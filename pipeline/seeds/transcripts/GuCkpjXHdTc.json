{"text": " Yeah, so I guess the thing I've kind of learned, I guess I should share my screen and say what I feel like I've learned from this suite. Grab it, Kettle. So on pets, this is the top 15 after running a few more sweeps to see if there's some better options. But basically, I think what's interesting is in the top 15 on pets, you have like a bit of everything, right? You've got ResNet RS and ResNet V2 distilled. You've got, you know, VIT, Transformers ones, VIT, Swin, mobile VIT. Classic ResNet 26D still in there. It's actually the fastest of all, which maybe reflects like maybe that's the most well-optimized kind of because it's been around a while and Nvidia's probably worked hard on that. ResNet, GPU memory, like, yeah, it's kind of interesting the way there's all these very different approaches, but they all kind of end up in there. I think, you know, one thing interesting when you look at it on the graph is this, these green ones is VIT, which they kind of cut off here at this fit time of 150. And I think that's because the larger vision Transformers only work on larger images, and I only did 124 pixel images. So if I included larger images, we might see VIT at the very best in terms of error rate. I was pleasantly surprised to see that some of the VITs, you know, didn't actually use much memory. And they were pretty fast. And it's interesting, like, because that was quite a while ago, right? The vision Transformers paper came out quite a while ago. And yeah, it was kind of like the way I remember it, it was like just hacking together the first and most obvious thing people thought of when it came to kind of making Transformers work on vision. And yeah, the fact that it still works so well seems that people haven't improved on it much other than perhaps Swin. So I guess that my takeaways on the, like, small and fast models. Yeah, I guess I hadn't really looked much at ResNet-RS before, so it's interesting to see that one's right up there. And then on planet, it doesn't look that different, except, you know, it really is just VIT, Swin and ConvNext, in fact, entirely is VIT, Swin and ConvNext, the whole top 15. Yeah, so I guess that's some of the things I noticed. That's cool. Yeah, look at this one. It's cool that you're able to try so many different kinds of models and so many things. Yeah, I ran in less than 24 hours on my three GPUs, something like 1200 runs or something. Yeah, I thought this was interesting. Look at this one, VIT small patch 32. Memory usage is amazing. And speed is fastest. And yeah, third from the top of the smaller, faster ones. I have a question. Yeah. Can you hear me? Sure. Yes, hi. Just a small question. So you mentioned small and large. Does large mean the size of the image? Sometimes, when I was talking about VIT, yes, it does. For VIT, they, so I was specifically just doing 224 by 224 pixel images, and pretty much all the transformer based ones are fixed. They can only do a one size. And so the VIT models, I don't think they have, so there's two meanings of larger here. There's larger as in more bigger models, like more layers, wider layers. And I think all the VIT models, which are larger, more capacity, slower, probably more accurate, I think only run on literally on bigger images, which is like, doesn't have to be that way. That's just what they happen to do. So that's why, yeah, the IT only goes this far. There are bigger VIT models that should be more accurate, but they don't work on 224 by 224 pixel images. Is there like a good threshold to know when is a good time to use large versus the small one, or is it just all experimental? Larger images. You basically do everything on a smaller image as you can, as long as it gets you reasonable results because you want to iterate quickly. And then when you're finished and you want to, like in the case of Kaggle, you want the best accuracy you can. So you try bigger and bigger images to see what you can get away with and keep doing that as long as the accuracy improves. In a production environment, it's kind of similar, but you make them bigger and bigger until the latency of your model is too high, and you find the right trade off between model latency and accuracy. Generally speaking, larger images will give you more accurate results. But a bit slower. Correct. I mean, a lot slower, right? Because if you increase it from 224 to, on one side, to 360, then you're going from 224 squared to 360 squared. So you end up with a lot more pixels. So like for, for example, an application would be like for object detection for video, for example, like live video, then even if it's a larger size, it will still be good to do small because it's faster. Certainly for your iterating, there's no need to have a really accurate model for your iterating because you're trying to find out what data pre-processing works best or what architecture works best or whatever. So yeah, there's no point using large models and large images generally, as long as they're big enough to get the job done reasonably well. Okay. Thank you very much. Yeah, no worries. If you were, you were going to do this in like a business context, let's say, and someone said, Hey, Jeremy, I want to, you know, have a vision model. Would you, would you kind of just pick a reasonable one and just kind of go with that? And if the results were good, do you just use it or? I would do exactly what I'm doing here, you know, which is to try a few things on, you know, small, fast models on small images on a small subset of the data to find out what data pre-processing to use and what architecture to use. And then I would look at, yeah, what are the constraints in terms of operationalizing this? How much RAM do we have? How much latency can we get away with? How expensive is it going to be to then scale it up to the point at which, you know, we're getting acceptable results using acceptable resources. So, yeah, it wouldn't look very different at all to, you know, a Kaggle competition in terms of the modeling, but then there'd be a whole piece of analysis around user requirements and costs and stuff like that. I see. Jeremy? I tried doing what you're doing with going from smaller to larger models and mine somehow started out with much lower accuracy. Is it just a fluke or? I mean, I had several issues happen that and then... Oh, it's not a fluke. It means you press the wrong buttons somehow. So I will... I think I already have, haven't I shared my notebooks? So if I haven't, I'll certainly share them today. Maybe I haven't yet. So I'll share my notebooks today. So what I suggest you do is like, you know, go from mine, make sure you can rerun them and then look at yours and see how they're different and then figure out where you went wrong. But also like, yeah, you know, I always tell people when debugging, like to look at the inputs and the outputs. So what predictions are you making? Are you always predicting zero, for example? You know, did you run the LR finder to find what learning rate works well? Yeah, stuff like that. Thank you. No worries. Jeremy, on the question for the professional, I tried to... After your work for last week, you did mention learn.export and then later on we can learn.no. I found that it's maybe that's about the... Because when I load them, they actually looking for the suffix is.pth. But when we save and they say to the models folder and then you can give whatever name you want. But when you want to load them, they actually have the suffix at the end. So I'm not sure there's some... Yeah. So just to make sure you save it with a.pth suffix. But yeah, it certainly would make sense. You're asked to do that automatically. But in the documentation, it seems it's a safe in PICL. The format is PICL. But this is just PyTorch. So PyTorch uses a variant of the PICL format and they normally use PTH as their extension. So yes, it is PICL and just it does use the PTH extension. Jeremy, when you were opening this window, you typed in something in the URL bar to get... What were you... Oh, I just typed my port number because I know that the only thing that has 8888 and that is... Oh, okay. Oh yeah, some magic going on. Yeah, nothing like that. Let me just shut these down. I did have one more idea about this competition, which is... Oh, it's that CSV file, right? Yes. Yeah, train.csv, that's right. And it has this variety thing. And I want for variety, tf.variety. So there's 10,000 rows and 7,000 of them are one variety. But there are 3,000 rows that contain other varieties. So the only idea I had for this was something which is a bit counterintuitive, but those of you that did, I can't remember, 2017 or 2018, faster, I might remember. Sometimes if there's two different things, in this case, what kind of rice is it and what kind of disease is it, sometimes trying to get your model to predict both of those things makes them better at both. So if we try to get our model to predict what kind of disease is it and what kind of rice is it, it might actually get better at predicting the kind of disease, which might sound counterintuitive or I find it counterintuitive because it sounds like it's got more work to do. But you're also giving it more signal, like there's more things you're teaching it to look for. And so maybe if it knows how to recognise different types of rice, it can use that information to also recognise how different kinds of rice are impacted by different diseases. So I have no idea if that's going to be useful or not, but I thought it would be an interesting exercise to try to do that. So that's what I thought we might have a go at today, if that sounds of interest. Which also is frankly a good exercise in delving into models in a way we've never done before. So this is going to be much more sophisticated than anything we've done with deep learning before, which means it's very much up to you folks to stop me anytime something slightly confusing because I actually want everybody to understand this. And it's a really good test of how well you understand what's going on inside a neural network. So if you're not understanding it, that's a sign I haven't explained it very well. So let me try it. Let's have a look. Okay. So one thing I just did yesterday afternoon was I just trained a model three times to see what the error rate was, because I wanted to get a sense of like how much variation is there. And I found if I use a learning rate of 0.02 and just train for three epochs, I seem to pretty consistently get reasonable results. So here's something I can now do in two minutes to see how I'm going. So I thought that would be good. So this is one thing I really like doing. People are often very into doing reproducible training where they have like set the seed for their training and run the same thing every time. I think that's normally a bad idea because I actually want to see like what the natural variation is. And so if I make a change, I want to know whether that's, you know, changes the difference I see in the result is might be just due to natural variation or it's actually something significant. So that's why I did these. The natural variation is really large. Does that ever just weight you? Yeah, that's going to be tough to see like, did I improve things? But then if the natural variation is so large that improvements are invisible, then trying to improve it seems pointless, right? Because it sounds like you haven't really found a way to stably train something. And normally that happens because my learning rate is too big. So if you try this yourself and bump the learning rate up to 0.04, you'll see like, at least for me, I got like 5%, 6%, 5.5%. You know, it's like all over the place. So yeah, trading for more epochs at a lower learning rate will generally give you more stable results. And there's a compromise because doing more epochs is slow. So that's why I was trying to find a learning rate and a number of epochs, which is fast and stable. You could also try using a smaller subset of the data or I don't know, like in the end, sometimes things just will be slow in such as life. But most of the time I find I can get a compromise and I certainly did here, I think. With six epochs at half the learning rate, I certainly can do better. I can get to 4%, you know, rather than 5. But that's okay. I just want something for testing. One thing that was always counterintuitive to me that I think you talk about is like these improvements that you make on a small scale, like show up on the larger scale, like always. Oh yeah, absolutely. Basically, they pretty much always will. Yeah, because they're the same models with just more layers or wider activations. Yeah, if you find something that's going to, some pre-processing step that works well on a convexed tiny, it's going to work also well on a convexed large 99.9% of the time. Most people act as if that's not true, I find, but you know, like in academia and stuff. I feel like you have to do a full suite of everything, which most people just never think to try, but intuitively, of course it's the same, you know? Why wouldn't it be the same? Like it is the same thing, just scaled up a bit, they behave very similarly. I mean, it's hard to argue with you because it works. So I mean, but like it wasn't that intuitive. You can argue that it's not intuitive, that's fine. But like, I feel like the only reason it would be not intuitive is because everybody's told you for years that it doesn't work that way. Do you know what I mean? Yeah, that's fair. If nobody told you that, I think you'd be like, yeah, of course it works that way. That's fair. Okay. So, okay, let's do something crazy. Let's actually look at a model. So inside our learner, there's basically two main things. There's the data loaders, learn.deals, and there's the model, learn.model. And we've seen these before. And if you've forgotten, then yeah, go back and have a look at the older videos from the course. So the model itself, basically, yeah, it's got like things in it. And in this case, the first thing in it is called a timbody. And the timbody has things in it. And the first thing in it is called model. And then timbody.model has things in it, and the first thing is called the stem, and the next thing is called the stages, and so forth. So you can see how it's this kind of tree. And we actually want to go all the way to the bottom. So the basic top, there's two things in it at the very top level. There's a timbody, and there's a thing here, which doesn't actually have a name, but we always call it the head. And so the body is the bit that basically does all the hard work of looking at the pixels and trying to find features and stuff like that. That's something we call a convolutional neural network. And at the very end of that, it spits out, yeah, a whole bunch of information about those pixels. And the head is the thing that then tries to make sense of that and make some predictions about what we're looking at. And so this is the head. And as you can see, the head is pretty simple, whereas the body, which goes from here all the way to here, is not so simple. And we want to predict two things, what kind of rice it is and what disease it has. Now look at the very, very, very last layer. It's a linear layer. So a linear layer, if you remember, is just something that does a matrix product. And the matrix product is a matrix which takes as input 512 features and spits out 10 features. So it's a 512 by 10 matrix. So let's do a few things. Let's grab the head. So the head is the index one thing in the model. So there's our head. Quick question. I've seen these model whatever you want to call it, x-rays a lot. Have you ever wanted to, is there a way that maybe I don't know about to see the shape of the tensors as it flows, or the shape of the data as it flows through the model? You know, like, yeah, man, there it is. Oh, I didn't even know about this. Okay. Dude, you should try watching some fast AI lectures. Okay. Yeah. So this will tell you how many parameters there are. And yeah, the shape as it goes through. And so the key thing is, since we're predicting 10 probabilities, one probability for each of the 10 possible diseases, we end up with a shape of 64 by 10. The 64 is because we're using a batch size of 64. And for each image, we're predicting 10 probabilities. It's very thorough. It shows the callbacks. Wow. I don't remember this. Yeah, we don't look around, man. Here in fast AI, we're thorough. So yeah, so that's the question because that's a great thing for us to look at. So yeah, so in the head, let's create something called the last layer, which is going to be the end of the head. And the very end of the head. So our last layer is this linear thing, right? And so this is so we could actually see the parameters themselves. I hope it does that. A lot of these things are generated lazily, right? So when you see this thing saying generator object, it's just it's literally the word is lazy. It's too lazy to actually bother calculating what it is. So it doesn't bother until you force it to. So if you turn it into a list, it actually forces to generate it. Okay. So it's a list of one thing, which is not surprising, right? There it is. And so the last layer parameters. Is a matrix. Which is there we go, 10 by 512. So it's transposed to what I said, but that's okay. So we're getting 512 inputs. And when we multiply this by this matrix, we end up with 10 outputs. So my daughter's watching me. Sorry about that. Homeschooling transitions always require some input. All right. So we're going to basically have to. If we got rid of this, right, then then our last linear layer here would be taking in 1536 features and spitting out 512 features. So what we could do would be to delete this layer and instead take those 1536, sorry, this 512 features and create two linear layers. One with 10 outputs as before. And one with however many varieties there are. Which have a look. Hi Jeremy. Yes. So I was just contemplating whether back in that linear layer where it was the output was 10 by 512. Yes. Not the output. The output was 10. The output is 64 by 10. So when you want to mix diseases with like rice in the output, I was wondering whether that might be a, like, I don't know how many rice types there are. So there's five rice types. Okay. So that 10 might be a 10 by 10 matrix output. No, two by 10. So you want one probability of what type of race is it and one probability of what disease does it have? Okay. Yes. So just two by 10. So let's go ahead and do the easy thing first, which is to delete the layer we don't want. So this says sequential. So sequential means like literally PyTorch is going to go through and calculate this and take the output of that and pass it to this and take the output of that and pass it to this and so forth. So if we delete the last layer, that's no problem. It's just won't ever call it. So I can't quite remember if we can do this in sequential, but let's assume it works like normal Python. We should be able to go delete H minus one. That looked hopeful. Yep, we can. Okay. So it's got normal Python list semantics. So this model will now be returning 512 output. So we want to basically wrap it in a model which instead has two linear layers. So there's a couple of ways we can do this, but let's do it like the most step by step way. Let's take a look. So we're going to create a class. So in PyTorch, modules are classes. So we're going to take a class which includes this model. So let's call this class disease and type classifier. Now that is a PyTorch calls all things that it basically uses as layers in a neural net module. So this is a neural net module. Now if you haven't done any OO programming in Python before, it would be very helpful to read a tutorial about basic Python OO programming because PyTorch assumes that you are pretty familiar with it. If you've done any kind of OO programming before, I'm going to work on the assumption you have. Then the constructor, there's a lot of weird things in Python. The constructor is called Dundee init. So this is, so Dundee means underscores on each side. And it always passes in the object being constructed or the object we're calling it on first. So we'll give that a name. And so we're basically going to create two linear layers. And one easy way to create the correct kind of layer would be self.l1 equals, we could do that. So one question is like, I understand this subclassing thing. Is there some other way that you could push two additional layers onto the existing thing? Or does that not make any sense? Yeah, we could try that. Let's see if we get this one working and then we'll try the other way. How about that? That'd be fun. We could also try using fast AI has a create head function as well. So we'll see how we go. So here's linear layer number one. And as you can see, I literally just copied and pasted. It's inside the nn submodule. So I just had to add that. But the representation of it is nice and convenient in that I can just copy and paste it. In real life, we'd have it normally write the in features and out features. Everybody kind of knows that the first two things are in and out features. So I might just make it look more normal. So then the second layer, and then maybe we'll just give ourselves a note here. So we'll use this one for rice type and we'll use this one for disease. So at this point, once we create this, there's going to be these things that are going to be in it. And then we also need to wrap the actual model. So we'll just call that m and we'll just store that away. M equals m. So what happens is when PyTorch calls, like basically modules act exactly like functions. In Python terms, they're called callables. They act exactly like functions. But the way PyTorch sets it up is when you call this function, which is actually a module, it will always call a specially named method in your class. And the name of that is forward. So you have to create something called forward. And it will pass the current set of features to it, which I normally I always call x. I think most people call it x, if I remember correctly. So this is going to contain a 64 by 512 tensor. Okay. So no, it's not going to contain a 64 by 512 tensor. It's going to contain an input tensor because it's going to be a model. So we need to create the 64 by 512 tensor from it by calling the model like so. So results. In fact, what we often do is we'll go x equals because we're kind of making it like a sequential model. We're going x equals. Oh, you know, another idea is we something else we can try is we can make this whole thing as a sequential model. Let's do that next. So this is probably going to be the least easy way is what I'm doing it here, the most manual way. So we first of all call the original model. And then basically we're going to create two separate outputs. The race type output. And the disease type output. And so then we could return both of them. So that's so what I would then do is I would say let's create a new model. So disease type classifier. So we'd create it like this. And we need to pass in the existing model, which is this thing here, right? Oh, yes. And you always have to call the super classes done to in it to construct the object before you do anything else. There's a lot of annoying boilerplate in Python. Oh, I'm afraid. Okay. There we go. Okay. I just wanted to. Yeah. Point out how cool it is that you created the model without the last layer by doing the atomic thing that is so good. I didn't know this existed. Yeah. I had to write this up in Python code because how can you delete stuff like that? It's not a list and has only functionality to support this. Yeah. Yeah. I mean, yeah, I kind of like it's nice. I generally find I can work on the assumption that PyTorch classes are well designed because it turns out they generally are. And so to me, a well designed collection class would have the exact same behavior as Python. For example, fast cause L collection class has the exact same behavior as Python. So yeah, PyTorch is very nicely made. That thing where you deleted the thing, that's a PyTorch thing? That's not a fast thing, I think. Yeah. It's a PyTorch thing. This is just a regular, this is just part of the sequential. Yeah. The sequential class. So yeah. Okay. Nice. Wow. The way I asked this, I would explode the model into layers and then reconstruct it using sequential without the layers that I need. But hey, you can actually do this. This is so nice. Yeah, exactly. So let's create a new learner. Just be a copy of the last one, right? And then let's set the model to our new model. So we've now got a learner that contains our new model. So that's cool. I guess at this point, I guess we should be able to get some predictions, right? Wait, one question. Oh yeah. So the main thing I'm waiting for is the loss function. Like how are you going to... Yeah, yeah, yeah. That's what we're going to get there. Let's do this first. And suppose you're doing the predictions just to verify the plumbing is working. Exactly. Okay. Okay. Nope. And it's not. Let's see. Stack trace input type. Oh, right, right, right. Floating point 16. Okay. Fair enough. I think to simplify things, we're going to remove the.2 FP16 and we'll worry about that later. Is that some kind of mixed precision? That is exactly mixed precision. Yes. So let's just pretend that doesn't exist for a moment. And we'll come back to that. Okay. What on earth just happened? All right. So let's go back even simpler. So it's useful to like, if you talk about what's going through your mind when you see this error. Like you see the trace. Yeah. So let's create like a minimum reproducible example. So let's just like create a learner and then copy it and then not change it at all. Yeah. I can't even do that. All right. So this so then I would be like, okay, let's not even copy it. But instead let's just call it directly. What it learned to. Okay. That works. So doing a copy apparently doesn't work. Oh, so generally speaking, I would be inclined to change copy to deep copy at this point. Wait, but you still got the stack trace and then, right? Most things though. Oh, why are we still getting a half precision somewhere? That's very, yes, isn't it? Oh, it's probably because our data loaders got changed somehow. Let's recreate the data loaders as well. It nearly made it, didn't it? Yeah. It looked like it was working. And then at the very end. Oh, there we go. How are we getting half precision? What on earth is making it half precision? That's odd. Do you think resetting your kernel and I think so. I don't see how this would help, but there's never any harm, right? Maybe one of the callbacks somehow. Define atch. Okay. In some states. Yeah, maybe. Yep. That's exactly what happened. Okay. Well, that's that shouldn't happen. So that's not a great, you know what happened? Sorry. I don't know what happened. Like some, like, like I said, some, something has some state that's keeping things in half precision, which yeah, shouldn't be happening. And so at some point we can try to figure out what that is, but not now. Okay. So let's make a copy of the learner and check this in into it. Actually before we do, we're just going to use the copy directly. So we'll just make as few changes each time as possible. Okay. That worked. What are you looking for in this? Just saying, like, I'm trying to see why it's returning. I thought that was a decoded thing. So I was just wondering why it's being returned. Says here with this decoded equals false. Oh, they're the targets. That's why. That's why. That's why. So it actually returns preds, comma, targets. That's what it's returning. Okay. All right. So now it's working quite nicely. And so I would be now inclined to, like, create a really minimal model, which is like a, I'm going to call it dummy classifier. And all it does is it calls the original model. And let's see if that works. Because if this works, then we're at a point where we can then try out new models, right? It's interesting. I would have just gone straight back to the full model and tried that next, but you're slowly walking away. Yeah, I probably should have done it that way in the, you know, done it more slowly in the first place, but I got over enthusiastic. Okay, great. Oopsie daisy. We could do this inside our model, I guess. This is all pretty hacky, but we're just trying to get something working. So the head is the number one thing in the model. The last layer is the end of the head. We don't need that. We delete that last thing. Yes, we don't need that. Okay. So we might as well inline that. Keep it simple. Okay. So we delete the head and store it away. Okay. So we're going to create our learner. Okay. So create a learner. Create a class. This time we call the disease, et cetera, classifier. Set the model to that. Okay. Cool. Okay. So we're now at the point where it's trying to calculate loss and it has no way to do that. I'm slightly surprised it's trying to calculate loss at all since with loss is false. That's fine. So okay. So the loss function to remind you is the thing which is like a number which says how good is this model? And the loss function that we were using was designed on something that only returned a single tensor and we're returning a couple of tensors. And so that's why when it tries to call the loss function, it gets confused, which is fair enough. So the loss function is another thing that is stored inside the learner. Okay. There it is. So what we could do is we could, what's the best way to do this? One thing would be we could look at the source code for vision learner and see how that creates the loss function. So I'll just pass this to learner. Okay. So let's look at that. Okay, so it's trying to get it from the training data set. So the training data set knows what function, loss function to use, which is pretty nifty. So to start with, we could, let's create a loss function. So let's create a really simple loss function. So disease and type classifier loss. So we're going to be passed some, we're going to be passed predictions and actuals. Okay. So we're going to be passed predictions or sometimes called the targets. And what we could do is we could just say like for now, let's say the current loss function is whatever loss function we had before. And let's just try to predict, let's just try to get it so it's working just on the disease prediction, which is this bit here. So predictions will be a tuple. So this will be rice predictions and it will be disease predictions. That'll be what's in our preds. And so just to start with, let's just keep getting this, keep, get this so it keeps your works on disease predictions. So we'll just return whatever the current loss function was and we'll call it on the disease predictions. Okay. So now we need to go learn to dot loss function is that function we just created. That's interesting. Sorry when you did that to like set the current loss, set the gloss function and the learner didn't want this mess up your code. I guess like you need to create the learner again. Nevermind. Sorry. Okay. January, do you want to go up a little bit back to your loss function? Is it you actually want to pass the predict target? This is disease target. Okay. No, no, sorry. So I'm just going to ignore the rice type prediction for now and just try to get it our new thing working to continue to do exactly what it did before, but with this new structure around it. Do we have to split the targets as well? No, because at the moment our targets, we haven't included anything other than just the diseases in the targets. So yeah, we're going to have to change our data loading as well to include the rice type as well, but we haven't done that yet. Okay. Yeah. Okay. Ah, yes. Okay. So then we've got metrics. So metrics are the things that just get printed out as you go. And we don't yet have a metric that works on this. So a very easy way to fix that is just to remove metrics for now. Great. Now preds.shape shouldn't work. Good. It doesn't because now we've got two sets of predictions. Right? We've got a tuple because that's the predictions is just whatever the model creates. The model is creating two things, not one. So we've now got rice predictions and disease predictions. So that's actually pretty good progress, I think. But you know, like for those of you who are involved in fast AI development, you know, it's pretty clear to me and try to do this that this is far harder than it should be and it feels like something that should be easy to do. I used to see Andrew using the magic, the percentage and then have a patch and then just get some little thing on top of it. Yeah, it's not so much about patching. It's about I feel like there might even be some multi loss thing. If there's not, I feel like this is something we should add to fast AI to make it easier. Can you explain a little bit about why the loss is stored in the data loader? Like how that is a good thing? Yeah, sure. So generally speaking, what is the appropriate loss function to use or at least a reasonable default depends on what kind of data you have. So if your data is, you know, a single continuous output, you probably have a regression problems. You probably want mean squared error. If it's a single categorical variable, you probably want cross entropy loss. If you have a multi categorical variable, you know, you probably want that log loss without the soft max and so forth. So yeah, basically by having it come from the from the data set means that you can get sensible defaults that ought to work for that data set. I see. So that's why we generally most of the time don't have to specify what loss function to use unless we're doing something kind of nonstandard. All right. So we're about to wrap up. The last thing I think I might do is just try to put this back and we can do it exactly the same way, which is to say DTC error. So pop this a bit higher. So we'll just return error rate on the disease predictions. Learn two dot metrics equals DTC error. Cool. So I guess we should now be able to do things like learn two dot LR find, for example. And this should we should be able to just replicate our disease model at this point. Because we're not doing anything with this extra rice type thing yet. And fine tune. One epoch. 0.05 say 0.01 say. While I wait for that, let's see if I search like fast AI multiple loss function or something. 2018 is going to be too long ago. Functions. Nothing there. Multitask learning. I got to go, but yeah, thanks a lot. No worries. OK, so it looks like this person did something pretty similar. They created their own little multitask loss wrapper. Cool. Well, I think we're at a good place to stop. That's we've got back. So it's not totally broken. So that's good. And next time we will try and plug this stuff in. We have any questions or anything before we wrap up? Just a quick question, Jeremy. It says the valley is point zero zero one, but you use point zero one for the fine tune. Yeah, I'm not sure. This is pretty this is it's picked out something pretty early in the curve. I thought something down here seems more reasonable. Just by bowling it, you know, it tends to recommend like rather conservative values. So, yeah, I tend to kind of look for the bit that's I kind of look for the bit that's as far to the right as possible, but still looks pretty steep gradient. I guess it's OK. Thank you. All right. Again. Thanks. Yeah.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.200000000000001, "text": " Yeah, so I guess the thing I've kind of learned, I guess I should share my screen and say what", "tokens": [865, 11, 370, 286, 2041, 264, 551, 286, 600, 733, 295, 3264, 11, 286, 2041, 286, 820, 2073, 452, 2568, 293, 584, 437], "temperature": 0.0, "avg_logprob": -0.37502731421054936, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.08732433617115021}, {"id": 1, "seek": 0, "start": 13.200000000000001, "end": 23.28, "text": " I feel like I've learned from this suite.", "tokens": [286, 841, 411, 286, 600, 3264, 490, 341, 14205, 13], "temperature": 0.0, "avg_logprob": -0.37502731421054936, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.08732433617115021}, {"id": 2, "seek": 2328, "start": 23.28, "end": 43.480000000000004, "text": " Grab it, Kettle.", "tokens": [20357, 309, 11, 591, 28444, 13], "temperature": 0.0, "avg_logprob": -0.8779231071472168, "compression_ratio": 0.6666666666666666, "no_speech_prob": 0.0002883697743527591}, {"id": 3, "seek": 4348, "start": 43.48, "end": 56.199999999999996, "text": " So on pets, this is the top 15 after running a few more sweeps to see if there's some better", "tokens": [407, 322, 19897, 11, 341, 307, 264, 1192, 2119, 934, 2614, 257, 1326, 544, 2484, 10653, 281, 536, 498, 456, 311, 512, 1101], "temperature": 0.0, "avg_logprob": -0.19953919695569322, "compression_ratio": 1.4043715846994536, "no_speech_prob": 1.0127703717444092e-05}, {"id": 4, "seek": 4348, "start": 56.199999999999996, "end": 57.199999999999996, "text": " options.", "tokens": [3956, 13], "temperature": 0.0, "avg_logprob": -0.19953919695569322, "compression_ratio": 1.4043715846994536, "no_speech_prob": 1.0127703717444092e-05}, {"id": 5, "seek": 4348, "start": 57.199999999999996, "end": 64.75999999999999, "text": " But basically, I think what's interesting is in the top 15 on pets, you have like a", "tokens": [583, 1936, 11, 286, 519, 437, 311, 1880, 307, 294, 264, 1192, 2119, 322, 19897, 11, 291, 362, 411, 257], "temperature": 0.0, "avg_logprob": -0.19953919695569322, "compression_ratio": 1.4043715846994536, "no_speech_prob": 1.0127703717444092e-05}, {"id": 6, "seek": 4348, "start": 64.75999999999999, "end": 66.0, "text": " bit of everything, right?", "tokens": [857, 295, 1203, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19953919695569322, "compression_ratio": 1.4043715846994536, "no_speech_prob": 1.0127703717444092e-05}, {"id": 7, "seek": 4348, "start": 66.0, "end": 71.75999999999999, "text": " You've got ResNet RS and ResNet V2 distilled.", "tokens": [509, 600, 658, 5015, 31890, 25855, 293, 5015, 31890, 691, 17, 1483, 6261, 13], "temperature": 0.0, "avg_logprob": -0.19953919695569322, "compression_ratio": 1.4043715846994536, "no_speech_prob": 1.0127703717444092e-05}, {"id": 8, "seek": 7176, "start": 71.76, "end": 81.80000000000001, "text": " You've got, you know, VIT, Transformers ones, VIT, Swin, mobile VIT.", "tokens": [509, 600, 658, 11, 291, 458, 11, 691, 3927, 11, 27938, 433, 2306, 11, 691, 3927, 11, 3926, 259, 11, 6013, 691, 3927, 13], "temperature": 0.0, "avg_logprob": -0.32567148323518685, "compression_ratio": 1.3853658536585365, "no_speech_prob": 2.177972419303842e-05}, {"id": 9, "seek": 7176, "start": 81.80000000000001, "end": 86.32000000000001, "text": " Classic ResNet 26D still in there.", "tokens": [25008, 5015, 31890, 7551, 35, 920, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.32567148323518685, "compression_ratio": 1.3853658536585365, "no_speech_prob": 2.177972419303842e-05}, {"id": 10, "seek": 7176, "start": 86.32000000000001, "end": 91.16000000000001, "text": " It's actually the fastest of all, which maybe reflects like maybe that's the most well-optimized", "tokens": [467, 311, 767, 264, 14573, 295, 439, 11, 597, 1310, 18926, 411, 1310, 300, 311, 264, 881, 731, 12, 5747, 332, 1602], "temperature": 0.0, "avg_logprob": -0.32567148323518685, "compression_ratio": 1.3853658536585365, "no_speech_prob": 2.177972419303842e-05}, {"id": 11, "seek": 7176, "start": 91.16000000000001, "end": 97.72, "text": " kind of because it's been around a while and Nvidia's probably worked hard on that.", "tokens": [733, 295, 570, 309, 311, 668, 926, 257, 1339, 293, 46284, 311, 1391, 2732, 1152, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.32567148323518685, "compression_ratio": 1.3853658536585365, "no_speech_prob": 2.177972419303842e-05}, {"id": 12, "seek": 9772, "start": 97.72, "end": 103.76, "text": " ResNet, GPU memory, like, yeah, it's kind of interesting the way there's all these very", "tokens": [5015, 31890, 11, 18407, 4675, 11, 411, 11, 1338, 11, 309, 311, 733, 295, 1880, 264, 636, 456, 311, 439, 613, 588], "temperature": 0.0, "avg_logprob": -0.15438757836818695, "compression_ratio": 1.4658385093167703, "no_speech_prob": 3.2673873647581786e-05}, {"id": 13, "seek": 9772, "start": 103.76, "end": 113.4, "text": " different approaches, but they all kind of end up in there.", "tokens": [819, 11587, 11, 457, 436, 439, 733, 295, 917, 493, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.15438757836818695, "compression_ratio": 1.4658385093167703, "no_speech_prob": 3.2673873647581786e-05}, {"id": 14, "seek": 9772, "start": 113.4, "end": 120.32, "text": " I think, you know, one thing interesting when you look at it on the graph is this, these", "tokens": [286, 519, 11, 291, 458, 11, 472, 551, 1880, 562, 291, 574, 412, 309, 322, 264, 4295, 307, 341, 11, 613], "temperature": 0.0, "avg_logprob": -0.15438757836818695, "compression_ratio": 1.4658385093167703, "no_speech_prob": 3.2673873647581786e-05}, {"id": 15, "seek": 12032, "start": 120.32, "end": 131.28, "text": " green ones is VIT, which they kind of cut off here at this fit time of 150.", "tokens": [3092, 2306, 307, 691, 3927, 11, 597, 436, 733, 295, 1723, 766, 510, 412, 341, 3318, 565, 295, 8451, 13], "temperature": 0.0, "avg_logprob": -0.1943635410732693, "compression_ratio": 1.3197278911564625, "no_speech_prob": 3.500691946101142e-06}, {"id": 16, "seek": 12032, "start": 131.28, "end": 138.79999999999998, "text": " And I think that's because the larger vision Transformers only work on larger images, and", "tokens": [400, 286, 519, 300, 311, 570, 264, 4833, 5201, 27938, 433, 787, 589, 322, 4833, 5267, 11, 293], "temperature": 0.0, "avg_logprob": -0.1943635410732693, "compression_ratio": 1.3197278911564625, "no_speech_prob": 3.500691946101142e-06}, {"id": 17, "seek": 12032, "start": 138.79999999999998, "end": 143.35999999999999, "text": " I only did 124 pixel images.", "tokens": [286, 787, 630, 2272, 19, 19261, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1943635410732693, "compression_ratio": 1.3197278911564625, "no_speech_prob": 3.500691946101142e-06}, {"id": 18, "seek": 14336, "start": 143.36, "end": 153.72000000000003, "text": " So if I included larger images, we might see VIT at the very best in terms of error rate.", "tokens": [407, 498, 286, 5556, 4833, 5267, 11, 321, 1062, 536, 691, 3927, 412, 264, 588, 1151, 294, 2115, 295, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.13548284623681045, "compression_ratio": 1.4422110552763818, "no_speech_prob": 4.5920319280412514e-07}, {"id": 19, "seek": 14336, "start": 153.72000000000003, "end": 163.76000000000002, "text": " I was pleasantly surprised to see that some of the VITs, you know, didn't actually use", "tokens": [286, 390, 35122, 3627, 6100, 281, 536, 300, 512, 295, 264, 691, 3927, 82, 11, 291, 458, 11, 994, 380, 767, 764], "temperature": 0.0, "avg_logprob": -0.13548284623681045, "compression_ratio": 1.4422110552763818, "no_speech_prob": 4.5920319280412514e-07}, {"id": 20, "seek": 14336, "start": 163.76000000000002, "end": 166.76000000000002, "text": " much memory.", "tokens": [709, 4675, 13], "temperature": 0.0, "avg_logprob": -0.13548284623681045, "compression_ratio": 1.4422110552763818, "no_speech_prob": 4.5920319280412514e-07}, {"id": 21, "seek": 14336, "start": 166.76000000000002, "end": 167.76000000000002, "text": " And they were pretty fast.", "tokens": [400, 436, 645, 1238, 2370, 13], "temperature": 0.0, "avg_logprob": -0.13548284623681045, "compression_ratio": 1.4422110552763818, "no_speech_prob": 4.5920319280412514e-07}, {"id": 22, "seek": 14336, "start": 167.76000000000002, "end": 170.20000000000002, "text": " And it's interesting, like, because that was quite a while ago, right?", "tokens": [400, 309, 311, 1880, 11, 411, 11, 570, 300, 390, 1596, 257, 1339, 2057, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13548284623681045, "compression_ratio": 1.4422110552763818, "no_speech_prob": 4.5920319280412514e-07}, {"id": 23, "seek": 17020, "start": 170.2, "end": 173.6, "text": " The vision Transformers paper came out quite a while ago.", "tokens": [440, 5201, 27938, 433, 3035, 1361, 484, 1596, 257, 1339, 2057, 13], "temperature": 0.0, "avg_logprob": -0.21818240145419507, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.88469236693345e-05}, {"id": 24, "seek": 17020, "start": 173.6, "end": 181.67999999999998, "text": " And yeah, it was kind of like the way I remember it, it was like just hacking together the", "tokens": [400, 1338, 11, 309, 390, 733, 295, 411, 264, 636, 286, 1604, 309, 11, 309, 390, 411, 445, 31422, 1214, 264], "temperature": 0.0, "avg_logprob": -0.21818240145419507, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.88469236693345e-05}, {"id": 25, "seek": 17020, "start": 181.67999999999998, "end": 185.6, "text": " first and most obvious thing people thought of when it came to kind of making Transformers", "tokens": [700, 293, 881, 6322, 551, 561, 1194, 295, 562, 309, 1361, 281, 733, 295, 1455, 27938, 433], "temperature": 0.0, "avg_logprob": -0.21818240145419507, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.88469236693345e-05}, {"id": 26, "seek": 17020, "start": 185.6, "end": 186.6, "text": " work on vision.", "tokens": [589, 322, 5201, 13], "temperature": 0.0, "avg_logprob": -0.21818240145419507, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.88469236693345e-05}, {"id": 27, "seek": 17020, "start": 186.6, "end": 192.83999999999997, "text": " And yeah, the fact that it still works so well seems that people haven't improved on", "tokens": [400, 1338, 11, 264, 1186, 300, 309, 920, 1985, 370, 731, 2544, 300, 561, 2378, 380, 9689, 322], "temperature": 0.0, "avg_logprob": -0.21818240145419507, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.88469236693345e-05}, {"id": 28, "seek": 17020, "start": 192.83999999999997, "end": 196.07999999999998, "text": " it much other than perhaps Swin.", "tokens": [309, 709, 661, 813, 4317, 3926, 259, 13], "temperature": 0.0, "avg_logprob": -0.21818240145419507, "compression_ratio": 1.6651785714285714, "no_speech_prob": 2.88469236693345e-05}, {"id": 29, "seek": 19608, "start": 196.08, "end": 205.8, "text": " So I guess that my takeaways on the, like, small and fast models.", "tokens": [407, 286, 2041, 300, 452, 45584, 322, 264, 11, 411, 11, 1359, 293, 2370, 5245, 13], "temperature": 0.0, "avg_logprob": -0.23255255164169683, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.0784058506251313e-05}, {"id": 30, "seek": 19608, "start": 205.8, "end": 209.68, "text": " Yeah, I guess I hadn't really looked much at ResNet-RS before, so it's interesting to", "tokens": [865, 11, 286, 2041, 286, 8782, 380, 534, 2956, 709, 412, 5015, 31890, 12, 43580, 949, 11, 370, 309, 311, 1880, 281], "temperature": 0.0, "avg_logprob": -0.23255255164169683, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.0784058506251313e-05}, {"id": 31, "seek": 19608, "start": 209.68, "end": 216.88000000000002, "text": " see that one's right up there.", "tokens": [536, 300, 472, 311, 558, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.23255255164169683, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.0784058506251313e-05}, {"id": 32, "seek": 19608, "start": 216.88000000000002, "end": 225.20000000000002, "text": " And then on planet, it doesn't look that different, except, you know, it really is just VIT, Swin", "tokens": [400, 550, 322, 5054, 11, 309, 1177, 380, 574, 300, 819, 11, 3993, 11, 291, 458, 11, 309, 534, 307, 445, 691, 3927, 11, 3926, 259], "temperature": 0.0, "avg_logprob": -0.23255255164169683, "compression_ratio": 1.4213197969543148, "no_speech_prob": 2.0784058506251313e-05}, {"id": 33, "seek": 22520, "start": 225.2, "end": 234.88, "text": " and ConvNext, in fact, entirely is VIT, Swin and ConvNext, the whole top 15.", "tokens": [293, 2656, 85, 31002, 11, 294, 1186, 11, 7696, 307, 691, 3927, 11, 3926, 259, 293, 2656, 85, 31002, 11, 264, 1379, 1192, 2119, 13], "temperature": 0.0, "avg_logprob": -0.31803706487019856, "compression_ratio": 1.2846153846153847, "no_speech_prob": 2.506583768990822e-05}, {"id": 34, "seek": 22520, "start": 234.88, "end": 247.07999999999998, "text": " Yeah, so I guess that's some of the things I noticed.", "tokens": [865, 11, 370, 286, 2041, 300, 311, 512, 295, 264, 721, 286, 5694, 13], "temperature": 0.0, "avg_logprob": -0.31803706487019856, "compression_ratio": 1.2846153846153847, "no_speech_prob": 2.506583768990822e-05}, {"id": 35, "seek": 22520, "start": 247.07999999999998, "end": 248.07999999999998, "text": " That's cool.", "tokens": [663, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.31803706487019856, "compression_ratio": 1.2846153846153847, "no_speech_prob": 2.506583768990822e-05}, {"id": 36, "seek": 22520, "start": 248.07999999999998, "end": 249.07999999999998, "text": " Yeah, look at this one.", "tokens": [865, 11, 574, 412, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.31803706487019856, "compression_ratio": 1.2846153846153847, "no_speech_prob": 2.506583768990822e-05}, {"id": 37, "seek": 24908, "start": 249.08, "end": 259.24, "text": " It's cool that you're able to try so many different kinds of models and so many things.", "tokens": [467, 311, 1627, 300, 291, 434, 1075, 281, 853, 370, 867, 819, 3685, 295, 5245, 293, 370, 867, 721, 13], "temperature": 0.0, "avg_logprob": -0.3043642777663011, "compression_ratio": 1.4455958549222798, "no_speech_prob": 4.22258608523407e-06}, {"id": 38, "seek": 24908, "start": 259.24, "end": 269.68, "text": " Yeah, I ran in less than 24 hours on my three GPUs, something like 1200 runs or something.", "tokens": [865, 11, 286, 5872, 294, 1570, 813, 4022, 2496, 322, 452, 1045, 18407, 82, 11, 746, 411, 29139, 6676, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.3043642777663011, "compression_ratio": 1.4455958549222798, "no_speech_prob": 4.22258608523407e-06}, {"id": 39, "seek": 24908, "start": 269.68, "end": 272.36, "text": " Yeah, I thought this was interesting.", "tokens": [865, 11, 286, 1194, 341, 390, 1880, 13], "temperature": 0.0, "avg_logprob": -0.3043642777663011, "compression_ratio": 1.4455958549222798, "no_speech_prob": 4.22258608523407e-06}, {"id": 40, "seek": 24908, "start": 272.36, "end": 275.72, "text": " Look at this one, VIT small patch 32.", "tokens": [2053, 412, 341, 472, 11, 691, 3927, 1359, 9972, 8858, 13], "temperature": 0.0, "avg_logprob": -0.3043642777663011, "compression_ratio": 1.4455958549222798, "no_speech_prob": 4.22258608523407e-06}, {"id": 41, "seek": 24908, "start": 275.72, "end": 277.72, "text": " Memory usage is amazing.", "tokens": [38203, 14924, 307, 2243, 13], "temperature": 0.0, "avg_logprob": -0.3043642777663011, "compression_ratio": 1.4455958549222798, "no_speech_prob": 4.22258608523407e-06}, {"id": 42, "seek": 27772, "start": 277.72, "end": 286.16, "text": " And speed is fastest.", "tokens": [400, 3073, 307, 14573, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 43, "seek": 27772, "start": 286.16, "end": 289.64000000000004, "text": " And yeah, third from the top of the smaller, faster ones.", "tokens": [400, 1338, 11, 2636, 490, 264, 1192, 295, 264, 4356, 11, 4663, 2306, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 44, "seek": 27772, "start": 289.64000000000004, "end": 290.64000000000004, "text": " I have a question.", "tokens": [286, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 45, "seek": 27772, "start": 290.64000000000004, "end": 291.64000000000004, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 46, "seek": 27772, "start": 291.64000000000004, "end": 292.64000000000004, "text": " Can you hear me?", "tokens": [1664, 291, 1568, 385, 30], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 47, "seek": 27772, "start": 292.64000000000004, "end": 293.64000000000004, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 48, "seek": 27772, "start": 293.64000000000004, "end": 294.64000000000004, "text": " Yes, hi.", "tokens": [1079, 11, 4879, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 49, "seek": 27772, "start": 294.64000000000004, "end": 298.76000000000005, "text": " Just a small question.", "tokens": [1449, 257, 1359, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 50, "seek": 27772, "start": 298.76000000000005, "end": 301.04, "text": " So you mentioned small and large.", "tokens": [407, 291, 2835, 1359, 293, 2416, 13], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 51, "seek": 27772, "start": 301.04, "end": 303.8, "text": " Does large mean the size of the image?", "tokens": [4402, 2416, 914, 264, 2744, 295, 264, 3256, 30], "temperature": 0.0, "avg_logprob": -0.3233204364776611, "compression_ratio": 1.4683544303797469, "no_speech_prob": 8.588458149461076e-05}, {"id": 52, "seek": 30380, "start": 303.8, "end": 308.8, "text": " Sometimes, when I was talking about VIT, yes, it does.", "tokens": [4803, 11, 562, 286, 390, 1417, 466, 691, 3927, 11, 2086, 11, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.22152832116973534, "compression_ratio": 1.444976076555024, "no_speech_prob": 6.960971859371057e-06}, {"id": 53, "seek": 30380, "start": 308.8, "end": 317.8, "text": " For VIT, they, so I was specifically just doing 224 by 224 pixel images, and pretty", "tokens": [1171, 691, 3927, 11, 436, 11, 370, 286, 390, 4682, 445, 884, 5853, 19, 538, 5853, 19, 19261, 5267, 11, 293, 1238], "temperature": 0.0, "avg_logprob": -0.22152832116973534, "compression_ratio": 1.444976076555024, "no_speech_prob": 6.960971859371057e-06}, {"id": 54, "seek": 30380, "start": 317.8, "end": 320.76, "text": " much all the transformer based ones are fixed.", "tokens": [709, 439, 264, 31782, 2361, 2306, 366, 6806, 13], "temperature": 0.0, "avg_logprob": -0.22152832116973534, "compression_ratio": 1.444976076555024, "no_speech_prob": 6.960971859371057e-06}, {"id": 55, "seek": 30380, "start": 320.76, "end": 324.44, "text": " They can only do a one size.", "tokens": [814, 393, 787, 360, 257, 472, 2744, 13], "temperature": 0.0, "avg_logprob": -0.22152832116973534, "compression_ratio": 1.444976076555024, "no_speech_prob": 6.960971859371057e-06}, {"id": 56, "seek": 30380, "start": 324.44, "end": 331.72, "text": " And so the VIT models, I don't think they have, so there's two meanings of larger here.", "tokens": [400, 370, 264, 691, 3927, 5245, 11, 286, 500, 380, 519, 436, 362, 11, 370, 456, 311, 732, 28138, 295, 4833, 510, 13], "temperature": 0.0, "avg_logprob": -0.22152832116973534, "compression_ratio": 1.444976076555024, "no_speech_prob": 6.960971859371057e-06}, {"id": 57, "seek": 33172, "start": 331.72, "end": 338.12, "text": " There's larger as in more bigger models, like more layers, wider layers.", "tokens": [821, 311, 4833, 382, 294, 544, 3801, 5245, 11, 411, 544, 7914, 11, 11842, 7914, 13], "temperature": 0.0, "avg_logprob": -0.14468668908188023, "compression_ratio": 1.6121495327102804, "no_speech_prob": 9.079213668883312e-06}, {"id": 58, "seek": 33172, "start": 338.12, "end": 343.28000000000003, "text": " And I think all the VIT models, which are larger, more capacity, slower, probably more", "tokens": [400, 286, 519, 439, 264, 691, 3927, 5245, 11, 597, 366, 4833, 11, 544, 6042, 11, 14009, 11, 1391, 544], "temperature": 0.0, "avg_logprob": -0.14468668908188023, "compression_ratio": 1.6121495327102804, "no_speech_prob": 9.079213668883312e-06}, {"id": 59, "seek": 33172, "start": 343.28000000000003, "end": 348.68, "text": " accurate, I think only run on literally on bigger images, which is like, doesn't have", "tokens": [8559, 11, 286, 519, 787, 1190, 322, 3736, 322, 3801, 5267, 11, 597, 307, 411, 11, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.14468668908188023, "compression_ratio": 1.6121495327102804, "no_speech_prob": 9.079213668883312e-06}, {"id": 60, "seek": 33172, "start": 348.68, "end": 349.68, "text": " to be that way.", "tokens": [281, 312, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.14468668908188023, "compression_ratio": 1.6121495327102804, "no_speech_prob": 9.079213668883312e-06}, {"id": 61, "seek": 33172, "start": 349.68, "end": 351.64000000000004, "text": " That's just what they happen to do.", "tokens": [663, 311, 445, 437, 436, 1051, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14468668908188023, "compression_ratio": 1.6121495327102804, "no_speech_prob": 9.079213668883312e-06}, {"id": 62, "seek": 33172, "start": 351.64000000000004, "end": 357.08000000000004, "text": " So that's why, yeah, the IT only goes this far.", "tokens": [407, 300, 311, 983, 11, 1338, 11, 264, 6783, 787, 1709, 341, 1400, 13], "temperature": 0.0, "avg_logprob": -0.14468668908188023, "compression_ratio": 1.6121495327102804, "no_speech_prob": 9.079213668883312e-06}, {"id": 63, "seek": 35708, "start": 357.08, "end": 361.84, "text": " There are bigger VIT models that should be more accurate, but they don't work on 224", "tokens": [821, 366, 3801, 691, 3927, 5245, 300, 820, 312, 544, 8559, 11, 457, 436, 500, 380, 589, 322, 5853, 19], "temperature": 0.0, "avg_logprob": -0.160141123403417, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.0288242265232839e-05}, {"id": 64, "seek": 35708, "start": 361.84, "end": 363.68, "text": " by 224 pixel images.", "tokens": [538, 5853, 19, 19261, 5267, 13], "temperature": 0.0, "avg_logprob": -0.160141123403417, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.0288242265232839e-05}, {"id": 65, "seek": 35708, "start": 363.68, "end": 369.64, "text": " Is there like a good threshold to know when is a good time to use large versus the small", "tokens": [1119, 456, 411, 257, 665, 14678, 281, 458, 562, 307, 257, 665, 565, 281, 764, 2416, 5717, 264, 1359], "temperature": 0.0, "avg_logprob": -0.160141123403417, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.0288242265232839e-05}, {"id": 66, "seek": 35708, "start": 369.64, "end": 372.32, "text": " one, or is it just all experimental?", "tokens": [472, 11, 420, 307, 309, 445, 439, 17069, 30], "temperature": 0.0, "avg_logprob": -0.160141123403417, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.0288242265232839e-05}, {"id": 67, "seek": 35708, "start": 372.32, "end": 374.36, "text": " Larger images.", "tokens": [11569, 1321, 5267, 13], "temperature": 0.0, "avg_logprob": -0.160141123403417, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.0288242265232839e-05}, {"id": 68, "seek": 35708, "start": 374.36, "end": 379.91999999999996, "text": " You basically do everything on a smaller image as you can, as long as it gets you reasonable", "tokens": [509, 1936, 360, 1203, 322, 257, 4356, 3256, 382, 291, 393, 11, 382, 938, 382, 309, 2170, 291, 10585], "temperature": 0.0, "avg_logprob": -0.160141123403417, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.0288242265232839e-05}, {"id": 69, "seek": 35708, "start": 379.91999999999996, "end": 384.15999999999997, "text": " results because you want to iterate quickly.", "tokens": [3542, 570, 291, 528, 281, 44497, 2661, 13], "temperature": 0.0, "avg_logprob": -0.160141123403417, "compression_ratio": 1.5802469135802468, "no_speech_prob": 1.0288242265232839e-05}, {"id": 70, "seek": 38416, "start": 384.16, "end": 389.28000000000003, "text": " And then when you're finished and you want to, like in the case of Kaggle, you want the", "tokens": [400, 550, 562, 291, 434, 4335, 293, 291, 528, 281, 11, 411, 294, 264, 1389, 295, 48751, 22631, 11, 291, 528, 264], "temperature": 0.0, "avg_logprob": -0.13287773499122033, "compression_ratio": 1.6857142857142857, "no_speech_prob": 4.356516456027748e-06}, {"id": 71, "seek": 38416, "start": 389.28000000000003, "end": 391.52000000000004, "text": " best accuracy you can.", "tokens": [1151, 14170, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.13287773499122033, "compression_ratio": 1.6857142857142857, "no_speech_prob": 4.356516456027748e-06}, {"id": 72, "seek": 38416, "start": 391.52000000000004, "end": 395.84000000000003, "text": " So you try bigger and bigger images to see what you can get away with and keep doing", "tokens": [407, 291, 853, 3801, 293, 3801, 5267, 281, 536, 437, 291, 393, 483, 1314, 365, 293, 1066, 884], "temperature": 0.0, "avg_logprob": -0.13287773499122033, "compression_ratio": 1.6857142857142857, "no_speech_prob": 4.356516456027748e-06}, {"id": 73, "seek": 38416, "start": 395.84000000000003, "end": 400.28000000000003, "text": " that as long as the accuracy improves.", "tokens": [300, 382, 938, 382, 264, 14170, 24771, 13], "temperature": 0.0, "avg_logprob": -0.13287773499122033, "compression_ratio": 1.6857142857142857, "no_speech_prob": 4.356516456027748e-06}, {"id": 74, "seek": 38416, "start": 400.28000000000003, "end": 404.36, "text": " In a production environment, it's kind of similar, but you make them bigger and bigger", "tokens": [682, 257, 4265, 2823, 11, 309, 311, 733, 295, 2531, 11, 457, 291, 652, 552, 3801, 293, 3801], "temperature": 0.0, "avg_logprob": -0.13287773499122033, "compression_ratio": 1.6857142857142857, "no_speech_prob": 4.356516456027748e-06}, {"id": 75, "seek": 38416, "start": 404.36, "end": 411.36, "text": " until the latency of your model is too high, and you find the right trade off between model", "tokens": [1826, 264, 27043, 295, 428, 2316, 307, 886, 1090, 11, 293, 291, 915, 264, 558, 4923, 766, 1296, 2316], "temperature": 0.0, "avg_logprob": -0.13287773499122033, "compression_ratio": 1.6857142857142857, "no_speech_prob": 4.356516456027748e-06}, {"id": 76, "seek": 41136, "start": 411.36, "end": 414.6, "text": " latency and accuracy.", "tokens": [27043, 293, 14170, 13], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 77, "seek": 41136, "start": 414.6, "end": 419.8, "text": " Generally speaking, larger images will give you more accurate results.", "tokens": [21082, 4124, 11, 4833, 5267, 486, 976, 291, 544, 8559, 3542, 13], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 78, "seek": 41136, "start": 419.8, "end": 421.8, "text": " But a bit slower.", "tokens": [583, 257, 857, 14009, 13], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 79, "seek": 41136, "start": 421.8, "end": 422.8, "text": " Correct.", "tokens": [12753, 13], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 80, "seek": 41136, "start": 422.8, "end": 425.2, "text": " I mean, a lot slower, right?", "tokens": [286, 914, 11, 257, 688, 14009, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 81, "seek": 41136, "start": 425.2, "end": 434.08000000000004, "text": " Because if you increase it from 224 to, on one side, to 360, then you're going from", "tokens": [1436, 498, 291, 3488, 309, 490, 5853, 19, 281, 11, 322, 472, 1252, 11, 281, 13898, 11, 550, 291, 434, 516, 490], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 82, "seek": 41136, "start": 434.08000000000004, "end": 436.24, "text": " 224 squared to 360 squared.", "tokens": [5853, 19, 8889, 281, 13898, 8889, 13], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 83, "seek": 41136, "start": 436.24, "end": 439.36, "text": " So you end up with a lot more pixels.", "tokens": [407, 291, 917, 493, 365, 257, 688, 544, 18668, 13], "temperature": 0.0, "avg_logprob": -0.2105455505713988, "compression_ratio": 1.4679802955665024, "no_speech_prob": 4.936616278428119e-06}, {"id": 84, "seek": 43936, "start": 439.36, "end": 444.88, "text": " So like for, for example, an application would be like for object detection for video, for", "tokens": [407, 411, 337, 11, 337, 1365, 11, 364, 3861, 576, 312, 411, 337, 2657, 17784, 337, 960, 11, 337], "temperature": 0.0, "avg_logprob": -0.11301831098703238, "compression_ratio": 1.7702127659574467, "no_speech_prob": 8.396649718633853e-06}, {"id": 85, "seek": 43936, "start": 444.88, "end": 453.0, "text": " example, like live video, then even if it's a larger size, it will still be good to do", "tokens": [1365, 11, 411, 1621, 960, 11, 550, 754, 498, 309, 311, 257, 4833, 2744, 11, 309, 486, 920, 312, 665, 281, 360], "temperature": 0.0, "avg_logprob": -0.11301831098703238, "compression_ratio": 1.7702127659574467, "no_speech_prob": 8.396649718633853e-06}, {"id": 86, "seek": 43936, "start": 453.0, "end": 456.88, "text": " small because it's faster.", "tokens": [1359, 570, 309, 311, 4663, 13], "temperature": 0.0, "avg_logprob": -0.11301831098703238, "compression_ratio": 1.7702127659574467, "no_speech_prob": 8.396649718633853e-06}, {"id": 87, "seek": 43936, "start": 456.88, "end": 462.8, "text": " Certainly for your iterating, there's no need to have a really accurate model for your iterating", "tokens": [16628, 337, 428, 17138, 990, 11, 456, 311, 572, 643, 281, 362, 257, 534, 8559, 2316, 337, 428, 17138, 990], "temperature": 0.0, "avg_logprob": -0.11301831098703238, "compression_ratio": 1.7702127659574467, "no_speech_prob": 8.396649718633853e-06}, {"id": 88, "seek": 43936, "start": 462.8, "end": 466.68, "text": " because you're trying to find out what data pre-processing works best or what architecture", "tokens": [570, 291, 434, 1382, 281, 915, 484, 437, 1412, 659, 12, 41075, 278, 1985, 1151, 420, 437, 9482], "temperature": 0.0, "avg_logprob": -0.11301831098703238, "compression_ratio": 1.7702127659574467, "no_speech_prob": 8.396649718633853e-06}, {"id": 89, "seek": 43936, "start": 466.68, "end": 467.88, "text": " works best or whatever.", "tokens": [1985, 1151, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.11301831098703238, "compression_ratio": 1.7702127659574467, "no_speech_prob": 8.396649718633853e-06}, {"id": 90, "seek": 46788, "start": 467.88, "end": 472.96, "text": " So yeah, there's no point using large models and large images generally, as long as they're", "tokens": [407, 1338, 11, 456, 311, 572, 935, 1228, 2416, 5245, 293, 2416, 5267, 5101, 11, 382, 938, 382, 436, 434], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 91, "seek": 46788, "start": 472.96, "end": 477.48, "text": " big enough to get the job done reasonably well.", "tokens": [955, 1547, 281, 483, 264, 1691, 1096, 23551, 731, 13], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 92, "seek": 46788, "start": 477.48, "end": 478.48, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 93, "seek": 46788, "start": 478.48, "end": 479.48, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 94, "seek": 46788, "start": 479.48, "end": 480.92, "text": " Yeah, no worries.", "tokens": [865, 11, 572, 16340, 13], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 95, "seek": 46788, "start": 480.92, "end": 485.48, "text": " If you were, you were going to do this in like a business context, let's say, and someone", "tokens": [759, 291, 645, 11, 291, 645, 516, 281, 360, 341, 294, 411, 257, 1606, 4319, 11, 718, 311, 584, 11, 293, 1580], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 96, "seek": 46788, "start": 485.48, "end": 490.36, "text": " said, Hey, Jeremy, I want to, you know, have a vision model.", "tokens": [848, 11, 1911, 11, 17809, 11, 286, 528, 281, 11, 291, 458, 11, 362, 257, 5201, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 97, "seek": 46788, "start": 490.36, "end": 495.52, "text": " Would you, would you kind of just pick a reasonable one and just kind of go with that?", "tokens": [6068, 291, 11, 576, 291, 733, 295, 445, 1888, 257, 10585, 472, 293, 445, 733, 295, 352, 365, 300, 30], "temperature": 0.0, "avg_logprob": -0.16318384806315103, "compression_ratio": 1.586466165413534, "no_speech_prob": 8.266340955742635e-06}, {"id": 98, "seek": 49552, "start": 495.52, "end": 498.24, "text": " And if the results were good, do you just use it or?", "tokens": [400, 498, 264, 3542, 645, 665, 11, 360, 291, 445, 764, 309, 420, 30], "temperature": 0.0, "avg_logprob": -0.14243474553843014, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.6449377957033e-06}, {"id": 99, "seek": 49552, "start": 498.24, "end": 502.4, "text": " I would do exactly what I'm doing here, you know, which is to try a few things on, you", "tokens": [286, 576, 360, 2293, 437, 286, 478, 884, 510, 11, 291, 458, 11, 597, 307, 281, 853, 257, 1326, 721, 322, 11, 291], "temperature": 0.0, "avg_logprob": -0.14243474553843014, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.6449377957033e-06}, {"id": 100, "seek": 49552, "start": 502.4, "end": 510.35999999999996, "text": " know, small, fast models on small images on a small subset of the data to find out what", "tokens": [458, 11, 1359, 11, 2370, 5245, 322, 1359, 5267, 322, 257, 1359, 25993, 295, 264, 1412, 281, 915, 484, 437], "temperature": 0.0, "avg_logprob": -0.14243474553843014, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.6449377957033e-06}, {"id": 101, "seek": 49552, "start": 510.35999999999996, "end": 516.16, "text": " data pre-processing to use and what architecture to use.", "tokens": [1412, 659, 12, 41075, 278, 281, 764, 293, 437, 9482, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.14243474553843014, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.6449377957033e-06}, {"id": 102, "seek": 49552, "start": 516.16, "end": 521.56, "text": " And then I would look at, yeah, what are the constraints in terms of operationalizing this?", "tokens": [400, 550, 286, 576, 574, 412, 11, 1338, 11, 437, 366, 264, 18491, 294, 2115, 295, 16607, 3319, 341, 30], "temperature": 0.0, "avg_logprob": -0.14243474553843014, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.6449377957033e-06}, {"id": 103, "seek": 49552, "start": 521.56, "end": 522.9399999999999, "text": " How much RAM do we have?", "tokens": [1012, 709, 14561, 360, 321, 362, 30], "temperature": 0.0, "avg_logprob": -0.14243474553843014, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.6449377957033e-06}, {"id": 104, "seek": 49552, "start": 522.9399999999999, "end": 525.24, "text": " How much latency can we get away with?", "tokens": [1012, 709, 27043, 393, 321, 483, 1314, 365, 30], "temperature": 0.0, "avg_logprob": -0.14243474553843014, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.6449377957033e-06}, {"id": 105, "seek": 52524, "start": 525.24, "end": 531.92, "text": " How expensive is it going to be to then scale it up to the point at which, you know, we're", "tokens": [1012, 5124, 307, 309, 516, 281, 312, 281, 550, 4373, 309, 493, 281, 264, 935, 412, 597, 11, 291, 458, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.16714112758636473, "compression_ratio": 1.5523809523809524, "no_speech_prob": 2.1107072825543582e-05}, {"id": 106, "seek": 52524, "start": 531.92, "end": 535.52, "text": " getting acceptable results using acceptable resources.", "tokens": [1242, 15513, 3542, 1228, 15513, 3593, 13], "temperature": 0.0, "avg_logprob": -0.16714112758636473, "compression_ratio": 1.5523809523809524, "no_speech_prob": 2.1107072825543582e-05}, {"id": 107, "seek": 52524, "start": 535.52, "end": 544.36, "text": " So, yeah, it wouldn't look very different at all to, you know, a Kaggle competition", "tokens": [407, 11, 1338, 11, 309, 2759, 380, 574, 588, 819, 412, 439, 281, 11, 291, 458, 11, 257, 48751, 22631, 6211], "temperature": 0.0, "avg_logprob": -0.16714112758636473, "compression_ratio": 1.5523809523809524, "no_speech_prob": 2.1107072825543582e-05}, {"id": 108, "seek": 52524, "start": 544.36, "end": 551.76, "text": " in terms of the modeling, but then there'd be a whole piece of analysis around user requirements", "tokens": [294, 2115, 295, 264, 15983, 11, 457, 550, 456, 1116, 312, 257, 1379, 2522, 295, 5215, 926, 4195, 7728], "temperature": 0.0, "avg_logprob": -0.16714112758636473, "compression_ratio": 1.5523809523809524, "no_speech_prob": 2.1107072825543582e-05}, {"id": 109, "seek": 55176, "start": 551.76, "end": 555.3199999999999, "text": " and costs and stuff like that.", "tokens": [293, 5497, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 110, "seek": 55176, "start": 555.3199999999999, "end": 557.3199999999999, "text": " I see.", "tokens": [286, 536, 13], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 111, "seek": 55176, "start": 557.3199999999999, "end": 559.24, "text": " Jeremy?", "tokens": [17809, 30], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 112, "seek": 55176, "start": 559.24, "end": 567.3199999999999, "text": " I tried doing what you're doing with going from smaller to larger models and mine somehow", "tokens": [286, 3031, 884, 437, 291, 434, 884, 365, 516, 490, 4356, 281, 4833, 5245, 293, 3892, 6063], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 113, "seek": 55176, "start": 567.3199999999999, "end": 571.76, "text": " started out with much lower accuracy.", "tokens": [1409, 484, 365, 709, 3126, 14170, 13], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 114, "seek": 55176, "start": 571.76, "end": 573.88, "text": " Is it just a fluke or?", "tokens": [1119, 309, 445, 257, 5029, 330, 420, 30], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 115, "seek": 55176, "start": 573.88, "end": 577.48, "text": " I mean, I had several issues happen that and then...", "tokens": [286, 914, 11, 286, 632, 2940, 2663, 1051, 300, 293, 550, 485], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 116, "seek": 55176, "start": 577.48, "end": 580.48, "text": " Oh, it's not a fluke.", "tokens": [876, 11, 309, 311, 406, 257, 5029, 330, 13], "temperature": 0.0, "avg_logprob": -0.3664347866931594, "compression_ratio": 1.4808743169398908, "no_speech_prob": 1.3210355064074975e-05}, {"id": 117, "seek": 58048, "start": 580.48, "end": 585.4, "text": " It means you press the wrong buttons somehow.", "tokens": [467, 1355, 291, 1886, 264, 2085, 9905, 6063, 13], "temperature": 0.0, "avg_logprob": -0.32311237392140857, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.2795796465070453e-05}, {"id": 118, "seek": 58048, "start": 585.4, "end": 586.4, "text": " So I will...", "tokens": [407, 286, 486, 485], "temperature": 0.0, "avg_logprob": -0.32311237392140857, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.2795796465070453e-05}, {"id": 119, "seek": 58048, "start": 586.4, "end": 594.96, "text": " I think I already have, haven't I shared my notebooks?", "tokens": [286, 519, 286, 1217, 362, 11, 2378, 380, 286, 5507, 452, 43782, 30], "temperature": 0.0, "avg_logprob": -0.32311237392140857, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.2795796465070453e-05}, {"id": 120, "seek": 58048, "start": 594.96, "end": 599.96, "text": " So if I haven't, I'll certainly share them today.", "tokens": [407, 498, 286, 2378, 380, 11, 286, 603, 3297, 2073, 552, 965, 13], "temperature": 0.0, "avg_logprob": -0.32311237392140857, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.2795796465070453e-05}, {"id": 121, "seek": 58048, "start": 599.96, "end": 606.4, "text": " Maybe I haven't yet.", "tokens": [2704, 286, 2378, 380, 1939, 13], "temperature": 0.0, "avg_logprob": -0.32311237392140857, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.2795796465070453e-05}, {"id": 122, "seek": 58048, "start": 606.4, "end": 607.48, "text": " So I'll share my notebooks today.", "tokens": [407, 286, 603, 2073, 452, 43782, 965, 13], "temperature": 0.0, "avg_logprob": -0.32311237392140857, "compression_ratio": 1.472972972972973, "no_speech_prob": 1.2795796465070453e-05}, {"id": 123, "seek": 60748, "start": 607.48, "end": 613.04, "text": " So what I suggest you do is like, you know, go from mine, make sure you can rerun them", "tokens": [407, 437, 286, 3402, 291, 360, 307, 411, 11, 291, 458, 11, 352, 490, 3892, 11, 652, 988, 291, 393, 43819, 409, 552], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 124, "seek": 60748, "start": 613.04, "end": 617.4, "text": " and then look at yours and see how they're different and then figure out where you went", "tokens": [293, 550, 574, 412, 6342, 293, 536, 577, 436, 434, 819, 293, 550, 2573, 484, 689, 291, 1437], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 125, "seek": 60748, "start": 617.4, "end": 618.4, "text": " wrong.", "tokens": [2085, 13], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 126, "seek": 60748, "start": 618.4, "end": 622.6, "text": " But also like, yeah, you know, I always tell people when debugging, like to look at the", "tokens": [583, 611, 411, 11, 1338, 11, 291, 458, 11, 286, 1009, 980, 561, 562, 45592, 11, 411, 281, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 127, "seek": 60748, "start": 622.6, "end": 623.88, "text": " inputs and the outputs.", "tokens": [15743, 293, 264, 23930, 13], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 128, "seek": 60748, "start": 623.88, "end": 625.36, "text": " So what predictions are you making?", "tokens": [407, 437, 21264, 366, 291, 1455, 30], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 129, "seek": 60748, "start": 625.36, "end": 628.12, "text": " Are you always predicting zero, for example?", "tokens": [2014, 291, 1009, 32884, 4018, 11, 337, 1365, 30], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 130, "seek": 60748, "start": 628.12, "end": 634.36, "text": " You know, did you run the LR finder to find what learning rate works well?", "tokens": [509, 458, 11, 630, 291, 1190, 264, 441, 49, 915, 260, 281, 915, 437, 2539, 3314, 1985, 731, 30], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 131, "seek": 60748, "start": 634.36, "end": 637.32, "text": " Yeah, stuff like that.", "tokens": [865, 11, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2142129898071289, "compression_ratio": 1.7611940298507462, "no_speech_prob": 2.9471050311258296e-06}, {"id": 132, "seek": 63732, "start": 637.32, "end": 639.6, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.4039948687833898, "compression_ratio": 1.4444444444444444, "no_speech_prob": 9.753622725838795e-05}, {"id": 133, "seek": 63732, "start": 639.6, "end": 642.84, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.4039948687833898, "compression_ratio": 1.4444444444444444, "no_speech_prob": 9.753622725838795e-05}, {"id": 134, "seek": 63732, "start": 642.84, "end": 649.84, "text": " Jeremy, on the question for the professional, I tried to...", "tokens": [17809, 11, 322, 264, 1168, 337, 264, 4843, 11, 286, 3031, 281, 485], "temperature": 0.0, "avg_logprob": -0.4039948687833898, "compression_ratio": 1.4444444444444444, "no_speech_prob": 9.753622725838795e-05}, {"id": 135, "seek": 63732, "start": 649.84, "end": 657.0400000000001, "text": " After your work for last week, you did mention learn.export and then later on we can learn", "tokens": [2381, 428, 589, 337, 1036, 1243, 11, 291, 630, 2152, 1466, 13, 3121, 2707, 293, 550, 1780, 322, 321, 393, 1466], "temperature": 0.0, "avg_logprob": -0.4039948687833898, "compression_ratio": 1.4444444444444444, "no_speech_prob": 9.753622725838795e-05}, {"id": 136, "seek": 63732, "start": 657.0400000000001, "end": 658.0400000000001, "text": ".no.", "tokens": [2411, 1771, 13], "temperature": 0.0, "avg_logprob": -0.4039948687833898, "compression_ratio": 1.4444444444444444, "no_speech_prob": 9.753622725838795e-05}, {"id": 137, "seek": 63732, "start": 658.0400000000001, "end": 661.2800000000001, "text": " I found that it's maybe that's about the...", "tokens": [286, 1352, 300, 309, 311, 1310, 300, 311, 466, 264, 485], "temperature": 0.0, "avg_logprob": -0.4039948687833898, "compression_ratio": 1.4444444444444444, "no_speech_prob": 9.753622725838795e-05}, {"id": 138, "seek": 66128, "start": 661.28, "end": 669.4399999999999, "text": " Because when I load them, they actually looking for the suffix is.pth.", "tokens": [1436, 562, 286, 3677, 552, 11, 436, 767, 1237, 337, 264, 3889, 970, 307, 2411, 79, 392, 13], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 139, "seek": 66128, "start": 669.4399999999999, "end": 677.0, "text": " But when we save and they say to the models folder and then you can give whatever name", "tokens": [583, 562, 321, 3155, 293, 436, 584, 281, 264, 5245, 10820, 293, 550, 291, 393, 976, 2035, 1315], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 140, "seek": 66128, "start": 677.0, "end": 678.28, "text": " you want.", "tokens": [291, 528, 13], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 141, "seek": 66128, "start": 678.28, "end": 682.92, "text": " But when you want to load them, they actually have the suffix at the end.", "tokens": [583, 562, 291, 528, 281, 3677, 552, 11, 436, 767, 362, 264, 3889, 970, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 142, "seek": 66128, "start": 682.92, "end": 684.4399999999999, "text": " So I'm not sure there's some...", "tokens": [407, 286, 478, 406, 988, 456, 311, 512, 485], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 143, "seek": 66128, "start": 684.4399999999999, "end": 685.4399999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 144, "seek": 66128, "start": 685.4399999999999, "end": 687.92, "text": " So just to make sure you save it with a.pth suffix.", "tokens": [407, 445, 281, 652, 988, 291, 3155, 309, 365, 257, 2411, 79, 392, 3889, 970, 13], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 145, "seek": 66128, "start": 687.92, "end": 690.3199999999999, "text": " But yeah, it certainly would make sense.", "tokens": [583, 1338, 11, 309, 3297, 576, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.2045370394045168, "compression_ratio": 1.7714285714285714, "no_speech_prob": 5.6482433137716725e-05}, {"id": 146, "seek": 69032, "start": 690.32, "end": 692.0, "text": " You're asked to do that automatically.", "tokens": [509, 434, 2351, 281, 360, 300, 6772, 13], "temperature": 0.0, "avg_logprob": -0.2891873319943746, "compression_ratio": 1.5846994535519126, "no_speech_prob": 3.21830520988442e-05}, {"id": 147, "seek": 69032, "start": 692.0, "end": 697.7600000000001, "text": " But in the documentation, it seems it's a safe in PICL.", "tokens": [583, 294, 264, 14333, 11, 309, 2544, 309, 311, 257, 3273, 294, 430, 2532, 43, 13], "temperature": 0.0, "avg_logprob": -0.2891873319943746, "compression_ratio": 1.5846994535519126, "no_speech_prob": 3.21830520988442e-05}, {"id": 148, "seek": 69032, "start": 697.7600000000001, "end": 701.8000000000001, "text": " The format is PICL.", "tokens": [440, 7877, 307, 430, 2532, 43, 13], "temperature": 0.0, "avg_logprob": -0.2891873319943746, "compression_ratio": 1.5846994535519126, "no_speech_prob": 3.21830520988442e-05}, {"id": 149, "seek": 69032, "start": 701.8000000000001, "end": 703.72, "text": " But this is just PyTorch.", "tokens": [583, 341, 307, 445, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.2891873319943746, "compression_ratio": 1.5846994535519126, "no_speech_prob": 3.21830520988442e-05}, {"id": 150, "seek": 69032, "start": 703.72, "end": 711.08, "text": " So PyTorch uses a variant of the PICL format and they normally use PTH as their extension.", "tokens": [407, 9953, 51, 284, 339, 4960, 257, 17501, 295, 264, 430, 2532, 43, 7877, 293, 436, 5646, 764, 430, 9620, 382, 641, 10320, 13], "temperature": 0.0, "avg_logprob": -0.2891873319943746, "compression_ratio": 1.5846994535519126, "no_speech_prob": 3.21830520988442e-05}, {"id": 151, "seek": 69032, "start": 711.08, "end": 716.44, "text": " So yes, it is PICL and just it does use the PTH extension.", "tokens": [407, 2086, 11, 309, 307, 430, 2532, 43, 293, 445, 309, 775, 764, 264, 430, 9620, 10320, 13], "temperature": 0.0, "avg_logprob": -0.2891873319943746, "compression_ratio": 1.5846994535519126, "no_speech_prob": 3.21830520988442e-05}, {"id": 152, "seek": 71644, "start": 716.44, "end": 727.1600000000001, "text": " Jeremy, when you were opening this window, you typed in something in the URL bar to get...", "tokens": [17809, 11, 562, 291, 645, 5193, 341, 4910, 11, 291, 33941, 294, 746, 294, 264, 12905, 2159, 281, 483, 485], "temperature": 0.0, "avg_logprob": -0.33703799364043446, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.320029645692557e-05}, {"id": 153, "seek": 71644, "start": 727.1600000000001, "end": 728.1600000000001, "text": " What were you...", "tokens": [708, 645, 291, 485], "temperature": 0.0, "avg_logprob": -0.33703799364043446, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.320029645692557e-05}, {"id": 154, "seek": 71644, "start": 728.1600000000001, "end": 731.6800000000001, "text": " Oh, I just typed my port number because I know that the only thing that has 8888 and", "tokens": [876, 11, 286, 445, 33941, 452, 2436, 1230, 570, 286, 458, 300, 264, 787, 551, 300, 575, 1649, 16919, 23, 293], "temperature": 0.0, "avg_logprob": -0.33703799364043446, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.320029645692557e-05}, {"id": 155, "seek": 71644, "start": 731.6800000000001, "end": 732.6800000000001, "text": " that is...", "tokens": [300, 307, 485], "temperature": 0.0, "avg_logprob": -0.33703799364043446, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.320029645692557e-05}, {"id": 156, "seek": 71644, "start": 732.6800000000001, "end": 733.6800000000001, "text": " Oh, okay.", "tokens": [876, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.33703799364043446, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.320029645692557e-05}, {"id": 157, "seek": 71644, "start": 733.6800000000001, "end": 737.48, "text": " Oh yeah, some magic going on.", "tokens": [876, 1338, 11, 512, 5585, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.33703799364043446, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.320029645692557e-05}, {"id": 158, "seek": 71644, "start": 737.48, "end": 740.48, "text": " Yeah, nothing like that.", "tokens": [865, 11, 1825, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.33703799364043446, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.320029645692557e-05}, {"id": 159, "seek": 74048, "start": 740.48, "end": 754.4, "text": " Let me just shut these down.", "tokens": [961, 385, 445, 5309, 613, 760, 13], "temperature": 0.0, "avg_logprob": -0.23975599895824085, "compression_ratio": 0.7777777777777778, "no_speech_prob": 7.833327254047617e-05}, {"id": 160, "seek": 75440, "start": 754.4, "end": 773.0799999999999, "text": " I did have one more idea about this competition, which is...", "tokens": [286, 630, 362, 472, 544, 1558, 466, 341, 6211, 11, 597, 307, 485], "temperature": 0.0, "avg_logprob": -0.21401803633745978, "compression_ratio": 0.9523809523809523, "no_speech_prob": 1.1474345228634775e-05}, {"id": 161, "seek": 77308, "start": 773.08, "end": 787.2800000000001, "text": " Oh, it's that CSV file, right?", "tokens": [876, 11, 309, 311, 300, 48814, 3991, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.5854824913872613, "compression_ratio": 0.813953488372093, "no_speech_prob": 2.5846466087386943e-05}, {"id": 162, "seek": 77308, "start": 787.2800000000001, "end": 788.2800000000001, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.5854824913872613, "compression_ratio": 0.813953488372093, "no_speech_prob": 2.5846466087386943e-05}, {"id": 163, "seek": 78828, "start": 788.28, "end": 804.0799999999999, "text": " Yeah, train.csv, that's right.", "tokens": [865, 11, 3847, 13, 14368, 85, 11, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.303847074508667, "compression_ratio": 0.953125, "no_speech_prob": 6.960851806070423e-06}, {"id": 164, "seek": 78828, "start": 804.0799999999999, "end": 814.48, "text": " And it has this variety thing.", "tokens": [400, 309, 575, 341, 5673, 551, 13], "temperature": 0.0, "avg_logprob": -0.303847074508667, "compression_ratio": 0.953125, "no_speech_prob": 6.960851806070423e-06}, {"id": 165, "seek": 81448, "start": 814.48, "end": 827.08, "text": " And I want for variety, tf.variety.", "tokens": [400, 286, 528, 337, 5673, 11, 256, 69, 13, 8517, 4014, 13], "temperature": 0.0, "avg_logprob": -0.1555821165746572, "compression_ratio": 1.3577981651376148, "no_speech_prob": 1.5443956726812758e-05}, {"id": 166, "seek": 81448, "start": 827.08, "end": 834.2, "text": " So there's 10,000 rows and 7,000 of them are one variety.", "tokens": [407, 456, 311, 1266, 11, 1360, 13241, 293, 1614, 11, 1360, 295, 552, 366, 472, 5673, 13], "temperature": 0.0, "avg_logprob": -0.1555821165746572, "compression_ratio": 1.3577981651376148, "no_speech_prob": 1.5443956726812758e-05}, {"id": 167, "seek": 81448, "start": 834.2, "end": 840.52, "text": " But there are 3,000 rows that contain other varieties.", "tokens": [583, 456, 366, 805, 11, 1360, 13241, 300, 5304, 661, 22092, 13], "temperature": 0.0, "avg_logprob": -0.1555821165746572, "compression_ratio": 1.3577981651376148, "no_speech_prob": 1.5443956726812758e-05}, {"id": 168, "seek": 84052, "start": 840.52, "end": 849.88, "text": " So the only idea I had for this was something which is a bit counterintuitive, but those", "tokens": [407, 264, 787, 1558, 286, 632, 337, 341, 390, 746, 597, 307, 257, 857, 5682, 686, 48314, 11, 457, 729], "temperature": 0.0, "avg_logprob": -0.1458059847354889, "compression_ratio": 1.6157205240174672, "no_speech_prob": 3.761013431358151e-05}, {"id": 169, "seek": 84052, "start": 849.88, "end": 856.04, "text": " of you that did, I can't remember, 2017 or 2018, faster, I might remember.", "tokens": [295, 291, 300, 630, 11, 286, 393, 380, 1604, 11, 6591, 420, 6096, 11, 4663, 11, 286, 1062, 1604, 13], "temperature": 0.0, "avg_logprob": -0.1458059847354889, "compression_ratio": 1.6157205240174672, "no_speech_prob": 3.761013431358151e-05}, {"id": 170, "seek": 84052, "start": 856.04, "end": 859.8, "text": " Sometimes if there's two different things, in this case, what kind of rice is it and", "tokens": [4803, 498, 456, 311, 732, 819, 721, 11, 294, 341, 1389, 11, 437, 733, 295, 5090, 307, 309, 293], "temperature": 0.0, "avg_logprob": -0.1458059847354889, "compression_ratio": 1.6157205240174672, "no_speech_prob": 3.761013431358151e-05}, {"id": 171, "seek": 84052, "start": 859.8, "end": 864.0799999999999, "text": " what kind of disease is it, sometimes trying to get your model to predict both of those", "tokens": [437, 733, 295, 4752, 307, 309, 11, 2171, 1382, 281, 483, 428, 2316, 281, 6069, 1293, 295, 729], "temperature": 0.0, "avg_logprob": -0.1458059847354889, "compression_ratio": 1.6157205240174672, "no_speech_prob": 3.761013431358151e-05}, {"id": 172, "seek": 84052, "start": 864.0799999999999, "end": 868.04, "text": " things makes them better at both.", "tokens": [721, 1669, 552, 1101, 412, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1458059847354889, "compression_ratio": 1.6157205240174672, "no_speech_prob": 3.761013431358151e-05}, {"id": 173, "seek": 86804, "start": 868.04, "end": 872.0, "text": " So if we try to get our model to predict what kind of disease is it and what kind of rice", "tokens": [407, 498, 321, 853, 281, 483, 527, 2316, 281, 6069, 437, 733, 295, 4752, 307, 309, 293, 437, 733, 295, 5090], "temperature": 0.0, "avg_logprob": -0.09833163228528254, "compression_ratio": 1.7816091954022988, "no_speech_prob": 3.070763705181889e-05}, {"id": 174, "seek": 86804, "start": 872.0, "end": 880.88, "text": " is it, it might actually get better at predicting the kind of disease, which might sound counterintuitive", "tokens": [307, 309, 11, 309, 1062, 767, 483, 1101, 412, 32884, 264, 733, 295, 4752, 11, 597, 1062, 1626, 5682, 686, 48314], "temperature": 0.0, "avg_logprob": -0.09833163228528254, "compression_ratio": 1.7816091954022988, "no_speech_prob": 3.070763705181889e-05}, {"id": 175, "seek": 86804, "start": 880.88, "end": 885.8399999999999, "text": " or I find it counterintuitive because it sounds like it's got more work to do.", "tokens": [420, 286, 915, 309, 5682, 686, 48314, 570, 309, 3263, 411, 309, 311, 658, 544, 589, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.09833163228528254, "compression_ratio": 1.7816091954022988, "no_speech_prob": 3.070763705181889e-05}, {"id": 176, "seek": 86804, "start": 885.8399999999999, "end": 891.36, "text": " But you're also giving it more signal, like there's more things you're teaching it to", "tokens": [583, 291, 434, 611, 2902, 309, 544, 6358, 11, 411, 456, 311, 544, 721, 291, 434, 4571, 309, 281], "temperature": 0.0, "avg_logprob": -0.09833163228528254, "compression_ratio": 1.7816091954022988, "no_speech_prob": 3.070763705181889e-05}, {"id": 177, "seek": 86804, "start": 891.36, "end": 892.36, "text": " look for.", "tokens": [574, 337, 13], "temperature": 0.0, "avg_logprob": -0.09833163228528254, "compression_ratio": 1.7816091954022988, "no_speech_prob": 3.070763705181889e-05}, {"id": 178, "seek": 86804, "start": 892.36, "end": 896.8, "text": " And so maybe if it knows how to recognise different types of rice, it can use that information", "tokens": [400, 370, 1310, 498, 309, 3255, 577, 281, 23991, 819, 3467, 295, 5090, 11, 309, 393, 764, 300, 1589], "temperature": 0.0, "avg_logprob": -0.09833163228528254, "compression_ratio": 1.7816091954022988, "no_speech_prob": 3.070763705181889e-05}, {"id": 179, "seek": 89680, "start": 896.8, "end": 901.64, "text": " to also recognise how different kinds of rice are impacted by different diseases.", "tokens": [281, 611, 23991, 577, 819, 3685, 295, 5090, 366, 15653, 538, 819, 11044, 13], "temperature": 0.0, "avg_logprob": -0.10507808504877864, "compression_ratio": 1.6123595505617978, "no_speech_prob": 1.2214642993058078e-05}, {"id": 180, "seek": 89680, "start": 901.64, "end": 906.76, "text": " So I have no idea if that's going to be useful or not, but I thought it would be an interesting", "tokens": [407, 286, 362, 572, 1558, 498, 300, 311, 516, 281, 312, 4420, 420, 406, 11, 457, 286, 1194, 309, 576, 312, 364, 1880], "temperature": 0.0, "avg_logprob": -0.10507808504877864, "compression_ratio": 1.6123595505617978, "no_speech_prob": 1.2214642993058078e-05}, {"id": 181, "seek": 89680, "start": 906.76, "end": 910.64, "text": " exercise to try to do that.", "tokens": [5380, 281, 853, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.10507808504877864, "compression_ratio": 1.6123595505617978, "no_speech_prob": 1.2214642993058078e-05}, {"id": 182, "seek": 89680, "start": 910.64, "end": 918.0, "text": " So that's what I thought we might have a go at today, if that sounds of interest.", "tokens": [407, 300, 311, 437, 286, 1194, 321, 1062, 362, 257, 352, 412, 965, 11, 498, 300, 3263, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.10507808504877864, "compression_ratio": 1.6123595505617978, "no_speech_prob": 1.2214642993058078e-05}, {"id": 183, "seek": 91800, "start": 918.0, "end": 931.4, "text": " Which also is frankly a good exercise in delving into models in a way we've never done before.", "tokens": [3013, 611, 307, 11939, 257, 665, 5380, 294, 1103, 798, 666, 5245, 294, 257, 636, 321, 600, 1128, 1096, 949, 13], "temperature": 0.0, "avg_logprob": -0.13091830773787064, "compression_ratio": 1.3602941176470589, "no_speech_prob": 7.406109943985939e-06}, {"id": 184, "seek": 91800, "start": 931.4, "end": 942.68, "text": " So this is going to be much more sophisticated than anything we've done with deep learning", "tokens": [407, 341, 307, 516, 281, 312, 709, 544, 16950, 813, 1340, 321, 600, 1096, 365, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.13091830773787064, "compression_ratio": 1.3602941176470589, "no_speech_prob": 7.406109943985939e-06}, {"id": 185, "seek": 94268, "start": 942.68, "end": 950.8, "text": " before, which means it's very much up to you folks to stop me anytime something slightly", "tokens": [949, 11, 597, 1355, 309, 311, 588, 709, 493, 281, 291, 4024, 281, 1590, 385, 13038, 746, 4748], "temperature": 0.0, "avg_logprob": -0.1253601356788918, "compression_ratio": 1.5687203791469195, "no_speech_prob": 5.0608003220986575e-05}, {"id": 186, "seek": 94268, "start": 950.8, "end": 957.2399999999999, "text": " confusing because I actually want everybody to understand this.", "tokens": [13181, 570, 286, 767, 528, 2201, 281, 1223, 341, 13], "temperature": 0.0, "avg_logprob": -0.1253601356788918, "compression_ratio": 1.5687203791469195, "no_speech_prob": 5.0608003220986575e-05}, {"id": 187, "seek": 94268, "start": 957.2399999999999, "end": 965.0, "text": " And it's a really good test of how well you understand what's going on inside a neural", "tokens": [400, 309, 311, 257, 534, 665, 1500, 295, 577, 731, 291, 1223, 437, 311, 516, 322, 1854, 257, 18161], "temperature": 0.0, "avg_logprob": -0.1253601356788918, "compression_ratio": 1.5687203791469195, "no_speech_prob": 5.0608003220986575e-05}, {"id": 188, "seek": 94268, "start": 965.0, "end": 966.0, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.1253601356788918, "compression_ratio": 1.5687203791469195, "no_speech_prob": 5.0608003220986575e-05}, {"id": 189, "seek": 94268, "start": 966.0, "end": 970.12, "text": " So if you're not understanding it, that's a sign I haven't explained it very well.", "tokens": [407, 498, 291, 434, 406, 3701, 309, 11, 300, 311, 257, 1465, 286, 2378, 380, 8825, 309, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.1253601356788918, "compression_ratio": 1.5687203791469195, "no_speech_prob": 5.0608003220986575e-05}, {"id": 190, "seek": 97012, "start": 970.12, "end": 975.4, "text": " So let me try it.", "tokens": [407, 718, 385, 853, 309, 13], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 191, "seek": 97012, "start": 975.4, "end": 976.4, "text": " Let's have a look.", "tokens": [961, 311, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 192, "seek": 97012, "start": 976.4, "end": 977.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 193, "seek": 97012, "start": 977.4, "end": 985.08, "text": " So one thing I just did yesterday afternoon was I just trained a model three times to", "tokens": [407, 472, 551, 286, 445, 630, 5186, 6499, 390, 286, 445, 8895, 257, 2316, 1045, 1413, 281], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 194, "seek": 97012, "start": 985.08, "end": 989.5600000000001, "text": " see what the error rate was, because I wanted to get a sense of like how much variation", "tokens": [536, 437, 264, 6713, 3314, 390, 11, 570, 286, 1415, 281, 483, 257, 2020, 295, 411, 577, 709, 12990], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 195, "seek": 97012, "start": 989.5600000000001, "end": 990.84, "text": " is there.", "tokens": [307, 456, 13], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 196, "seek": 97012, "start": 990.84, "end": 996.04, "text": " And I found if I use a learning rate of 0.02 and just train for three epochs, I seem to", "tokens": [400, 286, 1352, 498, 286, 764, 257, 2539, 3314, 295, 1958, 13, 12756, 293, 445, 3847, 337, 1045, 30992, 28346, 11, 286, 1643, 281], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 197, "seek": 97012, "start": 996.04, "end": 998.64, "text": " pretty consistently get reasonable results.", "tokens": [1238, 14961, 483, 10585, 3542, 13], "temperature": 0.0, "avg_logprob": -0.13323746105231862, "compression_ratio": 1.5169491525423728, "no_speech_prob": 7.409590580209624e-06}, {"id": 198, "seek": 99864, "start": 998.64, "end": 1004.4399999999999, "text": " So here's something I can now do in two minutes to see how I'm going.", "tokens": [407, 510, 311, 746, 286, 393, 586, 360, 294, 732, 2077, 281, 536, 577, 286, 478, 516, 13], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 199, "seek": 99864, "start": 1004.4399999999999, "end": 1007.3199999999999, "text": " So I thought that would be good.", "tokens": [407, 286, 1194, 300, 576, 312, 665, 13], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 200, "seek": 99864, "start": 1007.3199999999999, "end": 1008.84, "text": " So this is one thing I really like doing.", "tokens": [407, 341, 307, 472, 551, 286, 534, 411, 884, 13], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 201, "seek": 99864, "start": 1008.84, "end": 1012.88, "text": " People are often very into doing reproducible training where they have like set the seed", "tokens": [3432, 366, 2049, 588, 666, 884, 11408, 32128, 3097, 689, 436, 362, 411, 992, 264, 8871], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 202, "seek": 99864, "start": 1012.88, "end": 1015.92, "text": " for their training and run the same thing every time.", "tokens": [337, 641, 3097, 293, 1190, 264, 912, 551, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 203, "seek": 99864, "start": 1015.92, "end": 1020.6, "text": " I think that's normally a bad idea because I actually want to see like what the natural", "tokens": [286, 519, 300, 311, 5646, 257, 1578, 1558, 570, 286, 767, 528, 281, 536, 411, 437, 264, 3303], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 204, "seek": 99864, "start": 1020.6, "end": 1021.6, "text": " variation is.", "tokens": [12990, 307, 13], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 205, "seek": 99864, "start": 1021.6, "end": 1027.94, "text": " And so if I make a change, I want to know whether that's, you know, changes the difference", "tokens": [400, 370, 498, 286, 652, 257, 1319, 11, 286, 528, 281, 458, 1968, 300, 311, 11, 291, 458, 11, 2962, 264, 2649], "temperature": 0.0, "avg_logprob": -0.12600065046741116, "compression_ratio": 1.7204301075268817, "no_speech_prob": 6.961723101994721e-06}, {"id": 206, "seek": 102794, "start": 1027.94, "end": 1033.0800000000002, "text": " I see in the result is might be just due to natural variation or it's actually something", "tokens": [286, 536, 294, 264, 1874, 307, 1062, 312, 445, 3462, 281, 3303, 12990, 420, 309, 311, 767, 746], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 207, "seek": 102794, "start": 1033.0800000000002, "end": 1034.52, "text": " significant.", "tokens": [4776, 13], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 208, "seek": 102794, "start": 1034.52, "end": 1035.52, "text": " So that's why I did these.", "tokens": [407, 300, 311, 983, 286, 630, 613, 13], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 209, "seek": 102794, "start": 1035.52, "end": 1038.3600000000001, "text": " The natural variation is really large.", "tokens": [440, 3303, 12990, 307, 534, 2416, 13], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 210, "seek": 102794, "start": 1038.3600000000001, "end": 1040.3600000000001, "text": " Does that ever just weight you?", "tokens": [4402, 300, 1562, 445, 3364, 291, 30], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 211, "seek": 102794, "start": 1040.3600000000001, "end": 1044.52, "text": " Yeah, that's going to be tough to see like, did I improve things?", "tokens": [865, 11, 300, 311, 516, 281, 312, 4930, 281, 536, 411, 11, 630, 286, 3470, 721, 30], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 212, "seek": 102794, "start": 1044.52, "end": 1048.8400000000001, "text": " But then if the natural variation is so large that improvements are invisible, then trying", "tokens": [583, 550, 498, 264, 3303, 12990, 307, 370, 2416, 300, 13797, 366, 14603, 11, 550, 1382], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 213, "seek": 102794, "start": 1048.8400000000001, "end": 1050.64, "text": " to improve it seems pointless, right?", "tokens": [281, 3470, 309, 2544, 32824, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 214, "seek": 102794, "start": 1050.64, "end": 1056.6000000000001, "text": " Because it sounds like you haven't really found a way to stably train something.", "tokens": [1436, 309, 3263, 411, 291, 2378, 380, 534, 1352, 257, 636, 281, 342, 1188, 3847, 746, 13], "temperature": 0.0, "avg_logprob": -0.2302100658416748, "compression_ratio": 1.7723880597014925, "no_speech_prob": 6.338943421724252e-06}, {"id": 215, "seek": 105660, "start": 1056.6, "end": 1062.1599999999999, "text": " And normally that happens because my learning rate is too big.", "tokens": [400, 5646, 300, 2314, 570, 452, 2539, 3314, 307, 886, 955, 13], "temperature": 0.0, "avg_logprob": -0.17028380729056694, "compression_ratio": 1.6090534979423867, "no_speech_prob": 7.2959778663062025e-06}, {"id": 216, "seek": 105660, "start": 1062.1599999999999, "end": 1065.9199999999998, "text": " So if you try this yourself and bump the learning rate up to 0.04, you'll see like, at least", "tokens": [407, 498, 291, 853, 341, 1803, 293, 9961, 264, 2539, 3314, 493, 281, 1958, 13, 14565, 11, 291, 603, 536, 411, 11, 412, 1935], "temperature": 0.0, "avg_logprob": -0.17028380729056694, "compression_ratio": 1.6090534979423867, "no_speech_prob": 7.2959778663062025e-06}, {"id": 217, "seek": 105660, "start": 1065.9199999999998, "end": 1069.24, "text": " for me, I got like 5%, 6%, 5.5%.", "tokens": [337, 385, 11, 286, 658, 411, 1025, 8923, 1386, 8923, 1025, 13, 20, 6856], "temperature": 0.0, "avg_logprob": -0.17028380729056694, "compression_ratio": 1.6090534979423867, "no_speech_prob": 7.2959778663062025e-06}, {"id": 218, "seek": 105660, "start": 1069.24, "end": 1073.36, "text": " You know, it's like all over the place.", "tokens": [509, 458, 11, 309, 311, 411, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.17028380729056694, "compression_ratio": 1.6090534979423867, "no_speech_prob": 7.2959778663062025e-06}, {"id": 219, "seek": 105660, "start": 1073.36, "end": 1078.48, "text": " So yeah, trading for more epochs at a lower learning rate will generally give you more", "tokens": [407, 1338, 11, 9529, 337, 544, 30992, 28346, 412, 257, 3126, 2539, 3314, 486, 5101, 976, 291, 544], "temperature": 0.0, "avg_logprob": -0.17028380729056694, "compression_ratio": 1.6090534979423867, "no_speech_prob": 7.2959778663062025e-06}, {"id": 220, "seek": 105660, "start": 1078.48, "end": 1080.76, "text": " stable results.", "tokens": [8351, 3542, 13], "temperature": 0.0, "avg_logprob": -0.17028380729056694, "compression_ratio": 1.6090534979423867, "no_speech_prob": 7.2959778663062025e-06}, {"id": 221, "seek": 105660, "start": 1080.76, "end": 1083.24, "text": " And there's a compromise because doing more epochs is slow.", "tokens": [400, 456, 311, 257, 18577, 570, 884, 544, 30992, 28346, 307, 2964, 13], "temperature": 0.0, "avg_logprob": -0.17028380729056694, "compression_ratio": 1.6090534979423867, "no_speech_prob": 7.2959778663062025e-06}, {"id": 222, "seek": 108324, "start": 1083.24, "end": 1086.64, "text": " So that's why I was trying to find a learning rate and a number of epochs, which is fast", "tokens": [407, 300, 311, 983, 286, 390, 1382, 281, 915, 257, 2539, 3314, 293, 257, 1230, 295, 30992, 28346, 11, 597, 307, 2370], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 223, "seek": 108324, "start": 1086.64, "end": 1087.64, "text": " and stable.", "tokens": [293, 8351, 13], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 224, "seek": 108324, "start": 1087.64, "end": 1094.68, "text": " You could also try using a smaller subset of the data or I don't know, like in the end,", "tokens": [509, 727, 611, 853, 1228, 257, 4356, 25993, 295, 264, 1412, 420, 286, 500, 380, 458, 11, 411, 294, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 225, "seek": 108324, "start": 1094.68, "end": 1096.6, "text": " sometimes things just will be slow in such as life.", "tokens": [2171, 721, 445, 486, 312, 2964, 294, 1270, 382, 993, 13], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 226, "seek": 108324, "start": 1096.6, "end": 1103.28, "text": " But most of the time I find I can get a compromise and I certainly did here, I think.", "tokens": [583, 881, 295, 264, 565, 286, 915, 286, 393, 483, 257, 18577, 293, 286, 3297, 630, 510, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 227, "seek": 108324, "start": 1103.28, "end": 1106.52, "text": " With six epochs at half the learning rate, I certainly can do better.", "tokens": [2022, 2309, 30992, 28346, 412, 1922, 264, 2539, 3314, 11, 286, 3297, 393, 360, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 228, "seek": 108324, "start": 1106.52, "end": 1110.28, "text": " I can get to 4%, you know, rather than 5.", "tokens": [286, 393, 483, 281, 1017, 8923, 291, 458, 11, 2831, 813, 1025, 13], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 229, "seek": 108324, "start": 1110.28, "end": 1111.28, "text": " But that's okay.", "tokens": [583, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.1735736322766952, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3418178241408896e-05}, {"id": 230, "seek": 111128, "start": 1111.28, "end": 1114.72, "text": " I just want something for testing.", "tokens": [286, 445, 528, 746, 337, 4997, 13], "temperature": 0.0, "avg_logprob": -0.1701175233592158, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.070980245771352e-06}, {"id": 231, "seek": 111128, "start": 1114.72, "end": 1121.12, "text": " One thing that was always counterintuitive to me that I think you talk about is like", "tokens": [1485, 551, 300, 390, 1009, 5682, 686, 48314, 281, 385, 300, 286, 519, 291, 751, 466, 307, 411], "temperature": 0.0, "avg_logprob": -0.1701175233592158, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.070980245771352e-06}, {"id": 232, "seek": 111128, "start": 1121.12, "end": 1127.72, "text": " these improvements that you make on a small scale, like show up on the larger scale, like", "tokens": [613, 13797, 300, 291, 652, 322, 257, 1359, 4373, 11, 411, 855, 493, 322, 264, 4833, 4373, 11, 411], "temperature": 0.0, "avg_logprob": -0.1701175233592158, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.070980245771352e-06}, {"id": 233, "seek": 111128, "start": 1127.72, "end": 1128.72, "text": " always.", "tokens": [1009, 13], "temperature": 0.0, "avg_logprob": -0.1701175233592158, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.070980245771352e-06}, {"id": 234, "seek": 111128, "start": 1128.72, "end": 1129.72, "text": " Oh yeah, absolutely.", "tokens": [876, 1338, 11, 3122, 13], "temperature": 0.0, "avg_logprob": -0.1701175233592158, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.070980245771352e-06}, {"id": 235, "seek": 111128, "start": 1129.72, "end": 1131.16, "text": " Basically, they pretty much always will.", "tokens": [8537, 11, 436, 1238, 709, 1009, 486, 13], "temperature": 0.0, "avg_logprob": -0.1701175233592158, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.070980245771352e-06}, {"id": 236, "seek": 111128, "start": 1131.16, "end": 1137.48, "text": " Yeah, because they're the same models with just more layers or wider activations.", "tokens": [865, 11, 570, 436, 434, 264, 912, 5245, 365, 445, 544, 7914, 420, 11842, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.1701175233592158, "compression_ratio": 1.6188340807174888, "no_speech_prob": 7.070980245771352e-06}, {"id": 237, "seek": 113748, "start": 1137.48, "end": 1143.32, "text": " Yeah, if you find something that's going to, some pre-processing step that works well on", "tokens": [865, 11, 498, 291, 915, 746, 300, 311, 516, 281, 11, 512, 659, 12, 41075, 278, 1823, 300, 1985, 731, 322], "temperature": 0.0, "avg_logprob": -0.19998966563831677, "compression_ratio": 1.5232558139534884, "no_speech_prob": 1.6698539184289984e-05}, {"id": 238, "seek": 113748, "start": 1143.32, "end": 1154.2, "text": " a convexed tiny, it's going to work also well on a convexed large 99.9% of the time.", "tokens": [257, 42432, 292, 5870, 11, 309, 311, 516, 281, 589, 611, 731, 322, 257, 42432, 292, 2416, 11803, 13, 24, 4, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.19998966563831677, "compression_ratio": 1.5232558139534884, "no_speech_prob": 1.6698539184289984e-05}, {"id": 239, "seek": 113748, "start": 1154.2, "end": 1160.16, "text": " Most people act as if that's not true, I find, but you know, like in academia and stuff.", "tokens": [4534, 561, 605, 382, 498, 300, 311, 406, 2074, 11, 286, 915, 11, 457, 291, 458, 11, 411, 294, 28937, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.19998966563831677, "compression_ratio": 1.5232558139534884, "no_speech_prob": 1.6698539184289984e-05}, {"id": 240, "seek": 116016, "start": 1160.16, "end": 1168.64, "text": " I feel like you have to do a full suite of everything, which most people just never think", "tokens": [286, 841, 411, 291, 362, 281, 360, 257, 1577, 14205, 295, 1203, 11, 597, 881, 561, 445, 1128, 519], "temperature": 0.0, "avg_logprob": -0.29961476380797636, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.6440633771708235e-05}, {"id": 241, "seek": 116016, "start": 1168.64, "end": 1173.76, "text": " to try, but intuitively, of course it's the same, you know?", "tokens": [281, 853, 11, 457, 46506, 11, 295, 1164, 309, 311, 264, 912, 11, 291, 458, 30], "temperature": 0.0, "avg_logprob": -0.29961476380797636, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.6440633771708235e-05}, {"id": 242, "seek": 116016, "start": 1173.76, "end": 1175.0, "text": " Why wouldn't it be the same?", "tokens": [1545, 2759, 380, 309, 312, 264, 912, 30], "temperature": 0.0, "avg_logprob": -0.29961476380797636, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.6440633771708235e-05}, {"id": 243, "seek": 116016, "start": 1175.0, "end": 1179.3200000000002, "text": " Like it is the same thing, just scaled up a bit, they behave very similarly.", "tokens": [1743, 309, 307, 264, 912, 551, 11, 445, 36039, 493, 257, 857, 11, 436, 15158, 588, 14138, 13], "temperature": 0.0, "avg_logprob": -0.29961476380797636, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.6440633771708235e-05}, {"id": 244, "seek": 116016, "start": 1179.3200000000002, "end": 1185.52, "text": " I mean, it's hard to argue with you because it works.", "tokens": [286, 914, 11, 309, 311, 1152, 281, 9695, 365, 291, 570, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.29961476380797636, "compression_ratio": 1.5297029702970297, "no_speech_prob": 1.6440633771708235e-05}, {"id": 245, "seek": 118552, "start": 1185.52, "end": 1190.28, "text": " So I mean, but like it wasn't that intuitive.", "tokens": [407, 286, 914, 11, 457, 411, 309, 2067, 380, 300, 21769, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 246, "seek": 118552, "start": 1190.28, "end": 1192.16, "text": " You can argue that it's not intuitive, that's fine.", "tokens": [509, 393, 9695, 300, 309, 311, 406, 21769, 11, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 247, "seek": 118552, "start": 1192.16, "end": 1195.52, "text": " But like, I feel like the only reason it would be not intuitive is because everybody's told", "tokens": [583, 411, 11, 286, 841, 411, 264, 787, 1778, 309, 576, 312, 406, 21769, 307, 570, 2201, 311, 1907], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 248, "seek": 118552, "start": 1195.52, "end": 1197.28, "text": " you for years that it doesn't work that way.", "tokens": [291, 337, 924, 300, 309, 1177, 380, 589, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 249, "seek": 118552, "start": 1197.28, "end": 1198.28, "text": " Do you know what I mean?", "tokens": [1144, 291, 458, 437, 286, 914, 30], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 250, "seek": 118552, "start": 1198.28, "end": 1199.28, "text": " Yeah, that's fair.", "tokens": [865, 11, 300, 311, 3143, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 251, "seek": 118552, "start": 1199.28, "end": 1202.96, "text": " If nobody told you that, I think you'd be like, yeah, of course it works that way.", "tokens": [759, 5079, 1907, 291, 300, 11, 286, 519, 291, 1116, 312, 411, 11, 1338, 11, 295, 1164, 309, 1985, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 252, "seek": 118552, "start": 1202.96, "end": 1203.96, "text": " That's fair.", "tokens": [663, 311, 3143, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 253, "seek": 118552, "start": 1203.96, "end": 1204.96, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 254, "seek": 118552, "start": 1204.96, "end": 1207.76, "text": " So, okay, let's do something crazy.", "tokens": [407, 11, 1392, 11, 718, 311, 360, 746, 3219, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 255, "seek": 118552, "start": 1207.76, "end": 1210.92, "text": " Let's actually look at a model.", "tokens": [961, 311, 767, 574, 412, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 256, "seek": 118552, "start": 1210.92, "end": 1213.24, "text": " So inside our learner, there's basically two main things.", "tokens": [407, 1854, 527, 33347, 11, 456, 311, 1936, 732, 2135, 721, 13], "temperature": 0.0, "avg_logprob": -0.19061481325249924, "compression_ratio": 1.775438596491228, "no_speech_prob": 1.1658728908514604e-05}, {"id": 257, "seek": 121324, "start": 1213.24, "end": 1217.72, "text": " There's the data loaders, learn.deals, and there's the model, learn.model.", "tokens": [821, 311, 264, 1412, 3677, 433, 11, 1466, 13, 1479, 1124, 11, 293, 456, 311, 264, 2316, 11, 1466, 13, 8014, 338, 13], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 258, "seek": 121324, "start": 1217.72, "end": 1218.72, "text": " And we've seen these before.", "tokens": [400, 321, 600, 1612, 613, 949, 13], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 259, "seek": 121324, "start": 1218.72, "end": 1224.44, "text": " And if you've forgotten, then yeah, go back and have a look at the older videos from the", "tokens": [400, 498, 291, 600, 11832, 11, 550, 1338, 11, 352, 646, 293, 362, 257, 574, 412, 264, 4906, 2145, 490, 264], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 260, "seek": 121324, "start": 1224.44, "end": 1225.44, "text": " course.", "tokens": [1164, 13], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 261, "seek": 121324, "start": 1225.44, "end": 1232.84, "text": " So the model itself, basically, yeah, it's got like things in it.", "tokens": [407, 264, 2316, 2564, 11, 1936, 11, 1338, 11, 309, 311, 658, 411, 721, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 262, "seek": 121324, "start": 1232.84, "end": 1236.16, "text": " And in this case, the first thing in it is called a timbody.", "tokens": [400, 294, 341, 1389, 11, 264, 700, 551, 294, 309, 307, 1219, 257, 524, 1067, 13], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 263, "seek": 121324, "start": 1236.16, "end": 1237.6, "text": " And the timbody has things in it.", "tokens": [400, 264, 524, 1067, 575, 721, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 264, "seek": 121324, "start": 1237.6, "end": 1239.44, "text": " And the first thing in it is called model.", "tokens": [400, 264, 700, 551, 294, 309, 307, 1219, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2144120689330062, "compression_ratio": 1.9056603773584906, "no_speech_prob": 1.406253704772098e-05}, {"id": 265, "seek": 123944, "start": 1239.44, "end": 1244.2, "text": " And then timbody.model has things in it, and the first thing is called the stem, and the", "tokens": [400, 550, 524, 1067, 13, 8014, 338, 575, 721, 294, 309, 11, 293, 264, 700, 551, 307, 1219, 264, 12312, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.19619356896266466, "compression_ratio": 1.84304932735426, "no_speech_prob": 6.14401096754591e-06}, {"id": 266, "seek": 123944, "start": 1244.2, "end": 1245.92, "text": " next thing is called the stages, and so forth.", "tokens": [958, 551, 307, 1219, 264, 10232, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.19619356896266466, "compression_ratio": 1.84304932735426, "no_speech_prob": 6.14401096754591e-06}, {"id": 267, "seek": 123944, "start": 1245.92, "end": 1247.8, "text": " So you can see how it's this kind of tree.", "tokens": [407, 291, 393, 536, 577, 309, 311, 341, 733, 295, 4230, 13], "temperature": 0.0, "avg_logprob": -0.19619356896266466, "compression_ratio": 1.84304932735426, "no_speech_prob": 6.14401096754591e-06}, {"id": 268, "seek": 123944, "start": 1247.8, "end": 1251.8, "text": " And we actually want to go all the way to the bottom.", "tokens": [400, 321, 767, 528, 281, 352, 439, 264, 636, 281, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.19619356896266466, "compression_ratio": 1.84304932735426, "no_speech_prob": 6.14401096754591e-06}, {"id": 269, "seek": 123944, "start": 1251.8, "end": 1256.76, "text": " So the basic top, there's two things in it at the very top level.", "tokens": [407, 264, 3875, 1192, 11, 456, 311, 732, 721, 294, 309, 412, 264, 588, 1192, 1496, 13], "temperature": 0.0, "avg_logprob": -0.19619356896266466, "compression_ratio": 1.84304932735426, "no_speech_prob": 6.14401096754591e-06}, {"id": 270, "seek": 123944, "start": 1256.76, "end": 1263.56, "text": " There's a timbody, and there's a thing here, which doesn't actually have a name, but we", "tokens": [821, 311, 257, 524, 1067, 11, 293, 456, 311, 257, 551, 510, 11, 597, 1177, 380, 767, 362, 257, 1315, 11, 457, 321], "temperature": 0.0, "avg_logprob": -0.19619356896266466, "compression_ratio": 1.84304932735426, "no_speech_prob": 6.14401096754591e-06}, {"id": 271, "seek": 123944, "start": 1263.56, "end": 1265.4, "text": " always call it the head.", "tokens": [1009, 818, 309, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.19619356896266466, "compression_ratio": 1.84304932735426, "no_speech_prob": 6.14401096754591e-06}, {"id": 272, "seek": 126540, "start": 1265.4, "end": 1270.92, "text": " And so the body is the bit that basically does all the hard work of looking at the pixels", "tokens": [400, 370, 264, 1772, 307, 264, 857, 300, 1936, 775, 439, 264, 1152, 589, 295, 1237, 412, 264, 18668], "temperature": 0.0, "avg_logprob": -0.1003253664289202, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.178035538236145e-05}, {"id": 273, "seek": 126540, "start": 1270.92, "end": 1273.48, "text": " and trying to find features and stuff like that.", "tokens": [293, 1382, 281, 915, 4122, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1003253664289202, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.178035538236145e-05}, {"id": 274, "seek": 126540, "start": 1273.48, "end": 1277.2, "text": " That's something we call a convolutional neural network.", "tokens": [663, 311, 746, 321, 818, 257, 45216, 304, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1003253664289202, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.178035538236145e-05}, {"id": 275, "seek": 126540, "start": 1277.2, "end": 1285.2, "text": " And at the very end of that, it spits out, yeah, a whole bunch of information about those", "tokens": [400, 412, 264, 588, 917, 295, 300, 11, 309, 637, 1208, 484, 11, 1338, 11, 257, 1379, 3840, 295, 1589, 466, 729], "temperature": 0.0, "avg_logprob": -0.1003253664289202, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.178035538236145e-05}, {"id": 276, "seek": 126540, "start": 1285.2, "end": 1286.72, "text": " pixels.", "tokens": [18668, 13], "temperature": 0.0, "avg_logprob": -0.1003253664289202, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.178035538236145e-05}, {"id": 277, "seek": 126540, "start": 1286.72, "end": 1291.1000000000001, "text": " And the head is the thing that then tries to make sense of that and make some predictions", "tokens": [400, 264, 1378, 307, 264, 551, 300, 550, 9898, 281, 652, 2020, 295, 300, 293, 652, 512, 21264], "temperature": 0.0, "avg_logprob": -0.1003253664289202, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.178035538236145e-05}, {"id": 278, "seek": 126540, "start": 1291.1000000000001, "end": 1293.0, "text": " about what we're looking at.", "tokens": [466, 437, 321, 434, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.1003253664289202, "compression_ratio": 1.7166666666666666, "no_speech_prob": 2.178035538236145e-05}, {"id": 279, "seek": 129300, "start": 1293.0, "end": 1296.24, "text": " And so this is the head.", "tokens": [400, 370, 341, 307, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.11301924387613932, "compression_ratio": 1.625, "no_speech_prob": 6.747893166902941e-06}, {"id": 280, "seek": 129300, "start": 1296.24, "end": 1301.76, "text": " And as you can see, the head is pretty simple, whereas the body, which goes from here all", "tokens": [400, 382, 291, 393, 536, 11, 264, 1378, 307, 1238, 2199, 11, 9735, 264, 1772, 11, 597, 1709, 490, 510, 439], "temperature": 0.0, "avg_logprob": -0.11301924387613932, "compression_ratio": 1.625, "no_speech_prob": 6.747893166902941e-06}, {"id": 281, "seek": 129300, "start": 1301.76, "end": 1307.68, "text": " the way to here, is not so simple.", "tokens": [264, 636, 281, 510, 11, 307, 406, 370, 2199, 13], "temperature": 0.0, "avg_logprob": -0.11301924387613932, "compression_ratio": 1.625, "no_speech_prob": 6.747893166902941e-06}, {"id": 282, "seek": 129300, "start": 1307.68, "end": 1316.24, "text": " And we want to predict two things, what kind of rice it is and what disease it has.", "tokens": [400, 321, 528, 281, 6069, 732, 721, 11, 437, 733, 295, 5090, 309, 307, 293, 437, 4752, 309, 575, 13], "temperature": 0.0, "avg_logprob": -0.11301924387613932, "compression_ratio": 1.625, "no_speech_prob": 6.747893166902941e-06}, {"id": 283, "seek": 129300, "start": 1316.24, "end": 1319.08, "text": " Now look at the very, very, very last layer.", "tokens": [823, 574, 412, 264, 588, 11, 588, 11, 588, 1036, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11301924387613932, "compression_ratio": 1.625, "no_speech_prob": 6.747893166902941e-06}, {"id": 284, "seek": 129300, "start": 1319.08, "end": 1320.08, "text": " It's a linear layer.", "tokens": [467, 311, 257, 8213, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11301924387613932, "compression_ratio": 1.625, "no_speech_prob": 6.747893166902941e-06}, {"id": 285, "seek": 132008, "start": 1320.08, "end": 1329.28, "text": " So a linear layer, if you remember, is just something that does a matrix product.", "tokens": [407, 257, 8213, 4583, 11, 498, 291, 1604, 11, 307, 445, 746, 300, 775, 257, 8141, 1674, 13], "temperature": 0.0, "avg_logprob": -0.1784797625595264, "compression_ratio": 1.618279569892473, "no_speech_prob": 1.7603240394237218e-06}, {"id": 286, "seek": 132008, "start": 1329.28, "end": 1337.8799999999999, "text": " And the matrix product is a matrix which takes as input 512 features and spits out 10 features.", "tokens": [400, 264, 8141, 1674, 307, 257, 8141, 597, 2516, 382, 4846, 1025, 4762, 4122, 293, 637, 1208, 484, 1266, 4122, 13], "temperature": 0.0, "avg_logprob": -0.1784797625595264, "compression_ratio": 1.618279569892473, "no_speech_prob": 1.7603240394237218e-06}, {"id": 287, "seek": 132008, "start": 1337.8799999999999, "end": 1342.9199999999998, "text": " So it's a 512 by 10 matrix.", "tokens": [407, 309, 311, 257, 1025, 4762, 538, 1266, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1784797625595264, "compression_ratio": 1.618279569892473, "no_speech_prob": 1.7603240394237218e-06}, {"id": 288, "seek": 132008, "start": 1342.9199999999998, "end": 1344.1599999999999, "text": " So let's do a few things.", "tokens": [407, 718, 311, 360, 257, 1326, 721, 13], "temperature": 0.0, "avg_logprob": -0.1784797625595264, "compression_ratio": 1.618279569892473, "no_speech_prob": 1.7603240394237218e-06}, {"id": 289, "seek": 132008, "start": 1344.1599999999999, "end": 1345.1599999999999, "text": " Let's grab the head.", "tokens": [961, 311, 4444, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.1784797625595264, "compression_ratio": 1.618279569892473, "no_speech_prob": 1.7603240394237218e-06}, {"id": 290, "seek": 132008, "start": 1345.1599999999999, "end": 1349.96, "text": " So the head is the index one thing in the model.", "tokens": [407, 264, 1378, 307, 264, 8186, 472, 551, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1784797625595264, "compression_ratio": 1.618279569892473, "no_speech_prob": 1.7603240394237218e-06}, {"id": 291, "seek": 134996, "start": 1349.96, "end": 1353.68, "text": " So there's our head.", "tokens": [407, 456, 311, 527, 1378, 13], "temperature": 0.0, "avg_logprob": -0.2373711856794946, "compression_ratio": 1.5511363636363635, "no_speech_prob": 7.526862646045629e-06}, {"id": 292, "seek": 134996, "start": 1353.68, "end": 1356.52, "text": " Quick question.", "tokens": [12101, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2373711856794946, "compression_ratio": 1.5511363636363635, "no_speech_prob": 7.526862646045629e-06}, {"id": 293, "seek": 134996, "start": 1356.52, "end": 1363.04, "text": " I've seen these model whatever you want to call it, x-rays a lot.", "tokens": [286, 600, 1612, 613, 2316, 2035, 291, 528, 281, 818, 309, 11, 2031, 12, 36212, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.2373711856794946, "compression_ratio": 1.5511363636363635, "no_speech_prob": 7.526862646045629e-06}, {"id": 294, "seek": 134996, "start": 1363.04, "end": 1369.1200000000001, "text": " Have you ever wanted to, is there a way that maybe I don't know about to see the shape", "tokens": [3560, 291, 1562, 1415, 281, 11, 307, 456, 257, 636, 300, 1310, 286, 500, 380, 458, 466, 281, 536, 264, 3909], "temperature": 0.0, "avg_logprob": -0.2373711856794946, "compression_ratio": 1.5511363636363635, "no_speech_prob": 7.526862646045629e-06}, {"id": 295, "seek": 134996, "start": 1369.1200000000001, "end": 1373.92, "text": " of the tensors as it flows, or the shape of the data as it flows through the model?", "tokens": [295, 264, 10688, 830, 382, 309, 12867, 11, 420, 264, 3909, 295, 264, 1412, 382, 309, 12867, 807, 264, 2316, 30], "temperature": 0.0, "avg_logprob": -0.2373711856794946, "compression_ratio": 1.5511363636363635, "no_speech_prob": 7.526862646045629e-06}, {"id": 296, "seek": 137392, "start": 1373.92, "end": 1381.0, "text": " You know, like, yeah, man, there it is.", "tokens": [509, 458, 11, 411, 11, 1338, 11, 587, 11, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 297, "seek": 137392, "start": 1381.0, "end": 1383.88, "text": " Oh, I didn't even know about this.", "tokens": [876, 11, 286, 994, 380, 754, 458, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 298, "seek": 137392, "start": 1383.88, "end": 1384.88, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 299, "seek": 137392, "start": 1384.88, "end": 1388.8400000000001, "text": " Dude, you should try watching some fast AI lectures.", "tokens": [12042, 11, 291, 820, 853, 1976, 512, 2370, 7318, 16564, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 300, "seek": 137392, "start": 1388.8400000000001, "end": 1389.8400000000001, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 301, "seek": 137392, "start": 1389.8400000000001, "end": 1390.8400000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 302, "seek": 137392, "start": 1390.8400000000001, "end": 1394.8000000000002, "text": " So this will tell you how many parameters there are.", "tokens": [407, 341, 486, 980, 291, 577, 867, 9834, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 303, "seek": 137392, "start": 1394.8000000000002, "end": 1397.2, "text": " And yeah, the shape as it goes through.", "tokens": [400, 1338, 11, 264, 3909, 382, 309, 1709, 807, 13], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 304, "seek": 137392, "start": 1397.2, "end": 1402.68, "text": " And so the key thing is, since we're predicting 10 probabilities, one probability for each", "tokens": [400, 370, 264, 2141, 551, 307, 11, 1670, 321, 434, 32884, 1266, 33783, 11, 472, 8482, 337, 1184], "temperature": 0.0, "avg_logprob": -0.2935048690950028, "compression_ratio": 1.5161290322580645, "no_speech_prob": 2.2122958398540504e-05}, {"id": 305, "seek": 140268, "start": 1402.68, "end": 1407.24, "text": " of the 10 possible diseases, we end up with a shape of 64 by 10.", "tokens": [295, 264, 1266, 1944, 11044, 11, 321, 917, 493, 365, 257, 3909, 295, 12145, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 306, "seek": 140268, "start": 1407.24, "end": 1410.28, "text": " The 64 is because we're using a batch size of 64.", "tokens": [440, 12145, 307, 570, 321, 434, 1228, 257, 15245, 2744, 295, 12145, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 307, "seek": 140268, "start": 1410.28, "end": 1413.04, "text": " And for each image, we're predicting 10 probabilities.", "tokens": [400, 337, 1184, 3256, 11, 321, 434, 32884, 1266, 33783, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 308, "seek": 140268, "start": 1413.04, "end": 1415.04, "text": " It's very thorough.", "tokens": [467, 311, 588, 12934, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 309, "seek": 140268, "start": 1415.04, "end": 1416.52, "text": " It shows the callbacks.", "tokens": [467, 3110, 264, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 310, "seek": 140268, "start": 1416.52, "end": 1417.52, "text": " Wow.", "tokens": [3153, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 311, "seek": 140268, "start": 1417.52, "end": 1418.52, "text": " I don't remember this.", "tokens": [286, 500, 380, 1604, 341, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 312, "seek": 140268, "start": 1418.52, "end": 1419.96, "text": " Yeah, we don't look around, man.", "tokens": [865, 11, 321, 500, 380, 574, 926, 11, 587, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 313, "seek": 140268, "start": 1419.96, "end": 1422.4, "text": " Here in fast AI, we're thorough.", "tokens": [1692, 294, 2370, 7318, 11, 321, 434, 12934, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 314, "seek": 140268, "start": 1422.4, "end": 1430.28, "text": " So yeah, so that's the question because that's a great thing for us to look at.", "tokens": [407, 1338, 11, 370, 300, 311, 264, 1168, 570, 300, 311, 257, 869, 551, 337, 505, 281, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.2180974503194005, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.5864068447845057e-05}, {"id": 315, "seek": 143028, "start": 1430.28, "end": 1438.3999999999999, "text": " So yeah, so in the head, let's create something called the last layer, which is going to be", "tokens": [407, 1338, 11, 370, 294, 264, 1378, 11, 718, 311, 1884, 746, 1219, 264, 1036, 4583, 11, 597, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.2212844557232327, "compression_ratio": 1.5802469135802468, "no_speech_prob": 9.368171049572993e-06}, {"id": 316, "seek": 143028, "start": 1438.3999999999999, "end": 1441.6399999999999, "text": " the end of the head.", "tokens": [264, 917, 295, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.2212844557232327, "compression_ratio": 1.5802469135802468, "no_speech_prob": 9.368171049572993e-06}, {"id": 317, "seek": 143028, "start": 1441.6399999999999, "end": 1444.92, "text": " And the very end of the head.", "tokens": [400, 264, 588, 917, 295, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.2212844557232327, "compression_ratio": 1.5802469135802468, "no_speech_prob": 9.368171049572993e-06}, {"id": 318, "seek": 143028, "start": 1444.92, "end": 1450.2, "text": " So our last layer is this linear thing, right?", "tokens": [407, 527, 1036, 4583, 307, 341, 8213, 551, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2212844557232327, "compression_ratio": 1.5802469135802468, "no_speech_prob": 9.368171049572993e-06}, {"id": 319, "seek": 143028, "start": 1450.2, "end": 1455.2, "text": " And so this is so we could actually see the parameters themselves.", "tokens": [400, 370, 341, 307, 370, 321, 727, 767, 536, 264, 9834, 2969, 13], "temperature": 0.0, "avg_logprob": -0.2212844557232327, "compression_ratio": 1.5802469135802468, "no_speech_prob": 9.368171049572993e-06}, {"id": 320, "seek": 145520, "start": 1455.2, "end": 1460.2, "text": " I hope it does that.", "tokens": [286, 1454, 309, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 321, "seek": 145520, "start": 1460.2, "end": 1462.96, "text": " A lot of these things are generated lazily, right?", "tokens": [316, 688, 295, 613, 721, 366, 10833, 19320, 953, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 322, "seek": 145520, "start": 1462.96, "end": 1468.16, "text": " So when you see this thing saying generator object, it's just it's literally the word", "tokens": [407, 562, 291, 536, 341, 551, 1566, 19265, 2657, 11, 309, 311, 445, 309, 311, 3736, 264, 1349], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 323, "seek": 145520, "start": 1468.16, "end": 1469.16, "text": " is lazy.", "tokens": [307, 14847, 13], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 324, "seek": 145520, "start": 1469.16, "end": 1471.1200000000001, "text": " It's too lazy to actually bother calculating what it is.", "tokens": [467, 311, 886, 14847, 281, 767, 8677, 28258, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 325, "seek": 145520, "start": 1471.1200000000001, "end": 1474.3, "text": " So it doesn't bother until you force it to.", "tokens": [407, 309, 1177, 380, 8677, 1826, 291, 3464, 309, 281, 13], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 326, "seek": 145520, "start": 1474.3, "end": 1477.6000000000001, "text": " So if you turn it into a list, it actually forces to generate it.", "tokens": [407, 498, 291, 1261, 309, 666, 257, 1329, 11, 309, 767, 5874, 281, 8460, 309, 13], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 327, "seek": 145520, "start": 1477.6000000000001, "end": 1478.6000000000001, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 328, "seek": 145520, "start": 1478.6000000000001, "end": 1483.88, "text": " So it's a list of one thing, which is not surprising, right?", "tokens": [407, 309, 311, 257, 1329, 295, 472, 551, 11, 597, 307, 406, 8830, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 329, "seek": 145520, "start": 1483.88, "end": 1484.88, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17880299052254098, "compression_ratio": 1.75, "no_speech_prob": 1.54458757606335e-05}, {"id": 330, "seek": 148488, "start": 1484.88, "end": 1492.16, "text": " And so the last layer parameters.", "tokens": [400, 370, 264, 1036, 4583, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1943515650431315, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.0782071512949187e-05}, {"id": 331, "seek": 148488, "start": 1492.16, "end": 1494.44, "text": " Is a matrix.", "tokens": [1119, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1943515650431315, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.0782071512949187e-05}, {"id": 332, "seek": 148488, "start": 1494.44, "end": 1496.6000000000001, "text": " Which is there we go, 10 by 512.", "tokens": [3013, 307, 456, 321, 352, 11, 1266, 538, 1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.1943515650431315, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.0782071512949187e-05}, {"id": 333, "seek": 148488, "start": 1496.6000000000001, "end": 1502.0800000000002, "text": " So it's transposed to what I said, but that's okay.", "tokens": [407, 309, 311, 7132, 1744, 281, 437, 286, 848, 11, 457, 300, 311, 1392, 13], "temperature": 0.0, "avg_logprob": -0.1943515650431315, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.0782071512949187e-05}, {"id": 334, "seek": 148488, "start": 1502.0800000000002, "end": 1504.2800000000002, "text": " So we're getting 512 inputs.", "tokens": [407, 321, 434, 1242, 1025, 4762, 15743, 13], "temperature": 0.0, "avg_logprob": -0.1943515650431315, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.0782071512949187e-05}, {"id": 335, "seek": 148488, "start": 1504.2800000000002, "end": 1511.24, "text": " And when we multiply this by this matrix, we end up with 10 outputs.", "tokens": [400, 562, 321, 12972, 341, 538, 341, 8141, 11, 321, 917, 493, 365, 1266, 23930, 13], "temperature": 0.0, "avg_logprob": -0.1943515650431315, "compression_ratio": 1.3795180722891567, "no_speech_prob": 1.0782071512949187e-05}, {"id": 336, "seek": 151124, "start": 1511.24, "end": 1537.96, "text": " So my daughter's watching me.", "tokens": [407, 452, 4653, 311, 1976, 385, 13], "temperature": 0.0, "avg_logprob": -0.6293526129289106, "compression_ratio": 0.7837837837837838, "no_speech_prob": 9.015005343826488e-05}, {"id": 337, "seek": 153796, "start": 1537.96, "end": 1541.76, "text": " Sorry about that.", "tokens": [4919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.4011336697472466, "compression_ratio": 1.145631067961165, "no_speech_prob": 6.853924332972383e-06}, {"id": 338, "seek": 153796, "start": 1541.76, "end": 1548.04, "text": " Homeschooling transitions always require some input.", "tokens": [389, 18168, 21856, 278, 23767, 1009, 3651, 512, 4846, 13], "temperature": 0.0, "avg_logprob": -0.4011336697472466, "compression_ratio": 1.145631067961165, "no_speech_prob": 6.853924332972383e-06}, {"id": 339, "seek": 153796, "start": 1548.04, "end": 1551.88, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.4011336697472466, "compression_ratio": 1.145631067961165, "no_speech_prob": 6.853924332972383e-06}, {"id": 340, "seek": 153796, "start": 1551.88, "end": 1563.64, "text": " So we're going to basically have to.", "tokens": [407, 321, 434, 516, 281, 1936, 362, 281, 13], "temperature": 0.0, "avg_logprob": -0.4011336697472466, "compression_ratio": 1.145631067961165, "no_speech_prob": 6.853924332972383e-06}, {"id": 341, "seek": 156364, "start": 1563.64, "end": 1577.16, "text": " If we got rid of this, right, then then our last linear layer here would be taking in", "tokens": [759, 321, 658, 3973, 295, 341, 11, 558, 11, 550, 550, 527, 1036, 8213, 4583, 510, 576, 312, 1940, 294], "temperature": 0.0, "avg_logprob": -0.2217381967080606, "compression_ratio": 1.2380952380952381, "no_speech_prob": 4.2889168980764225e-06}, {"id": 342, "seek": 156364, "start": 1577.16, "end": 1584.2800000000002, "text": " 1536 features and spitting out 512 features.", "tokens": [2119, 11309, 4122, 293, 637, 2414, 484, 1025, 4762, 4122, 13], "temperature": 0.0, "avg_logprob": -0.2217381967080606, "compression_ratio": 1.2380952380952381, "no_speech_prob": 4.2889168980764225e-06}, {"id": 343, "seek": 158428, "start": 1584.28, "end": 1599.56, "text": " So what we could do would be to delete this layer and instead take those 1536, sorry,", "tokens": [407, 437, 321, 727, 360, 576, 312, 281, 12097, 341, 4583, 293, 2602, 747, 729, 2119, 11309, 11, 2597, 11], "temperature": 0.0, "avg_logprob": -0.27873450385199655, "compression_ratio": 1.312, "no_speech_prob": 1.482322431911598e-06}, {"id": 344, "seek": 158428, "start": 1599.56, "end": 1603.96, "text": " this 512 features and create two linear layers.", "tokens": [341, 1025, 4762, 4122, 293, 1884, 732, 8213, 7914, 13], "temperature": 0.0, "avg_logprob": -0.27873450385199655, "compression_ratio": 1.312, "no_speech_prob": 1.482322431911598e-06}, {"id": 345, "seek": 158428, "start": 1603.96, "end": 1607.36, "text": " One with 10 outputs as before.", "tokens": [1485, 365, 1266, 23930, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.27873450385199655, "compression_ratio": 1.312, "no_speech_prob": 1.482322431911598e-06}, {"id": 346, "seek": 160736, "start": 1607.36, "end": 1614.7199999999998, "text": " And one with however many varieties there are.", "tokens": [400, 472, 365, 4461, 867, 22092, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 347, "seek": 160736, "start": 1614.7199999999998, "end": 1621.4399999999998, "text": " Which have a look.", "tokens": [3013, 362, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 348, "seek": 160736, "start": 1621.4399999999998, "end": 1624.3999999999999, "text": " Hi Jeremy.", "tokens": [2421, 17809, 13], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 349, "seek": 160736, "start": 1624.3999999999999, "end": 1625.56, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 350, "seek": 160736, "start": 1625.56, "end": 1633.56, "text": " So I was just contemplating whether back in that linear layer where it was the output", "tokens": [407, 286, 390, 445, 19935, 990, 1968, 646, 294, 300, 8213, 4583, 689, 309, 390, 264, 5598], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 351, "seek": 160736, "start": 1633.56, "end": 1635.28, "text": " was 10 by 512.", "tokens": [390, 1266, 538, 1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 352, "seek": 160736, "start": 1635.28, "end": 1636.28, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 353, "seek": 160736, "start": 1636.28, "end": 1637.28, "text": " Not the output.", "tokens": [1726, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.3768696062492602, "compression_ratio": 1.3443708609271523, "no_speech_prob": 1.4504594219033606e-05}, {"id": 354, "seek": 163728, "start": 1637.28, "end": 1638.28, "text": " The output was 10.", "tokens": [440, 5598, 390, 1266, 13], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 355, "seek": 163728, "start": 1638.28, "end": 1641.28, "text": " The output is 64 by 10.", "tokens": [440, 5598, 307, 12145, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 356, "seek": 163728, "start": 1641.28, "end": 1647.48, "text": " So when you want to mix diseases with like rice in the output, I was wondering whether", "tokens": [407, 562, 291, 528, 281, 2890, 11044, 365, 411, 5090, 294, 264, 5598, 11, 286, 390, 6359, 1968], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 357, "seek": 163728, "start": 1647.48, "end": 1651.56, "text": " that might be a, like, I don't know how many rice types there are.", "tokens": [300, 1062, 312, 257, 11, 411, 11, 286, 500, 380, 458, 577, 867, 5090, 3467, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 358, "seek": 163728, "start": 1651.56, "end": 1653.04, "text": " So there's five rice types.", "tokens": [407, 456, 311, 1732, 5090, 3467, 13], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 359, "seek": 163728, "start": 1653.04, "end": 1654.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 360, "seek": 163728, "start": 1654.04, "end": 1658.24, "text": " So that 10 might be a 10 by 10 matrix output.", "tokens": [407, 300, 1266, 1062, 312, 257, 1266, 538, 1266, 8141, 5598, 13], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 361, "seek": 163728, "start": 1658.24, "end": 1660.16, "text": " No, two by 10.", "tokens": [883, 11, 732, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 362, "seek": 163728, "start": 1660.16, "end": 1665.3999999999999, "text": " So you want one probability of what type of race is it and one probability of what disease", "tokens": [407, 291, 528, 472, 8482, 295, 437, 2010, 295, 4569, 307, 309, 293, 472, 8482, 295, 437, 4752], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 363, "seek": 163728, "start": 1665.3999999999999, "end": 1666.3999999999999, "text": " does it have?", "tokens": [775, 309, 362, 30], "temperature": 0.0, "avg_logprob": -0.2679807839273405, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.00011771679419325665}, {"id": 364, "seek": 166640, "start": 1666.4, "end": 1667.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 365, "seek": 166640, "start": 1667.4, "end": 1668.4, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 366, "seek": 166640, "start": 1668.4, "end": 1670.96, "text": " So just two by 10.", "tokens": [407, 445, 732, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 367, "seek": 166640, "start": 1670.96, "end": 1677.0800000000002, "text": " So let's go ahead and do the easy thing first, which is to delete the layer we don't want.", "tokens": [407, 718, 311, 352, 2286, 293, 360, 264, 1858, 551, 700, 11, 597, 307, 281, 12097, 264, 4583, 321, 500, 380, 528, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 368, "seek": 166640, "start": 1677.0800000000002, "end": 1678.6000000000001, "text": " So this says sequential.", "tokens": [407, 341, 1619, 42881, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 369, "seek": 166640, "start": 1678.6000000000001, "end": 1683.5600000000002, "text": " So sequential means like literally PyTorch is going to go through and calculate this", "tokens": [407, 42881, 1355, 411, 3736, 9953, 51, 284, 339, 307, 516, 281, 352, 807, 293, 8873, 341], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 370, "seek": 166640, "start": 1683.5600000000002, "end": 1686.88, "text": " and take the output of that and pass it to this and take the output of that and pass", "tokens": [293, 747, 264, 5598, 295, 300, 293, 1320, 309, 281, 341, 293, 747, 264, 5598, 295, 300, 293, 1320], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 371, "seek": 166640, "start": 1686.88, "end": 1689.0, "text": " it to this and so forth.", "tokens": [309, 281, 341, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 372, "seek": 166640, "start": 1689.0, "end": 1692.48, "text": " So if we delete the last layer, that's no problem.", "tokens": [407, 498, 321, 12097, 264, 1036, 4583, 11, 300, 311, 572, 1154, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 373, "seek": 166640, "start": 1692.48, "end": 1695.68, "text": " It's just won't ever call it.", "tokens": [467, 311, 445, 1582, 380, 1562, 818, 309, 13], "temperature": 0.0, "avg_logprob": -0.13354884338378906, "compression_ratio": 1.8146551724137931, "no_speech_prob": 2.144152131222654e-05}, {"id": 374, "seek": 169568, "start": 1695.68, "end": 1702.92, "text": " So I can't quite remember if we can do this in sequential, but let's assume it works like", "tokens": [407, 286, 393, 380, 1596, 1604, 498, 321, 393, 360, 341, 294, 42881, 11, 457, 718, 311, 6552, 309, 1985, 411], "temperature": 0.0, "avg_logprob": -0.2174588441848755, "compression_ratio": 1.3855421686746987, "no_speech_prob": 7.296266630874015e-06}, {"id": 375, "seek": 169568, "start": 1702.92, "end": 1703.92, "text": " normal Python.", "tokens": [2710, 15329, 13], "temperature": 0.0, "avg_logprob": -0.2174588441848755, "compression_ratio": 1.3855421686746987, "no_speech_prob": 7.296266630874015e-06}, {"id": 376, "seek": 169568, "start": 1703.92, "end": 1709.24, "text": " We should be able to go delete H minus one.", "tokens": [492, 820, 312, 1075, 281, 352, 12097, 389, 3175, 472, 13], "temperature": 0.0, "avg_logprob": -0.2174588441848755, "compression_ratio": 1.3855421686746987, "no_speech_prob": 7.296266630874015e-06}, {"id": 377, "seek": 169568, "start": 1709.24, "end": 1712.28, "text": " That looked hopeful.", "tokens": [663, 2956, 20531, 13], "temperature": 0.0, "avg_logprob": -0.2174588441848755, "compression_ratio": 1.3855421686746987, "no_speech_prob": 7.296266630874015e-06}, {"id": 378, "seek": 169568, "start": 1712.28, "end": 1714.16, "text": " Yep, we can.", "tokens": [7010, 11, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.2174588441848755, "compression_ratio": 1.3855421686746987, "no_speech_prob": 7.296266630874015e-06}, {"id": 379, "seek": 169568, "start": 1714.16, "end": 1715.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2174588441848755, "compression_ratio": 1.3855421686746987, "no_speech_prob": 7.296266630874015e-06}, {"id": 380, "seek": 169568, "start": 1715.16, "end": 1720.92, "text": " So it's got normal Python list semantics.", "tokens": [407, 309, 311, 658, 2710, 15329, 1329, 4361, 45298, 13], "temperature": 0.0, "avg_logprob": -0.2174588441848755, "compression_ratio": 1.3855421686746987, "no_speech_prob": 7.296266630874015e-06}, {"id": 381, "seek": 172092, "start": 1720.92, "end": 1733.4, "text": " So this model will now be returning 512 output.", "tokens": [407, 341, 2316, 486, 586, 312, 12678, 1025, 4762, 5598, 13], "temperature": 0.0, "avg_logprob": -0.07258722361396341, "compression_ratio": 1.2211538461538463, "no_speech_prob": 1.4367344647325808e-06}, {"id": 382, "seek": 172092, "start": 1733.4, "end": 1746.1000000000001, "text": " So we want to basically wrap it in a model which instead has two linear layers.", "tokens": [407, 321, 528, 281, 1936, 7019, 309, 294, 257, 2316, 597, 2602, 575, 732, 8213, 7914, 13], "temperature": 0.0, "avg_logprob": -0.07258722361396341, "compression_ratio": 1.2211538461538463, "no_speech_prob": 1.4367344647325808e-06}, {"id": 383, "seek": 174610, "start": 1746.1, "end": 1752.24, "text": " So there's a couple of ways we can do this, but let's do it like the most step by step", "tokens": [407, 456, 311, 257, 1916, 295, 2098, 321, 393, 360, 341, 11, 457, 718, 311, 360, 309, 411, 264, 881, 1823, 538, 1823], "temperature": 0.0, "avg_logprob": -0.20563595111553484, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.816197482185089e-06}, {"id": 384, "seek": 174610, "start": 1752.24, "end": 1753.24, "text": " way.", "tokens": [636, 13], "temperature": 0.0, "avg_logprob": -0.20563595111553484, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.816197482185089e-06}, {"id": 385, "seek": 174610, "start": 1753.24, "end": 1756.78, "text": " Let's take a look.", "tokens": [961, 311, 747, 257, 574, 13], "temperature": 0.0, "avg_logprob": -0.20563595111553484, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.816197482185089e-06}, {"id": 386, "seek": 174610, "start": 1756.78, "end": 1758.76, "text": " So we're going to create a class.", "tokens": [407, 321, 434, 516, 281, 1884, 257, 1508, 13], "temperature": 0.0, "avg_logprob": -0.20563595111553484, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.816197482185089e-06}, {"id": 387, "seek": 174610, "start": 1758.76, "end": 1761.6799999999998, "text": " So in PyTorch, modules are classes.", "tokens": [407, 294, 9953, 51, 284, 339, 11, 16679, 366, 5359, 13], "temperature": 0.0, "avg_logprob": -0.20563595111553484, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.816197482185089e-06}, {"id": 388, "seek": 174610, "start": 1761.6799999999998, "end": 1765.4599999999998, "text": " So we're going to take a class which includes this model.", "tokens": [407, 321, 434, 516, 281, 747, 257, 1508, 597, 5974, 341, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20563595111553484, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.816197482185089e-06}, {"id": 389, "seek": 176546, "start": 1765.46, "end": 1779.28, "text": " So let's call this class disease and type classifier.", "tokens": [407, 718, 311, 818, 341, 1508, 4752, 293, 2010, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.21885024376635281, "compression_ratio": 1.4836065573770492, "no_speech_prob": 1.1365505088178907e-06}, {"id": 390, "seek": 176546, "start": 1779.28, "end": 1786.4, "text": " Now that is a PyTorch calls all things that it basically uses as layers in a neural net", "tokens": [823, 300, 307, 257, 9953, 51, 284, 339, 5498, 439, 721, 300, 309, 1936, 4960, 382, 7914, 294, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.21885024376635281, "compression_ratio": 1.4836065573770492, "no_speech_prob": 1.1365505088178907e-06}, {"id": 391, "seek": 176546, "start": 1786.4, "end": 1787.4, "text": " module.", "tokens": [10088, 13], "temperature": 0.0, "avg_logprob": -0.21885024376635281, "compression_ratio": 1.4836065573770492, "no_speech_prob": 1.1365505088178907e-06}, {"id": 392, "seek": 176546, "start": 1787.4, "end": 1791.16, "text": " So this is a neural net module.", "tokens": [407, 341, 307, 257, 18161, 2533, 10088, 13], "temperature": 0.0, "avg_logprob": -0.21885024376635281, "compression_ratio": 1.4836065573770492, "no_speech_prob": 1.1365505088178907e-06}, {"id": 393, "seek": 179116, "start": 1791.16, "end": 1796.0, "text": " Now if you haven't done any OO programming in Python before, it would be very helpful", "tokens": [823, 498, 291, 2378, 380, 1096, 604, 422, 46, 9410, 294, 15329, 949, 11, 309, 576, 312, 588, 4961], "temperature": 0.0, "avg_logprob": -0.20709135872977122, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.853988452348858e-06}, {"id": 394, "seek": 179116, "start": 1796.0, "end": 1803.4, "text": " to read a tutorial about basic Python OO programming because PyTorch assumes that you are pretty", "tokens": [281, 1401, 257, 7073, 466, 3875, 15329, 422, 46, 9410, 570, 9953, 51, 284, 339, 37808, 300, 291, 366, 1238], "temperature": 0.0, "avg_logprob": -0.20709135872977122, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.853988452348858e-06}, {"id": 395, "seek": 179116, "start": 1803.4, "end": 1805.48, "text": " familiar with it.", "tokens": [4963, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.20709135872977122, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.853988452348858e-06}, {"id": 396, "seek": 179116, "start": 1805.48, "end": 1809.48, "text": " If you've done any kind of OO programming before, I'm going to work on the assumption", "tokens": [759, 291, 600, 1096, 604, 733, 295, 422, 46, 9410, 949, 11, 286, 478, 516, 281, 589, 322, 264, 15302], "temperature": 0.0, "avg_logprob": -0.20709135872977122, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.853988452348858e-06}, {"id": 397, "seek": 179116, "start": 1809.48, "end": 1811.3600000000001, "text": " you have.", "tokens": [291, 362, 13], "temperature": 0.0, "avg_logprob": -0.20709135872977122, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.853988452348858e-06}, {"id": 398, "seek": 179116, "start": 1811.3600000000001, "end": 1814.8400000000001, "text": " Then the constructor, there's a lot of weird things in Python.", "tokens": [1396, 264, 47479, 11, 456, 311, 257, 688, 295, 3657, 721, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.20709135872977122, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.853988452348858e-06}, {"id": 399, "seek": 179116, "start": 1814.8400000000001, "end": 1817.0400000000002, "text": " The constructor is called Dundee init.", "tokens": [440, 47479, 307, 1219, 413, 997, 1653, 3157, 13], "temperature": 0.0, "avg_logprob": -0.20709135872977122, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.853988452348858e-06}, {"id": 400, "seek": 181704, "start": 1817.04, "end": 1822.72, "text": " So this is, so Dundee means underscores on each side.", "tokens": [407, 341, 307, 11, 370, 413, 997, 1653, 1355, 16692, 66, 2706, 322, 1184, 1252, 13], "temperature": 0.0, "avg_logprob": -0.19931565631519665, "compression_ratio": 1.5792079207920793, "no_speech_prob": 5.014535872760462e-06}, {"id": 401, "seek": 181704, "start": 1822.72, "end": 1827.3999999999999, "text": " And it always passes in the object being constructed or the object we're calling it on first.", "tokens": [400, 309, 1009, 11335, 294, 264, 2657, 885, 17083, 420, 264, 2657, 321, 434, 5141, 309, 322, 700, 13], "temperature": 0.0, "avg_logprob": -0.19931565631519665, "compression_ratio": 1.5792079207920793, "no_speech_prob": 5.014535872760462e-06}, {"id": 402, "seek": 181704, "start": 1827.3999999999999, "end": 1830.94, "text": " So we'll give that a name.", "tokens": [407, 321, 603, 976, 300, 257, 1315, 13], "temperature": 0.0, "avg_logprob": -0.19931565631519665, "compression_ratio": 1.5792079207920793, "no_speech_prob": 5.014535872760462e-06}, {"id": 403, "seek": 181704, "start": 1830.94, "end": 1836.6, "text": " And so we're basically going to create two linear layers.", "tokens": [400, 370, 321, 434, 1936, 516, 281, 1884, 732, 8213, 7914, 13], "temperature": 0.0, "avg_logprob": -0.19931565631519665, "compression_ratio": 1.5792079207920793, "no_speech_prob": 5.014535872760462e-06}, {"id": 404, "seek": 181704, "start": 1836.6, "end": 1845.36, "text": " And one easy way to create the correct kind of layer would be self.l1 equals, we could", "tokens": [400, 472, 1858, 636, 281, 1884, 264, 3006, 733, 295, 4583, 576, 312, 2698, 13, 75, 16, 6915, 11, 321, 727], "temperature": 0.0, "avg_logprob": -0.19931565631519665, "compression_ratio": 1.5792079207920793, "no_speech_prob": 5.014535872760462e-06}, {"id": 405, "seek": 184536, "start": 1845.36, "end": 1847.9599999999998, "text": " do that.", "tokens": [360, 300, 13], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 406, "seek": 184536, "start": 1847.9599999999998, "end": 1854.1599999999999, "text": " So one question is like, I understand this subclassing thing.", "tokens": [407, 472, 1168, 307, 411, 11, 286, 1223, 341, 1422, 11665, 278, 551, 13], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 407, "seek": 184536, "start": 1854.1599999999999, "end": 1861.1999999999998, "text": " Is there some other way that you could push two additional layers onto the existing thing?", "tokens": [1119, 456, 512, 661, 636, 300, 291, 727, 2944, 732, 4497, 7914, 3911, 264, 6741, 551, 30], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 408, "seek": 184536, "start": 1861.1999999999998, "end": 1862.56, "text": " Or does that not make any sense?", "tokens": [1610, 775, 300, 406, 652, 604, 2020, 30], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 409, "seek": 184536, "start": 1862.56, "end": 1863.56, "text": " Yeah, we could try that.", "tokens": [865, 11, 321, 727, 853, 300, 13], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 410, "seek": 184536, "start": 1863.56, "end": 1866.3999999999999, "text": " Let's see if we get this one working and then we'll try the other way.", "tokens": [961, 311, 536, 498, 321, 483, 341, 472, 1364, 293, 550, 321, 603, 853, 264, 661, 636, 13], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 411, "seek": 184536, "start": 1866.3999999999999, "end": 1867.3999999999999, "text": " How about that?", "tokens": [1012, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 412, "seek": 184536, "start": 1867.3999999999999, "end": 1868.3999999999999, "text": " That'd be fun.", "tokens": [663, 1116, 312, 1019, 13], "temperature": 0.0, "avg_logprob": -0.17545616880376289, "compression_ratio": 1.5358851674641147, "no_speech_prob": 6.747161478415364e-06}, {"id": 413, "seek": 186840, "start": 1868.4, "end": 1875.6000000000001, "text": " We could also try using fast AI has a create head function as well.", "tokens": [492, 727, 611, 853, 1228, 2370, 7318, 575, 257, 1884, 1378, 2445, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 414, "seek": 186840, "start": 1875.6000000000001, "end": 1877.1200000000001, "text": " So we'll see how we go.", "tokens": [407, 321, 603, 536, 577, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 415, "seek": 186840, "start": 1877.1200000000001, "end": 1881.0800000000002, "text": " So here's linear layer number one.", "tokens": [407, 510, 311, 8213, 4583, 1230, 472, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 416, "seek": 186840, "start": 1881.0800000000002, "end": 1883.6000000000001, "text": " And as you can see, I literally just copied and pasted.", "tokens": [400, 382, 291, 393, 536, 11, 286, 3736, 445, 25365, 293, 1791, 292, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 417, "seek": 186840, "start": 1883.6000000000001, "end": 1884.6000000000001, "text": " It's inside the nn submodule.", "tokens": [467, 311, 1854, 264, 297, 77, 1422, 8014, 2271, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 418, "seek": 186840, "start": 1884.6000000000001, "end": 1886.88, "text": " So I just had to add that.", "tokens": [407, 286, 445, 632, 281, 909, 300, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 419, "seek": 186840, "start": 1886.88, "end": 1893.8000000000002, "text": " But the representation of it is nice and convenient in that I can just copy and paste it.", "tokens": [583, 264, 10290, 295, 309, 307, 1481, 293, 10851, 294, 300, 286, 393, 445, 5055, 293, 9163, 309, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 420, "seek": 186840, "start": 1893.8000000000002, "end": 1897.14, "text": " In real life, we'd have it normally write the in features and out features.", "tokens": [682, 957, 993, 11, 321, 1116, 362, 309, 5646, 2464, 264, 294, 4122, 293, 484, 4122, 13], "temperature": 0.0, "avg_logprob": -0.2545801309438852, "compression_ratio": 1.6330645161290323, "no_speech_prob": 9.817584214033559e-06}, {"id": 421, "seek": 189714, "start": 1897.14, "end": 1901.3200000000002, "text": " Everybody kind of knows that the first two things are in and out features.", "tokens": [7646, 733, 295, 3255, 300, 264, 700, 732, 721, 366, 294, 293, 484, 4122, 13], "temperature": 0.0, "avg_logprob": -0.20866033554077149, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3006743756704964e-05}, {"id": 422, "seek": 189714, "start": 1901.3200000000002, "end": 1905.1200000000001, "text": " So I might just make it look more normal.", "tokens": [407, 286, 1062, 445, 652, 309, 574, 544, 2710, 13], "temperature": 0.0, "avg_logprob": -0.20866033554077149, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3006743756704964e-05}, {"id": 423, "seek": 189714, "start": 1905.1200000000001, "end": 1910.0400000000002, "text": " So then the second layer, and then maybe we'll just give ourselves a note here.", "tokens": [407, 550, 264, 1150, 4583, 11, 293, 550, 1310, 321, 603, 445, 976, 4175, 257, 3637, 510, 13], "temperature": 0.0, "avg_logprob": -0.20866033554077149, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3006743756704964e-05}, {"id": 424, "seek": 189714, "start": 1910.0400000000002, "end": 1919.92, "text": " So we'll use this one for rice type and we'll use this one for disease.", "tokens": [407, 321, 603, 764, 341, 472, 337, 5090, 2010, 293, 321, 603, 764, 341, 472, 337, 4752, 13], "temperature": 0.0, "avg_logprob": -0.20866033554077149, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3006743756704964e-05}, {"id": 425, "seek": 189714, "start": 1919.92, "end": 1924.24, "text": " So at this point, once we create this, there's going to be these things that are going to", "tokens": [407, 412, 341, 935, 11, 1564, 321, 1884, 341, 11, 456, 311, 516, 281, 312, 613, 721, 300, 366, 516, 281], "temperature": 0.0, "avg_logprob": -0.20866033554077149, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3006743756704964e-05}, {"id": 426, "seek": 189714, "start": 1924.24, "end": 1925.3200000000002, "text": " be in it.", "tokens": [312, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.20866033554077149, "compression_ratio": 1.7037037037037037, "no_speech_prob": 1.3006743756704964e-05}, {"id": 427, "seek": 192532, "start": 1925.32, "end": 1929.48, "text": " And then we also need to wrap the actual model.", "tokens": [400, 550, 321, 611, 643, 281, 7019, 264, 3539, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24029526529432851, "compression_ratio": 1.6079545454545454, "no_speech_prob": 4.222753887006547e-06}, {"id": 428, "seek": 192532, "start": 1929.48, "end": 1935.6799999999998, "text": " So we'll just call that m and we'll just store that away.", "tokens": [407, 321, 603, 445, 818, 300, 275, 293, 321, 603, 445, 3531, 300, 1314, 13], "temperature": 0.0, "avg_logprob": -0.24029526529432851, "compression_ratio": 1.6079545454545454, "no_speech_prob": 4.222753887006547e-06}, {"id": 429, "seek": 192532, "start": 1935.6799999999998, "end": 1937.48, "text": " M equals m.", "tokens": [376, 6915, 275, 13], "temperature": 0.0, "avg_logprob": -0.24029526529432851, "compression_ratio": 1.6079545454545454, "no_speech_prob": 4.222753887006547e-06}, {"id": 430, "seek": 192532, "start": 1937.48, "end": 1947.2, "text": " So what happens is when PyTorch calls, like basically modules act exactly like functions.", "tokens": [407, 437, 2314, 307, 562, 9953, 51, 284, 339, 5498, 11, 411, 1936, 16679, 605, 2293, 411, 6828, 13], "temperature": 0.0, "avg_logprob": -0.24029526529432851, "compression_ratio": 1.6079545454545454, "no_speech_prob": 4.222753887006547e-06}, {"id": 431, "seek": 192532, "start": 1947.2, "end": 1948.84, "text": " In Python terms, they're called callables.", "tokens": [682, 15329, 2115, 11, 436, 434, 1219, 818, 2965, 13], "temperature": 0.0, "avg_logprob": -0.24029526529432851, "compression_ratio": 1.6079545454545454, "no_speech_prob": 4.222753887006547e-06}, {"id": 432, "seek": 192532, "start": 1948.84, "end": 1950.84, "text": " They act exactly like functions.", "tokens": [814, 605, 2293, 411, 6828, 13], "temperature": 0.0, "avg_logprob": -0.24029526529432851, "compression_ratio": 1.6079545454545454, "no_speech_prob": 4.222753887006547e-06}, {"id": 433, "seek": 195084, "start": 1950.84, "end": 1956.6399999999999, "text": " But the way PyTorch sets it up is when you call this function, which is actually a module,", "tokens": [583, 264, 636, 9953, 51, 284, 339, 6352, 309, 493, 307, 562, 291, 818, 341, 2445, 11, 597, 307, 767, 257, 10088, 11], "temperature": 0.0, "avg_logprob": -0.10968680333609533, "compression_ratio": 1.6359649122807018, "no_speech_prob": 3.555927605702891e-06}, {"id": 434, "seek": 195084, "start": 1956.6399999999999, "end": 1961.08, "text": " it will always call a specially named method in your class.", "tokens": [309, 486, 1009, 818, 257, 22549, 4926, 3170, 294, 428, 1508, 13], "temperature": 0.0, "avg_logprob": -0.10968680333609533, "compression_ratio": 1.6359649122807018, "no_speech_prob": 3.555927605702891e-06}, {"id": 435, "seek": 195084, "start": 1961.08, "end": 1963.36, "text": " And the name of that is forward.", "tokens": [400, 264, 1315, 295, 300, 307, 2128, 13], "temperature": 0.0, "avg_logprob": -0.10968680333609533, "compression_ratio": 1.6359649122807018, "no_speech_prob": 3.555927605702891e-06}, {"id": 436, "seek": 195084, "start": 1963.36, "end": 1965.9599999999998, "text": " So you have to create something called forward.", "tokens": [407, 291, 362, 281, 1884, 746, 1219, 2128, 13], "temperature": 0.0, "avg_logprob": -0.10968680333609533, "compression_ratio": 1.6359649122807018, "no_speech_prob": 3.555927605702891e-06}, {"id": 437, "seek": 195084, "start": 1965.9599999999998, "end": 1976.12, "text": " And it will pass the current set of features to it, which I normally I always call x.", "tokens": [400, 309, 486, 1320, 264, 2190, 992, 295, 4122, 281, 309, 11, 597, 286, 5646, 286, 1009, 818, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10968680333609533, "compression_ratio": 1.6359649122807018, "no_speech_prob": 3.555927605702891e-06}, {"id": 438, "seek": 195084, "start": 1976.12, "end": 1980.28, "text": " I think most people call it x, if I remember correctly.", "tokens": [286, 519, 881, 561, 818, 309, 2031, 11, 498, 286, 1604, 8944, 13], "temperature": 0.0, "avg_logprob": -0.10968680333609533, "compression_ratio": 1.6359649122807018, "no_speech_prob": 3.555927605702891e-06}, {"id": 439, "seek": 198028, "start": 1980.28, "end": 1990.3999999999999, "text": " So this is going to contain a 64 by 512 tensor.", "tokens": [407, 341, 307, 516, 281, 5304, 257, 12145, 538, 1025, 4762, 40863, 13], "temperature": 0.0, "avg_logprob": -0.21232519478633485, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.014523139834637e-06}, {"id": 440, "seek": 198028, "start": 1990.3999999999999, "end": 1992.62, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21232519478633485, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.014523139834637e-06}, {"id": 441, "seek": 198028, "start": 1992.62, "end": 2001.36, "text": " So no, it's not going to contain a 64 by 512 tensor.", "tokens": [407, 572, 11, 309, 311, 406, 516, 281, 5304, 257, 12145, 538, 1025, 4762, 40863, 13], "temperature": 0.0, "avg_logprob": -0.21232519478633485, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.014523139834637e-06}, {"id": 442, "seek": 198028, "start": 2001.36, "end": 2004.54, "text": " It's going to contain an input tensor because it's going to be a model.", "tokens": [467, 311, 516, 281, 5304, 364, 4846, 40863, 570, 309, 311, 516, 281, 312, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21232519478633485, "compression_ratio": 1.5892857142857142, "no_speech_prob": 5.014523139834637e-06}, {"id": 443, "seek": 200454, "start": 2004.54, "end": 2015.28, "text": " So we need to create the 64 by 512 tensor from it by calling the model like so.", "tokens": [407, 321, 643, 281, 1884, 264, 12145, 538, 1025, 4762, 40863, 490, 309, 538, 5141, 264, 2316, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 444, "seek": 200454, "start": 2015.28, "end": 2016.28, "text": " So results.", "tokens": [407, 3542, 13], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 445, "seek": 200454, "start": 2016.28, "end": 2020.12, "text": " In fact, what we often do is we'll go x equals because we're kind of making it like a sequential", "tokens": [682, 1186, 11, 437, 321, 2049, 360, 307, 321, 603, 352, 2031, 6915, 570, 321, 434, 733, 295, 1455, 309, 411, 257, 42881], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 446, "seek": 200454, "start": 2020.12, "end": 2021.12, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 447, "seek": 200454, "start": 2021.12, "end": 2022.12, "text": " We're going x equals.", "tokens": [492, 434, 516, 2031, 6915, 13], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 448, "seek": 200454, "start": 2022.12, "end": 2029.24, "text": " Oh, you know, another idea is we something else we can try is we can make this whole", "tokens": [876, 11, 291, 458, 11, 1071, 1558, 307, 321, 746, 1646, 321, 393, 853, 307, 321, 393, 652, 341, 1379], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 449, "seek": 200454, "start": 2029.24, "end": 2030.76, "text": " thing as a sequential model.", "tokens": [551, 382, 257, 42881, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 450, "seek": 200454, "start": 2030.76, "end": 2032.68, "text": " Let's do that next.", "tokens": [961, 311, 360, 300, 958, 13], "temperature": 0.0, "avg_logprob": -0.16718855270972618, "compression_ratio": 1.588235294117647, "no_speech_prob": 2.260269866383169e-06}, {"id": 451, "seek": 203268, "start": 2032.68, "end": 2036.96, "text": " So this is probably going to be the least easy way is what I'm doing it here, the most", "tokens": [407, 341, 307, 1391, 516, 281, 312, 264, 1935, 1858, 636, 307, 437, 286, 478, 884, 309, 510, 11, 264, 881], "temperature": 0.0, "avg_logprob": -0.18069303539437306, "compression_ratio": 1.5515151515151515, "no_speech_prob": 3.905392077285796e-06}, {"id": 452, "seek": 203268, "start": 2036.96, "end": 2038.1200000000001, "text": " manual way.", "tokens": [9688, 636, 13], "temperature": 0.0, "avg_logprob": -0.18069303539437306, "compression_ratio": 1.5515151515151515, "no_speech_prob": 3.905392077285796e-06}, {"id": 453, "seek": 203268, "start": 2038.1200000000001, "end": 2041.0, "text": " So we first of all call the original model.", "tokens": [407, 321, 700, 295, 439, 818, 264, 3380, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18069303539437306, "compression_ratio": 1.5515151515151515, "no_speech_prob": 3.905392077285796e-06}, {"id": 454, "seek": 203268, "start": 2041.0, "end": 2049.28, "text": " And then basically we're going to create two separate outputs.", "tokens": [400, 550, 1936, 321, 434, 516, 281, 1884, 732, 4994, 23930, 13], "temperature": 0.0, "avg_logprob": -0.18069303539437306, "compression_ratio": 1.5515151515151515, "no_speech_prob": 3.905392077285796e-06}, {"id": 455, "seek": 203268, "start": 2049.28, "end": 2055.12, "text": " The race type output.", "tokens": [440, 4569, 2010, 5598, 13], "temperature": 0.0, "avg_logprob": -0.18069303539437306, "compression_ratio": 1.5515151515151515, "no_speech_prob": 3.905392077285796e-06}, {"id": 456, "seek": 203268, "start": 2055.12, "end": 2059.04, "text": " And the disease type output.", "tokens": [400, 264, 4752, 2010, 5598, 13], "temperature": 0.0, "avg_logprob": -0.18069303539437306, "compression_ratio": 1.5515151515151515, "no_speech_prob": 3.905392077285796e-06}, {"id": 457, "seek": 205904, "start": 2059.04, "end": 2064.16, "text": " And so then we could return both of them.", "tokens": [400, 370, 550, 321, 727, 2736, 1293, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.14928801854451498, "compression_ratio": 1.4065040650406504, "no_speech_prob": 4.356777026259806e-06}, {"id": 458, "seek": 205904, "start": 2064.16, "end": 2073.24, "text": " So that's so what I would then do is I would say let's create a new model.", "tokens": [407, 300, 311, 370, 437, 286, 576, 550, 360, 307, 286, 576, 584, 718, 311, 1884, 257, 777, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14928801854451498, "compression_ratio": 1.4065040650406504, "no_speech_prob": 4.356777026259806e-06}, {"id": 459, "seek": 205904, "start": 2073.24, "end": 2077.72, "text": " So disease type classifier.", "tokens": [407, 4752, 2010, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.14928801854451498, "compression_ratio": 1.4065040650406504, "no_speech_prob": 4.356777026259806e-06}, {"id": 460, "seek": 205904, "start": 2077.72, "end": 2080.52, "text": " So we'd create it like this.", "tokens": [407, 321, 1116, 1884, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14928801854451498, "compression_ratio": 1.4065040650406504, "no_speech_prob": 4.356777026259806e-06}, {"id": 461, "seek": 208052, "start": 2080.52, "end": 2093.4, "text": " And we need to pass in the existing model, which is this thing here, right?", "tokens": [400, 321, 643, 281, 1320, 294, 264, 6741, 2316, 11, 597, 307, 341, 551, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.26700425581498577, "compression_ratio": 1.3971631205673758, "no_speech_prob": 9.817294085223693e-06}, {"id": 462, "seek": 208052, "start": 2093.4, "end": 2099.88, "text": " Oh, yes.", "tokens": [876, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.26700425581498577, "compression_ratio": 1.3971631205673758, "no_speech_prob": 9.817294085223693e-06}, {"id": 463, "seek": 208052, "start": 2099.88, "end": 2105.88, "text": " And you always have to call the super classes done to in it to construct the object before", "tokens": [400, 291, 1009, 362, 281, 818, 264, 1687, 5359, 1096, 281, 294, 309, 281, 7690, 264, 2657, 949], "temperature": 0.0, "avg_logprob": -0.26700425581498577, "compression_ratio": 1.3971631205673758, "no_speech_prob": 9.817294085223693e-06}, {"id": 464, "seek": 208052, "start": 2105.88, "end": 2109.36, "text": " you do anything else.", "tokens": [291, 360, 1340, 1646, 13], "temperature": 0.0, "avg_logprob": -0.26700425581498577, "compression_ratio": 1.3971631205673758, "no_speech_prob": 9.817294085223693e-06}, {"id": 465, "seek": 210936, "start": 2109.36, "end": 2114.08, "text": " There's a lot of annoying boilerplate in Python.", "tokens": [821, 311, 257, 688, 295, 11304, 39228, 37008, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 466, "seek": 210936, "start": 2114.08, "end": 2116.08, "text": " Oh, I'm afraid.", "tokens": [876, 11, 286, 478, 4638, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 467, "seek": 210936, "start": 2116.08, "end": 2117.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 468, "seek": 210936, "start": 2117.08, "end": 2118.08, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 469, "seek": 210936, "start": 2118.08, "end": 2119.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 470, "seek": 210936, "start": 2119.08, "end": 2120.08, "text": " I just wanted to.", "tokens": [286, 445, 1415, 281, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 471, "seek": 210936, "start": 2120.08, "end": 2121.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 472, "seek": 210936, "start": 2121.08, "end": 2130.56, "text": " Point out how cool it is that you created the model without the last layer by doing", "tokens": [12387, 484, 577, 1627, 309, 307, 300, 291, 2942, 264, 2316, 1553, 264, 1036, 4583, 538, 884], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 473, "seek": 210936, "start": 2130.56, "end": 2133.56, "text": " the atomic thing that is so good.", "tokens": [264, 22275, 551, 300, 307, 370, 665, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 474, "seek": 210936, "start": 2133.56, "end": 2134.56, "text": " I didn't know this existed.", "tokens": [286, 994, 380, 458, 341, 13135, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 475, "seek": 210936, "start": 2134.56, "end": 2135.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.4130753411187066, "compression_ratio": 1.4722222222222223, "no_speech_prob": 3.218548954464495e-05}, {"id": 476, "seek": 213556, "start": 2135.56, "end": 2140.4, "text": " I had to write this up in Python code because how can you delete stuff like that?", "tokens": [286, 632, 281, 2464, 341, 493, 294, 15329, 3089, 570, 577, 393, 291, 12097, 1507, 411, 300, 30], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 477, "seek": 213556, "start": 2140.4, "end": 2144.12, "text": " It's not a list and has only functionality to support this.", "tokens": [467, 311, 406, 257, 1329, 293, 575, 787, 14980, 281, 1406, 341, 13], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 478, "seek": 213556, "start": 2144.12, "end": 2145.12, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 479, "seek": 213556, "start": 2145.12, "end": 2146.12, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 480, "seek": 213556, "start": 2146.12, "end": 2147.44, "text": " I mean, yeah, I kind of like it's nice.", "tokens": [286, 914, 11, 1338, 11, 286, 733, 295, 411, 309, 311, 1481, 13], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 481, "seek": 213556, "start": 2147.44, "end": 2151.68, "text": " I generally find I can work on the assumption that PyTorch classes are well designed because", "tokens": [286, 5101, 915, 286, 393, 589, 322, 264, 15302, 300, 9953, 51, 284, 339, 5359, 366, 731, 4761, 570], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 482, "seek": 213556, "start": 2151.68, "end": 2153.44, "text": " it turns out they generally are.", "tokens": [309, 4523, 484, 436, 5101, 366, 13], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 483, "seek": 213556, "start": 2153.44, "end": 2161.2799999999997, "text": " And so to me, a well designed collection class would have the exact same behavior as Python.", "tokens": [400, 370, 281, 385, 11, 257, 731, 4761, 5765, 1508, 576, 362, 264, 1900, 912, 5223, 382, 15329, 13], "temperature": 0.0, "avg_logprob": -0.2610181172688802, "compression_ratio": 1.648, "no_speech_prob": 2.0782988940482028e-05}, {"id": 484, "seek": 216128, "start": 2161.28, "end": 2166.2400000000002, "text": " For example, fast cause L collection class has the exact same behavior as Python.", "tokens": [1171, 1365, 11, 2370, 3082, 441, 5765, 1508, 575, 264, 1900, 912, 5223, 382, 15329, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 485, "seek": 216128, "start": 2166.2400000000002, "end": 2174.1200000000003, "text": " So yeah, PyTorch is very nicely made.", "tokens": [407, 1338, 11, 9953, 51, 284, 339, 307, 588, 9594, 1027, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 486, "seek": 216128, "start": 2174.1200000000003, "end": 2177.0400000000004, "text": " That thing where you deleted the thing, that's a PyTorch thing?", "tokens": [663, 551, 689, 291, 22981, 264, 551, 11, 300, 311, 257, 9953, 51, 284, 339, 551, 30], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 487, "seek": 216128, "start": 2177.0400000000004, "end": 2178.0400000000004, "text": " That's not a fast thing, I think.", "tokens": [663, 311, 406, 257, 2370, 551, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 488, "seek": 216128, "start": 2178.0400000000004, "end": 2179.0400000000004, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 489, "seek": 216128, "start": 2179.0400000000004, "end": 2180.0400000000004, "text": " It's a PyTorch thing.", "tokens": [467, 311, 257, 9953, 51, 284, 339, 551, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 490, "seek": 216128, "start": 2180.0400000000004, "end": 2183.32, "text": " This is just a regular, this is just part of the sequential.", "tokens": [639, 307, 445, 257, 3890, 11, 341, 307, 445, 644, 295, 264, 42881, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 491, "seek": 216128, "start": 2183.32, "end": 2184.32, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 492, "seek": 216128, "start": 2184.32, "end": 2185.32, "text": " The sequential class.", "tokens": [440, 42881, 1508, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 493, "seek": 216128, "start": 2185.32, "end": 2186.32, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 494, "seek": 216128, "start": 2186.32, "end": 2187.32, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 495, "seek": 216128, "start": 2187.32, "end": 2188.32, "text": " Nice.", "tokens": [5490, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 496, "seek": 216128, "start": 2188.32, "end": 2189.32, "text": " Wow.", "tokens": [3153, 13], "temperature": 0.0, "avg_logprob": -0.2683197579732755, "compression_ratio": 1.643835616438356, "no_speech_prob": 4.2226206460327376e-06}, {"id": 497, "seek": 218932, "start": 2189.32, "end": 2196.6400000000003, "text": " The way I asked this, I would explode the model into layers and then reconstruct it", "tokens": [440, 636, 286, 2351, 341, 11, 286, 576, 21411, 264, 2316, 666, 7914, 293, 550, 31499, 309], "temperature": 0.0, "avg_logprob": -0.35576535478422916, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.5935194824123755e-05}, {"id": 498, "seek": 218932, "start": 2196.6400000000003, "end": 2201.0, "text": " using sequential without the layers that I need.", "tokens": [1228, 42881, 1553, 264, 7914, 300, 286, 643, 13], "temperature": 0.0, "avg_logprob": -0.35576535478422916, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.5935194824123755e-05}, {"id": 499, "seek": 218932, "start": 2201.0, "end": 2202.6000000000004, "text": " But hey, you can actually do this.", "tokens": [583, 4177, 11, 291, 393, 767, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.35576535478422916, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.5935194824123755e-05}, {"id": 500, "seek": 218932, "start": 2202.6000000000004, "end": 2204.6000000000004, "text": " This is so nice.", "tokens": [639, 307, 370, 1481, 13], "temperature": 0.0, "avg_logprob": -0.35576535478422916, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.5935194824123755e-05}, {"id": 501, "seek": 218932, "start": 2204.6000000000004, "end": 2206.2400000000002, "text": " Yeah, exactly.", "tokens": [865, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.35576535478422916, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.5935194824123755e-05}, {"id": 502, "seek": 218932, "start": 2206.2400000000002, "end": 2207.2400000000002, "text": " So let's create a new learner.", "tokens": [407, 718, 311, 1884, 257, 777, 33347, 13], "temperature": 0.0, "avg_logprob": -0.35576535478422916, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.5935194824123755e-05}, {"id": 503, "seek": 218932, "start": 2207.2400000000002, "end": 2214.36, "text": " Just be a copy of the last one, right?", "tokens": [1449, 312, 257, 5055, 295, 264, 1036, 472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.35576535478422916, "compression_ratio": 1.4308510638297873, "no_speech_prob": 1.5935194824123755e-05}, {"id": 504, "seek": 221436, "start": 2214.36, "end": 2225.48, "text": " And then let's set the model to our new model.", "tokens": [400, 550, 718, 311, 992, 264, 2316, 281, 527, 777, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13051095761750875, "compression_ratio": 1.3258426966292134, "no_speech_prob": 2.2603003344556782e-06}, {"id": 505, "seek": 221436, "start": 2225.48, "end": 2234.56, "text": " So we've now got a learner that contains our new model.", "tokens": [407, 321, 600, 586, 658, 257, 33347, 300, 8306, 527, 777, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13051095761750875, "compression_ratio": 1.3258426966292134, "no_speech_prob": 2.2603003344556782e-06}, {"id": 506, "seek": 221436, "start": 2234.56, "end": 2240.6400000000003, "text": " So that's cool.", "tokens": [407, 300, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.13051095761750875, "compression_ratio": 1.3258426966292134, "no_speech_prob": 2.2603003344556782e-06}, {"id": 507, "seek": 224064, "start": 2240.64, "end": 2254.7599999999998, "text": " I guess at this point, I guess we should be able to get some predictions, right?", "tokens": [286, 2041, 412, 341, 935, 11, 286, 2041, 321, 820, 312, 1075, 281, 483, 512, 21264, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 508, "seek": 224064, "start": 2254.7599999999998, "end": 2257.4, "text": " Wait, one question.", "tokens": [3802, 11, 472, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 509, "seek": 224064, "start": 2257.4, "end": 2259.7599999999998, "text": " Oh yeah.", "tokens": [876, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 510, "seek": 224064, "start": 2259.7599999999998, "end": 2263.12, "text": " So the main thing I'm waiting for is the loss function.", "tokens": [407, 264, 2135, 551, 286, 478, 3806, 337, 307, 264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 511, "seek": 224064, "start": 2263.12, "end": 2264.12, "text": " Like how are you going to...", "tokens": [1743, 577, 366, 291, 516, 281, 485], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 512, "seek": 224064, "start": 2264.12, "end": 2265.12, "text": " Yeah, yeah, yeah.", "tokens": [865, 11, 1338, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 513, "seek": 224064, "start": 2265.12, "end": 2266.12, "text": " That's what we're going to get there.", "tokens": [663, 311, 437, 321, 434, 516, 281, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 514, "seek": 224064, "start": 2266.12, "end": 2269.12, "text": " Let's do this first.", "tokens": [961, 311, 360, 341, 700, 13], "temperature": 0.0, "avg_logprob": -0.3655901064817933, "compression_ratio": 1.4648648648648648, "no_speech_prob": 2.0902862161165103e-06}, {"id": 515, "seek": 226912, "start": 2269.12, "end": 2279.52, "text": " And suppose you're doing the predictions just to verify the plumbing is working.", "tokens": [400, 7297, 291, 434, 884, 264, 21264, 445, 281, 16888, 264, 39993, 307, 1364, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 516, "seek": 226912, "start": 2279.52, "end": 2280.52, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 517, "seek": 226912, "start": 2280.52, "end": 2281.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 518, "seek": 226912, "start": 2281.52, "end": 2282.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 519, "seek": 226912, "start": 2282.52, "end": 2283.52, "text": " Nope.", "tokens": [12172, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 520, "seek": 226912, "start": 2283.52, "end": 2284.52, "text": " And it's not.", "tokens": [400, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 521, "seek": 226912, "start": 2284.52, "end": 2285.52, "text": " Let's see.", "tokens": [961, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 522, "seek": 226912, "start": 2285.52, "end": 2286.52, "text": " Stack trace input type.", "tokens": [37649, 13508, 4846, 2010, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 523, "seek": 226912, "start": 2286.52, "end": 2287.52, "text": " Oh, right, right, right.", "tokens": [876, 11, 558, 11, 558, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 524, "seek": 226912, "start": 2287.52, "end": 2288.52, "text": " Floating point 16.", "tokens": [15153, 990, 935, 3165, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 525, "seek": 226912, "start": 2288.52, "end": 2289.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.31122744710821854, "compression_ratio": 1.355263157894737, "no_speech_prob": 5.506894922291394e-06}, {"id": 526, "seek": 228952, "start": 2289.52, "end": 2309.88, "text": " Fair enough.", "tokens": [12157, 1547, 13], "temperature": 0.0, "avg_logprob": -0.30815226031887916, "compression_ratio": 1.075268817204301, "no_speech_prob": 3.393057795619825e-06}, {"id": 527, "seek": 228952, "start": 2309.88, "end": 2319.12, "text": " I think to simplify things, we're going to remove the.2 FP16 and we'll worry about that", "tokens": [286, 519, 281, 20460, 721, 11, 321, 434, 516, 281, 4159, 264, 2411, 17, 36655, 6866, 293, 321, 603, 3292, 466, 300], "temperature": 0.0, "avg_logprob": -0.30815226031887916, "compression_ratio": 1.075268817204301, "no_speech_prob": 3.393057795619825e-06}, {"id": 528, "seek": 231912, "start": 2319.12, "end": 2324.3199999999997, "text": " later.", "tokens": [1780, 13], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 529, "seek": 231912, "start": 2324.3199999999997, "end": 2326.4, "text": " Is that some kind of mixed precision?", "tokens": [1119, 300, 512, 733, 295, 7467, 18356, 30], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 530, "seek": 231912, "start": 2326.4, "end": 2328.4, "text": " That is exactly mixed precision.", "tokens": [663, 307, 2293, 7467, 18356, 13], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 531, "seek": 231912, "start": 2328.4, "end": 2329.4, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 532, "seek": 231912, "start": 2329.4, "end": 2333.92, "text": " So let's just pretend that doesn't exist for a moment.", "tokens": [407, 718, 311, 445, 11865, 300, 1177, 380, 2514, 337, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 533, "seek": 231912, "start": 2333.92, "end": 2337.3199999999997, "text": " And we'll come back to that.", "tokens": [400, 321, 603, 808, 646, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 534, "seek": 231912, "start": 2337.3199999999997, "end": 2339.3199999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 535, "seek": 231912, "start": 2339.3199999999997, "end": 2346.52, "text": " What on earth just happened?", "tokens": [708, 322, 4120, 445, 2011, 30], "temperature": 0.0, "avg_logprob": -0.4228319901686448, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.9763632483081892e-05}, {"id": 536, "seek": 234652, "start": 2346.52, "end": 2350.6, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2563661677496774, "compression_ratio": 1.330708661417323, "no_speech_prob": 4.157030616624979e-06}, {"id": 537, "seek": 234652, "start": 2350.6, "end": 2363.8, "text": " So let's go back even simpler.", "tokens": [407, 718, 311, 352, 646, 754, 18587, 13], "temperature": 0.0, "avg_logprob": -0.2563661677496774, "compression_ratio": 1.330708661417323, "no_speech_prob": 4.157030616624979e-06}, {"id": 538, "seek": 234652, "start": 2363.8, "end": 2367.92, "text": " So it's useful to like, if you talk about what's going through your mind when you see", "tokens": [407, 309, 311, 4420, 281, 411, 11, 498, 291, 751, 466, 437, 311, 516, 807, 428, 1575, 562, 291, 536], "temperature": 0.0, "avg_logprob": -0.2563661677496774, "compression_ratio": 1.330708661417323, "no_speech_prob": 4.157030616624979e-06}, {"id": 539, "seek": 234652, "start": 2367.92, "end": 2368.92, "text": " this error.", "tokens": [341, 6713, 13], "temperature": 0.0, "avg_logprob": -0.2563661677496774, "compression_ratio": 1.330708661417323, "no_speech_prob": 4.157030616624979e-06}, {"id": 540, "seek": 234652, "start": 2368.92, "end": 2369.92, "text": " Like you see the trace.", "tokens": [1743, 291, 536, 264, 13508, 13], "temperature": 0.0, "avg_logprob": -0.2563661677496774, "compression_ratio": 1.330708661417323, "no_speech_prob": 4.157030616624979e-06}, {"id": 541, "seek": 234652, "start": 2369.92, "end": 2370.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2563661677496774, "compression_ratio": 1.330708661417323, "no_speech_prob": 4.157030616624979e-06}, {"id": 542, "seek": 237092, "start": 2370.92, "end": 2377.36, "text": " So let's create like a minimum reproducible example.", "tokens": [407, 718, 311, 1884, 411, 257, 7285, 11408, 32128, 1365, 13], "temperature": 0.0, "avg_logprob": -0.28215771993001304, "compression_ratio": 1.515923566878981, "no_speech_prob": 1.0615224709908944e-05}, {"id": 543, "seek": 237092, "start": 2377.36, "end": 2388.0, "text": " So let's just like create a learner and then copy it and then not change it at all.", "tokens": [407, 718, 311, 445, 411, 1884, 257, 33347, 293, 550, 5055, 309, 293, 550, 406, 1319, 309, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.28215771993001304, "compression_ratio": 1.515923566878981, "no_speech_prob": 1.0615224709908944e-05}, {"id": 544, "seek": 237092, "start": 2388.0, "end": 2389.4, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.28215771993001304, "compression_ratio": 1.515923566878981, "no_speech_prob": 1.0615224709908944e-05}, {"id": 545, "seek": 237092, "start": 2389.4, "end": 2392.2400000000002, "text": " I can't even do that.", "tokens": [286, 393, 380, 754, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.28215771993001304, "compression_ratio": 1.515923566878981, "no_speech_prob": 1.0615224709908944e-05}, {"id": 546, "seek": 237092, "start": 2392.2400000000002, "end": 2393.2400000000002, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.28215771993001304, "compression_ratio": 1.515923566878981, "no_speech_prob": 1.0615224709908944e-05}, {"id": 547, "seek": 237092, "start": 2393.2400000000002, "end": 2398.6800000000003, "text": " So this so then I would be like, okay, let's not even copy it.", "tokens": [407, 341, 370, 550, 286, 576, 312, 411, 11, 1392, 11, 718, 311, 406, 754, 5055, 309, 13], "temperature": 0.0, "avg_logprob": -0.28215771993001304, "compression_ratio": 1.515923566878981, "no_speech_prob": 1.0615224709908944e-05}, {"id": 548, "seek": 239868, "start": 2398.68, "end": 2403.3599999999997, "text": " But instead let's just call it directly.", "tokens": [583, 2602, 718, 311, 445, 818, 309, 3838, 13], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 549, "seek": 239868, "start": 2403.3599999999997, "end": 2407.3199999999997, "text": " What it learned to.", "tokens": [708, 309, 3264, 281, 13], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 550, "seek": 239868, "start": 2407.3199999999997, "end": 2409.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 551, "seek": 239868, "start": 2409.52, "end": 2410.52, "text": " That works.", "tokens": [663, 1985, 13], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 552, "seek": 239868, "start": 2410.52, "end": 2412.7999999999997, "text": " So doing a copy apparently doesn't work.", "tokens": [407, 884, 257, 5055, 7970, 1177, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 553, "seek": 239868, "start": 2412.7999999999997, "end": 2417.64, "text": " Oh, so generally speaking, I would be inclined to change copy to deep copy at this point.", "tokens": [876, 11, 370, 5101, 4124, 11, 286, 576, 312, 28173, 281, 1319, 5055, 281, 2452, 5055, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 554, "seek": 239868, "start": 2417.64, "end": 2421.56, "text": " Wait, but you still got the stack trace and then, right?", "tokens": [3802, 11, 457, 291, 920, 658, 264, 8630, 13508, 293, 550, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 555, "seek": 239868, "start": 2421.56, "end": 2423.04, "text": " Most things though.", "tokens": [4534, 721, 1673, 13], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 556, "seek": 239868, "start": 2423.04, "end": 2427.8399999999997, "text": " Oh, why are we still getting a half precision somewhere?", "tokens": [876, 11, 983, 366, 321, 920, 1242, 257, 1922, 18356, 4079, 30], "temperature": 0.0, "avg_logprob": -0.40550858633858816, "compression_ratio": 1.5244444444444445, "no_speech_prob": 4.784949396707816e-06}, {"id": 557, "seek": 242784, "start": 2427.84, "end": 2437.84, "text": " That's very, yes, isn't it?", "tokens": [663, 311, 588, 11, 2086, 11, 1943, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.35715469360351565, "compression_ratio": 1.40625, "no_speech_prob": 1.184220764116617e-05}, {"id": 558, "seek": 242784, "start": 2437.84, "end": 2444.1600000000003, "text": " Oh, it's probably because our data loaders got changed somehow.", "tokens": [876, 11, 309, 311, 1391, 570, 527, 1412, 3677, 433, 658, 3105, 6063, 13], "temperature": 0.0, "avg_logprob": -0.35715469360351565, "compression_ratio": 1.40625, "no_speech_prob": 1.184220764116617e-05}, {"id": 559, "seek": 242784, "start": 2444.1600000000003, "end": 2446.2400000000002, "text": " Let's recreate the data loaders as well.", "tokens": [961, 311, 25833, 264, 1412, 3677, 433, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.35715469360351565, "compression_ratio": 1.40625, "no_speech_prob": 1.184220764116617e-05}, {"id": 560, "seek": 242784, "start": 2446.2400000000002, "end": 2450.84, "text": " It nearly made it, didn't it?", "tokens": [467, 6217, 1027, 309, 11, 994, 380, 309, 30], "temperature": 0.0, "avg_logprob": -0.35715469360351565, "compression_ratio": 1.40625, "no_speech_prob": 1.184220764116617e-05}, {"id": 561, "seek": 242784, "start": 2450.84, "end": 2451.84, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.35715469360351565, "compression_ratio": 1.40625, "no_speech_prob": 1.184220764116617e-05}, {"id": 562, "seek": 242784, "start": 2451.84, "end": 2454.1200000000003, "text": " It looked like it was working.", "tokens": [467, 2956, 411, 309, 390, 1364, 13], "temperature": 0.0, "avg_logprob": -0.35715469360351565, "compression_ratio": 1.40625, "no_speech_prob": 1.184220764116617e-05}, {"id": 563, "seek": 242784, "start": 2454.1200000000003, "end": 2455.92, "text": " And then at the very end.", "tokens": [400, 550, 412, 264, 588, 917, 13], "temperature": 0.0, "avg_logprob": -0.35715469360351565, "compression_ratio": 1.40625, "no_speech_prob": 1.184220764116617e-05}, {"id": 564, "seek": 245592, "start": 2455.92, "end": 2458.92, "text": " Oh, there we go.", "tokens": [876, 11, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.3344470705304827, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.0449997716932558e-05}, {"id": 565, "seek": 245592, "start": 2458.92, "end": 2459.92, "text": " How are we getting half precision?", "tokens": [1012, 366, 321, 1242, 1922, 18356, 30], "temperature": 0.0, "avg_logprob": -0.3344470705304827, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.0449997716932558e-05}, {"id": 566, "seek": 245592, "start": 2459.92, "end": 2469.16, "text": " What on earth is making it half precision?", "tokens": [708, 322, 4120, 307, 1455, 309, 1922, 18356, 30], "temperature": 0.0, "avg_logprob": -0.3344470705304827, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.0449997716932558e-05}, {"id": 567, "seek": 245592, "start": 2469.16, "end": 2475.12, "text": " That's odd.", "tokens": [663, 311, 7401, 13], "temperature": 0.0, "avg_logprob": -0.3344470705304827, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.0449997716932558e-05}, {"id": 568, "seek": 245592, "start": 2475.12, "end": 2479.7200000000003, "text": " Do you think resetting your kernel and I think so.", "tokens": [1144, 291, 519, 14322, 783, 428, 28256, 293, 286, 519, 370, 13], "temperature": 0.0, "avg_logprob": -0.3344470705304827, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.0449997716932558e-05}, {"id": 569, "seek": 245592, "start": 2479.7200000000003, "end": 2485.84, "text": " I don't see how this would help, but there's never any harm, right?", "tokens": [286, 500, 380, 536, 577, 341, 576, 854, 11, 457, 456, 311, 1128, 604, 6491, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3344470705304827, "compression_ratio": 1.4423076923076923, "no_speech_prob": 1.0449997716932558e-05}, {"id": 570, "seek": 248584, "start": 2485.84, "end": 2493.1600000000003, "text": " Maybe one of the callbacks somehow.", "tokens": [2704, 472, 295, 264, 818, 17758, 6063, 13], "temperature": 0.0, "avg_logprob": -0.846688490647536, "compression_ratio": 1.0232558139534884, "no_speech_prob": 6.198195478646085e-05}, {"id": 571, "seek": 248584, "start": 2493.1600000000003, "end": 2496.1600000000003, "text": " Define atch.", "tokens": [9548, 533, 412, 339, 13], "temperature": 0.0, "avg_logprob": -0.846688490647536, "compression_ratio": 1.0232558139534884, "no_speech_prob": 6.198195478646085e-05}, {"id": 572, "seek": 248584, "start": 2496.1600000000003, "end": 2497.1600000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.846688490647536, "compression_ratio": 1.0232558139534884, "no_speech_prob": 6.198195478646085e-05}, {"id": 573, "seek": 248584, "start": 2497.1600000000003, "end": 2502.1600000000003, "text": " In some states.", "tokens": [682, 512, 4368, 13], "temperature": 0.0, "avg_logprob": -0.846688490647536, "compression_ratio": 1.0232558139534884, "no_speech_prob": 6.198195478646085e-05}, {"id": 574, "seek": 248584, "start": 2502.1600000000003, "end": 2506.2400000000002, "text": " Yeah, maybe.", "tokens": [865, 11, 1310, 13], "temperature": 0.0, "avg_logprob": -0.846688490647536, "compression_ratio": 1.0232558139534884, "no_speech_prob": 6.198195478646085e-05}, {"id": 575, "seek": 248584, "start": 2506.2400000000002, "end": 2509.32, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.846688490647536, "compression_ratio": 1.0232558139534884, "no_speech_prob": 6.198195478646085e-05}, {"id": 576, "seek": 250932, "start": 2509.32, "end": 2516.7200000000003, "text": " That's exactly what happened.", "tokens": [663, 311, 2293, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 577, "seek": 250932, "start": 2516.7200000000003, "end": 2517.7200000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 578, "seek": 250932, "start": 2517.7200000000003, "end": 2519.8, "text": " Well, that's that shouldn't happen.", "tokens": [1042, 11, 300, 311, 300, 4659, 380, 1051, 13], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 579, "seek": 250932, "start": 2519.8, "end": 2522.84, "text": " So that's not a great, you know what happened?", "tokens": [407, 300, 311, 406, 257, 869, 11, 291, 458, 437, 2011, 30], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 580, "seek": 250932, "start": 2522.84, "end": 2523.84, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 581, "seek": 250932, "start": 2523.84, "end": 2524.84, "text": " I don't know what happened.", "tokens": [286, 500, 380, 458, 437, 2011, 13], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 582, "seek": 250932, "start": 2524.84, "end": 2531.1200000000003, "text": " Like some, like, like I said, some, something has some state that's keeping things in half", "tokens": [1743, 512, 11, 411, 11, 411, 286, 848, 11, 512, 11, 746, 575, 512, 1785, 300, 311, 5145, 721, 294, 1922], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 583, "seek": 250932, "start": 2531.1200000000003, "end": 2535.04, "text": " precision, which yeah, shouldn't be happening.", "tokens": [18356, 11, 597, 1338, 11, 4659, 380, 312, 2737, 13], "temperature": 0.0, "avg_logprob": -0.2754498843489022, "compression_ratio": 1.6724137931034482, "no_speech_prob": 3.726297336470452e-06}, {"id": 584, "seek": 253504, "start": 2535.04, "end": 2541.24, "text": " And so at some point we can try to figure out what that is, but not now.", "tokens": [400, 370, 412, 512, 935, 321, 393, 853, 281, 2573, 484, 437, 300, 307, 11, 457, 406, 586, 13], "temperature": 0.0, "avg_logprob": -0.2533582216733462, "compression_ratio": 1.4748603351955307, "no_speech_prob": 8.39751555758994e-06}, {"id": 585, "seek": 253504, "start": 2541.24, "end": 2542.24, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2533582216733462, "compression_ratio": 1.4748603351955307, "no_speech_prob": 8.39751555758994e-06}, {"id": 586, "seek": 253504, "start": 2542.24, "end": 2554.7599999999998, "text": " So let's make a copy of the learner and check this in into it.", "tokens": [407, 718, 311, 652, 257, 5055, 295, 264, 33347, 293, 1520, 341, 294, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.2533582216733462, "compression_ratio": 1.4748603351955307, "no_speech_prob": 8.39751555758994e-06}, {"id": 587, "seek": 253504, "start": 2554.7599999999998, "end": 2559.64, "text": " Actually before we do, we're just going to use the copy directly.", "tokens": [5135, 949, 321, 360, 11, 321, 434, 445, 516, 281, 764, 264, 5055, 3838, 13], "temperature": 0.0, "avg_logprob": -0.2533582216733462, "compression_ratio": 1.4748603351955307, "no_speech_prob": 8.39751555758994e-06}, {"id": 588, "seek": 253504, "start": 2559.64, "end": 2563.6, "text": " So we'll just make as few changes each time as possible.", "tokens": [407, 321, 603, 445, 652, 382, 1326, 2962, 1184, 565, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.2533582216733462, "compression_ratio": 1.4748603351955307, "no_speech_prob": 8.39751555758994e-06}, {"id": 589, "seek": 256360, "start": 2563.6, "end": 2565.3199999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.31476748719507336, "compression_ratio": 1.2083333333333333, "no_speech_prob": 1.0782274330267683e-05}, {"id": 590, "seek": 256360, "start": 2565.3199999999997, "end": 2569.3199999999997, "text": " That worked.", "tokens": [663, 2732, 13], "temperature": 0.0, "avg_logprob": -0.31476748719507336, "compression_ratio": 1.2083333333333333, "no_speech_prob": 1.0782274330267683e-05}, {"id": 591, "seek": 256360, "start": 2569.3199999999997, "end": 2584.88, "text": " What are you looking for in this?", "tokens": [708, 366, 291, 1237, 337, 294, 341, 30], "temperature": 0.0, "avg_logprob": -0.31476748719507336, "compression_ratio": 1.2083333333333333, "no_speech_prob": 1.0782274330267683e-05}, {"id": 592, "seek": 256360, "start": 2584.88, "end": 2588.6, "text": " Just saying, like, I'm trying to see why it's returning.", "tokens": [1449, 1566, 11, 411, 11, 286, 478, 1382, 281, 536, 983, 309, 311, 12678, 13], "temperature": 0.0, "avg_logprob": -0.31476748719507336, "compression_ratio": 1.2083333333333333, "no_speech_prob": 1.0782274330267683e-05}, {"id": 593, "seek": 256360, "start": 2588.6, "end": 2590.2799999999997, "text": " I thought that was a decoded thing.", "tokens": [286, 1194, 300, 390, 257, 979, 12340, 551, 13], "temperature": 0.0, "avg_logprob": -0.31476748719507336, "compression_ratio": 1.2083333333333333, "no_speech_prob": 1.0782274330267683e-05}, {"id": 594, "seek": 259028, "start": 2590.28, "end": 2595.0800000000004, "text": " So I was just wondering why it's being returned.", "tokens": [407, 286, 390, 445, 6359, 983, 309, 311, 885, 8752, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 595, "seek": 259028, "start": 2595.0800000000004, "end": 2597.6800000000003, "text": " Says here with this decoded equals false.", "tokens": [36780, 510, 365, 341, 979, 12340, 6915, 7908, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 596, "seek": 259028, "start": 2597.6800000000003, "end": 2600.48, "text": " Oh, they're the targets.", "tokens": [876, 11, 436, 434, 264, 12911, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 597, "seek": 259028, "start": 2600.48, "end": 2601.48, "text": " That's why.", "tokens": [663, 311, 983, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 598, "seek": 259028, "start": 2601.48, "end": 2602.48, "text": " That's why.", "tokens": [663, 311, 983, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 599, "seek": 259028, "start": 2602.48, "end": 2603.48, "text": " That's why.", "tokens": [663, 311, 983, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 600, "seek": 259028, "start": 2603.48, "end": 2607.7200000000003, "text": " So it actually returns preds, comma, targets.", "tokens": [407, 309, 767, 11247, 3852, 82, 11, 22117, 11, 12911, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 601, "seek": 259028, "start": 2607.7200000000003, "end": 2610.7200000000003, "text": " That's what it's returning.", "tokens": [663, 311, 437, 309, 311, 12678, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 602, "seek": 259028, "start": 2610.7200000000003, "end": 2612.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 603, "seek": 259028, "start": 2612.52, "end": 2616.48, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.36287873131888254, "compression_ratio": 1.5714285714285714, "no_speech_prob": 4.6376922000490595e-06}, {"id": 604, "seek": 261648, "start": 2616.48, "end": 2626.8, "text": " So now it's working quite nicely.", "tokens": [407, 586, 309, 311, 1364, 1596, 9594, 13], "temperature": 0.0, "avg_logprob": -0.19696944952011108, "compression_ratio": 1.2677165354330708, "no_speech_prob": 1.4367321909958264e-06}, {"id": 605, "seek": 261648, "start": 2626.8, "end": 2636.28, "text": " And so I would be now inclined to, like, create a really minimal model, which is like a, I'm", "tokens": [400, 370, 286, 576, 312, 586, 28173, 281, 11, 411, 11, 1884, 257, 534, 13206, 2316, 11, 597, 307, 411, 257, 11, 286, 478], "temperature": 0.0, "avg_logprob": -0.19696944952011108, "compression_ratio": 1.2677165354330708, "no_speech_prob": 1.4367321909958264e-06}, {"id": 606, "seek": 261648, "start": 2636.28, "end": 2640.68, "text": " going to call it dummy classifier.", "tokens": [516, 281, 818, 309, 35064, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.19696944952011108, "compression_ratio": 1.2677165354330708, "no_speech_prob": 1.4367321909958264e-06}, {"id": 607, "seek": 264068, "start": 2640.68, "end": 2648.7599999999998, "text": " And all it does is it calls the original model.", "tokens": [400, 439, 309, 775, 307, 309, 5498, 264, 3380, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1613030983851506, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.368735067648231e-06}, {"id": 608, "seek": 264068, "start": 2648.7599999999998, "end": 2650.3199999999997, "text": " And let's see if that works.", "tokens": [400, 718, 311, 536, 498, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1613030983851506, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.368735067648231e-06}, {"id": 609, "seek": 264068, "start": 2650.3199999999997, "end": 2658.3199999999997, "text": " Because if this works, then we're at a point where we can then try out new models, right?", "tokens": [1436, 498, 341, 1985, 11, 550, 321, 434, 412, 257, 935, 689, 321, 393, 550, 853, 484, 777, 5245, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1613030983851506, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.368735067648231e-06}, {"id": 610, "seek": 264068, "start": 2658.3199999999997, "end": 2659.3199999999997, "text": " It's interesting.", "tokens": [467, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1613030983851506, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.368735067648231e-06}, {"id": 611, "seek": 264068, "start": 2659.3199999999997, "end": 2664.3199999999997, "text": " I would have just gone straight back to the full model and tried that next, but you're", "tokens": [286, 576, 362, 445, 2780, 2997, 646, 281, 264, 1577, 2316, 293, 3031, 300, 958, 11, 457, 291, 434], "temperature": 0.0, "avg_logprob": -0.1613030983851506, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.368735067648231e-06}, {"id": 612, "seek": 264068, "start": 2664.3199999999997, "end": 2665.3199999999997, "text": " slowly walking away.", "tokens": [5692, 4494, 1314, 13], "temperature": 0.0, "avg_logprob": -0.1613030983851506, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.368735067648231e-06}, {"id": 613, "seek": 264068, "start": 2665.3199999999997, "end": 2668.8399999999997, "text": " Yeah, I probably should have done it that way in the, you know, done it more slowly", "tokens": [865, 11, 286, 1391, 820, 362, 1096, 309, 300, 636, 294, 264, 11, 291, 458, 11, 1096, 309, 544, 5692], "temperature": 0.0, "avg_logprob": -0.1613030983851506, "compression_ratio": 1.6277056277056277, "no_speech_prob": 2.368735067648231e-06}, {"id": 614, "seek": 266884, "start": 2668.84, "end": 2674.56, "text": " in the first place, but I got over enthusiastic.", "tokens": [294, 264, 700, 1081, 11, 457, 286, 658, 670, 28574, 13], "temperature": 0.0, "avg_logprob": -0.5248064654214042, "compression_ratio": 1.0, "no_speech_prob": 3.6461166018852964e-05}, {"id": 615, "seek": 266884, "start": 2674.56, "end": 2680.4, "text": " Okay, great.", "tokens": [1033, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.5248064654214042, "compression_ratio": 1.0, "no_speech_prob": 3.6461166018852964e-05}, {"id": 616, "seek": 266884, "start": 2680.4, "end": 2691.44, "text": " Oopsie daisy.", "tokens": [21726, 414, 1120, 14169, 13], "temperature": 0.0, "avg_logprob": -0.5248064654214042, "compression_ratio": 1.0, "no_speech_prob": 3.6461166018852964e-05}, {"id": 617, "seek": 269144, "start": 2691.44, "end": 2713.16, "text": " We could do this inside our model, I guess.", "tokens": [492, 727, 360, 341, 1854, 527, 2316, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.14137249834397259, "compression_ratio": 1.1470588235294117, "no_speech_prob": 1.4062973605177831e-05}, {"id": 618, "seek": 269144, "start": 2713.16, "end": 2718.28, "text": " This is all pretty hacky, but we're just trying to get something working.", "tokens": [639, 307, 439, 1238, 10339, 88, 11, 457, 321, 434, 445, 1382, 281, 483, 746, 1364, 13], "temperature": 0.0, "avg_logprob": -0.14137249834397259, "compression_ratio": 1.1470588235294117, "no_speech_prob": 1.4062973605177831e-05}, {"id": 619, "seek": 271828, "start": 2718.28, "end": 2722.0800000000004, "text": " So the head is the number one thing in the model.", "tokens": [407, 264, 1378, 307, 264, 1230, 472, 551, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 620, "seek": 271828, "start": 2722.0800000000004, "end": 2725.0800000000004, "text": " The last layer is the end of the head.", "tokens": [440, 1036, 4583, 307, 264, 917, 295, 264, 1378, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 621, "seek": 271828, "start": 2725.0800000000004, "end": 2727.92, "text": " We don't need that.", "tokens": [492, 500, 380, 643, 300, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 622, "seek": 271828, "start": 2727.92, "end": 2730.1200000000003, "text": " We delete that last thing.", "tokens": [492, 12097, 300, 1036, 551, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 623, "seek": 271828, "start": 2730.1200000000003, "end": 2733.1200000000003, "text": " Yes, we don't need that.", "tokens": [1079, 11, 321, 500, 380, 643, 300, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 624, "seek": 271828, "start": 2733.1200000000003, "end": 2734.1200000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 625, "seek": 271828, "start": 2734.1200000000003, "end": 2736.96, "text": " So we might as well inline that.", "tokens": [407, 321, 1062, 382, 731, 294, 1889, 300, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 626, "seek": 271828, "start": 2736.96, "end": 2740.0400000000004, "text": " Keep it simple.", "tokens": [5527, 309, 2199, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 627, "seek": 271828, "start": 2740.0400000000004, "end": 2741.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 628, "seek": 271828, "start": 2741.28, "end": 2747.28, "text": " So we delete the head and store it away.", "tokens": [407, 321, 12097, 264, 1378, 293, 3531, 309, 1314, 13], "temperature": 0.0, "avg_logprob": -0.376052982204563, "compression_ratio": 1.6903225806451614, "no_speech_prob": 9.51579295360716e-06}, {"id": 629, "seek": 274728, "start": 2747.28, "end": 2749.2000000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6101287953993854, "compression_ratio": 1.2647058823529411, "no_speech_prob": 1.1842552339658141e-05}, {"id": 630, "seek": 274728, "start": 2749.2000000000003, "end": 2760.6800000000003, "text": " So we're going to create our learner.", "tokens": [407, 321, 434, 516, 281, 1884, 527, 33347, 13], "temperature": 0.0, "avg_logprob": -0.6101287953993854, "compression_ratio": 1.2647058823529411, "no_speech_prob": 1.1842552339658141e-05}, {"id": 631, "seek": 274728, "start": 2760.6800000000003, "end": 2763.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6101287953993854, "compression_ratio": 1.2647058823529411, "no_speech_prob": 1.1842552339658141e-05}, {"id": 632, "seek": 274728, "start": 2763.28, "end": 2768.96, "text": " So create a learner.", "tokens": [407, 1884, 257, 33347, 13], "temperature": 0.0, "avg_logprob": -0.6101287953993854, "compression_ratio": 1.2647058823529411, "no_speech_prob": 1.1842552339658141e-05}, {"id": 633, "seek": 274728, "start": 2768.96, "end": 2769.96, "text": " Create a class.", "tokens": [20248, 257, 1508, 13], "temperature": 0.0, "avg_logprob": -0.6101287953993854, "compression_ratio": 1.2647058823529411, "no_speech_prob": 1.1842552339658141e-05}, {"id": 634, "seek": 276996, "start": 2769.96, "end": 2777.84, "text": " This time we call the disease, et cetera, classifier.", "tokens": [639, 565, 321, 818, 264, 4752, 11, 1030, 11458, 11, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.4736483672569538, "compression_ratio": 1.0512820512820513, "no_speech_prob": 6.301575194811448e-05}, {"id": 635, "seek": 276996, "start": 2777.84, "end": 2787.96, "text": " Set the model to that.", "tokens": [8928, 264, 2316, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.4736483672569538, "compression_ratio": 1.0512820512820513, "no_speech_prob": 6.301575194811448e-05}, {"id": 636, "seek": 276996, "start": 2787.96, "end": 2792.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4736483672569538, "compression_ratio": 1.0512820512820513, "no_speech_prob": 6.301575194811448e-05}, {"id": 637, "seek": 279212, "start": 2792.12, "end": 2800.8399999999997, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.31842878702524546, "compression_ratio": 1.1195652173913044, "no_speech_prob": 8.139472811308224e-06}, {"id": 638, "seek": 279212, "start": 2800.8399999999997, "end": 2804.88, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.31842878702524546, "compression_ratio": 1.1195652173913044, "no_speech_prob": 8.139472811308224e-06}, {"id": 639, "seek": 279212, "start": 2804.88, "end": 2812.2799999999997, "text": " So we're now at the point where it's trying to calculate loss and it has no way to do", "tokens": [407, 321, 434, 586, 412, 264, 935, 689, 309, 311, 1382, 281, 8873, 4470, 293, 309, 575, 572, 636, 281, 360], "temperature": 0.0, "avg_logprob": -0.31842878702524546, "compression_ratio": 1.1195652173913044, "no_speech_prob": 8.139472811308224e-06}, {"id": 640, "seek": 279212, "start": 2812.2799999999997, "end": 2815.2799999999997, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.31842878702524546, "compression_ratio": 1.1195652173913044, "no_speech_prob": 8.139472811308224e-06}, {"id": 641, "seek": 281528, "start": 2815.28, "end": 2828.1600000000003, "text": " I'm slightly surprised it's trying to calculate loss at all since with loss is false.", "tokens": [286, 478, 4748, 6100, 309, 311, 1382, 281, 8873, 4470, 412, 439, 1670, 365, 4470, 307, 7908, 13], "temperature": 0.0, "avg_logprob": -0.25444921457542563, "compression_ratio": 1.4191176470588236, "no_speech_prob": 1.5779411342009553e-06}, {"id": 642, "seek": 281528, "start": 2828.1600000000003, "end": 2831.7200000000003, "text": " That's fine.", "tokens": [663, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.25444921457542563, "compression_ratio": 1.4191176470588236, "no_speech_prob": 1.5779411342009553e-06}, {"id": 643, "seek": 281528, "start": 2831.7200000000003, "end": 2832.7200000000003, "text": " So okay.", "tokens": [407, 1392, 13], "temperature": 0.0, "avg_logprob": -0.25444921457542563, "compression_ratio": 1.4191176470588236, "no_speech_prob": 1.5779411342009553e-06}, {"id": 644, "seek": 281528, "start": 2832.7200000000003, "end": 2843.52, "text": " So the loss function to remind you is the thing which is like a number which says how", "tokens": [407, 264, 4470, 2445, 281, 4160, 291, 307, 264, 551, 597, 307, 411, 257, 1230, 597, 1619, 577], "temperature": 0.0, "avg_logprob": -0.25444921457542563, "compression_ratio": 1.4191176470588236, "no_speech_prob": 1.5779411342009553e-06}, {"id": 645, "seek": 284352, "start": 2843.52, "end": 2847.68, "text": " good is this model?", "tokens": [665, 307, 341, 2316, 30], "temperature": 0.0, "avg_logprob": -0.1995178252931625, "compression_ratio": 1.559748427672956, "no_speech_prob": 5.173709723749198e-06}, {"id": 646, "seek": 284352, "start": 2847.68, "end": 2860.6, "text": " And the loss function that we were using was designed on something that only returned a", "tokens": [400, 264, 4470, 2445, 300, 321, 645, 1228, 390, 4761, 322, 746, 300, 787, 8752, 257], "temperature": 0.0, "avg_logprob": -0.1995178252931625, "compression_ratio": 1.559748427672956, "no_speech_prob": 5.173709723749198e-06}, {"id": 647, "seek": 284352, "start": 2860.6, "end": 2866.68, "text": " single tensor and we're returning a couple of tensors.", "tokens": [2167, 40863, 293, 321, 434, 12678, 257, 1916, 295, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.1995178252931625, "compression_ratio": 1.559748427672956, "no_speech_prob": 5.173709723749198e-06}, {"id": 648, "seek": 284352, "start": 2866.68, "end": 2872.6, "text": " And so that's why when it tries to call the loss function, it gets confused, which is", "tokens": [400, 370, 300, 311, 983, 562, 309, 9898, 281, 818, 264, 4470, 2445, 11, 309, 2170, 9019, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.1995178252931625, "compression_ratio": 1.559748427672956, "no_speech_prob": 5.173709723749198e-06}, {"id": 649, "seek": 287260, "start": 2872.6, "end": 2874.92, "text": " fair enough.", "tokens": [3143, 1547, 13], "temperature": 0.0, "avg_logprob": -0.216019919424346, "compression_ratio": 1.1685393258426966, "no_speech_prob": 4.356716544862138e-06}, {"id": 650, "seek": 287260, "start": 2874.92, "end": 2881.8399999999997, "text": " So the loss function is another thing that is stored inside the learner.", "tokens": [407, 264, 4470, 2445, 307, 1071, 551, 300, 307, 12187, 1854, 264, 33347, 13], "temperature": 0.0, "avg_logprob": -0.216019919424346, "compression_ratio": 1.1685393258426966, "no_speech_prob": 4.356716544862138e-06}, {"id": 651, "seek": 287260, "start": 2881.8399999999997, "end": 2885.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.216019919424346, "compression_ratio": 1.1685393258426966, "no_speech_prob": 4.356716544862138e-06}, {"id": 652, "seek": 287260, "start": 2885.08, "end": 2893.74, "text": " There it is.", "tokens": [821, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.216019919424346, "compression_ratio": 1.1685393258426966, "no_speech_prob": 4.356716544862138e-06}, {"id": 653, "seek": 289374, "start": 2893.74, "end": 2909.2, "text": " So what we could do is we could, what's the best way to do this?", "tokens": [407, 437, 321, 727, 360, 307, 321, 727, 11, 437, 311, 264, 1151, 636, 281, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.20098304748535156, "compression_ratio": 1.4206349206349207, "no_speech_prob": 2.2958911358728074e-06}, {"id": 654, "seek": 289374, "start": 2909.2, "end": 2913.3199999999997, "text": " One thing would be we could look at the source code for vision learner and see how that creates", "tokens": [1485, 551, 576, 312, 321, 727, 574, 412, 264, 4009, 3089, 337, 5201, 33347, 293, 536, 577, 300, 7829], "temperature": 0.0, "avg_logprob": -0.20098304748535156, "compression_ratio": 1.4206349206349207, "no_speech_prob": 2.2958911358728074e-06}, {"id": 655, "seek": 289374, "start": 2913.3199999999997, "end": 2916.3199999999997, "text": " the loss function.", "tokens": [264, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.20098304748535156, "compression_ratio": 1.4206349206349207, "no_speech_prob": 2.2958911358728074e-06}, {"id": 656, "seek": 291632, "start": 2916.32, "end": 2927.04, "text": " So I'll just pass this to learner.", "tokens": [407, 286, 603, 445, 1320, 341, 281, 33347, 13], "temperature": 0.0, "avg_logprob": -0.8706682645357572, "compression_ratio": 0.9545454545454546, "no_speech_prob": 3.089466645178618e-06}, {"id": 657, "seek": 291632, "start": 2927.04, "end": 2929.2400000000002, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.8706682645357572, "compression_ratio": 0.9545454545454546, "no_speech_prob": 3.089466645178618e-06}, {"id": 658, "seek": 291632, "start": 2929.2400000000002, "end": 2945.04, "text": " So let's look at that.", "tokens": [407, 718, 311, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.8706682645357572, "compression_ratio": 0.9545454545454546, "no_speech_prob": 3.089466645178618e-06}, {"id": 659, "seek": 294504, "start": 2945.04, "end": 2948.72, "text": " Okay, so it's trying to get it from the training data set.", "tokens": [1033, 11, 370, 309, 311, 1382, 281, 483, 309, 490, 264, 3097, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.16518666744232177, "compression_ratio": 1.774390243902439, "no_speech_prob": 8.013053047761787e-06}, {"id": 660, "seek": 294504, "start": 2948.72, "end": 2957.48, "text": " So the training data set knows what function, loss function to use, which is pretty nifty.", "tokens": [407, 264, 3097, 1412, 992, 3255, 437, 2445, 11, 4470, 2445, 281, 764, 11, 597, 307, 1238, 297, 37177, 13], "temperature": 0.0, "avg_logprob": -0.16518666744232177, "compression_ratio": 1.774390243902439, "no_speech_prob": 8.013053047761787e-06}, {"id": 661, "seek": 294504, "start": 2957.48, "end": 2962.62, "text": " So to start with, we could, let's create a loss function.", "tokens": [407, 281, 722, 365, 11, 321, 727, 11, 718, 311, 1884, 257, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16518666744232177, "compression_ratio": 1.774390243902439, "no_speech_prob": 8.013053047761787e-06}, {"id": 662, "seek": 294504, "start": 2962.62, "end": 2965.84, "text": " So let's create a really simple loss function.", "tokens": [407, 718, 311, 1884, 257, 534, 2199, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.16518666744232177, "compression_ratio": 1.774390243902439, "no_speech_prob": 8.013053047761787e-06}, {"id": 663, "seek": 294504, "start": 2965.84, "end": 2969.16, "text": " So disease and type classifier loss.", "tokens": [407, 4752, 293, 2010, 1508, 9902, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16518666744232177, "compression_ratio": 1.774390243902439, "no_speech_prob": 8.013053047761787e-06}, {"id": 664, "seek": 296916, "start": 2969.16, "end": 2980.64, "text": " So we're going to be passed some, we're going to be passed predictions and actuals.", "tokens": [407, 321, 434, 516, 281, 312, 4678, 512, 11, 321, 434, 516, 281, 312, 4678, 21264, 293, 3539, 82, 13], "temperature": 0.0, "avg_logprob": -0.2310953140258789, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.173771569388919e-06}, {"id": 665, "seek": 296916, "start": 2980.64, "end": 2981.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2310953140258789, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.173771569388919e-06}, {"id": 666, "seek": 296916, "start": 2981.64, "end": 2992.48, "text": " So we're going to be passed predictions or sometimes called the targets.", "tokens": [407, 321, 434, 516, 281, 312, 4678, 21264, 420, 2171, 1219, 264, 12911, 13], "temperature": 0.0, "avg_logprob": -0.2310953140258789, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.173771569388919e-06}, {"id": 667, "seek": 299248, "start": 2992.48, "end": 3001.2400000000002, "text": " And what we could do is we could just say like for now, let's say the current loss function", "tokens": [400, 437, 321, 727, 360, 307, 321, 727, 445, 584, 411, 337, 586, 11, 718, 311, 584, 264, 2190, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.1274709463119507, "compression_ratio": 1.6878612716763006, "no_speech_prob": 5.59424961465993e-06}, {"id": 668, "seek": 299248, "start": 3001.2400000000002, "end": 3007.28, "text": " is whatever loss function we had before.", "tokens": [307, 2035, 4470, 2445, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.1274709463119507, "compression_ratio": 1.6878612716763006, "no_speech_prob": 5.59424961465993e-06}, {"id": 669, "seek": 299248, "start": 3007.28, "end": 3015.4, "text": " And let's just try to predict, let's just try to get it so it's working just on the", "tokens": [400, 718, 311, 445, 853, 281, 6069, 11, 718, 311, 445, 853, 281, 483, 309, 370, 309, 311, 1364, 445, 322, 264], "temperature": 0.0, "avg_logprob": -0.1274709463119507, "compression_ratio": 1.6878612716763006, "no_speech_prob": 5.59424961465993e-06}, {"id": 670, "seek": 299248, "start": 3015.4, "end": 3018.38, "text": " disease prediction, which is this bit here.", "tokens": [4752, 17630, 11, 597, 307, 341, 857, 510, 13], "temperature": 0.0, "avg_logprob": -0.1274709463119507, "compression_ratio": 1.6878612716763006, "no_speech_prob": 5.59424961465993e-06}, {"id": 671, "seek": 299248, "start": 3018.38, "end": 3022.02, "text": " So predictions will be a tuple.", "tokens": [407, 21264, 486, 312, 257, 2604, 781, 13], "temperature": 0.0, "avg_logprob": -0.1274709463119507, "compression_ratio": 1.6878612716763006, "no_speech_prob": 5.59424961465993e-06}, {"id": 672, "seek": 302202, "start": 3022.02, "end": 3030.44, "text": " So this will be rice predictions and it will be disease predictions.", "tokens": [407, 341, 486, 312, 5090, 21264, 293, 309, 486, 312, 4752, 21264, 13], "temperature": 0.0, "avg_logprob": -0.163998196634014, "compression_ratio": 1.8087431693989071, "no_speech_prob": 4.785025794262765e-06}, {"id": 673, "seek": 302202, "start": 3030.44, "end": 3032.56, "text": " That'll be what's in our preds.", "tokens": [663, 603, 312, 437, 311, 294, 527, 3852, 82, 13], "temperature": 0.0, "avg_logprob": -0.163998196634014, "compression_ratio": 1.8087431693989071, "no_speech_prob": 4.785025794262765e-06}, {"id": 674, "seek": 302202, "start": 3032.56, "end": 3036.4, "text": " And so just to start with, let's just keep getting this, keep, get this so it keeps your", "tokens": [400, 370, 445, 281, 722, 365, 11, 718, 311, 445, 1066, 1242, 341, 11, 1066, 11, 483, 341, 370, 309, 5965, 428], "temperature": 0.0, "avg_logprob": -0.163998196634014, "compression_ratio": 1.8087431693989071, "no_speech_prob": 4.785025794262765e-06}, {"id": 675, "seek": 302202, "start": 3036.4, "end": 3038.6, "text": " works on disease predictions.", "tokens": [1985, 322, 4752, 21264, 13], "temperature": 0.0, "avg_logprob": -0.163998196634014, "compression_ratio": 1.8087431693989071, "no_speech_prob": 4.785025794262765e-06}, {"id": 676, "seek": 302202, "start": 3038.6, "end": 3044.92, "text": " So we'll just return whatever the current loss function was and we'll call it on the", "tokens": [407, 321, 603, 445, 2736, 2035, 264, 2190, 4470, 2445, 390, 293, 321, 603, 818, 309, 322, 264], "temperature": 0.0, "avg_logprob": -0.163998196634014, "compression_ratio": 1.8087431693989071, "no_speech_prob": 4.785025794262765e-06}, {"id": 677, "seek": 302202, "start": 3044.92, "end": 3047.96, "text": " disease predictions.", "tokens": [4752, 21264, 13], "temperature": 0.0, "avg_logprob": -0.163998196634014, "compression_ratio": 1.8087431693989071, "no_speech_prob": 4.785025794262765e-06}, {"id": 678, "seek": 302202, "start": 3047.96, "end": 3051.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.163998196634014, "compression_ratio": 1.8087431693989071, "no_speech_prob": 4.785025794262765e-06}, {"id": 679, "seek": 305108, "start": 3051.08, "end": 3070.16, "text": " So now we need to go learn to dot loss function is that function we just created.", "tokens": [407, 586, 321, 643, 281, 352, 1466, 281, 5893, 4470, 2445, 307, 300, 2445, 321, 445, 2942, 13], "temperature": 0.0, "avg_logprob": -0.4100302782925693, "compression_ratio": 1.1095890410958904, "no_speech_prob": 2.1110659872647375e-05}, {"id": 680, "seek": 307016, "start": 3070.16, "end": 3081.6, "text": " That's interesting.", "tokens": [663, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.2751409646236535, "compression_ratio": 1.4671052631578947, "no_speech_prob": 5.953516392764868e-06}, {"id": 681, "seek": 307016, "start": 3081.6, "end": 3088.16, "text": " Sorry when you did that to like set the current loss, set the gloss function and the learner", "tokens": [4919, 562, 291, 630, 300, 281, 411, 992, 264, 2190, 4470, 11, 992, 264, 19574, 2445, 293, 264, 33347], "temperature": 0.0, "avg_logprob": -0.2751409646236535, "compression_ratio": 1.4671052631578947, "no_speech_prob": 5.953516392764868e-06}, {"id": 682, "seek": 307016, "start": 3088.16, "end": 3092.24, "text": " didn't want this mess up your code.", "tokens": [994, 380, 528, 341, 2082, 493, 428, 3089, 13], "temperature": 0.0, "avg_logprob": -0.2751409646236535, "compression_ratio": 1.4671052631578947, "no_speech_prob": 5.953516392764868e-06}, {"id": 683, "seek": 307016, "start": 3092.24, "end": 3096.16, "text": " I guess like you need to create the learner again.", "tokens": [286, 2041, 411, 291, 643, 281, 1884, 264, 33347, 797, 13], "temperature": 0.0, "avg_logprob": -0.2751409646236535, "compression_ratio": 1.4671052631578947, "no_speech_prob": 5.953516392764868e-06}, {"id": 684, "seek": 307016, "start": 3096.16, "end": 3097.16, "text": " Nevermind.", "tokens": [7344, 13733, 13], "temperature": 0.0, "avg_logprob": -0.2751409646236535, "compression_ratio": 1.4671052631578947, "no_speech_prob": 5.953516392764868e-06}, {"id": 685, "seek": 307016, "start": 3097.16, "end": 3098.16, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.2751409646236535, "compression_ratio": 1.4671052631578947, "no_speech_prob": 5.953516392764868e-06}, {"id": 686, "seek": 307016, "start": 3098.16, "end": 3099.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2751409646236535, "compression_ratio": 1.4671052631578947, "no_speech_prob": 5.953516392764868e-06}, {"id": 687, "seek": 309916, "start": 3099.16, "end": 3107.2799999999997, "text": " January, do you want to go up a little bit back to your loss function?", "tokens": [7061, 11, 360, 291, 528, 281, 352, 493, 257, 707, 857, 646, 281, 428, 4470, 2445, 30], "temperature": 0.0, "avg_logprob": -0.3132078840925887, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.224596316111274e-05}, {"id": 688, "seek": 309916, "start": 3107.2799999999997, "end": 3111.04, "text": " Is it you actually want to pass the predict target?", "tokens": [1119, 309, 291, 767, 528, 281, 1320, 264, 6069, 3779, 30], "temperature": 0.0, "avg_logprob": -0.3132078840925887, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.224596316111274e-05}, {"id": 689, "seek": 309916, "start": 3111.04, "end": 3113.16, "text": " This is disease target.", "tokens": [639, 307, 4752, 3779, 13], "temperature": 0.0, "avg_logprob": -0.3132078840925887, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.224596316111274e-05}, {"id": 690, "seek": 309916, "start": 3113.16, "end": 3114.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3132078840925887, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.224596316111274e-05}, {"id": 691, "seek": 309916, "start": 3114.16, "end": 3115.64, "text": " No, no, sorry.", "tokens": [883, 11, 572, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.3132078840925887, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.224596316111274e-05}, {"id": 692, "seek": 309916, "start": 3115.64, "end": 3122.96, "text": " So I'm just going to ignore the rice type prediction for now and just try to get it", "tokens": [407, 286, 478, 445, 516, 281, 11200, 264, 5090, 2010, 17630, 337, 586, 293, 445, 853, 281, 483, 309], "temperature": 0.0, "avg_logprob": -0.3132078840925887, "compression_ratio": 1.4425287356321839, "no_speech_prob": 5.224596316111274e-05}, {"id": 693, "seek": 312296, "start": 3122.96, "end": 3130.56, "text": " our new thing working to continue to do exactly what it did before, but with this new structure", "tokens": [527, 777, 551, 1364, 281, 2354, 281, 360, 2293, 437, 309, 630, 949, 11, 457, 365, 341, 777, 3877], "temperature": 0.0, "avg_logprob": -0.18134603927384563, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.95443043494015e-06}, {"id": 694, "seek": 312296, "start": 3130.56, "end": 3131.56, "text": " around it.", "tokens": [926, 309, 13], "temperature": 0.0, "avg_logprob": -0.18134603927384563, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.95443043494015e-06}, {"id": 695, "seek": 312296, "start": 3131.56, "end": 3138.68, "text": " Do we have to split the targets as well?", "tokens": [1144, 321, 362, 281, 7472, 264, 12911, 382, 731, 30], "temperature": 0.0, "avg_logprob": -0.18134603927384563, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.95443043494015e-06}, {"id": 696, "seek": 312296, "start": 3138.68, "end": 3145.28, "text": " No, because at the moment our targets, we haven't included anything other than just", "tokens": [883, 11, 570, 412, 264, 1623, 527, 12911, 11, 321, 2378, 380, 5556, 1340, 661, 813, 445], "temperature": 0.0, "avg_logprob": -0.18134603927384563, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.95443043494015e-06}, {"id": 697, "seek": 312296, "start": 3145.28, "end": 3147.32, "text": " the diseases in the targets.", "tokens": [264, 11044, 294, 264, 12911, 13], "temperature": 0.0, "avg_logprob": -0.18134603927384563, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.95443043494015e-06}, {"id": 698, "seek": 314732, "start": 3147.32, "end": 3158.84, "text": " So yeah, we're going to have to change our data loading as well to include the rice type", "tokens": [407, 1338, 11, 321, 434, 516, 281, 362, 281, 1319, 527, 1412, 15114, 382, 731, 281, 4090, 264, 5090, 2010], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 699, "seek": 314732, "start": 3158.84, "end": 3161.92, "text": " as well, but we haven't done that yet.", "tokens": [382, 731, 11, 457, 321, 2378, 380, 1096, 300, 1939, 13], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 700, "seek": 314732, "start": 3161.92, "end": 3163.32, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 701, "seek": 314732, "start": 3163.32, "end": 3164.84, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 702, "seek": 314732, "start": 3164.84, "end": 3166.32, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 703, "seek": 314732, "start": 3166.32, "end": 3169.2000000000003, "text": " Ah, yes.", "tokens": [2438, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 704, "seek": 314732, "start": 3169.2000000000003, "end": 3170.84, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 705, "seek": 314732, "start": 3170.84, "end": 3176.2400000000002, "text": " So then we've got metrics.", "tokens": [407, 550, 321, 600, 658, 16367, 13], "temperature": 0.0, "avg_logprob": -0.3646579910727108, "compression_ratio": 1.3955223880597014, "no_speech_prob": 5.421766672952799e-06}, {"id": 706, "seek": 317624, "start": 3176.24, "end": 3179.08, "text": " So metrics are the things that just get printed out as you go.", "tokens": [407, 16367, 366, 264, 721, 300, 445, 483, 13567, 484, 382, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 707, "seek": 317624, "start": 3179.08, "end": 3182.9199999999996, "text": " And we don't yet have a metric that works on this.", "tokens": [400, 321, 500, 380, 1939, 362, 257, 20678, 300, 1985, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 708, "seek": 317624, "start": 3182.9199999999996, "end": 3192.24, "text": " So a very easy way to fix that is just to remove metrics for now.", "tokens": [407, 257, 588, 1858, 636, 281, 3191, 300, 307, 445, 281, 4159, 16367, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 709, "seek": 317624, "start": 3192.24, "end": 3197.04, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 710, "seek": 317624, "start": 3197.04, "end": 3199.24, "text": " Now preds.shape shouldn't work.", "tokens": [823, 3852, 82, 13, 82, 42406, 4659, 380, 589, 13], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 711, "seek": 317624, "start": 3199.24, "end": 3200.24, "text": " Good.", "tokens": [2205, 13], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 712, "seek": 317624, "start": 3200.24, "end": 3203.3999999999996, "text": " It doesn't because now we've got two sets of predictions.", "tokens": [467, 1177, 380, 570, 586, 321, 600, 658, 732, 6352, 295, 21264, 13], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 713, "seek": 317624, "start": 3203.3999999999996, "end": 3204.64, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.2700924555460612, "compression_ratio": 1.4896907216494846, "no_speech_prob": 3.966942131228279e-06}, {"id": 714, "seek": 320464, "start": 3204.64, "end": 3211.08, "text": " We've got a tuple because that's the predictions is just whatever the model creates.", "tokens": [492, 600, 658, 257, 2604, 781, 570, 300, 311, 264, 21264, 307, 445, 2035, 264, 2316, 7829, 13], "temperature": 0.0, "avg_logprob": -0.16917812063338908, "compression_ratio": 1.5121951219512195, "no_speech_prob": 1.2606345990207046e-05}, {"id": 715, "seek": 320464, "start": 3211.08, "end": 3214.44, "text": " The model is creating two things, not one.", "tokens": [440, 2316, 307, 4084, 732, 721, 11, 406, 472, 13], "temperature": 0.0, "avg_logprob": -0.16917812063338908, "compression_ratio": 1.5121951219512195, "no_speech_prob": 1.2606345990207046e-05}, {"id": 716, "seek": 320464, "start": 3214.44, "end": 3231.92, "text": " So we've now got rice predictions and disease predictions.", "tokens": [407, 321, 600, 586, 658, 5090, 21264, 293, 4752, 21264, 13], "temperature": 0.0, "avg_logprob": -0.16917812063338908, "compression_ratio": 1.5121951219512195, "no_speech_prob": 1.2606345990207046e-05}, {"id": 717, "seek": 323192, "start": 3231.92, "end": 3237.32, "text": " So that's actually pretty good progress, I think.", "tokens": [407, 300, 311, 767, 1238, 665, 4205, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.1851497773201235, "compression_ratio": 1.6589861751152073, "no_speech_prob": 8.800312571111135e-06}, {"id": 718, "seek": 323192, "start": 3237.32, "end": 3242.12, "text": " But you know, like for those of you who are involved in fast AI development, you know,", "tokens": [583, 291, 458, 11, 411, 337, 729, 295, 291, 567, 366, 3288, 294, 2370, 7318, 3250, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1851497773201235, "compression_ratio": 1.6589861751152073, "no_speech_prob": 8.800312571111135e-06}, {"id": 719, "seek": 323192, "start": 3242.12, "end": 3247.96, "text": " it's pretty clear to me and try to do this that this is far harder than it should be", "tokens": [309, 311, 1238, 1850, 281, 385, 293, 853, 281, 360, 341, 300, 341, 307, 1400, 6081, 813, 309, 820, 312], "temperature": 0.0, "avg_logprob": -0.1851497773201235, "compression_ratio": 1.6589861751152073, "no_speech_prob": 8.800312571111135e-06}, {"id": 720, "seek": 323192, "start": 3247.96, "end": 3251.16, "text": " and it feels like something that should be easy to do.", "tokens": [293, 309, 3417, 411, 746, 300, 820, 312, 1858, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1851497773201235, "compression_ratio": 1.6589861751152073, "no_speech_prob": 8.800312571111135e-06}, {"id": 721, "seek": 323192, "start": 3251.16, "end": 3261.6800000000003, "text": " I used to see Andrew using the magic, the percentage and then have a patch and then", "tokens": [286, 1143, 281, 536, 10110, 1228, 264, 5585, 11, 264, 9668, 293, 550, 362, 257, 9972, 293, 550], "temperature": 0.0, "avg_logprob": -0.1851497773201235, "compression_ratio": 1.6589861751152073, "no_speech_prob": 8.800312571111135e-06}, {"id": 722, "seek": 326168, "start": 3261.68, "end": 3265.96, "text": " just get some little thing on top of it.", "tokens": [445, 483, 512, 707, 551, 322, 1192, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.19146328706007737, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.068102498422377e-05}, {"id": 723, "seek": 326168, "start": 3265.96, "end": 3269.48, "text": " Yeah, it's not so much about patching.", "tokens": [865, 11, 309, 311, 406, 370, 709, 466, 9972, 278, 13], "temperature": 0.0, "avg_logprob": -0.19146328706007737, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.068102498422377e-05}, {"id": 724, "seek": 326168, "start": 3269.48, "end": 3276.7599999999998, "text": " It's about I feel like there might even be some multi loss thing.", "tokens": [467, 311, 466, 286, 841, 411, 456, 1062, 754, 312, 512, 4825, 4470, 551, 13], "temperature": 0.0, "avg_logprob": -0.19146328706007737, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.068102498422377e-05}, {"id": 725, "seek": 326168, "start": 3276.7599999999998, "end": 3282.9199999999996, "text": " If there's not, I feel like this is something we should add to fast AI to make it easier.", "tokens": [759, 456, 311, 406, 11, 286, 841, 411, 341, 307, 746, 321, 820, 909, 281, 2370, 7318, 281, 652, 309, 3571, 13], "temperature": 0.0, "avg_logprob": -0.19146328706007737, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.068102498422377e-05}, {"id": 726, "seek": 326168, "start": 3282.9199999999996, "end": 3287.64, "text": " Can you explain a little bit about why the loss is stored in the data loader?", "tokens": [1664, 291, 2903, 257, 707, 857, 466, 983, 264, 4470, 307, 12187, 294, 264, 1412, 3677, 260, 30], "temperature": 0.0, "avg_logprob": -0.19146328706007737, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.068102498422377e-05}, {"id": 727, "seek": 326168, "start": 3287.64, "end": 3290.2799999999997, "text": " Like how that is a good thing?", "tokens": [1743, 577, 300, 307, 257, 665, 551, 30], "temperature": 0.0, "avg_logprob": -0.19146328706007737, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.068102498422377e-05}, {"id": 728, "seek": 326168, "start": 3290.2799999999997, "end": 3291.3999999999996, "text": " Yeah, sure.", "tokens": [865, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.19146328706007737, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.068102498422377e-05}, {"id": 729, "seek": 329140, "start": 3291.4, "end": 3298.48, "text": " So generally speaking, what is the appropriate loss function to use or at least a reasonable", "tokens": [407, 5101, 4124, 11, 437, 307, 264, 6854, 4470, 2445, 281, 764, 420, 412, 1935, 257, 10585], "temperature": 0.0, "avg_logprob": -0.12052036193479974, "compression_ratio": 1.6342592592592593, "no_speech_prob": 9.368125574837904e-06}, {"id": 730, "seek": 329140, "start": 3298.48, "end": 3303.14, "text": " default depends on what kind of data you have.", "tokens": [7576, 5946, 322, 437, 733, 295, 1412, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.12052036193479974, "compression_ratio": 1.6342592592592593, "no_speech_prob": 9.368125574837904e-06}, {"id": 731, "seek": 329140, "start": 3303.14, "end": 3308.36, "text": " So if your data is, you know, a single continuous output, you probably have a regression problems.", "tokens": [407, 498, 428, 1412, 307, 11, 291, 458, 11, 257, 2167, 10957, 5598, 11, 291, 1391, 362, 257, 24590, 2740, 13], "temperature": 0.0, "avg_logprob": -0.12052036193479974, "compression_ratio": 1.6342592592592593, "no_speech_prob": 9.368125574837904e-06}, {"id": 732, "seek": 329140, "start": 3308.36, "end": 3311.0, "text": " You probably want mean squared error.", "tokens": [509, 1391, 528, 914, 8889, 6713, 13], "temperature": 0.0, "avg_logprob": -0.12052036193479974, "compression_ratio": 1.6342592592592593, "no_speech_prob": 9.368125574837904e-06}, {"id": 733, "seek": 329140, "start": 3311.0, "end": 3315.88, "text": " If it's a single categorical variable, you probably want cross entropy loss.", "tokens": [759, 309, 311, 257, 2167, 19250, 804, 7006, 11, 291, 1391, 528, 3278, 30867, 4470, 13], "temperature": 0.0, "avg_logprob": -0.12052036193479974, "compression_ratio": 1.6342592592592593, "no_speech_prob": 9.368125574837904e-06}, {"id": 734, "seek": 331588, "start": 3315.88, "end": 3326.36, "text": " If you have a multi categorical variable, you know, you probably want that log loss", "tokens": [759, 291, 362, 257, 4825, 19250, 804, 7006, 11, 291, 458, 11, 291, 1391, 528, 300, 3565, 4470], "temperature": 0.0, "avg_logprob": -0.1910416152742174, "compression_ratio": 1.5402298850574712, "no_speech_prob": 4.565440576698165e-06}, {"id": 735, "seek": 331588, "start": 3326.36, "end": 3328.76, "text": " without the soft max and so forth.", "tokens": [1553, 264, 2787, 11469, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1910416152742174, "compression_ratio": 1.5402298850574712, "no_speech_prob": 4.565440576698165e-06}, {"id": 736, "seek": 331588, "start": 3328.76, "end": 3334.36, "text": " So yeah, basically by having it come from the from the data set means that you can get", "tokens": [407, 1338, 11, 1936, 538, 1419, 309, 808, 490, 264, 490, 264, 1412, 992, 1355, 300, 291, 393, 483], "temperature": 0.0, "avg_logprob": -0.1910416152742174, "compression_ratio": 1.5402298850574712, "no_speech_prob": 4.565440576698165e-06}, {"id": 737, "seek": 331588, "start": 3334.36, "end": 3339.7200000000003, "text": " sensible defaults that ought to work for that data set.", "tokens": [25380, 7576, 82, 300, 13416, 281, 589, 337, 300, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1910416152742174, "compression_ratio": 1.5402298850574712, "no_speech_prob": 4.565440576698165e-06}, {"id": 738, "seek": 331588, "start": 3339.7200000000003, "end": 3343.36, "text": " I see.", "tokens": [286, 536, 13], "temperature": 0.0, "avg_logprob": -0.1910416152742174, "compression_ratio": 1.5402298850574712, "no_speech_prob": 4.565440576698165e-06}, {"id": 739, "seek": 334336, "start": 3343.36, "end": 3347.36, "text": " So that's why we generally most of the time don't have to specify what loss function to", "tokens": [407, 300, 311, 983, 321, 5101, 881, 295, 264, 565, 500, 380, 362, 281, 16500, 437, 4470, 2445, 281], "temperature": 0.0, "avg_logprob": -0.20375540415445964, "compression_ratio": 1.4751381215469612, "no_speech_prob": 5.254992174741346e-06}, {"id": 740, "seek": 334336, "start": 3347.36, "end": 3355.4, "text": " use unless we're doing something kind of nonstandard.", "tokens": [764, 5969, 321, 434, 884, 746, 733, 295, 2107, 1115, 515, 13], "temperature": 0.0, "avg_logprob": -0.20375540415445964, "compression_ratio": 1.4751381215469612, "no_speech_prob": 5.254992174741346e-06}, {"id": 741, "seek": 334336, "start": 3355.4, "end": 3361.8, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.20375540415445964, "compression_ratio": 1.4751381215469612, "no_speech_prob": 5.254992174741346e-06}, {"id": 742, "seek": 334336, "start": 3361.8, "end": 3365.6800000000003, "text": " So we're about to wrap up.", "tokens": [407, 321, 434, 466, 281, 7019, 493, 13], "temperature": 0.0, "avg_logprob": -0.20375540415445964, "compression_ratio": 1.4751381215469612, "no_speech_prob": 5.254992174741346e-06}, {"id": 743, "seek": 334336, "start": 3365.6800000000003, "end": 3370.76, "text": " The last thing I think I might do is just try to put this back and we can do it exactly", "tokens": [440, 1036, 551, 286, 519, 286, 1062, 360, 307, 445, 853, 281, 829, 341, 646, 293, 321, 393, 360, 309, 2293], "temperature": 0.0, "avg_logprob": -0.20375540415445964, "compression_ratio": 1.4751381215469612, "no_speech_prob": 5.254992174741346e-06}, {"id": 744, "seek": 337076, "start": 3370.76, "end": 3384.88, "text": " the same way, which is to say DTC error.", "tokens": [264, 912, 636, 11, 597, 307, 281, 584, 413, 18238, 6713, 13], "temperature": 0.0, "avg_logprob": -0.3705971908569336, "compression_ratio": 0.9428571428571428, "no_speech_prob": 1.7491705875727348e-05}, {"id": 745, "seek": 337076, "start": 3384.88, "end": 3398.6800000000003, "text": " So pop this a bit higher.", "tokens": [407, 1665, 341, 257, 857, 2946, 13], "temperature": 0.0, "avg_logprob": -0.3705971908569336, "compression_ratio": 0.9428571428571428, "no_speech_prob": 1.7491705875727348e-05}, {"id": 746, "seek": 339868, "start": 3398.68, "end": 3406.68, "text": " So we'll just return error rate on the disease predictions.", "tokens": [407, 321, 603, 445, 2736, 6713, 3314, 322, 264, 4752, 21264, 13], "temperature": 0.0, "avg_logprob": -0.6187118407218687, "compression_ratio": 1.141304347826087, "no_speech_prob": 9.515400051895995e-06}, {"id": 747, "seek": 339868, "start": 3406.68, "end": 3422.56, "text": " Learn two dot metrics equals DTC error.", "tokens": [17216, 732, 5893, 16367, 6915, 413, 18238, 6713, 13], "temperature": 0.0, "avg_logprob": -0.6187118407218687, "compression_ratio": 1.141304347826087, "no_speech_prob": 9.515400051895995e-06}, {"id": 748, "seek": 339868, "start": 3422.56, "end": 3425.08, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.6187118407218687, "compression_ratio": 1.141304347826087, "no_speech_prob": 9.515400051895995e-06}, {"id": 749, "seek": 342508, "start": 3425.08, "end": 3442.2, "text": " So I guess we should now be able to do things like learn two dot LR find, for example.", "tokens": [407, 286, 2041, 321, 820, 586, 312, 1075, 281, 360, 721, 411, 1466, 732, 5893, 441, 49, 915, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.20104959275987414, "compression_ratio": 1.3902439024390243, "no_speech_prob": 8.013221304281615e-06}, {"id": 750, "seek": 342508, "start": 3442.2, "end": 3451.68, "text": " And this should we should be able to just replicate our disease model at this point.", "tokens": [400, 341, 820, 321, 820, 312, 1075, 281, 445, 25356, 527, 4752, 2316, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.20104959275987414, "compression_ratio": 1.3902439024390243, "no_speech_prob": 8.013221304281615e-06}, {"id": 751, "seek": 345168, "start": 3451.68, "end": 3456.3999999999996, "text": " Because we're not doing anything with this extra rice type thing yet.", "tokens": [1436, 321, 434, 406, 884, 1340, 365, 341, 2857, 5090, 2010, 551, 1939, 13], "temperature": 0.0, "avg_logprob": -0.5149926906678735, "compression_ratio": 1.1875, "no_speech_prob": 4.6829256461933255e-05}, {"id": 752, "seek": 345168, "start": 3456.3999999999996, "end": 3459.52, "text": " And fine tune.", "tokens": [400, 2489, 10864, 13], "temperature": 0.0, "avg_logprob": -0.5149926906678735, "compression_ratio": 1.1875, "no_speech_prob": 4.6829256461933255e-05}, {"id": 753, "seek": 345168, "start": 3459.52, "end": 3462.52, "text": " One epoch.", "tokens": [1485, 30992, 339, 13], "temperature": 0.0, "avg_logprob": -0.5149926906678735, "compression_ratio": 1.1875, "no_speech_prob": 4.6829256461933255e-05}, {"id": 754, "seek": 345168, "start": 3462.52, "end": 3480.7999999999997, "text": " 0.05 say 0.01 say.", "tokens": [1958, 13, 13328, 584, 1958, 13, 10607, 584, 13], "temperature": 0.0, "avg_logprob": -0.5149926906678735, "compression_ratio": 1.1875, "no_speech_prob": 4.6829256461933255e-05}, {"id": 755, "seek": 348080, "start": 3480.8, "end": 3487.48, "text": " While I wait for that, let's see if I search like fast AI multiple loss function or something.", "tokens": [3987, 286, 1699, 337, 300, 11, 718, 311, 536, 498, 286, 3164, 411, 2370, 7318, 3866, 4470, 2445, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.4935500518135402, "compression_ratio": 1.2222222222222223, "no_speech_prob": 2.668655179149937e-05}, {"id": 756, "seek": 348080, "start": 3487.48, "end": 3495.0800000000004, "text": " 2018 is going to be too long ago.", "tokens": [6096, 307, 516, 281, 312, 886, 938, 2057, 13], "temperature": 0.0, "avg_logprob": -0.4935500518135402, "compression_ratio": 1.2222222222222223, "no_speech_prob": 2.668655179149937e-05}, {"id": 757, "seek": 348080, "start": 3495.0800000000004, "end": 3496.0800000000004, "text": " Functions.", "tokens": [11166, 3916, 13], "temperature": 0.0, "avg_logprob": -0.4935500518135402, "compression_ratio": 1.2222222222222223, "no_speech_prob": 2.668655179149937e-05}, {"id": 758, "seek": 348080, "start": 3496.0800000000004, "end": 3501.0800000000004, "text": " Nothing there.", "tokens": [6693, 456, 13], "temperature": 0.0, "avg_logprob": -0.4935500518135402, "compression_ratio": 1.2222222222222223, "no_speech_prob": 2.668655179149937e-05}, {"id": 759, "seek": 350108, "start": 3501.08, "end": 3512.7599999999998, "text": " Multitask learning.", "tokens": [14665, 270, 3863, 2539, 13], "temperature": 0.0, "avg_logprob": -0.45569956579873727, "compression_ratio": 1.201834862385321, "no_speech_prob": 2.2116848413133994e-05}, {"id": 760, "seek": 350108, "start": 3512.7599999999998, "end": 3516.24, "text": " I got to go, but yeah, thanks a lot.", "tokens": [286, 658, 281, 352, 11, 457, 1338, 11, 3231, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.45569956579873727, "compression_ratio": 1.201834862385321, "no_speech_prob": 2.2116848413133994e-05}, {"id": 761, "seek": 350108, "start": 3516.24, "end": 3517.96, "text": " No worries.", "tokens": [883, 16340, 13], "temperature": 0.0, "avg_logprob": -0.45569956579873727, "compression_ratio": 1.201834862385321, "no_speech_prob": 2.2116848413133994e-05}, {"id": 762, "seek": 350108, "start": 3517.96, "end": 3526.04, "text": " OK, so it looks like this person did something pretty similar.", "tokens": [2264, 11, 370, 309, 1542, 411, 341, 954, 630, 746, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.45569956579873727, "compression_ratio": 1.201834862385321, "no_speech_prob": 2.2116848413133994e-05}, {"id": 763, "seek": 352604, "start": 3526.04, "end": 3532.16, "text": " They created their own little multitask loss wrapper.", "tokens": [814, 2942, 641, 1065, 707, 42338, 3863, 4470, 46906, 13], "temperature": 0.0, "avg_logprob": -0.267691691716512, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.295543127838755e-06}, {"id": 764, "seek": 352604, "start": 3532.16, "end": 3533.16, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.267691691716512, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.295543127838755e-06}, {"id": 765, "seek": 352604, "start": 3533.16, "end": 3536.12, "text": " Well, I think we're at a good place to stop.", "tokens": [1042, 11, 286, 519, 321, 434, 412, 257, 665, 1081, 281, 1590, 13], "temperature": 0.0, "avg_logprob": -0.267691691716512, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.295543127838755e-06}, {"id": 766, "seek": 352604, "start": 3536.12, "end": 3539.04, "text": " That's we've got back.", "tokens": [663, 311, 321, 600, 658, 646, 13], "temperature": 0.0, "avg_logprob": -0.267691691716512, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.295543127838755e-06}, {"id": 767, "seek": 352604, "start": 3539.04, "end": 3540.88, "text": " So it's not totally broken.", "tokens": [407, 309, 311, 406, 3879, 5463, 13], "temperature": 0.0, "avg_logprob": -0.267691691716512, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.295543127838755e-06}, {"id": 768, "seek": 352604, "start": 3540.88, "end": 3545.2799999999997, "text": " So that's good.", "tokens": [407, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.267691691716512, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.295543127838755e-06}, {"id": 769, "seek": 352604, "start": 3545.2799999999997, "end": 3551.8, "text": " And next time we will try and plug this stuff in.", "tokens": [400, 958, 565, 321, 486, 853, 293, 5452, 341, 1507, 294, 13], "temperature": 0.0, "avg_logprob": -0.267691691716512, "compression_ratio": 1.3726708074534162, "no_speech_prob": 7.295543127838755e-06}, {"id": 770, "seek": 355180, "start": 3551.8, "end": 3556.36, "text": " We have any questions or anything before we wrap up?", "tokens": [492, 362, 604, 1651, 420, 1340, 949, 321, 7019, 493, 30], "temperature": 0.0, "avg_logprob": -0.3190747774564303, "compression_ratio": 1.6175298804780875, "no_speech_prob": 1.669564881012775e-05}, {"id": 771, "seek": 355180, "start": 3556.36, "end": 3558.4, "text": " Just a quick question, Jeremy.", "tokens": [1449, 257, 1702, 1168, 11, 17809, 13], "temperature": 0.0, "avg_logprob": -0.3190747774564303, "compression_ratio": 1.6175298804780875, "no_speech_prob": 1.669564881012775e-05}, {"id": 772, "seek": 355180, "start": 3558.4, "end": 3564.32, "text": " It says the valley is point zero zero one, but you use point zero one for the fine tune.", "tokens": [467, 1619, 264, 17636, 307, 935, 4018, 4018, 472, 11, 457, 291, 764, 935, 4018, 472, 337, 264, 2489, 10864, 13], "temperature": 0.0, "avg_logprob": -0.3190747774564303, "compression_ratio": 1.6175298804780875, "no_speech_prob": 1.669564881012775e-05}, {"id": 773, "seek": 355180, "start": 3564.32, "end": 3566.1600000000003, "text": " Yeah, I'm not sure.", "tokens": [865, 11, 286, 478, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.3190747774564303, "compression_ratio": 1.6175298804780875, "no_speech_prob": 1.669564881012775e-05}, {"id": 774, "seek": 355180, "start": 3566.1600000000003, "end": 3570.4, "text": " This is pretty this is it's picked out something pretty early in the curve.", "tokens": [639, 307, 1238, 341, 307, 309, 311, 6183, 484, 746, 1238, 2440, 294, 264, 7605, 13], "temperature": 0.0, "avg_logprob": -0.3190747774564303, "compression_ratio": 1.6175298804780875, "no_speech_prob": 1.669564881012775e-05}, {"id": 775, "seek": 355180, "start": 3570.4, "end": 3573.84, "text": " I thought something down here seems more reasonable.", "tokens": [286, 1194, 746, 760, 510, 2544, 544, 10585, 13], "temperature": 0.0, "avg_logprob": -0.3190747774564303, "compression_ratio": 1.6175298804780875, "no_speech_prob": 1.669564881012775e-05}, {"id": 776, "seek": 355180, "start": 3573.84, "end": 3580.44, "text": " Just by bowling it, you know, it tends to recommend like rather conservative values.", "tokens": [1449, 538, 35537, 309, 11, 291, 458, 11, 309, 12258, 281, 2748, 411, 2831, 13780, 4190, 13], "temperature": 0.0, "avg_logprob": -0.3190747774564303, "compression_ratio": 1.6175298804780875, "no_speech_prob": 1.669564881012775e-05}, {"id": 777, "seek": 358044, "start": 3580.44, "end": 3586.88, "text": " So, yeah, I tend to kind of look for the bit that's I kind of look for the bit that's as", "tokens": [407, 11, 1338, 11, 286, 3928, 281, 733, 295, 574, 337, 264, 857, 300, 311, 286, 733, 295, 574, 337, 264, 857, 300, 311, 382], "temperature": 0.0, "avg_logprob": -0.20740877615438924, "compression_ratio": 1.5069444444444444, "no_speech_prob": 2.1760290110250935e-05}, {"id": 778, "seek": 358044, "start": 3586.88, "end": 3591.68, "text": " far to the right as possible, but still looks pretty steep gradient.", "tokens": [1400, 281, 264, 558, 382, 1944, 11, 457, 920, 1542, 1238, 16841, 16235, 13], "temperature": 0.0, "avg_logprob": -0.20740877615438924, "compression_ratio": 1.5069444444444444, "no_speech_prob": 2.1760290110250935e-05}, {"id": 779, "seek": 358044, "start": 3591.68, "end": 3594.68, "text": " I guess it's OK.", "tokens": [286, 2041, 309, 311, 2264, 13], "temperature": 0.0, "avg_logprob": -0.20740877615438924, "compression_ratio": 1.5069444444444444, "no_speech_prob": 2.1760290110250935e-05}, {"id": 780, "seek": 358044, "start": 3594.68, "end": 3595.68, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.20740877615438924, "compression_ratio": 1.5069444444444444, "no_speech_prob": 2.1760290110250935e-05}, {"id": 781, "seek": 358044, "start": 3595.68, "end": 3596.68, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.20740877615438924, "compression_ratio": 1.5069444444444444, "no_speech_prob": 2.1760290110250935e-05}, {"id": 782, "seek": 358044, "start": 3596.68, "end": 3597.68, "text": " Again.", "tokens": [3764, 13], "temperature": 0.0, "avg_logprob": -0.20740877615438924, "compression_ratio": 1.5069444444444444, "no_speech_prob": 2.1760290110250935e-05}, {"id": 783, "seek": 358044, "start": 3597.68, "end": 3598.68, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.20740877615438924, "compression_ratio": 1.5069444444444444, "no_speech_prob": 2.1760290110250935e-05}, {"id": 784, "seek": 359868, "start": 3598.68, "end": 3612.9199999999996, "text": " Yeah.", "tokens": [50364, 865, 13, 51076], "temperature": 0.0, "avg_logprob": -0.5787811279296875, "compression_ratio": 0.38461538461538464, "no_speech_prob": 5.1351715228520334e-05}], "language": "en"}