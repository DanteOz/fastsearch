{"text": " So I wanted to start off by showing you something I'm kind of excited about, which is here is the dogs and cats competition, which we all know so well. It was interesting that the winner of this competition won by a very big margin, a 1.1% error versus a 1.7% error. This is very unusual in a Kaggle competition to see anybody win by a 50-60% margin. You can see after that people are generally clustering around 91.1, 91.9, 91.1, 91.8, about the same kind of number. So this was a pretty impressive performance. This is the guy who actually created a piece of deep learning software called Overfee. So I want to show you something pretty interesting, which is this week I tried something new and on dogs and cats got 98.95. So I want to show you how I did that. The way I did that was by using nearly only techniques I've already shown you, which is basically I created a standard model, which is basically a dense model, and then I pre-computed the last convolutional layer and then I trained the dense model lots of times. The other thing I did was to use some data augmentation, and I didn't actually have time to figure out the best data augmentation parameters, so I just picked some that seemed reasonable. I should also mention this 98.95 would be easy to make a lot better. I'm not doing any pseudo-labeling here and I'm not even using the full dataset. I put aside 2000 for the validation set. So with those two changes, we would definitely get well over 99% accuracy. The missing piece that I added is I added batch normalization to VGG. So batch normalization, if you guys remember, I said the important takeaway is that all modern networks should use batch norm because you can get 10x or more improvements in training speed and it tends to reduce overfitting. Because of the second one, it means you can use less dropout, and dropout of course is destroying some of your network, so you don't want to use more dropout than necessary. So why didn't VGG already have batch norm? Because it didn't exist. So VGG was kind of mid-to-late 2014 and batch norm was maybe early-to-mid 2015. So why haven't people added batch norm to VGG already? And the answer is actually interesting to think about. So to remind you what batch norm is, batch norm is something which first of all normalizes every intermediate layer. So it normalizes all of the activations by subtracting the mean and dividing by the standard deviation, which is always a good idea. And I know somebody on the forum today asked why is it a good idea, and I put a link to some more information about that. So anybody who wants to know more about why do normalization, check out the forum. But just doing that alone isn't enough because SGD is quite bloody-minded. And so if it was trying to denormalize the activations because it thought that was a good thing to do, it would do so anyway. So every time you try to normalize them, SGD would just undo it again. What batch norm does is it adds two additional trainable parameters to each layer, one which multiplies the activations and one which is added to the activations. So it basically allows it to undo the normalization, but not by changing every single weight, but by just changing two weights for each activation. So it makes things much more stable in practice. So you can't just go ahead and stick batch norm into a pre-trained network, because if you do, it's going to take that layer and it's going to subtract the mean and divide by the standard deviation, which means now those pre-trained weights from then on are now wrong because those weights were created for a completely different set of activations. So it's not rocket science, but I realized all we need to do is to insert a batch norm layer and figure out what the mean and standard deviation of the incoming activations would be for that data set and basically create the batch norm layer such that the two trainable parameters immediately undo that. That way we would insert a batch norm layer and it would not change the outputs at all. So I grabbed the whole of ImageNet and I created this standard dense layer model. I pre-computed the convolutional outputs for all of ImageNet and then I created two batch norm layers and I created a little function which allows us to insert a layer into an existing model. I inserted the layers just after the two dense layers. And then here's the key piece. I set the weights on the new batch norm layers equal to the variance and the mean, which I calculated on all of ImageNet. So I calculated the mean of each of those two layer outputs and the variance of each of those two layer outputs. So that allowed me to insert these batch norm layers into an existing model. And then afterwards I evaluated it and I checked that indeed it's giving me the same answers as it was before. As well as doing that, I then thought if you train a model with batch norm from the start, you're going to end up with weights which are designed to take advantage of the fact that the activations are being normalized. So I thought I wonder what would happen if we now fine-tuned the ImageNet network on all of ImageNet after we added these batch norm layers. So I then tried training it for one epoch on both the ImageNet images and the horizontally-flipped ImageNet images. So that's what these 2.5 million here are. You can see with modern GPUs, it only takes less than an hour to run the entirety of ImageNet twice. And the interesting thing was that my accuracy on the validation set went up from 63% to 67%. So adding batch norm actually improves ImageNet, which is cool. That wasn't the main reason I did it, the main reason I did it was so we can now use VGG with batch norm in our models. So I did all that, I saved the weights, I then edited our VGG model. So if we now look at the fully-connected block in our VGG model, it now has batch norm in there. I also saved to our website a new weights file called VGG16.bn for batch norm. So then when I did cats and dogs, I used that model. So now if you go and re-download from platform.ai the VGG16.py, it will automatically download the new weights. You will have this without any changes to your code. So I'd be interested to hear during the week if you try this out, just rerun the code you've got, whether you see improvements. Hopefully you will. Hopefully you'll find it trains more quickly and you get better results. At this stage, I've only added batch norm to the dense layers, not to the convolutional layers. There's no reason I shouldn't add it to the convolutional layers as well, I just had other things to do this week. But hopefully, since most of us are mainly fine-tuning just the dense layers, this is going to impact most of us the most anyway. So that's an exciting step which everybody can now use. The other thing to mention is now that you'll be using batch norm by default in your VGG networks, you should find that you can increase your learning rates. Because batch norm normalizes the activations, it makes sure that there's no activation that's gone really high or really low. That means that generally speaking, you can use higher learning rates. So if you try higher learning rates in your code than you were before, you should find that they work pretty well. You should also find that things that previously you couldn't get to train, now will start to train. Because often the reason that they don't train is because one of the activations shoots off into really high or really low and screws everything up, and that gets fixed when you use batch norm. So there's some things to try this week. I'll be interested to hear how you go. So last week we looked at collaborative filtering. And to remind you, we had a file that basically meant something like this. We had a bunch of movies and a bunch of users, and for some subset of those combinations, we had a review of that movie by that user. The way the actual file came to us didn't look like this, this is a crosstab. The way the file came to us looked like this. Each row was a single user rating a single movie with a single rating at a single time. So I showed you in Excel how we could take the crosstab version and we could create a table of dot products, where the dot products would be between a set of 5 random numbers for the movie and 5 random numbers for the user. And we could then use gradient descent to optimize those sets of 5 random numbers for every user and every movie. And if we did so, we end up getting pretty decent guesses as to the original ratings. And then we went a step further in the spreadsheet and we learned how you could take the dot product and you could also add on a single bias, a movie bias and a user bias. So we saw all that in Excel. And we also learned that Excel comes with a gradient descent solver called, funnily enough, Solver. And we saw that if we ran Solver on telling it that these are our varying cells and this is our target cell, then it came up with some pretty decent weight matrices. We learned that these kinds of weight matrices are called embeddings. An embedding is basically something where we can start with an integer, like 27, and look up movie number 27's vector of weights. That's called an embedding. It's also in collaborative filtering, this particular kind of embedding are known as latent factors. We hypothesized that once trained, each of these latent factors may mean something. And I said next week we might come back and have a look and see if we can figure out what they mean. So that was what I thought I would do now. So I'm going to take the bias model that we created. And the bias model we created was the one where we took a user embedding and a movie embedding and we took the dot product of the two and then we added to it a user bias and a movie bias. Those biases are just embeddings which have a single output. Just like in Excel, the bias was a single cell for each movie and a single cell for each user. So then we tried fitting that model and you might remember that we ended up getting an accuracy that was quite a bit higher than previous state-of-the-art. Actually, for that one we didn't. For the previous state-of-the-art, we broke by using the neural network. I discovered something interesting during the week, which is that I can get a state-of-the-art result using just this simple bias model. And the trick was that I just had to increase my regularization. So we haven't talked too much about regularization, we've briefly mentioned it a couple of times, but it's a very simple thing where we can basically say add to the loss function the sum of the squares of the weights. So we're trying to minimize the loss, so if you're adding the sum of the squares of the weights to the loss function, then the SGD solver is going to have to try to avoid increasing the weights where it can. And so we can pass to most Keras layers a parameter called wRegularizer, that stands for weight regularizer, and we can tell it how to regularize our weights. In this case, I say use an L2 norm, that means sum of the squares of how much, and that's something that I pass in, and I used 1a neg 4. And it turns out if I do that and then I train it for a while, it takes quite a lot longer to train, I got down to a loss of .7979, which is quite a bit better than the best results that that Stanford paper showed. It's not quite as good as the neural net, the neural net got 7.938 at best. But it's still interesting that this very simple approach actually gets results better than the academic state-of-the-art as of 2012 or 2013. I haven't been able to find more recent academic benchmarks than that. So I took this model and I wanted to find out what we can learn from these results. So obviously one thing we would do with this model is just to make predictions with it. So if you're building a website for recommending movies and a new user came along and said, I like these movies this much, what else would you recommend? You could just go through and do a prediction for each movie for that user ID and tell them which ones have the highest numbers. That's the normal way we would use collaborative filtering. We can do some other things. We can grab the top 2000 most popular movies, just to make this more interesting, and we can say let's just grab the bias term. I'll talk more about this particular syntax in just a moment, but just for now, this is a particularly simple kind of model. It's a model which simply takes the movie ID in and returns the movie bias out. In other words, it does a lookup in the movie bias table and returns the movie bias indexed by this movie ID. That's what these two lines do. I then combine that bias with the actual name of each rating and print out the top and bottom 15. According to MovieLens, the worst movie of all time is the Church of Scientology classic Battlefield Earth. This is interesting because these ratings are quite a lot more sophisticated than your average movie rating. What this is saying is that these have been normalized for some reviewers are more positive and negative than others. Some people are watching better or crappier films than others. So this bias is removing all of that noise and really telling us after removing all of that noise, these are the least good movies. Battlefield Earth is even worse than Spice World by a significant margin. On the other hand, here are the best. Miyazaki fans will be pleased to see Hell's Moving Castle at number 2. So that's interesting. Perhaps what's more interesting is to try and figure out what's going on, not in the biases but in the latent factors. The latent factors are a little bit harder to interpret because for every movie, we have 50 of them. In the Excel spreadsheet, we have 5. In our version, we have 50 of them. So what we want to do is we want to take from those 50 latent factors, we want to find 2 or 3 main components. The way we do this, the details aren't important, but a lot of you will already be familiar with it, which is that there's something called PCA, or Principal Components Analysis. Principal Components Analysis does exactly what I just said. It looks through a matrix, in this case it's got 50 columns, and it says what are the combinations of columns that we can add together because they tend to move in the same direction. And so in this case, we say start with our 50 columns and I want to create just 3 columns that capture all of the information of the original 50. If you're interested in learning more about how this works, PCA is something that is everywhere on the internet, so there's lots of information about it. But as I say, the details aren't important. The important thing to recognize is that we're just squishing our 50 latent factors down into 3. So if we look at the first PCA factor and we sort on it, we can see that at one end, we have fairly well-regarded movies like The Godfather, Pulp Fiction, Usual Suspects, these are all kind of classics. At the other end, we have things like Ace Ventura and Robocop 3, which are perhaps not so classic. So our first PCA factor is some kind of IsClassic score. On our second one, we have something similar but actually very different. At one end, we've got 10 movies that are huge Hollywood blockbusters with lots of special effects. And at the other end, we have things like Annie Hall and Brokeback Mountain, which are kind of dialogue-heavy, not big Hollywood hits. So there's another dimension. This is the first most important dimension by which people judge movies differently. This is the second most important one by which people judge movies differently. And then the third most important one by which people judge movies differently is something where at one end, we have a bunch of violent and scary movies, and at the other end, we have a bunch of very happy movies. And for those of you who haven't seen Babe, Australian movie, happiest movie ever. It's about a small pig and its adventures and its path to success. Happiest movie ever, according to MovieLens. So that's interesting. It's not saying that these factors are good or bad or anything like that. It's just saying that these are the things that when we've done this matrix decomposition have popped out as being the ways in which people are differing in their ratings for different kinds of movies. So one of the reasons I wanted to show you this is to say that these kinds of SGD-learned many-parameter networks are not inscrutable. Indeed it's not great to go in and look at every one of those 50 latent factor coefficients in detail, but you have to think about how to visualize them, how to look at them. In this case, I went a step further and I grabbed a couple of principal components and tried drawing a picture. And so with pictures, of course, you can start to see things in multiple dimensions. And so here I've got the first and third principal components. And so you can see at the far right-hand side here, we have more of the Hollywood-type movies, and at the far left, some of the more classic movies, and at the top, some of the more violent movies, and at the bottom, some of the happier movies. And so if you wanted to find a movie that was violent and classic, you would go into the top left, and Hubrix O'Clockwork Orange would probably be the one that most people would come up with first. Or if you wanted to come up with something that was very Hollywood and very non-violent, you would be down here in sleepless in Seattle. So you can really learn a lot by looking at these kinds of models, but you don't do it by looking at the coefficients, you do it by visualizations, you do it by interrogating it. And so I think this is a big difference. For any of you that have done much statistics before or have a background in the social sciences, you've spent most of your time doing regressions and looking at coefficients and t-tests and stuff. This is a very different world. This is a world where you're asking the model questions and getting the modeled results, which is kind of what we're doing here. I mentioned I would talk briefly about this syntax. And this syntax is something that we're going to be using a lot more of, and it's part of what's called the Keras Functional API. The Keras Functional API is a way of doing exactly the same things that you've already learned how to do using a different API. And that is not such a dumb idea. The API you've learned so far is the Sequential API. It's where you use the word sequential and then you write in order the layers of your neural network. That's all very well, but what if you want to do something like what we wanted to do just now, where we had two different things coming in. We had a user ID coming in and a movie ID coming in, and each one went through its own embedding and then they got multiplied together. How do you express that as a sequence? It's not very easy to do that. So the Functional API was designed to answer this question. The first thing to note about the Functional API is that you can do everything you can do in the Sequential API. And here's an example of something you can do perfectly well with the Sequential API, which is something with two dense layers. But it looks a bit different. Every Functional API model starts with an input layer and then you assign that to some variable. And then you list each of the layers in order and for each of them, after you've provided the details for that layer, you then immediately call the layer passing in the output of the previous layer. So this passes in inputs and calls it x. And then this passes in our x, and this is our new version of x. And then this next dense layer gets the next version of x and returns predictions. So you can see that each layer is saying what its previous layer is. So it's doing exactly the same thing as the Sequential API, just in a different way. Now as the docs note here, the sequential model is probably a better choice to implement this particular network because it's easier. This is just showing that you can do it. On the other hand, the model that we just looked at would be quite difficult, if not impossible to do with the Sequential API. But with the Functional API, it was very easy. We created a whole separate model which gave an output u for user. And that was the result of creating an embedding, where we said an embedding has its own input and then goes through an embedding layer. And then we returned the input to that and the embedding layer, like so. So that gave us our user input and our user embedding, and our movie input and our movie embedding. So there's like 2 separate little models. And then we did a similar thing to create 2 little models for our bias terms. They were both things that grabbed an embedding, returning a single output, and then flattened it. And that grabbed our biases. And so now we've got 4 separate models. And so we can merge them. There's this function called merge. It's pretty confusing. There's a small mmerge and a big mmerge. In general, you will be using the small mmerge. I'm not going to go into the details of why they're both there. They are there for a reason. If something weird happens to you with merge, try remembering to use the small mmerge. The small mmerge takes 2 previous outputs that you've just created using the functional API and combines them in whatever way you want, in this case, the dot product. So that grabs our user and movie embeddings and takes the dot product. We grab the output of that and take our user bias and the sum and the output of that and the movie bias and the sum. So that's a functional API to creating that model. At the end of which, we then use the model function to actually create our model saying what are the inputs to the model and what is the output of the model. So you can see this is different to usual because we've now got multiple inputs. So then when we call fit, we now have to pass in an array of inputs, a user ID and a movie ID. So the functional API is something that we're going to be using increasingly from now on. Now that we've learned all the basic architectures, we're going to be starting to build more exotic architectures for more special cases and we'll be using the functional API more and more. Question? Is the only reason to use an embedding layer so that you can provide a list of integers as input? Answer? That's a great question. Is the only reason to use an embedding layer so that you can use integers as input? Absolutely yes. So instead of using an embedding layer, we could have one-hot encoded all of those user IDs and one-hot encoded all of those movie IDs and created dense layers on top of them and it would have done exactly the same thing. Why choose 50 latent factors and then reduce them down with a principal component analysis? Why not just have 3 latent factors to begin with? I'm not quite sure why use both. If we only use 3 latent factors, then our predictive model would have been less accurate. So we want an accurate predictive model so that when people come to our website, we can do a good job of telling them what movie to watch. So 50 latent factors for that. But then for the purpose of our visualization of understanding what those factors are doing, we want a small number so that we can interpret them more easily. One thing you might want to try during the week is taking one or two of your models and converting them to use the Functional API. Just to start to get the hang of how this API looks. Question. Are these Functional Models how we would add additional information to images in CNNs, say driving speed or turning radius? Answer. Yes, absolutely. In general, the idea of adding additional information to a CNN is basically like adding metadata. This happens in collaborative filtering a lot. You might have a collaborative filtering model that as well as having the ratings table, you also have information about what genre the movie is in, maybe the demographic information about the user. So you can incorporate all that stuff by having additional inputs. With a CNN, for example, I'll give you a good example. The new Kaggle fish recognition competition, one of the things that turns out is a useful predictor, this is a leakage problem, is the size of the image. So you could have another input, which is the height and width of the image, just as integers, and have that as a separate input which is concatenated to the output of your convolutional layer after the first flattened layer, and then your dense layers can incorporate the convolutional outputs and your metadata. You might remember from last week that this whole thing about collaborative filtering was a journey to somewhere else. And the journey is to NLP, natural language processing. This is a question about collaborative filtering. So if we need to predict the missing values, the NANs or the 0.0, so if a user hasn't watched a movie, what would be the prediction? Or how do we go about predicting that? So this is really the key purpose of creating this model, so that you can make predictions for movie-user combinations you haven't seen before. The way you do that is to simply do something like this. You just call model.predict and pass in a movieID-userID pair that you haven't seen before. And all that's going to do is it's going to take the dot product of that movie's latent factors and that user's latent factors and add on those biases and return you back the answer. It's that easy. And so if this was a Kaggle competition, that would be how we would generate our submission for the Kaggle competition, would be to take their test set, which would be a bunch of movie-user pairs that we haven't seen before. Natural Language Processing. So collaborative filtering is extremely useful of itself. Without any doubt, it is far more commercially important right now than NLP is. Having said that, Fast.ai's mission is to impact society in as positive a way as possible and doing a better job at predicting movies is not necessarily the best way to do that. So we're maybe less excited about collaborative filtering than some people in industry are. So that's why it's not our main destination. NLP, on the other hand, can be a very big deal if you can do a good job, for example, of reading through lots of medical journal articles or family histories and patient notes. You could be a long way towards creating a fantastic diagnostic tool to use in the developing world to help bring medicine to people who don't currently have it, which is almost as good as telling them not to watch Battlefield Earth. They're both important. So let's talk a bit about NLP. In order to do this, we're going to look at a particular dataset. This dataset is like a really classic example of what people do with natural language processing and it's called sentiment analysis. Sentiment analysis means that you take a piece of text, it could be a phrase, a sentence, a paragraph or a whole document, and decide whether or not that is a positive or negative sentiment. Keras actually comes with such a dataset, which is called the IMDB sentiment dataset. The IMDB sentiment dataset was originally developed from the Stanford AI group. The paper about it was actually published in 2012. They talk about all the details about what people try to do with sentiment analysis. In general, although I think academic papers tend to be way more math-y than they should be, the introductory sections often do a great job of capturing why this is an interesting problem, what kind of approaches people have taken, and so forth. The other reason papers are super-helpful is that you can skip down to the experiment section. Every machine learning paper pretty much has an experiment section and find out what the score is. So here's their score section. So here they showed that using this dataset they created of IMDB movie reviews along with their sentiment, their full model plus an additional model got a score of 88.33% accuracy in predicting sentiment. They had another one here where they also added in some unlabeled data. We're not going to be looking at that today, that would be a semi-supervised learning problem. Today our goal is to beat 88.33 as being the academic state-of-the-art for this dataset, at least as of this time. To grab it, we can just say from keras.datasets import imdb. Keras actually kind of fiddles around with it in ways that I don't really like. So I actually copied and pasted from the keras file these 3 lines to import it directly without screwing with it. So that's why rather than using the keras dataset directly, I'm using these 3 lines. There are 25,000 movie reviews in the training set. Here's an example of one. Realmware Ohio is a cartoon comedy, around at the same time as some other programs, blah blah blah. So the dataset actually does not quite come to us in this format, it actually comes to us in this format, which is a list of IDs. And so these IDs we can look up in the word index, which is something that they provide. And so for example, if we look at IDXArray, the word index as you can see basically maps an integer to every word. It's in order of how frequently those words appeared in this particular corpus, which is kind of handy. Then I also create a reverse index, so it goes from word to ID. I can see that in the very first training example, the very first word is word number 23022. So if I look up index to word 23022, it is the word Bromwell. And so then I just go through and I map everything in that first review to index to word and then join it together with a space and that's how we can turn the data that they give us back into the movie review. As well as providing the reviews, they also provide labels. 1 is positive sentiment, 0 is negative sentiment. So our goal is to take these 25,000 reviews that look like this and predict whether it will be positive or negative in sentiment and the data is actually provided to us as a list of word IDs for each review. Is everybody clear on the problem we are trying to solve and how it's laid out? So there's a couple of things we can do to make it simpler. One is we can reduce the vocabulary. So currently there are some pretty unusual words, like word number 23022 is Bromwell. And if we're trying to figure out how to deal with all these different words, having to figure out the various ways in which the word Bromwell is used is probably not going to net as much for a lot of computation and memory cost. So we're going to truncate the vocabulary down to 5000. It's very easy to do that because the words are already ordered by frequency. I simply go through everything in our training set and I just say if the word ID is less than this vocab size of 5000, we'll leave it as it is. Otherwise we'll replace it with the number 5000. So at the end of this, we now have replaced all of our rare words with a single ID. Here's a quick look at the sentences. The reviews are sometimes up to 2493 words long. Some people spend far too much time in IMDb. Some are as short as 10 words. I've averaged there 237 words. As you'll see, we actually need to make all of our reviews the same length. So allowing this 2493 word review would use up a lot of memory and time. So we're going to decide to truncate every review at 500 words. That's more than twice as big as the mean. So we're not going to lose too much. What we now need to do is we have to create a rectangular... Question? What if the word 5000 gives a bias? So whatever the word 5000 is, let's find out what the word 5000 is. IDX2WORD 5000. Okay, it's the year, 1987. That's fine. We're about to learn a machine learning model. And so the vast majority of the time, it comes across the word 5000. It's actually going to mean rare word. It's not going to specifically mean 1987. It's going to learn to deal with that as best as it can. The idea is the rare words don't appear too often, so hopefully this is not going to cause too much problem. Question? And doesn't just using frequencies favor stop words? We're not just using frequencies. All we're doing is we're just truncating our vocabulary at this point. Can you put that close to your mouth? So the 5000 word, can we just replace it with some neutral word to take care of that bias thing? There's really not going to be a bias here. We're just replacing it with a random ID. The fact that occasionally the word 1987 actually pops up, it's totally insignificant. We could replace it with minus 1. It's just a sentinel value which has no meaning. Also, we're getting rid of all the words that are 5000 and beyond. It's not just a single word. It represents all of the less common words. It's one of these design decisions which it's not worth spending a lot of time thinking about because it's not significant. So I just picked whatever happened to be easiest at the time. As I said, I could have used minus 1, it's just not important. So what is important is that we have to create a rectangular matrix which we can pass to our machine learning model. So quite conveniently, Keras comes with something called pad-sequences that does that for us. It takes everything greater than this length and truncates it, and everything less than that length and pads it with whatever we asked for, which in this case is 0s. So at the end of this, the shape of our training set is now a numpy array of 25,000 rows by 500 columns. And as you can see, it's padded the front with 0s, such that it has 500 words in it. Other than that, it's exactly the same as before. And you can see Bromwell has now been not replaced with 5000, but with 4999. So this is our same movie review again after going through that padding process. Question, does it matter if we pad from the left or the right? I know that there's some reason that Keras decided to pad the front rather than the back. I don't recall what it is. Since it's what it does by default, I don't worry about it. I don't think it's important. So now that we have a rectangular matrix of numbers and we have some labels, we can use the exact techniques we've already learned to create a model. And as per usual, we should try to create the simplest possible model we can to start with. And we know that the simplest model we can is one with one hidden layer in the middle. Or at least this is the simplest model that we generally think ought to be pretty useful for just about everything. Now here is why we started with collaborative filtering. And that's because we're starting with an embedding. So if you think about it, our input are word IDs and we want to convert that into a vector. And that is what an embedding does. So again, rather than one-hot encoding this into a 5000 column long huge input thing and then doing a matrix product, an embedding just says look up that movie ID and grab that vector directly. So it's just a computational and memory shortcut to creating a one-hot encoding followed by a matrix product. So we're creating an embedding where we are going to have 5000 latent factors or 5000 things, each one we're going to have 32 items rather than 50. So then we're going to flatten that, have our single dense layer, a bit of dropout, and then our output to a sigmoid. So that's a pretty simple model. You can see it's a good idea to go through and make sure you understand why all these parameter counts are what they are. That's something you can do during the week and double-check that you're comfortable with all of those. So this is the size of each of the weight matrices at each point. And we can fit it. And after 2 epochs, we have 88% accuracy on the validation set. And so let's just compare that to Stanford where they had 88.3 and we have 88.04. So we're not yet there, but we're well on the right track. Question 2. Why 32? Answer. This is always the question about why have x number of filters in your convolutional layer or why have x number of outputs in your dense layer. It's just a case of trying things and seeing what works and also getting some intuition by looking at other models. In this case, I think 32 was the first I tried. I kind of felt like from my understanding of really big embedding models, which we'll learn about shortly, even 50 dimensions is enough to capture vocabularies of size 100,000 or more. So I felt like 32 was likely to be more than enough to capture a vocabulary of size 5,000. I tried it and I got a pretty good result, so I basically left it there. If at some point I discovered that I wasn't getting great results, I would try increasing it. Question 3. Why sigmoid in the final layer? You can always use a softmax instead of a sigmoid, it just means that you would have to change your labels. Remember our labels were just ones or zeros, they were just a single column. If I wanted to use a softmax, I would have to create two columns, it wouldn't just be one, it would be 1, 0, 1, 0, 1, 0. In the past, I've generally stuck to using softmax and then categorical cross-entropy loss just to be consistent, because then regardless of whether you have two classes or more than two classes, you can always do the same thing. In this case, I thought I want to show the other way you can do this, which is to just have a single column output. And remember a sigmoid is exactly the same thing as a softmax if you just have a binary output. So rather than using categorical cross-entropy, we use binary cross-entropy. Again, it's exactly the same thing, it just means I didn't have to worry about one-hot encoding the output because it's just a binary output. Question 4. Do we know what the inter-rater agreement is for this data set? No we don't. It's not something I have looked at. The important thing as far as I'm concerned is what is the benchmark that the Stanford people got and they compared it to a range of other previous benchmarks and they found that their technique was the best. So that's my goal here. I'm sure there have been other techniques that have come out since that are probably better, but I haven't seen them in any papers yet, so this is my target. You can see that we can in one second of training get an accuracy which is pretty competitive and it's just a simple neural net. So hopefully you're starting to get a sense that a neural net with one hidden layer is a great starting point for nearly everything. You now know how to create a pretty good sentiment analysis model and before today you didn't, so that's a good step. Question 2. Just to confirm, whenever we use binary cross-entropy, we use sigmoid for the final layer? Yes. And could you explain embedding again? What is the actual input into the dense layer for Word 2.3.4.5? This is an embedding. So an embedding is something that I think would be particularly helpful if we go back to our MovieLens recommendation dataset. And remember that the actual data coming in does not look like this, but it looks like this. So when we then come along and say what do we predict the rating would be for user ID 1 for movie ID 1172, we actually have to go through our list of movie IDs and find movie ID number 31, and then having found 31, look up its latent factor. And then we have to do the same thing for user ID number 1 and find its latent factor, and then we have to multiply the two together. So that step of taking an ID and finding it in a list and returning the vector that it's attached to, that is what an embedding is. So an embedding returns a vector which is of length, in this case, 32. So the output of this is that for each, the none always means your mini-batch size. So for each movie review, for each of the 500 words in that sequence, you're getting a 32-element vector. And so therefore you have a mini-batch size of 500x32 tensor coming out of this layer. That gets flattened, so 500x32 is 16,000, and that is the input into your first dense layer. Question For Josie I also think it might be helpful to show that for a review, instead of having that in words, that's being entered as a sequence of numbers where the number is how that word rangs. So we look at this first review, and remember this has now been truncated to 4999, and it's still 309, so it's going to take 309 and it's going to look up the 309th vector in the embedding and it's going to return it and it's going to concatenate it to create this tensor. So that's what embedding is. An embedding is a shortcut to a one-hot encoding followed by a matrix product. Question For Josie The interesting thing about NLP is that we're trying to capture something which is very subjective. In this case, you would have to read the original paper to find out how they got these particular labels. The way that people tend to get labels is either, in this case it's the IMDB dataset. IMDB has ratings, so you can just say anything higher than 8 is very positive and anything lower than 2 is very negative, and we'll throw away everything in the middle. The other way that people tend to label academic datasets is to send it off to Amazon Mechanical Turk and pay them a few cents to label each thing. So that's the kind of ways that you can label stuff. Question And there are places where people don't just use Mechanical Turk, but they specifically try to hire linguistics PhDs. Answer You certainly wouldn't do that for this, because the whole purpose here is to kind of capture normal people's sentiment. We know of a team at Google that does that. And I know when I was in medicine, we went through all these radiology reports and tried to capture which ones were critical findings and which ones weren't critical findings, and we used good radiologists rather than Mechanical Turk for that purpose. Question So we are not considering any sentence construction or bigrams, just a bag of words and the literal set of words that are being used in a comment? Answer It's not actually just a bag of words. If you think about it, this dense layer here has 1.6 million parameters. It's connecting every one of those 500 inputs to our output. So it's actually doing that for every one of the incoming factors. So it's creating a pretty complex, big Cartesian product of all of these weights. It's taking account of the position of a word in the overall sentence. It's not terribly sophisticated and it's not taking account of its position compared to other words, but it is taking account of whereabouts it occurs in the whole review. So it's not like the dumbest kind of model I could come up with, it's a good starting point. But we would expect that with a little bit of thought, which we're about to use, we could do a lot better. So why don't we go ahead and do that. So the slightly better, hopefully you guys have all predicted what that would be, it's a convolutional neural network. And the reason I hope you predicted that is because a, we've already talked about how CNNs are taking over the world, and b, specifically they're taking over the world any time we have some kind of ordered data. And clearly a sentence is ordered. One word comes after another word, it has a specific ordering. So therefore we can use a convolution. We can't use a 2D convolution because the sentence is not in 2D, the sentence is in 1D, so we're going to use a 1D convolution. So a 1D convolution is even simpler than a 2D convolution. We're just going to grab a string of a few words and we're going to take their embeddings and we're going to take that string and we're going to multiply it by some filter. And then we're going to move that sequence along our sentence. So this is our normal next place we go as we try to gradually increase the complexity, which is to grab our simplest possible CNN, which is a convolution, dropout, max-polling. And then flatten that and then we have our dense layer and our output. So this is exactly like what we did back when we were looking at gradually improving our state farm result. But rather than having convolution 2D, we have convolution 1D. The parameters are exactly the same. How many filters do you want to create and what is the size of your convolution. Originally I tried 3 here, 5 turned out to be better. So I'm looking at 5 words at a time and multiplying them by each one of 64 filters. So that is going to return, we're going to start with the same embedding as before. So we take our sentences and we turn them into a 500x32 matrix for each of our inputs. We then put it through our convolution and because our convolution has a border mode of same, we get back exactly the same shape that we gave it. We then put it through our 1D max-polling and that will halve its size. And then we stick it through the same dense layers as we had before. So that's a really simple convolutional neural network for words. So we compile it, run it, and we get 89.47 compared to 88.33. So we have already broken the academic state-of-the-art as at when this paper was written. And again, simple convolutional neural network gets us a very, very long way. Question 2. Convolution 2D for images is easier to understand, element-wise multiplication and addition, but what does it mean for a sequence of words? Don't think of it as a sequence of words, because remember it's been through an embedding. So it's a sequence of 32 element vectors. So it's doing exactly the same thing as we were doing in a 2D convolution, but rather than having 3 channels of color, we have 32 channels of embedding. So we're just going through and we're just like in our convolution spreadsheet, remember how in the second one, once we had two filters already, our filter had to be a 3x3x2 tensor in order to allow us to create the second layer. For us, we now don't have a 3x3x2 tensor, we have a 5x1x32, or more conveniently, a 5x32 matrix. So each convolution is going to go through each of the 5 words and each of the 32 embeddings, do an element-wise multiplication and add them all up. So the important thing to remember is that once we've done the embedding layer, which is always going to be our first step for every NLP model, is that we don't have words anymore. We now have vectors which are attempting to capture the information in that word in some way, just like our latent factors captured information about a movie and a user in our collaborative filtering. We haven't yet looked at what they do, we will in a moment, just like we did with the movie vectors. But we do know from our experience that SGD is going to try to fill out those 32 places with information about how that word is being used, which allow us to make these predictions. Just like when you first learned about 2D convolutions, it took you probably a few days of fiddling around with spreadsheets and pieces of paper and Python and checking inputs and outputs to get a really intuitive understanding of what a 2D convolution is doing. You may find it's the same with a 1D convolution, but it will take you probably a fifth of the time to get there because you've really done all the hard work already. I think now is a great time to have a break, so let's come back here at 7.57. So there's a couple of concepts that we come across from time to time in this class. There is no way that me lecturing to you is going to be enough to get an intuitive understanding of it. The first clearly is the 2D convolution. Hopefully you've had lots of opportunities to experiment and practice and read. These are things you have to tackle from many, many different directions to understand a 2D convolution. And 2D convolutions in a sense are really 3D, because if it's in full color, you've got 3 channels. Hopefully that's something you've all played with. And once you have multiple filters, later on in your image models, you still have 3D and you have more than 3 channels. You might have 32 filters or 64 filters. In this lesson, we've introduced one much simpler concept, but it's still new, which is the 1D convolution, which is really a 2D convolution because just like with images we had red, green, blue, now we have the 32 embedding factors. So that's something you definitely need to experiment with. Create a model with just an embedding layer, look at what the output is, what is its shape, what does it look like, and then how does a 1D convolution modify that. And then trying to understand what an embedding is is kind of your next big task if you're not already feeling comfortable with it. And if you haven't seen them before today, I'm sure you won't, because this is a big new concept. It's not in any way mathematically challenging, it's literally looking up an array and returning the thing at that ID. So an embedding looking at movie ID 3 is go to the 3rd column of the matrix and return what you see. That's all an embedding does. It couldn't be mathematically simpler, it's the simplest possible operation. Return the thing at this index. But the intuitive understanding of what happens when you put an embedding into an SGD and learn a vector which it turns out to be useful is something which is kind of mind-blowing. As we saw from the movie lens example, with just a dot product and this simple lookup something in an index operation, we ended up with vectors which captured all kinds of interesting features about movies without us in any way asking it to. So I kind of wanted to make sure that you guys really felt like after this class you're going to go away and try to find a dozen different ways of looking at these concepts. One of those ways is to look at how other people explain them. Chris Ola has one of the very, very best technical blogs I've come across and I quite often refer to him in this class. In his Understanding Convolutions post, he actually has a very interesting example of thinking about what a dropped ball does as a convolutional operation. He shows how you can think about a 1D convolution using this dropped ball analogy. Particularly if you have some background in electrical or mechanical engineering, I suspect you'll find this a very helpful example. There are many resources out there for thinking about convolutions and I hope some of you will share on the forums any that you come across. Question from audience 1. Essentially, are we training the input too? Answer. We are absolutely training the input. The only input we have is 25,000 sequences of 500 integers. So we take each of those integers and replace them with a lookup into a 500-column matrix. Initially that matrix is random, just like in our Excel example. We started with a random matrix, these are all random numbers, and then we created this loss function which was the sum of the squares of differences between the dot product and the rating. If we then use the gradient descent solver in Excel to solve that, it attempts to modify the two embedding matrices, and as you can see the objective is going down, to try and come up with the two embedding matrices which give us the best approximation of the original rating matrix. So this Excel spreadsheet is something which you can play with and do exactly what our first MovieLens example is doing in Python. The only difference is that our version in Python also has L2 regularization. This one's just finished here, so you can see it's come up with, these are no longer random, we've now got two embedding matrices which have got the loss function down from 40 to 5.6. So you can see, for example, these ratings are now very close to what they're meant to be. So this is exactly what Keras and SGD are doing in our Python example. So my question is, is it that we got an embedding in which each word is a vector of 32 elements? It's more clear that way, no? Yes, exactly. Each word in our vocabulary of 5,000 is being converted into a vector of 32 elements. Exactly right. Another question is, what would be the equivalent dense network if we didn't use a 2D embedding? This is in the initial model, the simple one. A dense layer with input of size embedding size of half size? I actually don't know what that means, sorry. Next question is, does it matter that encoded values which are close by are close in color in the case of pictures, which is not true for word vectors? For example, 254 and 255 are close as colors, but for words they have no relation. The important thing to realize is that the word IDs are not used mathematically in any way at all, other than as an index to look up into an integer. So the fact that this is movie number 27, the number 27 is not used in any way. We just take the number 27 and find its vector. So what's important is the values of each latent factor as to whether they're close together. So in the movie example, there were some latent factors that were something about is it a Hollywood blockbuster, and there were some latent factors that were something about is it a violent movie or not. It's the similarity on those factors that matters. The ID is never ever used, other than as an index to simply index into a matrix to return the vector that we found. So as Yannet was mentioning, in our case now for the word embeddings, we're looking up in our embeddings to return a 32-element vector of floats that are initially random and the model is trying to learn the 32 floats for each of our words that is semantically useful. In a moment, we're going to look at some visualizations of that to try and understand what it's actually learned. Question, What is the significance of the dropout on the embedding as well as on the next layer? So what is the significance and what is the difference between the two? So, you can apply the dropout parameter to the embedding layer itself. What that does is it zeroes out at random 20% of each of these 32 embeddings for each word. So it's basically avoiding overfitting the specifics of each word's embedding. This dropout, on the other hand, is removing at random some of the words, effectively some of the whole vectors. The significance of which one to use where is not something which I've seen anybody research in-depth, so I'm not sure that we have an answer that says use this amount in this place. I just tried a few different values in different places and it seems that putting the same amount of dropout in all these different spots seems to work pretty well in my experiments, so it's a reasonable rule of thumb. If you find you're massively overfitting or massively underfitting, try playing around with the various values and report back on the forum and tell us what you find. Maybe you'll find some different, better configurations than I've come up with. I'm sure some of you will. Great question. So let's think about what's going on here. We are taking each of our 5000 words in our vocabulary and we're replacing them with a 32-element long vector, which we are training to hopefully capture all of the information about what this word means and what it does and how it works. We might expect intuitively that somebody might have done this before. Just like with ImageNet and VGG, you can get a pre-trained network that says, oh, if you've got an image that looks a bit like a dog, well we've trained a network which has seen lots of dogs and so it will probably take your dog image and return some useful predictions because we've done lots of dog images before. The interesting thing here is your dog picture and the VGG author's dog pictures are not the same. They are going to be different in all kinds of ways. And so to get pre-trained weights for images, you have to give somebody a whole pre-trained network, which is like 500 megabytes worth of weights in a whole architecture. Words are much easier. In a document, the word dog always appears the same way. It's the word dog. It doesn't have different lighting conditions or facial expressions or whatever, it's just the word dog. So the cool thing is in NLP, we don't have to pass around pre-trained networks, we can pass around pre-trained embeddings, or as they're commonly known, pre-trained word vectors. That is to say, other people have already created big models with big text corpuses where they've attempted to build a 32-element vector or however long vector which captures all of the useful information about what that word is and how it behaves. So for example, if we type in word vector download, you can see that lots of questions and answers and pages about where we can download pre-trained word embeddings. That's pretty cool, but I guess what is a little unintuitive to me is that I think this means that if I can train a corpus on the works of Shakespeare, somehow that tells me something about how I can understand movie reviews. I imagine that in some sense that's true about how language is structured and whatnot, but the meaning of the word dog in Shakespeare is probably going to be used pretty differently. The word vectors that I'm going to be using, and I don't strongly recommend, but slightly recommend are the GloVe word vectors. The other main competition to these is called the Word2Vec word vectors. The GloVe word vectors come from a researcher named Jeffrey Pennington from Stanford. The Word2Vec word vectors come from Google. I will however mention that the TensorFlow documentation on the Word2Vec vectors is fantastic. I would definitely highly recommend checking this out. The GloVe word vectors have been pre-trained on a number of different corpuses. One of them has been pre-trained on all of Wikipedia and a huge database full of newspaper articles, a total of 6 billion words covering 400,000-size vocabulary. They provide 50-dimensional, 100-dimensional, 200-dimensional and 300-dimensional pre-trained vectors. They have another one which has been trained on 840 billion words of a huge dump of the entire internet. And then they have another one which has been trained on 2 billion tweets, which I believe all of the Donald Trump tweets have been carefully cleaned out prior to usage. So in my case, what I've done is I've downloaded the 6 billion token version and I will show you what one of these looks like. So here is ... Question, Are you losing context because of capital letters, punctuation? Answer, We'll look at that in a moment. Sometimes these are cased. So you can see for example this particular one includes case. There are 2.2 million items of vocabulary in this. Sometimes they're uncased. So we'll look at punctuation in a moment. Here is the start of the glove 50-dimensional word vectors trained on a corpus of 6 billion. Here is the word the. And here are the 50 floats which attempt to capture all of the information in the word the. Punctuation, well here is the word fullstop. And so here are the 50 floats that attempt to capture all of the information captured by a fullstop. So here is the word in, here is the word doublequote, here is apostrophe s. So you can see that the glove authors have tokenized their text in a very particular way. The idea that apostrophe s should be treated as a thing, that makes a lot of sense. It certainly has that thinginess in the English language. And so indeed the way the authors of a word-embedding corpus have chosen to tokenize their text definitely matters. One of the things I quite like about Glove is that they've been pretty smart in my opinion about how they've done this. What was the target variable? So the question is how does one create word vectors in general, what is the model that you're creating, and what are the labels that you're building. One of the things that we talked about getting to at some point is unsupervised learning. And this is a great example of unsupervised learning. We want to take 840 billion tokens of an internet dump and build a model of something. So what do we build a model of? This is a case of unsupervised learning. We're trying to capture some structure of this data, in this case how does English look, work and feel. The way that this is done, at least in the Word2Vec example, is quite cool. What they do is they take every sentence of say 11 words long, not just every sentence, but every 11 long string of words that appears in the corpus, and then they take the middle word and the first thing they do is create a copy of it, an exact copy. And then in the copy, they delete the middle word and replace it with some random word. So we now have two strings of 11 words, one of which makes sense because it's real, one of which probably doesn't make sense because the middle word has been replaced with something random. And so the model task that they create, the label is 1 if it's a real sentence or 0 if it's a fake sentence. And that's the task they give it. So you can see it's not a directly useful task in any way, unless somebody actually comes along and says, I just found this corpus in which somebody's replaced half of the middle words with random words. But it is something where in order to be able to tackle this task, you're going to have to know something about language. You're going to have to be able to recognize that this sentence doesn't make sense and this sentence does make sense. So this is a great example of unsupervised learning. Generally speaking in deep learning, unsupervised learning means coming up with a task which is as close to the task you're eventually going to be interested in as possible, but that doesn't require labels or where the labels are really cheap to generate. Question? I was just thinking about the language aspect. I'm talking about just English, it's just a stream of tokens. So if it's a different language, we're still turning them into a vector of floats. So how does that change across languages or in a mixed-language text? So it turns out that the embeddings that is created when you look at, say, Hindu and Japanese turn out to be nearly the same. And so one way to translate language is to create a bunch of word vectors in English for various words, and then to create a bunch of word vectors in, say, Japanese for various words. And then what you can do is you can say, I want to translate this word, which might be Queen, to Japanese. You can basically look up and find the nearest word in the same vector space in the Japanese corpus and it turns out it works. So it's a fascinating thing about language. In fact, Google has just announced that they've replaced Google Translate with a neural translation system and part of what that is doing is basically doing this. In fact, here are some interesting examples of some word embeddings. The word embedding for King and Queen has the same distance and direction as the word embeddings for Man and Woman. Ditto for Walking vs. Walked and Swimming vs. Swam. And ditto for Spain vs. Madrid and Italy vs. Rome. So the embeddings that have to get learned in order to solve this stupid meaningless random sentence task are quite amazing. And so I've actually downloaded those GloVe embeddings and I've pre-processed them and I'm going to upload these for you shortly into a form that's going to be really easy for you to use in Python. And I've created this little thing called LoadGloVe which loads the pre-processed stuff that I've created for you. And it's going to give you 3 things. It's going to give you the word vectors, which is the 400,000 by 50 dimensional vectors, a list of the words, the, comma, dot, of, to, and a list of the word indexes. So you can now take a word and call word2vec to get back its 50-dimensional array. And so then I drew a picture. In order to turn a 50-dimensional vector into something 2-dimensional that I can plot, we have to do something called dimensionality reduction. And there's a particular technique, the details don't really matter, called t-SNE, which attempts to find a way of taking your high-dimensional information and plot it on 2 dimensions such that things that were close in the 50 dimensions are still close in the 2 dimensions. And so I used t-SNE to plot the first 350 most common words and here they all are. And so you can see that bits of punctuation have appeared close to each other, numerals have appeared close to each other, written versions of numerals are close to each other, seasons, games, leagues, played are all close to each other, various things about politics, school and university, president, general, prime, minister, and Bush. Now this is a great example of where this t-SNE 2-dimensional projection is misleading about the level of complexity that's actually in these word vectors. In a different projection, Bush would be very close to Tree. The 2-dimensional projection is losing a lot of information. The true detail here is a lot more complex than us mere humans can see on a page. But hopefully you kind of get a sense of this. So all I've done here is I've just taken those 50-dimensional word vectors and I've plotted them in 2-dimensions. And so you can see that when you learn a word embedding, you end up with something. We've now seen, not just a word embedding, we've seen for movies, we were able to plot some movies in 2-dimensions and see how they relate to each other. We can do the same thing for words. In general, when you have some high-cardinality categorical variable, whether it be lots of movies or lots of reviewers or lots of words, you can turn it into a useful lower-dimensional space using this very simple technique of creating an embedding. Question Time 5 The explanation on how unsupervised learning was used in Word2Vec was pretty smart. How was it done in GloVe? I don't recall how it was done in GloVe. I believe it was something similar. I should mention though that both GloVe and Word2Vec did not use deep learning. They actually tried to create a linear model. The reason they did that was that they specifically wanted to create representations which had these kinds of linear relationships because they felt that this would be a useful characteristic of these representations. I'm not even sure if anybody has tried to create a similarly useful representation using a deeper model and whether that turns out to be better. But obviously with these linear models, it saves a lot of computational time as well. The embeddings, even though they were built using linear models, we can now use them as inputs to deep models, which is what we're about to do. Question Time 6 I was mentioning Word2Vec. Word2Vec has been around for 2.5 years, 2 years. All of these word vectors do all of these things. In that high-dimensional space, for example, you can see here is information about tense. So it's very easy to take a word vector and use it to create a part-of-speech recognizer. You just need a fairly small labeled corpus, and it's actually pretty easy to download a rather large labeled corpus, and build a simple model that goes from word vector to part-of-speech. There's a really interesting paper called Exploring the Limits of Language Modeling. That Parsi-McParseface thing got far more PR than it deserved. It was not really an advance over the state-of-the-art language models of the time. But since that time, there have been some much more interesting things. One of the interesting papers is Exploring the Limits of Language Modeling, which is looking at what happens when you take a very, very, very large dataset and spend shitloads of Google's money on lots and lots of GPUs for a very long time. They have some genuine, massive improvements to the state-of-the-art in language modeling. In general, when we're talking about language modeling, we're talking about things like is this a noun or a verb, is this a happy sentence or a sad sentence, is this a formal speech or an informal speech, and so on and so forth. And all of these things that NLP researchers do, we can now do super-easily with these embeddings. Question, did it use the optimizer as Adam, which automatically did generate all of those latent factors, which would translate into some of these verbs? This uses two techniques, one of which you know and one of which you're about to know, convolutional neural networks and recurrent neural networks, specifically a type called LSTM. You can check out this paper to see how they compare. Since this time, there's been an even newer paper that has furthered the state-of-the-art in language modeling and it's using a convolutional neural network. So right now, CNNs with pre-trained word embeddings are the state-of-the-art. So given that we can now download these pre-trained word embeddings, that leads to the question of why are we using randomly generated word embeddings when we do our sentiment analysis. That doesn't seem like a very good idea. And indeed, it's not a remotely good idea. You should never do that. From now on, you should now always use pre-trained word embeddings anytime you do NLP. And over the next few weeks, we will be gradually making this easier and easier. At this stage, it requires slightly less than a screen of code. You have to load the embeddings off-disk, creating your word vectors, your words and your word indexes. The next thing you have to do is the word indexes that come from GloVe are going to be different to the word indexes in your vocabulary. In our case, this was the word Bromwell, in the GloVe case it's probably not the word Bromwell. So this little piece of code is simply something that is mapping from one index to the other index. This createEmbedding function is then going to create an embedding matrix where the indexes are the indexes in the IMDB dataset and the embeddings are the embeddings from GloVe. So that's what EMB now contains. This embedding matrix are the GloVe word vectors indexed according to the IMDB dataset. Now I have simply copied and pasted the previous code and I have added this, weights equals my pre-trained embeddings. Since we think these embeddings are pretty good, I've set trainable to false. I won't leave it at false because we're going to need to fine-tune them, but we'll start it at false. One particular reason that we can't leave it at false is that sometimes I have had to create a random embedding because sometimes the word that I looked up in GloVe didn't exist. For example, anything that finishes with apostrophe S, in GloVe they tokenized that to have apostrophe S and the word as separate tokens, but in IMDB they were combined into one token. And so all of those things aren't vectors for them. So I just randomly created embeddings for anything that I couldn't find in the GloVe vocabulary. But for now, let's start using just the embeddings that we're given and we will set this to non-trainable and we will train a convolutional neural network using those embeddings for the IMDB task. And after 2 epochs, we have 89.8. Similarly with random embeddings, we had 89.5 and the academic state-of-the-art was 88.3. So we made significant improvements. Let's now go ahead and say first-layer trainable is true. Decrease the learning rate a bit and do just one more epoch and we're now up to 90.1. So we've got way beyond the academic state-of-the-art here. We're kind of cheating because we're now not just building a model, we're now using a pre-trained word embedding model that somebody else has provided for us. But why would you ever not do that now that that exists? So you can see that we've had a big jump and furthermore it's only taken us 12 seconds to train this network. So we started out with the pre-trained word embeddings. We set them initially to non-trainable in order to just train the layers that used them. Waited until that was stable, which took really 2 epochs, and then we set them to trainable and did one more little fine-tuning step. And this kind of approach of these 3 epochs of training is likely to work for a lot of the NLP stuff that you'll find in the wild. Question from joseph. Do you not need to compile the model after resetting the input layer to trainable equals true? Answer No, you don't, because the architecture of the model has not changed in any way. It's just changed the metadata attached to it. There's never any harm in compiling the model. Sometimes if you forget to compile, it just continues to use the old model. So best to err on the side of using it. Something that I thought was pretty cool is that during the week, one of our students here had an extremely popular post appear all over the place, as I saw it on the front page of Hacker News, talking about how his company, Quid, uses deep learning and very happy to see with small data, which is what we're all about. For those of you who don't know it, Quid is a company, quite a successful startup actually, that is processing millions and millions of documents, things like patents and stuff like that, and providing enterprise customers with really cool visualizations and interactive tools that let them analyze huge datasets. This is by Ben Bowles, one of our students here. He talked about how he compared 3 different approaches to a particular NLP classification task, one of which involved some pretty complex and slow-to-develop, carefully engineered features. But Model 3, in this example, was a convolutional neural network. So I think this is pretty cool and I was hoping to talk to Ben about this piece of work. Question, Could you give us a little bit of context on what you were doing in this project? Answer, Yes, so the task is about detecting marketing language from company descriptions. So it had the flavor of being very similar to sentiment analysis. You have two classes of things, they're kind of different in some kind of semantic way. You've got some examples here. One was like, our patent-pending support system is engineered to bring comfort and style. With your more marketing, I guess, and your spatial scanning software for mobile devices is more informative. Answer, Yes, the semantics of the marketing language is like, oh, this is exciting. There are certain types of meanings and semantics around which the marketing tends to cluster. I sort of realized, hey, this would be kind of a nice task for deep learning. How were these labeled, your data set, in the first place? Basically by a couple of us in the company, we basically just found some good ones and found the bad ones and then literally tried it out. It was literally as hacky as you could possibly imagine. It was kind of super, super scrappy. But it actually ended up being very useful for us. I think that's kind of a nice lesson. Sometimes scrappy gets you most of the way you need it. Think about how you get your data for your project. Well, you can actually just create it. I love this lesson, and so is startup. When I talk to big enterprise executives, they're all about their 5-year metadata and data repository infrastructure program, at the end of which maybe they'll actually try to get some value out of it. They're just like, okay, what have we got that we can do by Monday, let's throw it together and see if it works. The latter approach is so much better because by Monday you know whether it looks good, you know which kind of things are important, and you can decide on how much it's worth investing in. So one of the things I wanted to show is your convolutional neural network did something pretty neat. I wanted to use this same neat trick for our convolutional neural network, and it's a multi-size CNN. I mentioned earlier that when I built this CNN, I tried using a filter size of 5 and I found it better than 3. What Ben in his blog post points out is that there's a neat paper in which they describe doing something interesting, which is not just using one size convolution, but trying a few size convolutions. You can see here this is a great use of the functional API. I haven't exactly used your code, I've kind of rewritten it a little bit, but basically it's the same concept. Let's try size 3 and size 4 and size 5 convolutional filters. And so let's create a 1D convolutional filter of size 3, then size 4, then size 5, and then for each one, using the functional API, we'll add max pooling and we'll flatten it and we'll add it to a list of these different convolutions. And then at the end, we will merge them all together by simply concatenating them. So we're now going to have a single vector containing the result of the 3 and 4 and 5 size convolutions, like y-set of a 1. And then let's return that whole model as a little sub-model, which in Ben's code he called graph. The reason I assume you call this graph is because people tend to think of these things as a computational graph. A computational graph basically is saying this is a computation being expressed as various inputs and outputs. You can think of it as a graph. Once you've got this little multi-layer convolution module, you can stick it inside a standard sequential model by simply replacing the convolution 1D and max pooling piece with graph, where graph is the concatenated version of all of these different scales of convolution. And so trying this out, I got a slightly better answer again, which is 90.36%. And I hadn't seen that paper before, so thank you for giving that great idea. Did you have anything to add about this multi-scale convolution idea? Answer Question 2 Not really, other than I think it's super cool. But actually I'm still trying to figure out all the ins and outs of exactly how it works. In some ways, implementation is easier than understanding. Answer Question 3 Well, that's exactly right. In a lot of these things, the math is kind of ridiculously simple. And then you throw it at an SGD and let it do billions and billions of calculations in a fraction of a second, and what it comes up with is kind of hard to grasp. Answer Question 4 And you are using capital M merge in this example. Do you want to talk about that? Answer Question 3 Not really. Ben used capital M merge and I just did the same thing. Where were at me, I would have used small m merge, so we'll have to agree to disagree here. Let's not go there. So I think that's super fun. So we have a few minutes to talk about something enormous, so we're going to do a brief introduction to RNNs. And then next week, we will do a deep dive. So everything we've learned so far about convolutional neural networks does not necessarily do a great job of solving a problem like how would you model this. Now notice this markup here, it has to recognize when you have a start tag and know to close that tag, but then over a longer period of time, that it's inside a weird XMLE comment thing and to know that it has to finish off the weird XMLE comment thing. Which means it has to keep memory about what happened in the distant past if you're going to successfully do any kind of modeling with data that looks like this. And so with that kind of memory, therefore, it can handle long-term dependencies. Also think about these two different sentences. They both mean effectively the same thing, but in order to realize that, you're going to have to keep some kind of state that knows that after this has been read in, you're now talking about something that happened in 2009. And you then have to remember it all the way to here to know when it was that this thing happened that you did in Nepal. So we want to create some kind of stateful representation. Furthermore, it would be nice if we're going to deal with big long pieces of language like this with a lot of structure to be able to handle variable-length sequences so that we can handle some things that might be really long and some things that might be really short. So these are all things which convolutional neural networks don't necessarily do that well. So we're going to look at something else, which is a recurrent neural network, which handles that kind of thing well. Here is a great example of a good use of a recurrent neural network. At the top here, you can see that there is a convolutional neural network that is looking at images of house numbers. These images are coming from really big Google Street View pictures, so it has to figure out what part of the image should I look at next in order to figure out the house number. You can see that there's a little square box that is scanning through and figuring out I want to look at this piece next. And at the bottom, you can see it's then showing you what it's actually seeing after each time step. So the thing that is figuring out where to look next is a recurrent neural network. It's something which is taking its previous state and figuring out what should its next state be. This kind of model is called an attentional model. It's a really interesting avenue of research when it comes to dealing with things like very large images, images which might be too big for a single convolutional neural network with our current hardware constraints. On the left is another great example of a useful recurrent neural network, which is the very popular Android and iOS text entry system called SwiftKey. SwiftKey had a post up a few months ago in which they announced that they had just replaced their language model with a neural network of this kind, which basically looked at your previous words and figured out what word are you likely to be typing in next and then it could predict that word. A final example was Andrej Karpathy showed a really cool thing where he was able to generate random mathematical papers by generating random LaTeX. And to generate random LaTeX, you actually have to learn things like slash begin-proof and slash end-proof and these kind of long-term dependencies. He was able to do that successfully. This is actually a randomly generated piece of LaTeX which is being created with a recurrent neural network. So today I am not going to show you exactly how it works, I'm going to try to give you an intuition. I'm going to start off by showing you how to think about neural networks as computational graphs. This is coming back to that word Ben used earlier, this idea of a graph. So I started out by trying to draw, this is like my notation, you won't see this anywhere else but it will do for now. Here is a picture of a single hidden layer basic neural network. We can think of it as having an input which is going to be of batch size and contain width of number of inputs. This orange arrow represents something that we're doing to that matrix. So each of the boxes represents a matrix and each of the arrows represents one or more things we do to that. In this case, we do a matrix product and then we throw it through a rectified linear unit. Then we get a circle which represents a matrix, but it's now a hidden layer which is of batch size by number of activations. Number of activations is just when we created that dense layer, we would have said dense and then we would have had some number. That number is how many activations we create. And then we put that through another operation, which in this case is a matrix product followed by a softmax. And so triangle here represents an output matrix and that's going to be of batch size by 1000. So this is my little way of representing the computation graph of a basic neural network with a single hidden layer. I'm now going to create some slightly more complex models, but I'm going to slightly reduce the amount of stuff on the screen. One thing to note is that batch size appears all the time, so I'm going to get rid of it. So here's the same thing where I've removed batch size. Also the specific activation function, who gives a shit? It's probably ReLU everywhere except the last layer where it's softmax, so I've removed that as well. So let's now look at what a convolutional neural network with a single dense hidden layer would look like. So we'd have our input, which this time will be, remember I've removed batch size, number of channels by height by width. The operation, and we're ignoring the activation function, is going to be a convolution followed by a max-pull. Remember, any shape is representing a matrix. So that gives us a matrix, which will be size numfilters by height over 2 by width over 2, since we did a max-pulling. And then we take that and we flatten it. I've put flatten in parentheses because flattening mathematically does nothing at all. Flattening is just telling Keras to think of it as a vector. It doesn't actually calculate anything, it doesn't move anything, it doesn't really do anything. It just says think of it as being a different shape. That's why I put it in parentheses. So let's then take a matrix product, and remember I'm not putting in the activation functions anymore. So that would be our dense layer, gives us our first fully-connected layer, which will be of size number of activations. And then we put that through a final matrix product to get an output of size number of classes. So here is how we can represent a convolutional neural network with a single dense hidden layer. Question Time 5 The number of activations is the same as we had last time, it's whatever the n was that we wrote dense n. Just like the number of filters is when we write convolution2D, we say number of filters followed by its size. So I'm going to now create a slightly more complex computation graph, but again I'm going to slightly simplify what I put on the screen, which is this time I am going to remove all of the layer operations. Now that we have removed the activation function, you can see that in every case we have some kind of linear thing, either a matrix product or a convolution, and optionally there might also be a max-poll. So really this is not adding much additional information, so I'm going to get rid of it from now on. So we're now not showing the layer operations. So remember now, every arrow is representing one or more layer operations, which will generally be a convolution or a matrix product, followed by an activation function, and maybe there will be a max-polling in there as well. So let's say we wanted to predict the third word of a 3-word string based on the previous two words. Now there's all kinds of ways we could do this, but here is one interesting way which you will now recognize you could do with Keras' functional API. We could take word1 input, and that could be either a one-hot encoded thing, in which case its size would be vocab size, or it could be an embedding of it. Doesn't really matter either way. We then stick that through a layer operation to get a matrix output, which is our first fully-connected layer. And this thing here, we could then take and put through another layer operation, but this time we could also add in the word2 input, either of vocab size or the embedding of it. Put that through a layer operation of its own. When we have two arrows coming in together, that represents a merge. And a merge could either be done as a sum or as a concat. I'm not going to say one's better than the other, but there are two ways we can take two input vectors and combine them together. So now at this point, we have the input from word2 after sticking that through a layer. We have the input from word1 after sticking that through two layers. Merge them together and stick that through another layer to get our output, which we could then compare to word3 and try to train that to recognize word3 from words1 and 2. So you could try this. You could try and build this network using some corpus you find online, see how it goes. Pretty obviously then you could bring it up another level to say let's try and predict the fourth word of a 3-word string using words1 and 2 and 3. Now the reason I'm doing it in this way is that what's happening is each time I'm going through another layer operation and then bringing in word2 and going through a layer operation and then bringing in word3 and going through a layer operation, is I am collecting state. Each of these things has the ability to capture state about all of the words that have come so far and the order in which they've arrived. So by the time I get to predicting word4, this matrix has had the opportunity to learn what does it need to know about the previous word's orderings and how they're connected to each other and so forth in order to predict this fourth word. So we're actually capturing state here. It's important to note we have not yet previously built a model in Keras which has input coming in at anywhere other than the first layer, but there's no reason we can't. One of you asked a great question earlier, which was could we use this to bring in metadata like the speed a car was going to add it with a convolutional neural network's image data. I said yes we can. So in this case we're doing the same thing, which is we're bringing in an additional word's worth of data and remember each time you see two different arrows coming in, that represents a merge operation. So here's a perfectly reasonable way of trying to predict the fourth word from the previous three words. So this leads to a really interesting question, which was what if instead we said let's bring in our word1 and then we had a layer operation in order to create our hidden state. And that would be enough to predict word2. And then to predict word3, could we just do a layer operation and generate itself. And then that could be used to predict word3. And then run it again to predict word4 and run it again to predict word5. This is called an RNN. And everything that you see here is exactly the same structurally as everything I've shown before. The colored-in areas represent matrices and the arrows represent layer operations. One of the really interesting things about an RNN is each of these arrows, that you see three arrows, there's only one weight matrix attached to those. In other words, it's the equivalent thing of saying every time you see an arrow from a circle to a circle, those two weight matrices have to be exactly the same. Every time you see an arrow from a rectangle to a circle, those three matrices have to be exactly the same. And then finally you've got an arrow from a circle to a triangle and that weight matrix is separate. The idea being that if you have a word coming in and being added to some state, why would you want to treat it differently depending on whether it's the first word in a string or the third word in a string, given that generally speaking we kind of split up strings pretty much at random anyway. We're going to be having a whole bunch of 11-word strings. So one of the nice things about this way of thinking about it where you have it going back to itself is that you can very clearly see there is one layer operation, one weight matrix for input to hidden, one for hidden to hidden, circle to circle, and one for hidden to output, i.e. circle to triangle. So we're going to talk about that in a lot more detail next week. For now, I'm just going to quickly show you something in the last one minute, which is that we can train something which takes, for example, all of the text of Nietzsche. So here's a bit of his text, I've just read it in here. We could split it up into every sequence of length 40. So I've gone through the whole text and grabbed every sequence of length 40. And then I've created an RNN and its goal is to take the sentence which represents the indexes from i to i plus 40 and predict the sentence from i plus 1 to i plus 40 plus 1. So every string of length maxLen, I'm trying to predict the string one word after that. And so I can take that now and create a model, an LSTM is a kind of recurrent neural network which we'll talk about next week, which has a recurrent neural network, starts of course with an embedding. And then I can train that by passing in my sentences and my sentence one character later. I can then say, let's try and generate 300 characters by building a prediction of what do you think the next character would be. And so I have to seed it with something. So I seeded it with something that felt very Nietzschean, ethics is a basic foundation of all that, and see what happens. And after training it for only a few seconds, I get ethics is a basic foundation of all that blah blah blah. You can get the sense it's starting to learn a bit about the idea that, oh by the way, one thing to mention is this Nietzsche corpus is slightly annoying. It has carriage returns after every line. So you'll see it's going to throw carriage returns in all over the place. And it's got some pretty hideous formatting. So that was after about training for 30 seconds. I train it for another 30 seconds and I get to a point where it's kind of understanding the concept of punctuation and spacing. And then I've trained it for 640 seconds and it's starting to actually create real words. And then I've trained it for another 640 seconds and interestingly each section of Nietzsche starts with a numbered section that looks exactly like this. It's even starting to learn to close its quotation marks. It also notes that at the start of a chapter it always has 3 lines, so it's learned to start chapters. After another 640 seconds, and another 640 seconds. And so by this time it's actually got to a point where it's saying some things which are so obscure and difficult to understand it could really be Nietzsche. These car RNN models are fun and all, but the reason that this is interesting is that we're showing that we only provided that amount of text and it was able to generate text out here because it has state, it has recurrence. What that means is that we could use this kind of model to generate something like SwiftKey, whereas you're typing, it's saying this is the next thing you're going to type. I would love you to think about during the week whether this is likely to help our IMDB sentiment model or not. That would be an interesting thing to talk about. Next week we will look into the details of how RNNs work.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.26, "text": " So I wanted to start off by showing you something I'm kind of excited about, which is here is", "tokens": [50364, 407, 286, 1415, 281, 722, 766, 538, 4099, 291, 746, 286, 478, 733, 295, 2919, 466, 11, 597, 307, 510, 307, 50627], "temperature": 0.0, "avg_logprob": -0.27641255093604017, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006095441058278084}, {"id": 1, "seek": 0, "start": 5.26, "end": 8.92, "text": " the dogs and cats competition, which we all know so well.", "tokens": [50627, 264, 7197, 293, 11111, 6211, 11, 597, 321, 439, 458, 370, 731, 13, 50810], "temperature": 0.0, "avg_logprob": -0.27641255093604017, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006095441058278084}, {"id": 2, "seek": 0, "start": 8.92, "end": 16.4, "text": " It was interesting that the winner of this competition won by a very big margin, a 1.1%", "tokens": [50810, 467, 390, 1880, 300, 264, 8507, 295, 341, 6211, 1582, 538, 257, 588, 955, 10270, 11, 257, 502, 13, 16, 4, 51184], "temperature": 0.0, "avg_logprob": -0.27641255093604017, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006095441058278084}, {"id": 3, "seek": 0, "start": 16.4, "end": 20.12, "text": " error versus a 1.7% error.", "tokens": [51184, 6713, 5717, 257, 502, 13, 22, 4, 6713, 13, 51370], "temperature": 0.0, "avg_logprob": -0.27641255093604017, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006095441058278084}, {"id": 4, "seek": 0, "start": 20.12, "end": 25.76, "text": " This is very unusual in a Kaggle competition to see anybody win by a 50-60% margin.", "tokens": [51370, 639, 307, 588, 10901, 294, 257, 48751, 22631, 6211, 281, 536, 4472, 1942, 538, 257, 2625, 12, 4550, 4, 10270, 13, 51652], "temperature": 0.0, "avg_logprob": -0.27641255093604017, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006095441058278084}, {"id": 5, "seek": 2576, "start": 25.76, "end": 32.32, "text": " You can see after that people are generally clustering around 91.1, 91.9, 91.1, 91.8,", "tokens": [50364, 509, 393, 536, 934, 300, 561, 366, 5101, 596, 48673, 926, 31064, 13, 16, 11, 31064, 13, 24, 11, 31064, 13, 16, 11, 31064, 13, 23, 11, 50692], "temperature": 0.0, "avg_logprob": -0.27911829480937883, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.012431042268872261}, {"id": 6, "seek": 2576, "start": 32.32, "end": 34.04, "text": " about the same kind of number.", "tokens": [50692, 466, 264, 912, 733, 295, 1230, 13, 50778], "temperature": 0.0, "avg_logprob": -0.27911829480937883, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.012431042268872261}, {"id": 7, "seek": 2576, "start": 34.04, "end": 35.92, "text": " So this was a pretty impressive performance.", "tokens": [50778, 407, 341, 390, 257, 1238, 8992, 3389, 13, 50872], "temperature": 0.0, "avg_logprob": -0.27911829480937883, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.012431042268872261}, {"id": 8, "seek": 2576, "start": 35.92, "end": 42.480000000000004, "text": " This is the guy who actually created a piece of deep learning software called Overfee.", "tokens": [50872, 639, 307, 264, 2146, 567, 767, 2942, 257, 2522, 295, 2452, 2539, 4722, 1219, 4886, 69, 1653, 13, 51200], "temperature": 0.0, "avg_logprob": -0.27911829480937883, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.012431042268872261}, {"id": 9, "seek": 2576, "start": 42.480000000000004, "end": 48.120000000000005, "text": " So I want to show you something pretty interesting, which is this week I tried something new and", "tokens": [51200, 407, 286, 528, 281, 855, 291, 746, 1238, 1880, 11, 597, 307, 341, 1243, 286, 3031, 746, 777, 293, 51482], "temperature": 0.0, "avg_logprob": -0.27911829480937883, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.012431042268872261}, {"id": 10, "seek": 2576, "start": 48.120000000000005, "end": 53.8, "text": " on dogs and cats got 98.95.", "tokens": [51482, 322, 7197, 293, 11111, 658, 20860, 13, 15718, 13, 51766], "temperature": 0.0, "avg_logprob": -0.27911829480937883, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.012431042268872261}, {"id": 11, "seek": 5380, "start": 53.8, "end": 58.4, "text": " So I want to show you how I did that.", "tokens": [50364, 407, 286, 528, 281, 855, 291, 577, 286, 630, 300, 13, 50594], "temperature": 0.0, "avg_logprob": -0.2594300050001878, "compression_ratio": 1.675977653631285, "no_speech_prob": 0.009267876856029034}, {"id": 12, "seek": 5380, "start": 58.4, "end": 63.72, "text": " The way I did that was by using nearly only techniques I've already shown you, which is", "tokens": [50594, 440, 636, 286, 630, 300, 390, 538, 1228, 6217, 787, 7512, 286, 600, 1217, 4898, 291, 11, 597, 307, 50860], "temperature": 0.0, "avg_logprob": -0.2594300050001878, "compression_ratio": 1.675977653631285, "no_speech_prob": 0.009267876856029034}, {"id": 13, "seek": 5380, "start": 63.72, "end": 74.24, "text": " basically I created a standard model, which is basically a dense model, and then I pre-computed", "tokens": [50860, 1936, 286, 2942, 257, 3832, 2316, 11, 597, 307, 1936, 257, 18011, 2316, 11, 293, 550, 286, 659, 12, 1112, 2582, 292, 51386], "temperature": 0.0, "avg_logprob": -0.2594300050001878, "compression_ratio": 1.675977653631285, "no_speech_prob": 0.009267876856029034}, {"id": 14, "seek": 5380, "start": 74.24, "end": 83.08, "text": " the last convolutional layer and then I trained the dense model lots of times.", "tokens": [51386, 264, 1036, 45216, 304, 4583, 293, 550, 286, 8895, 264, 18011, 2316, 3195, 295, 1413, 13, 51828], "temperature": 0.0, "avg_logprob": -0.2594300050001878, "compression_ratio": 1.675977653631285, "no_speech_prob": 0.009267876856029034}, {"id": 15, "seek": 8308, "start": 83.12, "end": 87.64, "text": " The other thing I did was to use some data augmentation, and I didn't actually have time", "tokens": [50366, 440, 661, 551, 286, 630, 390, 281, 764, 512, 1412, 14501, 19631, 11, 293, 286, 994, 380, 767, 362, 565, 50592], "temperature": 0.0, "avg_logprob": -0.2492406762164572, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0012842704309150577}, {"id": 16, "seek": 8308, "start": 87.64, "end": 91.96, "text": " to figure out the best data augmentation parameters, so I just picked some that seemed reasonable.", "tokens": [50592, 281, 2573, 484, 264, 1151, 1412, 14501, 19631, 9834, 11, 370, 286, 445, 6183, 512, 300, 6576, 10585, 13, 50808], "temperature": 0.0, "avg_logprob": -0.2492406762164572, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0012842704309150577}, {"id": 17, "seek": 8308, "start": 91.96, "end": 98.08, "text": " I should also mention this 98.95 would be easy to make a lot better.", "tokens": [50808, 286, 820, 611, 2152, 341, 20860, 13, 15718, 576, 312, 1858, 281, 652, 257, 688, 1101, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2492406762164572, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0012842704309150577}, {"id": 18, "seek": 8308, "start": 98.08, "end": 102.16, "text": " I'm not doing any pseudo-labeling here and I'm not even using the full dataset.", "tokens": [51114, 286, 478, 406, 884, 604, 35899, 12, 44990, 11031, 510, 293, 286, 478, 406, 754, 1228, 264, 1577, 28872, 13, 51318], "temperature": 0.0, "avg_logprob": -0.2492406762164572, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0012842704309150577}, {"id": 19, "seek": 8308, "start": 102.16, "end": 104.92, "text": " I put aside 2000 for the validation set.", "tokens": [51318, 286, 829, 7359, 8132, 337, 264, 24071, 992, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2492406762164572, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0012842704309150577}, {"id": 20, "seek": 8308, "start": 104.92, "end": 111.44, "text": " So with those two changes, we would definitely get well over 99% accuracy.", "tokens": [51456, 407, 365, 729, 732, 2962, 11, 321, 576, 2138, 483, 731, 670, 11803, 4, 14170, 13, 51782], "temperature": 0.0, "avg_logprob": -0.2492406762164572, "compression_ratio": 1.6085409252669038, "no_speech_prob": 0.0012842704309150577}, {"id": 21, "seek": 11144, "start": 111.44, "end": 118.16, "text": " The missing piece that I added is I added batch normalization to VGG.", "tokens": [50364, 440, 5361, 2522, 300, 286, 3869, 307, 286, 3869, 15245, 2710, 2144, 281, 691, 27561, 13, 50700], "temperature": 0.0, "avg_logprob": -0.24634089368454953, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0046094986610114574}, {"id": 22, "seek": 11144, "start": 118.16, "end": 123.32, "text": " So batch normalization, if you guys remember, I said the important takeaway is that all", "tokens": [50700, 407, 15245, 2710, 2144, 11, 498, 291, 1074, 1604, 11, 286, 848, 264, 1021, 30681, 307, 300, 439, 50958], "temperature": 0.0, "avg_logprob": -0.24634089368454953, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0046094986610114574}, {"id": 23, "seek": 11144, "start": 123.32, "end": 128.88, "text": " modern networks should use batch norm because you can get 10x or more improvements in training", "tokens": [50958, 4363, 9590, 820, 764, 15245, 2026, 570, 291, 393, 483, 1266, 87, 420, 544, 13797, 294, 3097, 51236], "temperature": 0.0, "avg_logprob": -0.24634089368454953, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0046094986610114574}, {"id": 24, "seek": 11144, "start": 128.88, "end": 133.8, "text": " speed and it tends to reduce overfitting.", "tokens": [51236, 3073, 293, 309, 12258, 281, 5407, 670, 69, 2414, 13, 51482], "temperature": 0.0, "avg_logprob": -0.24634089368454953, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0046094986610114574}, {"id": 25, "seek": 11144, "start": 133.8, "end": 140.24, "text": " Because of the second one, it means you can use less dropout, and dropout of course is", "tokens": [51482, 1436, 295, 264, 1150, 472, 11, 309, 1355, 291, 393, 764, 1570, 3270, 346, 11, 293, 3270, 346, 295, 1164, 307, 51804], "temperature": 0.0, "avg_logprob": -0.24634089368454953, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.0046094986610114574}, {"id": 26, "seek": 14024, "start": 140.28, "end": 146.72, "text": " destroying some of your network, so you don't want to use more dropout than necessary.", "tokens": [50366, 19926, 512, 295, 428, 3209, 11, 370, 291, 500, 380, 528, 281, 764, 544, 3270, 346, 813, 4818, 13, 50688], "temperature": 0.0, "avg_logprob": -0.2631953968721278, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.013636685907840729}, {"id": 27, "seek": 14024, "start": 146.72, "end": 150.74, "text": " So why didn't VGG already have batch norm?", "tokens": [50688, 407, 983, 994, 380, 691, 27561, 1217, 362, 15245, 2026, 30, 50889], "temperature": 0.0, "avg_logprob": -0.2631953968721278, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.013636685907840729}, {"id": 28, "seek": 14024, "start": 150.74, "end": 152.4, "text": " Because it didn't exist.", "tokens": [50889, 1436, 309, 994, 380, 2514, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2631953968721278, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.013636685907840729}, {"id": 29, "seek": 14024, "start": 152.4, "end": 163.64000000000001, "text": " So VGG was kind of mid-to-late 2014 and batch norm was maybe early-to-mid 2015.", "tokens": [50972, 407, 691, 27561, 390, 733, 295, 2062, 12, 1353, 12, 17593, 8227, 293, 15245, 2026, 390, 1310, 2440, 12, 1353, 12, 25394, 7546, 13, 51534], "temperature": 0.0, "avg_logprob": -0.2631953968721278, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.013636685907840729}, {"id": 30, "seek": 14024, "start": 163.64000000000001, "end": 169.04000000000002, "text": " So why haven't people added batch norm to VGG already?", "tokens": [51534, 407, 983, 2378, 380, 561, 3869, 15245, 2026, 281, 691, 27561, 1217, 30, 51804], "temperature": 0.0, "avg_logprob": -0.2631953968721278, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.013636685907840729}, {"id": 31, "seek": 16904, "start": 169.04, "end": 171.4, "text": " And the answer is actually interesting to think about.", "tokens": [50364, 400, 264, 1867, 307, 767, 1880, 281, 519, 466, 13, 50482], "temperature": 0.0, "avg_logprob": -0.2699055819167304, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.024052901193499565}, {"id": 32, "seek": 16904, "start": 171.4, "end": 180.84, "text": " So to remind you what batch norm is, batch norm is something which first of all normalizes", "tokens": [50482, 407, 281, 4160, 291, 437, 15245, 2026, 307, 11, 15245, 2026, 307, 746, 597, 700, 295, 439, 2710, 5660, 50954], "temperature": 0.0, "avg_logprob": -0.2699055819167304, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.024052901193499565}, {"id": 33, "seek": 16904, "start": 180.84, "end": 182.12, "text": " every intermediate layer.", "tokens": [50954, 633, 19376, 4583, 13, 51018], "temperature": 0.0, "avg_logprob": -0.2699055819167304, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.024052901193499565}, {"id": 34, "seek": 16904, "start": 182.12, "end": 186.32, "text": " So it normalizes all of the activations by subtracting the mean and dividing by the standard", "tokens": [51018, 407, 309, 2710, 5660, 439, 295, 264, 2430, 763, 538, 16390, 278, 264, 914, 293, 26764, 538, 264, 3832, 51228], "temperature": 0.0, "avg_logprob": -0.2699055819167304, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.024052901193499565}, {"id": 35, "seek": 16904, "start": 186.32, "end": 191.44, "text": " deviation, which is always a good idea.", "tokens": [51228, 25163, 11, 597, 307, 1009, 257, 665, 1558, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2699055819167304, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.024052901193499565}, {"id": 36, "seek": 16904, "start": 191.44, "end": 195.28, "text": " And I know somebody on the forum today asked why is it a good idea, and I put a link to", "tokens": [51484, 400, 286, 458, 2618, 322, 264, 17542, 965, 2351, 983, 307, 309, 257, 665, 1558, 11, 293, 286, 829, 257, 2113, 281, 51676], "temperature": 0.0, "avg_logprob": -0.2699055819167304, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.024052901193499565}, {"id": 37, "seek": 19528, "start": 195.36, "end": 196.96, "text": " some more information about that.", "tokens": [50368, 512, 544, 1589, 466, 300, 13, 50448], "temperature": 0.0, "avg_logprob": -0.26702834564505273, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.27820947766304016}, {"id": 38, "seek": 19528, "start": 196.96, "end": 202.68, "text": " So anybody who wants to know more about why do normalization, check out the forum.", "tokens": [50448, 407, 4472, 567, 2738, 281, 458, 544, 466, 983, 360, 2710, 2144, 11, 1520, 484, 264, 17542, 13, 50734], "temperature": 0.0, "avg_logprob": -0.26702834564505273, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.27820947766304016}, {"id": 39, "seek": 19528, "start": 202.68, "end": 210.48, "text": " But just doing that alone isn't enough because SGD is quite bloody-minded.", "tokens": [50734, 583, 445, 884, 300, 3312, 1943, 380, 1547, 570, 34520, 35, 307, 1596, 18938, 12, 23310, 13, 51124], "temperature": 0.0, "avg_logprob": -0.26702834564505273, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.27820947766304016}, {"id": 40, "seek": 19528, "start": 210.48, "end": 216.84, "text": " And so if it was trying to denormalize the activations because it thought that was a", "tokens": [51124, 400, 370, 498, 309, 390, 1382, 281, 1441, 24440, 1125, 264, 2430, 763, 570, 309, 1194, 300, 390, 257, 51442], "temperature": 0.0, "avg_logprob": -0.26702834564505273, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.27820947766304016}, {"id": 41, "seek": 19528, "start": 216.84, "end": 219.36, "text": " good thing to do, it would do so anyway.", "tokens": [51442, 665, 551, 281, 360, 11, 309, 576, 360, 370, 4033, 13, 51568], "temperature": 0.0, "avg_logprob": -0.26702834564505273, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.27820947766304016}, {"id": 42, "seek": 19528, "start": 219.36, "end": 223.62, "text": " So every time you try to normalize them, SGD would just undo it again.", "tokens": [51568, 407, 633, 565, 291, 853, 281, 2710, 1125, 552, 11, 34520, 35, 576, 445, 23779, 309, 797, 13, 51781], "temperature": 0.0, "avg_logprob": -0.26702834564505273, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.27820947766304016}, {"id": 43, "seek": 22362, "start": 224.06, "end": 230.34, "text": " What batch norm does is it adds two additional trainable parameters to each layer, one which", "tokens": [50386, 708, 15245, 2026, 775, 307, 309, 10860, 732, 4497, 3847, 712, 9834, 281, 1184, 4583, 11, 472, 597, 50700], "temperature": 0.0, "avg_logprob": -0.23367575236729213, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.003172593656927347}, {"id": 44, "seek": 22362, "start": 230.34, "end": 235.46, "text": " multiplies the activations and one which is added to the activations.", "tokens": [50700, 12788, 530, 264, 2430, 763, 293, 472, 597, 307, 3869, 281, 264, 2430, 763, 13, 50956], "temperature": 0.0, "avg_logprob": -0.23367575236729213, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.003172593656927347}, {"id": 45, "seek": 22362, "start": 235.46, "end": 243.18, "text": " So it basically allows it to undo the normalization, but not by changing every single weight, but", "tokens": [50956, 407, 309, 1936, 4045, 309, 281, 23779, 264, 2710, 2144, 11, 457, 406, 538, 4473, 633, 2167, 3364, 11, 457, 51342], "temperature": 0.0, "avg_logprob": -0.23367575236729213, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.003172593656927347}, {"id": 46, "seek": 22362, "start": 243.18, "end": 246.42000000000002, "text": " by just changing two weights for each activation.", "tokens": [51342, 538, 445, 4473, 732, 17443, 337, 1184, 24433, 13, 51504], "temperature": 0.0, "avg_logprob": -0.23367575236729213, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.003172593656927347}, {"id": 47, "seek": 22362, "start": 246.42000000000002, "end": 251.06, "text": " So it makes things much more stable in practice.", "tokens": [51504, 407, 309, 1669, 721, 709, 544, 8351, 294, 3124, 13, 51736], "temperature": 0.0, "avg_logprob": -0.23367575236729213, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.003172593656927347}, {"id": 48, "seek": 25106, "start": 251.06, "end": 255.78, "text": " So you can't just go ahead and stick batch norm into a pre-trained network, because if", "tokens": [50364, 407, 291, 393, 380, 445, 352, 2286, 293, 2897, 15245, 2026, 666, 257, 659, 12, 17227, 2001, 3209, 11, 570, 498, 50600], "temperature": 0.0, "avg_logprob": -0.26914376651539523, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0063881357200443745}, {"id": 49, "seek": 25106, "start": 255.78, "end": 265.18, "text": " you do, it's going to take that layer and it's going to subtract the mean and divide", "tokens": [50600, 291, 360, 11, 309, 311, 516, 281, 747, 300, 4583, 293, 309, 311, 516, 281, 16390, 264, 914, 293, 9845, 51070], "temperature": 0.0, "avg_logprob": -0.26914376651539523, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0063881357200443745}, {"id": 50, "seek": 25106, "start": 265.18, "end": 270.34000000000003, "text": " by the standard deviation, which means now those pre-trained weights from then on are", "tokens": [51070, 538, 264, 3832, 25163, 11, 597, 1355, 586, 729, 659, 12, 17227, 2001, 17443, 490, 550, 322, 366, 51328], "temperature": 0.0, "avg_logprob": -0.26914376651539523, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0063881357200443745}, {"id": 51, "seek": 25106, "start": 270.34000000000003, "end": 276.3, "text": " now wrong because those weights were created for a completely different set of activations.", "tokens": [51328, 586, 2085, 570, 729, 17443, 645, 2942, 337, 257, 2584, 819, 992, 295, 2430, 763, 13, 51626], "temperature": 0.0, "avg_logprob": -0.26914376651539523, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0063881357200443745}, {"id": 52, "seek": 27630, "start": 276.3, "end": 285.06, "text": " So it's not rocket science, but I realized all we need to do is to insert a batch norm", "tokens": [50364, 407, 309, 311, 406, 13012, 3497, 11, 457, 286, 5334, 439, 321, 643, 281, 360, 307, 281, 8969, 257, 15245, 2026, 50802], "temperature": 0.0, "avg_logprob": -0.27919248853410994, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.03209862485527992}, {"id": 53, "seek": 27630, "start": 285.06, "end": 291.98, "text": " layer and figure out what the mean and standard deviation of the incoming activations would", "tokens": [50802, 4583, 293, 2573, 484, 437, 264, 914, 293, 3832, 25163, 295, 264, 22341, 2430, 763, 576, 51148], "temperature": 0.0, "avg_logprob": -0.27919248853410994, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.03209862485527992}, {"id": 54, "seek": 27630, "start": 291.98, "end": 298.62, "text": " be for that data set and basically create the batch norm layer such that the two trainable", "tokens": [51148, 312, 337, 300, 1412, 992, 293, 1936, 1884, 264, 15245, 2026, 4583, 1270, 300, 264, 732, 3847, 712, 51480], "temperature": 0.0, "avg_logprob": -0.27919248853410994, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.03209862485527992}, {"id": 55, "seek": 27630, "start": 298.62, "end": 301.86, "text": " parameters immediately undo that.", "tokens": [51480, 9834, 4258, 23779, 300, 13, 51642], "temperature": 0.0, "avg_logprob": -0.27919248853410994, "compression_ratio": 1.5699481865284974, "no_speech_prob": 0.03209862485527992}, {"id": 56, "seek": 30186, "start": 301.94, "end": 308.42, "text": " That way we would insert a batch norm layer and it would not change the outputs at all.", "tokens": [50368, 663, 636, 321, 576, 8969, 257, 15245, 2026, 4583, 293, 309, 576, 406, 1319, 264, 23930, 412, 439, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2610999975311622, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.029311085119843483}, {"id": 57, "seek": 30186, "start": 308.42, "end": 317.74, "text": " So I grabbed the whole of ImageNet and I created this standard dense layer model.", "tokens": [50692, 407, 286, 18607, 264, 1379, 295, 29903, 31890, 293, 286, 2942, 341, 3832, 18011, 4583, 2316, 13, 51158], "temperature": 0.0, "avg_logprob": -0.2610999975311622, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.029311085119843483}, {"id": 58, "seek": 30186, "start": 317.74, "end": 324.88, "text": " I pre-computed the convolutional outputs for all of ImageNet and then I created two batch", "tokens": [51158, 286, 659, 12, 1112, 2582, 292, 264, 45216, 304, 23930, 337, 439, 295, 29903, 31890, 293, 550, 286, 2942, 732, 15245, 51515], "temperature": 0.0, "avg_logprob": -0.2610999975311622, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.029311085119843483}, {"id": 59, "seek": 30186, "start": 324.88, "end": 330.58000000000004, "text": " norm layers and I created a little function which allows us to insert a layer into an", "tokens": [51515, 2026, 7914, 293, 286, 2942, 257, 707, 2445, 597, 4045, 505, 281, 8969, 257, 4583, 666, 364, 51800], "temperature": 0.0, "avg_logprob": -0.2610999975311622, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.029311085119843483}, {"id": 60, "seek": 30186, "start": 330.58000000000004, "end": 331.58000000000004, "text": " existing model.", "tokens": [51800, 6741, 2316, 13, 51850], "temperature": 0.0, "avg_logprob": -0.2610999975311622, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.029311085119843483}, {"id": 61, "seek": 33158, "start": 332.3, "end": 337.5, "text": " I inserted the layers just after the two dense layers.", "tokens": [50400, 286, 27992, 264, 7914, 445, 934, 264, 732, 18011, 7914, 13, 50660], "temperature": 0.0, "avg_logprob": -0.2512339856251177, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.015188816003501415}, {"id": 62, "seek": 33158, "start": 337.5, "end": 338.97999999999996, "text": " And then here's the key piece.", "tokens": [50660, 400, 550, 510, 311, 264, 2141, 2522, 13, 50734], "temperature": 0.0, "avg_logprob": -0.2512339856251177, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.015188816003501415}, {"id": 63, "seek": 33158, "start": 338.97999999999996, "end": 346.21999999999997, "text": " I set the weights on the new batch norm layers equal to the variance and the mean, which", "tokens": [50734, 286, 992, 264, 17443, 322, 264, 777, 15245, 2026, 7914, 2681, 281, 264, 21977, 293, 264, 914, 11, 597, 51096], "temperature": 0.0, "avg_logprob": -0.2512339856251177, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.015188816003501415}, {"id": 64, "seek": 33158, "start": 346.21999999999997, "end": 350.53999999999996, "text": " I calculated on all of ImageNet.", "tokens": [51096, 286, 15598, 322, 439, 295, 29903, 31890, 13, 51312], "temperature": 0.0, "avg_logprob": -0.2512339856251177, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.015188816003501415}, {"id": 65, "seek": 33158, "start": 350.53999999999996, "end": 354.7, "text": " So I calculated the mean of each of those two layer outputs and the variance of each", "tokens": [51312, 407, 286, 15598, 264, 914, 295, 1184, 295, 729, 732, 4583, 23930, 293, 264, 21977, 295, 1184, 51520], "temperature": 0.0, "avg_logprob": -0.2512339856251177, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.015188816003501415}, {"id": 66, "seek": 33158, "start": 354.7, "end": 356.7, "text": " of those two layer outputs.", "tokens": [51520, 295, 729, 732, 4583, 23930, 13, 51620], "temperature": 0.0, "avg_logprob": -0.2512339856251177, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.015188816003501415}, {"id": 67, "seek": 35670, "start": 356.7, "end": 363.26, "text": " So that allowed me to insert these batch norm layers into an existing model.", "tokens": [50364, 407, 300, 4350, 385, 281, 8969, 613, 15245, 2026, 7914, 666, 364, 6741, 2316, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2490847327492454, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.009708313271403313}, {"id": 68, "seek": 35670, "start": 363.26, "end": 368.53999999999996, "text": " And then afterwards I evaluated it and I checked that indeed it's giving me the same answers", "tokens": [50692, 400, 550, 10543, 286, 25509, 309, 293, 286, 10033, 300, 6451, 309, 311, 2902, 385, 264, 912, 6338, 50956], "temperature": 0.0, "avg_logprob": -0.2490847327492454, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.009708313271403313}, {"id": 69, "seek": 35670, "start": 368.53999999999996, "end": 371.56, "text": " as it was before.", "tokens": [50956, 382, 309, 390, 949, 13, 51107], "temperature": 0.0, "avg_logprob": -0.2490847327492454, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.009708313271403313}, {"id": 70, "seek": 35670, "start": 371.56, "end": 381.14, "text": " As well as doing that, I then thought if you train a model with batch norm from the start,", "tokens": [51107, 1018, 731, 382, 884, 300, 11, 286, 550, 1194, 498, 291, 3847, 257, 2316, 365, 15245, 2026, 490, 264, 722, 11, 51586], "temperature": 0.0, "avg_logprob": -0.2490847327492454, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.009708313271403313}, {"id": 71, "seek": 35670, "start": 381.14, "end": 385.18, "text": " you're going to end up with weights which are designed to take advantage of the fact", "tokens": [51586, 291, 434, 516, 281, 917, 493, 365, 17443, 597, 366, 4761, 281, 747, 5002, 295, 264, 1186, 51788], "temperature": 0.0, "avg_logprob": -0.2490847327492454, "compression_ratio": 1.6133333333333333, "no_speech_prob": 0.009708313271403313}, {"id": 72, "seek": 38518, "start": 385.22, "end": 387.38, "text": " that the activations are being normalized.", "tokens": [50366, 300, 264, 2430, 763, 366, 885, 48704, 13, 50474], "temperature": 0.0, "avg_logprob": -0.264988765921644, "compression_ratio": 1.6208530805687205, "no_speech_prob": 0.014728122390806675}, {"id": 73, "seek": 38518, "start": 387.38, "end": 392.74, "text": " So I thought I wonder what would happen if we now fine-tuned the ImageNet network on", "tokens": [50474, 407, 286, 1194, 286, 2441, 437, 576, 1051, 498, 321, 586, 2489, 12, 83, 43703, 264, 29903, 31890, 3209, 322, 50742], "temperature": 0.0, "avg_logprob": -0.264988765921644, "compression_ratio": 1.6208530805687205, "no_speech_prob": 0.014728122390806675}, {"id": 74, "seek": 38518, "start": 392.74, "end": 397.34000000000003, "text": " all of ImageNet after we added these batch norm layers.", "tokens": [50742, 439, 295, 29903, 31890, 934, 321, 3869, 613, 15245, 2026, 7914, 13, 50972], "temperature": 0.0, "avg_logprob": -0.264988765921644, "compression_ratio": 1.6208530805687205, "no_speech_prob": 0.014728122390806675}, {"id": 75, "seek": 38518, "start": 397.34000000000003, "end": 405.1, "text": " So I then tried training it for one epoch on both the ImageNet images and the horizontally-flipped", "tokens": [50972, 407, 286, 550, 3031, 3097, 309, 337, 472, 30992, 339, 322, 1293, 264, 29903, 31890, 5267, 293, 264, 33796, 12, 69, 2081, 3320, 51360], "temperature": 0.0, "avg_logprob": -0.264988765921644, "compression_ratio": 1.6208530805687205, "no_speech_prob": 0.014728122390806675}, {"id": 76, "seek": 38518, "start": 405.1, "end": 406.1, "text": " ImageNet images.", "tokens": [51360, 29903, 31890, 5267, 13, 51410], "temperature": 0.0, "avg_logprob": -0.264988765921644, "compression_ratio": 1.6208530805687205, "no_speech_prob": 0.014728122390806675}, {"id": 77, "seek": 38518, "start": 406.1, "end": 410.9, "text": " So that's what these 2.5 million here are.", "tokens": [51410, 407, 300, 311, 437, 613, 568, 13, 20, 2459, 510, 366, 13, 51650], "temperature": 0.0, "avg_logprob": -0.264988765921644, "compression_ratio": 1.6208530805687205, "no_speech_prob": 0.014728122390806675}, {"id": 78, "seek": 41090, "start": 410.9, "end": 419.17999999999995, "text": " You can see with modern GPUs, it only takes less than an hour to run the entirety of ImageNet", "tokens": [50364, 509, 393, 536, 365, 4363, 18407, 82, 11, 309, 787, 2516, 1570, 813, 364, 1773, 281, 1190, 264, 31557, 295, 29903, 31890, 50778], "temperature": 0.0, "avg_logprob": -0.3201466016871955, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.05108073353767395}, {"id": 79, "seek": 41090, "start": 419.17999999999995, "end": 420.5, "text": " twice.", "tokens": [50778, 6091, 13, 50844], "temperature": 0.0, "avg_logprob": -0.3201466016871955, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.05108073353767395}, {"id": 80, "seek": 41090, "start": 420.5, "end": 426.5, "text": " And the interesting thing was that my accuracy on the validation set went up from 63% to", "tokens": [50844, 400, 264, 1880, 551, 390, 300, 452, 14170, 322, 264, 24071, 992, 1437, 493, 490, 25082, 4, 281, 51144], "temperature": 0.0, "avg_logprob": -0.3201466016871955, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.05108073353767395}, {"id": 81, "seek": 41090, "start": 426.5, "end": 427.5, "text": " 67%.", "tokens": [51144, 23879, 6856, 51194], "temperature": 0.0, "avg_logprob": -0.3201466016871955, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.05108073353767395}, {"id": 82, "seek": 41090, "start": 427.5, "end": 433.29999999999995, "text": " So adding batch norm actually improves ImageNet, which is cool.", "tokens": [51194, 407, 5127, 15245, 2026, 767, 24771, 29903, 31890, 11, 597, 307, 1627, 13, 51484], "temperature": 0.0, "avg_logprob": -0.3201466016871955, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.05108073353767395}, {"id": 83, "seek": 41090, "start": 433.29999999999995, "end": 436.65999999999997, "text": " That wasn't the main reason I did it, the main reason I did it was so we can now use", "tokens": [51484, 663, 2067, 380, 264, 2135, 1778, 286, 630, 309, 11, 264, 2135, 1778, 286, 630, 309, 390, 370, 321, 393, 586, 764, 51652], "temperature": 0.0, "avg_logprob": -0.3201466016871955, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.05108073353767395}, {"id": 84, "seek": 43666, "start": 437.66, "end": 441.3, "text": " VGG with batch norm in our models.", "tokens": [50414, 691, 27561, 365, 15245, 2026, 294, 527, 5245, 13, 50596], "temperature": 0.0, "avg_logprob": -0.3147030437693876, "compression_ratio": 1.1318681318681318, "no_speech_prob": 0.3628881275653839}, {"id": 85, "seek": 43666, "start": 441.3, "end": 458.98, "text": " So I did all that, I saved the weights, I then edited our VGG model.", "tokens": [50596, 407, 286, 630, 439, 300, 11, 286, 6624, 264, 17443, 11, 286, 550, 23016, 527, 691, 27561, 2316, 13, 51480], "temperature": 0.0, "avg_logprob": -0.3147030437693876, "compression_ratio": 1.1318681318681318, "no_speech_prob": 0.3628881275653839}, {"id": 86, "seek": 45898, "start": 458.98, "end": 469.46000000000004, "text": " So if we now look at the fully-connected block in our VGG model, it now has batch norm", "tokens": [50364, 407, 498, 321, 586, 574, 412, 264, 4498, 12, 9826, 292, 3461, 294, 527, 691, 27561, 2316, 11, 309, 586, 575, 15245, 2026, 50888], "temperature": 0.0, "avg_logprob": -0.3171855785228588, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.0526106171309948}, {"id": 87, "seek": 45898, "start": 469.46000000000004, "end": 473.62, "text": " in there.", "tokens": [50888, 294, 456, 13, 51096], "temperature": 0.0, "avg_logprob": -0.3171855785228588, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.0526106171309948}, {"id": 88, "seek": 45898, "start": 473.62, "end": 482.66, "text": " I also saved to our website a new weights file called VGG16.bn for batch norm.", "tokens": [51096, 286, 611, 6624, 281, 527, 3144, 257, 777, 17443, 3991, 1219, 691, 27561, 6866, 13, 19404, 337, 15245, 2026, 13, 51548], "temperature": 0.0, "avg_logprob": -0.3171855785228588, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.0526106171309948}, {"id": 89, "seek": 48266, "start": 482.66, "end": 490.62, "text": " So then when I did cats and dogs, I used that model.", "tokens": [50364, 407, 550, 562, 286, 630, 11111, 293, 7197, 11, 286, 1143, 300, 2316, 13, 50762], "temperature": 0.0, "avg_logprob": -0.2856849553633709, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0939929336309433}, {"id": 90, "seek": 48266, "start": 490.62, "end": 499.74, "text": " So now if you go and re-download from platform.ai the VGG16.py, it will automatically download", "tokens": [50762, 407, 586, 498, 291, 352, 293, 319, 12, 5093, 2907, 490, 3663, 13, 1301, 264, 691, 27561, 6866, 13, 8200, 11, 309, 486, 6772, 5484, 51218], "temperature": 0.0, "avg_logprob": -0.2856849553633709, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0939929336309433}, {"id": 91, "seek": 48266, "start": 499.74, "end": 501.1, "text": " the new weights.", "tokens": [51218, 264, 777, 17443, 13, 51286], "temperature": 0.0, "avg_logprob": -0.2856849553633709, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0939929336309433}, {"id": 92, "seek": 48266, "start": 501.1, "end": 503.86, "text": " You will have this without any changes to your code.", "tokens": [51286, 509, 486, 362, 341, 1553, 604, 2962, 281, 428, 3089, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2856849553633709, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0939929336309433}, {"id": 93, "seek": 48266, "start": 503.86, "end": 507.54, "text": " So I'd be interested to hear during the week if you try this out, just rerun the code you've", "tokens": [51424, 407, 286, 1116, 312, 3102, 281, 1568, 1830, 264, 1243, 498, 291, 853, 341, 484, 11, 445, 43819, 409, 264, 3089, 291, 600, 51608], "temperature": 0.0, "avg_logprob": -0.2856849553633709, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0939929336309433}, {"id": 94, "seek": 48266, "start": 507.54, "end": 510.66, "text": " got, whether you see improvements.", "tokens": [51608, 658, 11, 1968, 291, 536, 13797, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2856849553633709, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0939929336309433}, {"id": 95, "seek": 51066, "start": 510.66, "end": 511.66, "text": " Hopefully you will.", "tokens": [50364, 10429, 291, 486, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 96, "seek": 51066, "start": 511.66, "end": 516.58, "text": " Hopefully you'll find it trains more quickly and you get better results.", "tokens": [50414, 10429, 291, 603, 915, 309, 16329, 544, 2661, 293, 291, 483, 1101, 3542, 13, 50660], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 97, "seek": 51066, "start": 516.58, "end": 521.44, "text": " At this stage, I've only added batch norm to the dense layers, not to the convolutional", "tokens": [50660, 1711, 341, 3233, 11, 286, 600, 787, 3869, 15245, 2026, 281, 264, 18011, 7914, 11, 406, 281, 264, 45216, 304, 50903], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 98, "seek": 51066, "start": 521.44, "end": 523.62, "text": " layers.", "tokens": [50903, 7914, 13, 51012], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 99, "seek": 51066, "start": 523.62, "end": 527.7, "text": " There's no reason I shouldn't add it to the convolutional layers as well, I just had other", "tokens": [51012, 821, 311, 572, 1778, 286, 4659, 380, 909, 309, 281, 264, 45216, 304, 7914, 382, 731, 11, 286, 445, 632, 661, 51216], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 100, "seek": 51066, "start": 527.7, "end": 529.82, "text": " things to do this week.", "tokens": [51216, 721, 281, 360, 341, 1243, 13, 51322], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 101, "seek": 51066, "start": 529.82, "end": 533.98, "text": " But hopefully, since most of us are mainly fine-tuning just the dense layers, this is", "tokens": [51322, 583, 4696, 11, 1670, 881, 295, 505, 366, 8704, 2489, 12, 83, 37726, 445, 264, 18011, 7914, 11, 341, 307, 51530], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 102, "seek": 51066, "start": 533.98, "end": 538.78, "text": " going to impact most of us the most anyway.", "tokens": [51530, 516, 281, 2712, 881, 295, 505, 264, 881, 4033, 13, 51770], "temperature": 0.0, "avg_logprob": -0.2644012617028278, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.13115990161895752}, {"id": 103, "seek": 53878, "start": 538.9, "end": 548.18, "text": " So that's an exciting step which everybody can now use.", "tokens": [50370, 407, 300, 311, 364, 4670, 1823, 597, 2201, 393, 586, 764, 13, 50834], "temperature": 0.0, "avg_logprob": -0.2903447208634342, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.002672954462468624}, {"id": 104, "seek": 53878, "start": 548.18, "end": 554.02, "text": " The other thing to mention is now that you'll be using batch norm by default in your VGG", "tokens": [50834, 440, 661, 551, 281, 2152, 307, 586, 300, 291, 603, 312, 1228, 15245, 2026, 538, 7576, 294, 428, 691, 27561, 51126], "temperature": 0.0, "avg_logprob": -0.2903447208634342, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.002672954462468624}, {"id": 105, "seek": 53878, "start": 554.02, "end": 558.14, "text": " networks, you should find that you can increase your learning rates.", "tokens": [51126, 9590, 11, 291, 820, 915, 300, 291, 393, 3488, 428, 2539, 6846, 13, 51332], "temperature": 0.0, "avg_logprob": -0.2903447208634342, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.002672954462468624}, {"id": 106, "seek": 53878, "start": 558.14, "end": 564.26, "text": " Because batch norm normalizes the activations, it makes sure that there's no activation that's", "tokens": [51332, 1436, 15245, 2026, 2710, 5660, 264, 2430, 763, 11, 309, 1669, 988, 300, 456, 311, 572, 24433, 300, 311, 51638], "temperature": 0.0, "avg_logprob": -0.2903447208634342, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.002672954462468624}, {"id": 107, "seek": 53878, "start": 564.26, "end": 566.6999999999999, "text": " gone really high or really low.", "tokens": [51638, 2780, 534, 1090, 420, 534, 2295, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2903447208634342, "compression_ratio": 1.596244131455399, "no_speech_prob": 0.002672954462468624}, {"id": 108, "seek": 56670, "start": 566.82, "end": 570.62, "text": " That means that generally speaking, you can use higher learning rates.", "tokens": [50370, 663, 1355, 300, 5101, 4124, 11, 291, 393, 764, 2946, 2539, 6846, 13, 50560], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 109, "seek": 56670, "start": 570.62, "end": 573.94, "text": " So if you try higher learning rates in your code than you were before, you should find", "tokens": [50560, 407, 498, 291, 853, 2946, 2539, 6846, 294, 428, 3089, 813, 291, 645, 949, 11, 291, 820, 915, 50726], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 110, "seek": 56670, "start": 573.94, "end": 576.2800000000001, "text": " that they work pretty well.", "tokens": [50726, 300, 436, 589, 1238, 731, 13, 50843], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 111, "seek": 56670, "start": 576.2800000000001, "end": 581.62, "text": " You should also find that things that previously you couldn't get to train, now will start", "tokens": [50843, 509, 820, 611, 915, 300, 721, 300, 8046, 291, 2809, 380, 483, 281, 3847, 11, 586, 486, 722, 51110], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 112, "seek": 56670, "start": 581.62, "end": 582.7, "text": " to train.", "tokens": [51110, 281, 3847, 13, 51164], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 113, "seek": 56670, "start": 582.7, "end": 588.0600000000001, "text": " Because often the reason that they don't train is because one of the activations shoots off", "tokens": [51164, 1436, 2049, 264, 1778, 300, 436, 500, 380, 3847, 307, 570, 472, 295, 264, 2430, 763, 20704, 766, 51432], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 114, "seek": 56670, "start": 588.0600000000001, "end": 595.26, "text": " into really high or really low and screws everything up, and that gets fixed when you", "tokens": [51432, 666, 534, 1090, 420, 534, 2295, 293, 13050, 1203, 493, 11, 293, 300, 2170, 6806, 562, 291, 51792], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 115, "seek": 56670, "start": 595.26, "end": 596.4200000000001, "text": " use batch norm.", "tokens": [51792, 764, 15245, 2026, 13, 51850], "temperature": 0.0, "avg_logprob": -0.26059177232825237, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.04813271760940552}, {"id": 116, "seek": 59642, "start": 597.14, "end": 598.14, "text": " So there's some things to try this week.", "tokens": [50400, 407, 456, 311, 512, 721, 281, 853, 341, 1243, 13, 50450], "temperature": 0.0, "avg_logprob": -0.3289391653878348, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.02442237176001072}, {"id": 117, "seek": 59642, "start": 598.14, "end": 602.06, "text": " I'll be interested to hear how you go.", "tokens": [50450, 286, 603, 312, 3102, 281, 1568, 577, 291, 352, 13, 50646], "temperature": 0.0, "avg_logprob": -0.3289391653878348, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.02442237176001072}, {"id": 118, "seek": 59642, "start": 602.06, "end": 608.3399999999999, "text": " So last week we looked at collaborative filtering.", "tokens": [50646, 407, 1036, 1243, 321, 2956, 412, 16555, 30822, 13, 50960], "temperature": 0.0, "avg_logprob": -0.3289391653878348, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.02442237176001072}, {"id": 119, "seek": 59642, "start": 608.3399999999999, "end": 616.42, "text": " And to remind you, we had a file that basically meant something like this.", "tokens": [50960, 400, 281, 4160, 291, 11, 321, 632, 257, 3991, 300, 1936, 4140, 746, 411, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3289391653878348, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.02442237176001072}, {"id": 120, "seek": 59642, "start": 616.42, "end": 622.14, "text": " We had a bunch of movies and a bunch of users, and for some subset of those combinations,", "tokens": [51364, 492, 632, 257, 3840, 295, 6233, 293, 257, 3840, 295, 5022, 11, 293, 337, 512, 25993, 295, 729, 21267, 11, 51650], "temperature": 0.0, "avg_logprob": -0.3289391653878348, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.02442237176001072}, {"id": 121, "seek": 62214, "start": 622.14, "end": 627.5, "text": " we had a review of that movie by that user.", "tokens": [50364, 321, 632, 257, 3131, 295, 300, 3169, 538, 300, 4195, 13, 50632], "temperature": 0.0, "avg_logprob": -0.22265495137965424, "compression_ratio": 1.8042328042328042, "no_speech_prob": 0.2782368063926697}, {"id": 122, "seek": 62214, "start": 627.5, "end": 632.5, "text": " The way the actual file came to us didn't look like this, this is a crosstab.", "tokens": [50632, 440, 636, 264, 3539, 3991, 1361, 281, 505, 994, 380, 574, 411, 341, 11, 341, 307, 257, 28108, 372, 455, 13, 50882], "temperature": 0.0, "avg_logprob": -0.22265495137965424, "compression_ratio": 1.8042328042328042, "no_speech_prob": 0.2782368063926697}, {"id": 123, "seek": 62214, "start": 632.5, "end": 636.18, "text": " The way the file came to us looked like this.", "tokens": [50882, 440, 636, 264, 3991, 1361, 281, 505, 2956, 411, 341, 13, 51066], "temperature": 0.0, "avg_logprob": -0.22265495137965424, "compression_ratio": 1.8042328042328042, "no_speech_prob": 0.2782368063926697}, {"id": 124, "seek": 62214, "start": 636.18, "end": 642.58, "text": " Each row was a single user rating a single movie with a single rating at a single time.", "tokens": [51066, 6947, 5386, 390, 257, 2167, 4195, 10990, 257, 2167, 3169, 365, 257, 2167, 10990, 412, 257, 2167, 565, 13, 51386], "temperature": 0.0, "avg_logprob": -0.22265495137965424, "compression_ratio": 1.8042328042328042, "no_speech_prob": 0.2782368063926697}, {"id": 125, "seek": 62214, "start": 642.58, "end": 651.42, "text": " So I showed you in Excel how we could take the crosstab version and we could create a", "tokens": [51386, 407, 286, 4712, 291, 294, 19060, 577, 321, 727, 747, 264, 28108, 372, 455, 3037, 293, 321, 727, 1884, 257, 51828], "temperature": 0.0, "avg_logprob": -0.22265495137965424, "compression_ratio": 1.8042328042328042, "no_speech_prob": 0.2782368063926697}, {"id": 126, "seek": 65142, "start": 651.42, "end": 660.26, "text": " table of dot products, where the dot products would be between a set of 5 random numbers", "tokens": [50364, 3199, 295, 5893, 3383, 11, 689, 264, 5893, 3383, 576, 312, 1296, 257, 992, 295, 1025, 4974, 3547, 50806], "temperature": 0.0, "avg_logprob": -0.31522982237768954, "compression_ratio": 1.767605633802817, "no_speech_prob": 0.004070077557116747}, {"id": 127, "seek": 65142, "start": 660.26, "end": 664.06, "text": " for the movie and 5 random numbers for the user.", "tokens": [50806, 337, 264, 3169, 293, 1025, 4974, 3547, 337, 264, 4195, 13, 50996], "temperature": 0.0, "avg_logprob": -0.31522982237768954, "compression_ratio": 1.767605633802817, "no_speech_prob": 0.004070077557116747}, {"id": 128, "seek": 65142, "start": 664.06, "end": 670.74, "text": " And we could then use gradient descent to optimize those sets of 5 random numbers for", "tokens": [50996, 400, 321, 727, 550, 764, 16235, 23475, 281, 19719, 729, 6352, 295, 1025, 4974, 3547, 337, 51330], "temperature": 0.0, "avg_logprob": -0.31522982237768954, "compression_ratio": 1.767605633802817, "no_speech_prob": 0.004070077557116747}, {"id": 129, "seek": 65142, "start": 670.74, "end": 673.54, "text": " every user and every movie.", "tokens": [51330, 633, 4195, 293, 633, 3169, 13, 51470], "temperature": 0.0, "avg_logprob": -0.31522982237768954, "compression_ratio": 1.767605633802817, "no_speech_prob": 0.004070077557116747}, {"id": 130, "seek": 67354, "start": 673.54, "end": 681.4599999999999, "text": " And if we did so, we end up getting pretty decent guesses as to the original ratings.", "tokens": [50364, 400, 498, 321, 630, 370, 11, 321, 917, 493, 1242, 1238, 8681, 42703, 382, 281, 264, 3380, 24603, 13, 50760], "temperature": 0.0, "avg_logprob": -0.28005821228027344, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.08631472289562225}, {"id": 131, "seek": 67354, "start": 681.4599999999999, "end": 687.3399999999999, "text": " And then we went a step further in the spreadsheet and we learned how you could take the dot", "tokens": [50760, 400, 550, 321, 1437, 257, 1823, 3052, 294, 264, 27733, 293, 321, 3264, 577, 291, 727, 747, 264, 5893, 51054], "temperature": 0.0, "avg_logprob": -0.28005821228027344, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.08631472289562225}, {"id": 132, "seek": 67354, "start": 687.3399999999999, "end": 695.5, "text": " product and you could also add on a single bias, a movie bias and a user bias.", "tokens": [51054, 1674, 293, 291, 727, 611, 909, 322, 257, 2167, 12577, 11, 257, 3169, 12577, 293, 257, 4195, 12577, 13, 51462], "temperature": 0.0, "avg_logprob": -0.28005821228027344, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.08631472289562225}, {"id": 133, "seek": 67354, "start": 695.5, "end": 697.5799999999999, "text": " So we saw all that in Excel.", "tokens": [51462, 407, 321, 1866, 439, 300, 294, 19060, 13, 51566], "temperature": 0.0, "avg_logprob": -0.28005821228027344, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.08631472289562225}, {"id": 134, "seek": 69758, "start": 697.58, "end": 704.94, "text": " And we also learned that Excel comes with a gradient descent solver called, funnily", "tokens": [50364, 400, 321, 611, 3264, 300, 19060, 1487, 365, 257, 16235, 23475, 1404, 331, 1219, 11, 1019, 77, 953, 50732], "temperature": 0.0, "avg_logprob": -0.2987660453433082, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.05834178999066353}, {"id": 135, "seek": 69758, "start": 704.94, "end": 706.46, "text": " enough, Solver.", "tokens": [50732, 1547, 11, 7026, 331, 13, 50808], "temperature": 0.0, "avg_logprob": -0.2987660453433082, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.05834178999066353}, {"id": 136, "seek": 69758, "start": 706.46, "end": 713.74, "text": " And we saw that if we ran Solver on telling it that these are our varying cells and this", "tokens": [50808, 400, 321, 1866, 300, 498, 321, 5872, 7026, 331, 322, 3585, 309, 300, 613, 366, 527, 22984, 5438, 293, 341, 51172], "temperature": 0.0, "avg_logprob": -0.2987660453433082, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.05834178999066353}, {"id": 137, "seek": 69758, "start": 713.74, "end": 721.14, "text": " is our target cell, then it came up with some pretty decent weight matrices.", "tokens": [51172, 307, 527, 3779, 2815, 11, 550, 309, 1361, 493, 365, 512, 1238, 8681, 3364, 32284, 13, 51542], "temperature": 0.0, "avg_logprob": -0.2987660453433082, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.05834178999066353}, {"id": 138, "seek": 69758, "start": 721.14, "end": 725.1400000000001, "text": " We learned that these kinds of weight matrices are called embeddings.", "tokens": [51542, 492, 3264, 300, 613, 3685, 295, 3364, 32284, 366, 1219, 12240, 29432, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2987660453433082, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.05834178999066353}, {"id": 139, "seek": 72514, "start": 725.14, "end": 730.3, "text": " An embedding is basically something where we can start with an integer, like 27, and", "tokens": [50364, 1107, 12240, 3584, 307, 1936, 746, 689, 321, 393, 722, 365, 364, 24922, 11, 411, 7634, 11, 293, 50622], "temperature": 0.0, "avg_logprob": -0.3054966926574707, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.034099217504262924}, {"id": 140, "seek": 72514, "start": 730.3, "end": 734.6999999999999, "text": " look up movie number 27's vector of weights.", "tokens": [50622, 574, 493, 3169, 1230, 7634, 311, 8062, 295, 17443, 13, 50842], "temperature": 0.0, "avg_logprob": -0.3054966926574707, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.034099217504262924}, {"id": 141, "seek": 72514, "start": 734.6999999999999, "end": 736.38, "text": " That's called an embedding.", "tokens": [50842, 663, 311, 1219, 364, 12240, 3584, 13, 50926], "temperature": 0.0, "avg_logprob": -0.3054966926574707, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.034099217504262924}, {"id": 142, "seek": 72514, "start": 736.38, "end": 740.46, "text": " It's also in collaborative filtering, this particular kind of embedding are known as", "tokens": [50926, 467, 311, 611, 294, 16555, 30822, 11, 341, 1729, 733, 295, 12240, 3584, 366, 2570, 382, 51130], "temperature": 0.0, "avg_logprob": -0.3054966926574707, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.034099217504262924}, {"id": 143, "seek": 72514, "start": 740.46, "end": 745.78, "text": " latent factors.", "tokens": [51130, 48994, 6771, 13, 51396], "temperature": 0.0, "avg_logprob": -0.3054966926574707, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.034099217504262924}, {"id": 144, "seek": 72514, "start": 745.78, "end": 753.38, "text": " We hypothesized that once trained, each of these latent factors may mean something.", "tokens": [51396, 492, 14276, 1602, 300, 1564, 8895, 11, 1184, 295, 613, 48994, 6771, 815, 914, 746, 13, 51776], "temperature": 0.0, "avg_logprob": -0.3054966926574707, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.034099217504262924}, {"id": 145, "seek": 75338, "start": 753.38, "end": 756.9, "text": " And I said next week we might come back and have a look and see if we can figure out what", "tokens": [50364, 400, 286, 848, 958, 1243, 321, 1062, 808, 646, 293, 362, 257, 574, 293, 536, 498, 321, 393, 2573, 484, 437, 50540], "temperature": 0.0, "avg_logprob": -0.27432883540286296, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0705578476190567}, {"id": 146, "seek": 75338, "start": 756.9, "end": 758.7, "text": " they mean.", "tokens": [50540, 436, 914, 13, 50630], "temperature": 0.0, "avg_logprob": -0.27432883540286296, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0705578476190567}, {"id": 147, "seek": 75338, "start": 758.7, "end": 761.98, "text": " So that was what I thought I would do now.", "tokens": [50630, 407, 300, 390, 437, 286, 1194, 286, 576, 360, 586, 13, 50794], "temperature": 0.0, "avg_logprob": -0.27432883540286296, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0705578476190567}, {"id": 148, "seek": 75338, "start": 761.98, "end": 767.46, "text": " So I'm going to take the bias model that we created.", "tokens": [50794, 407, 286, 478, 516, 281, 747, 264, 12577, 2316, 300, 321, 2942, 13, 51068], "temperature": 0.0, "avg_logprob": -0.27432883540286296, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0705578476190567}, {"id": 149, "seek": 75338, "start": 767.46, "end": 775.34, "text": " And the bias model we created was the one where we took a user embedding and a movie", "tokens": [51068, 400, 264, 12577, 2316, 321, 2942, 390, 264, 472, 689, 321, 1890, 257, 4195, 12240, 3584, 293, 257, 3169, 51462], "temperature": 0.0, "avg_logprob": -0.27432883540286296, "compression_ratio": 1.5965909090909092, "no_speech_prob": 0.0705578476190567}, {"id": 150, "seek": 77534, "start": 775.34, "end": 785.1, "text": " embedding and we took the dot product of the two and then we added to it a user bias", "tokens": [50364, 12240, 3584, 293, 321, 1890, 264, 5893, 1674, 295, 264, 732, 293, 550, 321, 3869, 281, 309, 257, 4195, 12577, 50852], "temperature": 0.0, "avg_logprob": -0.2712969444167446, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.3242001533508301}, {"id": 151, "seek": 77534, "start": 785.1, "end": 786.1, "text": " and a movie bias.", "tokens": [50852, 293, 257, 3169, 12577, 13, 50902], "temperature": 0.0, "avg_logprob": -0.2712969444167446, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.3242001533508301}, {"id": 152, "seek": 77534, "start": 786.1, "end": 791.26, "text": " Those biases are just embeddings which have a single output.", "tokens": [50902, 3950, 32152, 366, 445, 12240, 29432, 597, 362, 257, 2167, 5598, 13, 51160], "temperature": 0.0, "avg_logprob": -0.2712969444167446, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.3242001533508301}, {"id": 153, "seek": 77534, "start": 791.26, "end": 797.5, "text": " Just like in Excel, the bias was a single cell for each movie and a single cell for", "tokens": [51160, 1449, 411, 294, 19060, 11, 264, 12577, 390, 257, 2167, 2815, 337, 1184, 3169, 293, 257, 2167, 2815, 337, 51472], "temperature": 0.0, "avg_logprob": -0.2712969444167446, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.3242001533508301}, {"id": 154, "seek": 77534, "start": 797.5, "end": 801.1, "text": " each user.", "tokens": [51472, 1184, 4195, 13, 51652], "temperature": 0.0, "avg_logprob": -0.2712969444167446, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.3242001533508301}, {"id": 155, "seek": 80110, "start": 801.1, "end": 807.78, "text": " So then we tried fitting that model and you might remember that we ended up getting an", "tokens": [50364, 407, 550, 321, 3031, 15669, 300, 2316, 293, 291, 1062, 1604, 300, 321, 4590, 493, 1242, 364, 50698], "temperature": 0.0, "avg_logprob": -0.28188290198644, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.007815687917172909}, {"id": 156, "seek": 80110, "start": 807.78, "end": 816.5400000000001, "text": " accuracy that was quite a bit higher than previous state-of-the-art.", "tokens": [50698, 14170, 300, 390, 1596, 257, 857, 2946, 813, 3894, 1785, 12, 2670, 12, 3322, 12, 446, 13, 51136], "temperature": 0.0, "avg_logprob": -0.28188290198644, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.007815687917172909}, {"id": 157, "seek": 80110, "start": 816.5400000000001, "end": 819.66, "text": " Actually, for that one we didn't.", "tokens": [51136, 5135, 11, 337, 300, 472, 321, 994, 380, 13, 51292], "temperature": 0.0, "avg_logprob": -0.28188290198644, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.007815687917172909}, {"id": 158, "seek": 80110, "start": 819.66, "end": 824.0600000000001, "text": " For the previous state-of-the-art, we broke by using the neural network.", "tokens": [51292, 1171, 264, 3894, 1785, 12, 2670, 12, 3322, 12, 446, 11, 321, 6902, 538, 1228, 264, 18161, 3209, 13, 51512], "temperature": 0.0, "avg_logprob": -0.28188290198644, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.007815687917172909}, {"id": 159, "seek": 80110, "start": 824.0600000000001, "end": 828.6600000000001, "text": " I discovered something interesting during the week, which is that I can get a state-of-the-art", "tokens": [51512, 286, 6941, 746, 1880, 1830, 264, 1243, 11, 597, 307, 300, 286, 393, 483, 257, 1785, 12, 2670, 12, 3322, 12, 446, 51742], "temperature": 0.0, "avg_logprob": -0.28188290198644, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.007815687917172909}, {"id": 160, "seek": 82866, "start": 828.66, "end": 831.54, "text": " result using just this simple bias model.", "tokens": [50364, 1874, 1228, 445, 341, 2199, 12577, 2316, 13, 50508], "temperature": 0.0, "avg_logprob": -0.2824207636026236, "compression_ratio": 1.73109243697479, "no_speech_prob": 0.08632130920886993}, {"id": 161, "seek": 82866, "start": 831.54, "end": 837.78, "text": " And the trick was that I just had to increase my regularization.", "tokens": [50508, 400, 264, 4282, 390, 300, 286, 445, 632, 281, 3488, 452, 3890, 2144, 13, 50820], "temperature": 0.0, "avg_logprob": -0.2824207636026236, "compression_ratio": 1.73109243697479, "no_speech_prob": 0.08632130920886993}, {"id": 162, "seek": 82866, "start": 837.78, "end": 841.6999999999999, "text": " So we haven't talked too much about regularization, we've briefly mentioned it a couple of times,", "tokens": [50820, 407, 321, 2378, 380, 2825, 886, 709, 466, 3890, 2144, 11, 321, 600, 10515, 2835, 309, 257, 1916, 295, 1413, 11, 51016], "temperature": 0.0, "avg_logprob": -0.2824207636026236, "compression_ratio": 1.73109243697479, "no_speech_prob": 0.08632130920886993}, {"id": 163, "seek": 82866, "start": 841.6999999999999, "end": 848.06, "text": " but it's a very simple thing where we can basically say add to the loss function the", "tokens": [51016, 457, 309, 311, 257, 588, 2199, 551, 689, 321, 393, 1936, 584, 909, 281, 264, 4470, 2445, 264, 51334], "temperature": 0.0, "avg_logprob": -0.2824207636026236, "compression_ratio": 1.73109243697479, "no_speech_prob": 0.08632130920886993}, {"id": 164, "seek": 82866, "start": 848.06, "end": 851.06, "text": " sum of the squares of the weights.", "tokens": [51334, 2408, 295, 264, 19368, 295, 264, 17443, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2824207636026236, "compression_ratio": 1.73109243697479, "no_speech_prob": 0.08632130920886993}, {"id": 165, "seek": 82866, "start": 851.06, "end": 854.98, "text": " So we're trying to minimize the loss, so if you're adding the sum of the squares of the", "tokens": [51484, 407, 321, 434, 1382, 281, 17522, 264, 4470, 11, 370, 498, 291, 434, 5127, 264, 2408, 295, 264, 19368, 295, 264, 51680], "temperature": 0.0, "avg_logprob": -0.2824207636026236, "compression_ratio": 1.73109243697479, "no_speech_prob": 0.08632130920886993}, {"id": 166, "seek": 85498, "start": 854.98, "end": 861.46, "text": " weights to the loss function, then the SGD solver is going to have to try to avoid increasing", "tokens": [50364, 17443, 281, 264, 4470, 2445, 11, 550, 264, 34520, 35, 1404, 331, 307, 516, 281, 362, 281, 853, 281, 5042, 5662, 50688], "temperature": 0.0, "avg_logprob": -0.28542012507372566, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.32421329617500305}, {"id": 167, "seek": 85498, "start": 861.46, "end": 864.22, "text": " the weights where it can.", "tokens": [50688, 264, 17443, 689, 309, 393, 13, 50826], "temperature": 0.0, "avg_logprob": -0.28542012507372566, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.32421329617500305}, {"id": 168, "seek": 85498, "start": 864.22, "end": 873.7, "text": " And so we can pass to most Keras layers a parameter called wRegularizer, that stands", "tokens": [50826, 400, 370, 321, 393, 1320, 281, 881, 591, 6985, 7914, 257, 13075, 1219, 261, 40888, 1040, 6545, 11, 300, 7382, 51300], "temperature": 0.0, "avg_logprob": -0.28542012507372566, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.32421329617500305}, {"id": 169, "seek": 85498, "start": 873.7, "end": 877.14, "text": " for weight regularizer, and we can tell it how to regularize our weights.", "tokens": [51300, 337, 3364, 3890, 6545, 11, 293, 321, 393, 980, 309, 577, 281, 3890, 1125, 527, 17443, 13, 51472], "temperature": 0.0, "avg_logprob": -0.28542012507372566, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.32421329617500305}, {"id": 170, "seek": 85498, "start": 877.14, "end": 884.22, "text": " In this case, I say use an L2 norm, that means sum of the squares of how much, and that's", "tokens": [51472, 682, 341, 1389, 11, 286, 584, 764, 364, 441, 17, 2026, 11, 300, 1355, 2408, 295, 264, 19368, 295, 577, 709, 11, 293, 300, 311, 51826], "temperature": 0.0, "avg_logprob": -0.28542012507372566, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.32421329617500305}, {"id": 171, "seek": 88422, "start": 884.46, "end": 889.7, "text": " something that I pass in, and I used 1a neg 4.", "tokens": [50376, 746, 300, 286, 1320, 294, 11, 293, 286, 1143, 502, 64, 2485, 1017, 13, 50638], "temperature": 0.0, "avg_logprob": -0.29261173346103764, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.08035530149936676}, {"id": 172, "seek": 88422, "start": 889.7, "end": 895.34, "text": " And it turns out if I do that and then I train it for a while, it takes quite a lot longer", "tokens": [50638, 400, 309, 4523, 484, 498, 286, 360, 300, 293, 550, 286, 3847, 309, 337, 257, 1339, 11, 309, 2516, 1596, 257, 688, 2854, 50920], "temperature": 0.0, "avg_logprob": -0.29261173346103764, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.08035530149936676}, {"id": 173, "seek": 88422, "start": 895.34, "end": 910.22, "text": " to train, I got down to a loss of .7979, which is quite a bit better than the best results", "tokens": [50920, 281, 3847, 11, 286, 658, 760, 281, 257, 4470, 295, 2411, 22, 23247, 24, 11, 597, 307, 1596, 257, 857, 1101, 813, 264, 1151, 3542, 51664], "temperature": 0.0, "avg_logprob": -0.29261173346103764, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.08035530149936676}, {"id": 174, "seek": 88422, "start": 910.22, "end": 912.9, "text": " that that Stanford paper showed.", "tokens": [51664, 300, 300, 20374, 3035, 4712, 13, 51798], "temperature": 0.0, "avg_logprob": -0.29261173346103764, "compression_ratio": 1.4914285714285713, "no_speech_prob": 0.08035530149936676}, {"id": 175, "seek": 91290, "start": 912.9, "end": 919.9399999999999, "text": " It's not quite as good as the neural net, the neural net got 7.938 at best.", "tokens": [50364, 467, 311, 406, 1596, 382, 665, 382, 264, 18161, 2533, 11, 264, 18161, 2533, 658, 1614, 13, 24, 12625, 412, 1151, 13, 50716], "temperature": 0.0, "avg_logprob": -0.2935686613384046, "compression_ratio": 1.4845360824742269, "no_speech_prob": 0.37752506136894226}, {"id": 176, "seek": 91290, "start": 919.9399999999999, "end": 928.6999999999999, "text": " But it's still interesting that this very simple approach actually gets results better", "tokens": [50716, 583, 309, 311, 920, 1880, 300, 341, 588, 2199, 3109, 767, 2170, 3542, 1101, 51154], "temperature": 0.0, "avg_logprob": -0.2935686613384046, "compression_ratio": 1.4845360824742269, "no_speech_prob": 0.37752506136894226}, {"id": 177, "seek": 91290, "start": 928.6999999999999, "end": 934.14, "text": " than the academic state-of-the-art as of 2012 or 2013.", "tokens": [51154, 813, 264, 7778, 1785, 12, 2670, 12, 3322, 12, 446, 382, 295, 9125, 420, 9012, 13, 51426], "temperature": 0.0, "avg_logprob": -0.2935686613384046, "compression_ratio": 1.4845360824742269, "no_speech_prob": 0.37752506136894226}, {"id": 178, "seek": 91290, "start": 934.14, "end": 941.1, "text": " I haven't been able to find more recent academic benchmarks than that.", "tokens": [51426, 286, 2378, 380, 668, 1075, 281, 915, 544, 5162, 7778, 43751, 813, 300, 13, 51774], "temperature": 0.0, "avg_logprob": -0.2935686613384046, "compression_ratio": 1.4845360824742269, "no_speech_prob": 0.37752506136894226}, {"id": 179, "seek": 94110, "start": 941.1, "end": 952.14, "text": " So I took this model and I wanted to find out what we can learn from these results.", "tokens": [50364, 407, 286, 1890, 341, 2316, 293, 286, 1415, 281, 915, 484, 437, 321, 393, 1466, 490, 613, 3542, 13, 50916], "temperature": 0.0, "avg_logprob": -0.2788179856312426, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.046722766011953354}, {"id": 180, "seek": 94110, "start": 952.14, "end": 955.74, "text": " So obviously one thing we would do with this model is just to make predictions with it.", "tokens": [50916, 407, 2745, 472, 551, 321, 576, 360, 365, 341, 2316, 307, 445, 281, 652, 21264, 365, 309, 13, 51096], "temperature": 0.0, "avg_logprob": -0.2788179856312426, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.046722766011953354}, {"id": 181, "seek": 94110, "start": 955.74, "end": 963.1800000000001, "text": " So if you're building a website for recommending movies and a new user came along and said,", "tokens": [51096, 407, 498, 291, 434, 2390, 257, 3144, 337, 30559, 6233, 293, 257, 777, 4195, 1361, 2051, 293, 848, 11, 51468], "temperature": 0.0, "avg_logprob": -0.2788179856312426, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.046722766011953354}, {"id": 182, "seek": 94110, "start": 963.1800000000001, "end": 966.82, "text": " I like these movies this much, what else would you recommend?", "tokens": [51468, 286, 411, 613, 6233, 341, 709, 11, 437, 1646, 576, 291, 2748, 30, 51650], "temperature": 0.0, "avg_logprob": -0.2788179856312426, "compression_ratio": 1.6169154228855722, "no_speech_prob": 0.046722766011953354}, {"id": 183, "seek": 96682, "start": 966.82, "end": 971.7800000000001, "text": " You could just go through and do a prediction for each movie for that user ID and tell", "tokens": [50364, 509, 727, 445, 352, 807, 293, 360, 257, 17630, 337, 1184, 3169, 337, 300, 4195, 7348, 293, 980, 50612], "temperature": 0.0, "avg_logprob": -0.28878229473708966, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.20179902017116547}, {"id": 184, "seek": 96682, "start": 971.7800000000001, "end": 973.74, "text": " them which ones have the highest numbers.", "tokens": [50612, 552, 597, 2306, 362, 264, 6343, 3547, 13, 50710], "temperature": 0.0, "avg_logprob": -0.28878229473708966, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.20179902017116547}, {"id": 185, "seek": 96682, "start": 973.74, "end": 977.5400000000001, "text": " That's the normal way we would use collaborative filtering.", "tokens": [50710, 663, 311, 264, 2710, 636, 321, 576, 764, 16555, 30822, 13, 50900], "temperature": 0.0, "avg_logprob": -0.28878229473708966, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.20179902017116547}, {"id": 186, "seek": 96682, "start": 977.5400000000001, "end": 978.58, "text": " We can do some other things.", "tokens": [50900, 492, 393, 360, 512, 661, 721, 13, 50952], "temperature": 0.0, "avg_logprob": -0.28878229473708966, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.20179902017116547}, {"id": 187, "seek": 96682, "start": 978.58, "end": 984.1, "text": " We can grab the top 2000 most popular movies, just to make this more interesting, and we", "tokens": [50952, 492, 393, 4444, 264, 1192, 8132, 881, 3743, 6233, 11, 445, 281, 652, 341, 544, 1880, 11, 293, 321, 51228], "temperature": 0.0, "avg_logprob": -0.28878229473708966, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.20179902017116547}, {"id": 188, "seek": 96682, "start": 984.1, "end": 989.74, "text": " can say let's just grab the bias term.", "tokens": [51228, 393, 584, 718, 311, 445, 4444, 264, 12577, 1433, 13, 51510], "temperature": 0.0, "avg_logprob": -0.28878229473708966, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.20179902017116547}, {"id": 189, "seek": 96682, "start": 989.74, "end": 994.86, "text": " I'll talk more about this particular syntax in just a moment, but just for now, this is", "tokens": [51510, 286, 603, 751, 544, 466, 341, 1729, 28431, 294, 445, 257, 1623, 11, 457, 445, 337, 586, 11, 341, 307, 51766], "temperature": 0.0, "avg_logprob": -0.28878229473708966, "compression_ratio": 1.6339622641509435, "no_speech_prob": 0.20179902017116547}, {"id": 190, "seek": 99486, "start": 994.86, "end": 998.1800000000001, "text": " a particularly simple kind of model.", "tokens": [50364, 257, 4098, 2199, 733, 295, 2316, 13, 50530], "temperature": 0.0, "avg_logprob": -0.2805605133374532, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.010818233713507652}, {"id": 191, "seek": 99486, "start": 998.1800000000001, "end": 1003.54, "text": " It's a model which simply takes the movie ID in and returns the movie bias out.", "tokens": [50530, 467, 311, 257, 2316, 597, 2935, 2516, 264, 3169, 7348, 294, 293, 11247, 264, 3169, 12577, 484, 13, 50798], "temperature": 0.0, "avg_logprob": -0.2805605133374532, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.010818233713507652}, {"id": 192, "seek": 99486, "start": 1003.54, "end": 1010.1, "text": " In other words, it does a lookup in the movie bias table and returns the movie bias indexed", "tokens": [50798, 682, 661, 2283, 11, 309, 775, 257, 574, 1010, 294, 264, 3169, 12577, 3199, 293, 11247, 264, 3169, 12577, 8186, 292, 51126], "temperature": 0.0, "avg_logprob": -0.2805605133374532, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.010818233713507652}, {"id": 193, "seek": 99486, "start": 1010.1, "end": 1013.54, "text": " by this movie ID.", "tokens": [51126, 538, 341, 3169, 7348, 13, 51298], "temperature": 0.0, "avg_logprob": -0.2805605133374532, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.010818233713507652}, {"id": 194, "seek": 99486, "start": 1013.54, "end": 1014.86, "text": " That's what these two lines do.", "tokens": [51298, 663, 311, 437, 613, 732, 3876, 360, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2805605133374532, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.010818233713507652}, {"id": 195, "seek": 99486, "start": 1014.86, "end": 1020.62, "text": " I then combine that bias with the actual name of each rating and print out the top and bottom", "tokens": [51364, 286, 550, 10432, 300, 12577, 365, 264, 3539, 1315, 295, 1184, 10990, 293, 4482, 484, 264, 1192, 293, 2767, 51652], "temperature": 0.0, "avg_logprob": -0.2805605133374532, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.010818233713507652}, {"id": 196, "seek": 99486, "start": 1020.62, "end": 1022.02, "text": " 15.", "tokens": [51652, 2119, 13, 51722], "temperature": 0.0, "avg_logprob": -0.2805605133374532, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.010818233713507652}, {"id": 197, "seek": 102202, "start": 1022.18, "end": 1028.58, "text": " According to MovieLens, the worst movie of all time is the Church of Scientology classic", "tokens": [50372, 7328, 281, 28766, 43, 694, 11, 264, 5855, 3169, 295, 439, 565, 307, 264, 7882, 295, 18944, 1793, 7230, 50692], "temperature": 0.0, "avg_logprob": -0.29441487630208335, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.0012255567125976086}, {"id": 198, "seek": 102202, "start": 1028.58, "end": 1033.78, "text": " Battlefield Earth.", "tokens": [50692, 41091, 4755, 13, 50952], "temperature": 0.0, "avg_logprob": -0.29441487630208335, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.0012255567125976086}, {"id": 199, "seek": 102202, "start": 1033.78, "end": 1039.42, "text": " This is interesting because these ratings are quite a lot more sophisticated than your", "tokens": [50952, 639, 307, 1880, 570, 613, 24603, 366, 1596, 257, 688, 544, 16950, 813, 428, 51234], "temperature": 0.0, "avg_logprob": -0.29441487630208335, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.0012255567125976086}, {"id": 200, "seek": 102202, "start": 1039.42, "end": 1040.46, "text": " average movie rating.", "tokens": [51234, 4274, 3169, 10990, 13, 51286], "temperature": 0.0, "avg_logprob": -0.29441487630208335, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.0012255567125976086}, {"id": 201, "seek": 102202, "start": 1040.46, "end": 1047.58, "text": " What this is saying is that these have been normalized for some reviewers are more positive", "tokens": [51286, 708, 341, 307, 1566, 307, 300, 613, 362, 668, 48704, 337, 512, 45837, 366, 544, 3353, 51642], "temperature": 0.0, "avg_logprob": -0.29441487630208335, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.0012255567125976086}, {"id": 202, "seek": 102202, "start": 1047.58, "end": 1049.56, "text": " and negative than others.", "tokens": [51642, 293, 3671, 813, 2357, 13, 51741], "temperature": 0.0, "avg_logprob": -0.29441487630208335, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.0012255567125976086}, {"id": 203, "seek": 104956, "start": 1049.6, "end": 1052.9199999999998, "text": " Some people are watching better or crappier films than others.", "tokens": [50366, 2188, 561, 366, 1976, 1101, 420, 2094, 427, 811, 7796, 813, 2357, 13, 50532], "temperature": 0.0, "avg_logprob": -0.35818639554475484, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.039637185633182526}, {"id": 204, "seek": 104956, "start": 1052.9199999999998, "end": 1058.72, "text": " So this bias is removing all of that noise and really telling us after removing all of", "tokens": [50532, 407, 341, 12577, 307, 12720, 439, 295, 300, 5658, 293, 534, 3585, 505, 934, 12720, 439, 295, 50822], "temperature": 0.0, "avg_logprob": -0.35818639554475484, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.039637185633182526}, {"id": 205, "seek": 104956, "start": 1058.72, "end": 1062.0, "text": " that noise, these are the least good movies.", "tokens": [50822, 300, 5658, 11, 613, 366, 264, 1935, 665, 6233, 13, 50986], "temperature": 0.0, "avg_logprob": -0.35818639554475484, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.039637185633182526}, {"id": 206, "seek": 104956, "start": 1062.0, "end": 1069.1599999999999, "text": " Battlefield Earth is even worse than Spice World by a significant margin.", "tokens": [50986, 41091, 4755, 307, 754, 5324, 813, 1738, 573, 3937, 538, 257, 4776, 10270, 13, 51344], "temperature": 0.0, "avg_logprob": -0.35818639554475484, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.039637185633182526}, {"id": 207, "seek": 104956, "start": 1069.1599999999999, "end": 1072.12, "text": " On the other hand, here are the best.", "tokens": [51344, 1282, 264, 661, 1011, 11, 510, 366, 264, 1151, 13, 51492], "temperature": 0.0, "avg_logprob": -0.35818639554475484, "compression_ratio": 1.6105263157894736, "no_speech_prob": 0.039637185633182526}, {"id": 208, "seek": 107212, "start": 1072.12, "end": 1080.56, "text": " Miyazaki fans will be pleased to see Hell's Moving Castle at number 2.", "tokens": [50364, 26195, 921, 7421, 4499, 486, 312, 10587, 281, 536, 12090, 311, 14242, 21076, 412, 1230, 568, 13, 50786], "temperature": 0.0, "avg_logprob": -0.3037043933210702, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.10229798406362534}, {"id": 209, "seek": 107212, "start": 1080.56, "end": 1082.6799999999998, "text": " So that's interesting.", "tokens": [50786, 407, 300, 311, 1880, 13, 50892], "temperature": 0.0, "avg_logprob": -0.3037043933210702, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.10229798406362534}, {"id": 210, "seek": 107212, "start": 1082.6799999999998, "end": 1095.3999999999999, "text": " Perhaps what's more interesting is to try and figure out what's going on, not in the", "tokens": [50892, 10517, 437, 311, 544, 1880, 307, 281, 853, 293, 2573, 484, 437, 311, 516, 322, 11, 406, 294, 264, 51528], "temperature": 0.0, "avg_logprob": -0.3037043933210702, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.10229798406362534}, {"id": 211, "seek": 107212, "start": 1095.3999999999999, "end": 1099.1599999999999, "text": " biases but in the latent factors.", "tokens": [51528, 32152, 457, 294, 264, 48994, 6771, 13, 51716], "temperature": 0.0, "avg_logprob": -0.3037043933210702, "compression_ratio": 1.394736842105263, "no_speech_prob": 0.10229798406362534}, {"id": 212, "seek": 109916, "start": 1099.2, "end": 1104.72, "text": " The latent factors are a little bit harder to interpret because for every movie, we have", "tokens": [50366, 440, 48994, 6771, 366, 257, 707, 857, 6081, 281, 7302, 570, 337, 633, 3169, 11, 321, 362, 50642], "temperature": 0.0, "avg_logprob": -0.27422329095693737, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.008711190894246101}, {"id": 213, "seek": 109916, "start": 1104.72, "end": 1106.0400000000002, "text": " 50 of them.", "tokens": [50642, 2625, 295, 552, 13, 50708], "temperature": 0.0, "avg_logprob": -0.27422329095693737, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.008711190894246101}, {"id": 214, "seek": 109916, "start": 1106.0400000000002, "end": 1107.72, "text": " In the Excel spreadsheet, we have 5.", "tokens": [50708, 682, 264, 19060, 27733, 11, 321, 362, 1025, 13, 50792], "temperature": 0.0, "avg_logprob": -0.27422329095693737, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.008711190894246101}, {"id": 215, "seek": 109916, "start": 1107.72, "end": 1110.16, "text": " In our version, we have 50 of them.", "tokens": [50792, 682, 527, 3037, 11, 321, 362, 2625, 295, 552, 13, 50914], "temperature": 0.0, "avg_logprob": -0.27422329095693737, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.008711190894246101}, {"id": 216, "seek": 109916, "start": 1110.16, "end": 1116.16, "text": " So what we want to do is we want to take from those 50 latent factors, we want to find 2", "tokens": [50914, 407, 437, 321, 528, 281, 360, 307, 321, 528, 281, 747, 490, 729, 2625, 48994, 6771, 11, 321, 528, 281, 915, 568, 51214], "temperature": 0.0, "avg_logprob": -0.27422329095693737, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.008711190894246101}, {"id": 217, "seek": 109916, "start": 1116.16, "end": 1121.48, "text": " or 3 main components.", "tokens": [51214, 420, 805, 2135, 6677, 13, 51480], "temperature": 0.0, "avg_logprob": -0.27422329095693737, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.008711190894246101}, {"id": 218, "seek": 109916, "start": 1121.48, "end": 1126.68, "text": " The way we do this, the details aren't important, but a lot of you will already be familiar", "tokens": [51480, 440, 636, 321, 360, 341, 11, 264, 4365, 3212, 380, 1021, 11, 457, 257, 688, 295, 291, 486, 1217, 312, 4963, 51740], "temperature": 0.0, "avg_logprob": -0.27422329095693737, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.008711190894246101}, {"id": 219, "seek": 112668, "start": 1126.68, "end": 1132.0800000000002, "text": " with it, which is that there's something called PCA, or Principal Components Analysis.", "tokens": [50364, 365, 309, 11, 597, 307, 300, 456, 311, 746, 1219, 6465, 32, 11, 420, 38575, 6620, 40496, 38172, 13, 50634], "temperature": 0.0, "avg_logprob": -0.28718440843665083, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.5155524611473083}, {"id": 220, "seek": 112668, "start": 1132.0800000000002, "end": 1135.4, "text": " Principal Components Analysis does exactly what I just said.", "tokens": [50634, 38575, 6620, 40496, 38172, 775, 2293, 437, 286, 445, 848, 13, 50800], "temperature": 0.0, "avg_logprob": -0.28718440843665083, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.5155524611473083}, {"id": 221, "seek": 112668, "start": 1135.4, "end": 1141.18, "text": " It looks through a matrix, in this case it's got 50 columns, and it says what are the combinations", "tokens": [50800, 467, 1542, 807, 257, 8141, 11, 294, 341, 1389, 309, 311, 658, 2625, 13766, 11, 293, 309, 1619, 437, 366, 264, 21267, 51089], "temperature": 0.0, "avg_logprob": -0.28718440843665083, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.5155524611473083}, {"id": 222, "seek": 112668, "start": 1141.18, "end": 1146.28, "text": " of columns that we can add together because they tend to move in the same direction.", "tokens": [51089, 295, 13766, 300, 321, 393, 909, 1214, 570, 436, 3928, 281, 1286, 294, 264, 912, 3513, 13, 51344], "temperature": 0.0, "avg_logprob": -0.28718440843665083, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.5155524611473083}, {"id": 223, "seek": 112668, "start": 1146.28, "end": 1151.48, "text": " And so in this case, we say start with our 50 columns and I want to create just 3 columns", "tokens": [51344, 400, 370, 294, 341, 1389, 11, 321, 584, 722, 365, 527, 2625, 13766, 293, 286, 528, 281, 1884, 445, 805, 13766, 51604], "temperature": 0.0, "avg_logprob": -0.28718440843665083, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.5155524611473083}, {"id": 224, "seek": 112668, "start": 1151.48, "end": 1155.28, "text": " that capture all of the information of the original 50.", "tokens": [51604, 300, 7983, 439, 295, 264, 1589, 295, 264, 3380, 2625, 13, 51794], "temperature": 0.0, "avg_logprob": -0.28718440843665083, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.5155524611473083}, {"id": 225, "seek": 115528, "start": 1155.28, "end": 1159.92, "text": " If you're interested in learning more about how this works, PCA is something that is everywhere", "tokens": [50364, 759, 291, 434, 3102, 294, 2539, 544, 466, 577, 341, 1985, 11, 6465, 32, 307, 746, 300, 307, 5315, 50596], "temperature": 0.0, "avg_logprob": -0.2650494339442489, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.08151433616876602}, {"id": 226, "seek": 115528, "start": 1159.92, "end": 1162.16, "text": " on the internet, so there's lots of information about it.", "tokens": [50596, 322, 264, 4705, 11, 370, 456, 311, 3195, 295, 1589, 466, 309, 13, 50708], "temperature": 0.0, "avg_logprob": -0.2650494339442489, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.08151433616876602}, {"id": 227, "seek": 115528, "start": 1162.16, "end": 1164.36, "text": " But as I say, the details aren't important.", "tokens": [50708, 583, 382, 286, 584, 11, 264, 4365, 3212, 380, 1021, 13, 50818], "temperature": 0.0, "avg_logprob": -0.2650494339442489, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.08151433616876602}, {"id": 228, "seek": 115528, "start": 1164.36, "end": 1168.68, "text": " The important thing to recognize is that we're just squishing our 50 latent factors down", "tokens": [50818, 440, 1021, 551, 281, 5521, 307, 300, 321, 434, 445, 2339, 3807, 527, 2625, 48994, 6771, 760, 51034], "temperature": 0.0, "avg_logprob": -0.2650494339442489, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.08151433616876602}, {"id": 229, "seek": 115528, "start": 1168.68, "end": 1170.48, "text": " into 3.", "tokens": [51034, 666, 805, 13, 51124], "temperature": 0.0, "avg_logprob": -0.2650494339442489, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.08151433616876602}, {"id": 230, "seek": 115528, "start": 1170.48, "end": 1178.6399999999999, "text": " So if we look at the first PCA factor and we sort on it, we can see that at one end,", "tokens": [51124, 407, 498, 321, 574, 412, 264, 700, 6465, 32, 5952, 293, 321, 1333, 322, 309, 11, 321, 393, 536, 300, 412, 472, 917, 11, 51532], "temperature": 0.0, "avg_logprob": -0.2650494339442489, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.08151433616876602}, {"id": 231, "seek": 117864, "start": 1178.64, "end": 1189.8000000000002, "text": " we have fairly well-regarded movies like The Godfather, Pulp Fiction, Usual Suspects,", "tokens": [50364, 321, 362, 6457, 731, 12, 3375, 22803, 6233, 411, 440, 1265, 11541, 11, 35568, 79, 479, 4105, 11, 4958, 901, 9545, 1043, 82, 11, 50922], "temperature": 0.0, "avg_logprob": -0.3108913697392108, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.08755780011415482}, {"id": 232, "seek": 117864, "start": 1189.8000000000002, "end": 1193.0400000000002, "text": " these are all kind of classics.", "tokens": [50922, 613, 366, 439, 733, 295, 36110, 13, 51084], "temperature": 0.0, "avg_logprob": -0.3108913697392108, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.08755780011415482}, {"id": 233, "seek": 117864, "start": 1193.0400000000002, "end": 1197.94, "text": " At the other end, we have things like Ace Ventura and Robocop 3, which are perhaps not", "tokens": [51084, 1711, 264, 661, 917, 11, 321, 362, 721, 411, 24900, 28290, 2991, 293, 5424, 905, 404, 805, 11, 597, 366, 4317, 406, 51329], "temperature": 0.0, "avg_logprob": -0.3108913697392108, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.08755780011415482}, {"id": 234, "seek": 117864, "start": 1197.94, "end": 1199.64, "text": " so classic.", "tokens": [51329, 370, 7230, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3108913697392108, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.08755780011415482}, {"id": 235, "seek": 117864, "start": 1199.64, "end": 1207.72, "text": " So our first PCA factor is some kind of IsClassic score.", "tokens": [51414, 407, 527, 700, 6465, 32, 5952, 307, 512, 733, 295, 1119, 9966, 35685, 6175, 13, 51818], "temperature": 0.0, "avg_logprob": -0.3108913697392108, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.08755780011415482}, {"id": 236, "seek": 120772, "start": 1207.8, "end": 1212.08, "text": " On our second one, we have something similar but actually very different.", "tokens": [50368, 1282, 527, 1150, 472, 11, 321, 362, 746, 2531, 457, 767, 588, 819, 13, 50582], "temperature": 0.0, "avg_logprob": -0.2684361468786481, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.0008558944100514054}, {"id": 237, "seek": 120772, "start": 1212.08, "end": 1218.68, "text": " At one end, we've got 10 movies that are huge Hollywood blockbusters with lots of special", "tokens": [50582, 1711, 472, 917, 11, 321, 600, 658, 1266, 6233, 300, 366, 2603, 11628, 3461, 65, 17181, 365, 3195, 295, 2121, 50912], "temperature": 0.0, "avg_logprob": -0.2684361468786481, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.0008558944100514054}, {"id": 238, "seek": 120772, "start": 1218.68, "end": 1220.4, "text": " effects.", "tokens": [50912, 5065, 13, 50998], "temperature": 0.0, "avg_logprob": -0.2684361468786481, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.0008558944100514054}, {"id": 239, "seek": 120772, "start": 1220.4, "end": 1225.4, "text": " And at the other end, we have things like Annie Hall and Brokeback Mountain, which are", "tokens": [50998, 400, 412, 264, 661, 917, 11, 321, 362, 721, 411, 26781, 5434, 293, 5425, 330, 3207, 15586, 11, 597, 366, 51248], "temperature": 0.0, "avg_logprob": -0.2684361468786481, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.0008558944100514054}, {"id": 240, "seek": 120772, "start": 1225.4, "end": 1230.24, "text": " kind of dialogue-heavy, not big Hollywood hits.", "tokens": [51248, 733, 295, 10221, 12, 37157, 11, 406, 955, 11628, 8664, 13, 51490], "temperature": 0.0, "avg_logprob": -0.2684361468786481, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.0008558944100514054}, {"id": 241, "seek": 120772, "start": 1230.24, "end": 1235.52, "text": " So there's another dimension.", "tokens": [51490, 407, 456, 311, 1071, 10139, 13, 51754], "temperature": 0.0, "avg_logprob": -0.2684361468786481, "compression_ratio": 1.491150442477876, "no_speech_prob": 0.0008558944100514054}, {"id": 242, "seek": 123552, "start": 1235.52, "end": 1240.0, "text": " This is the first most important dimension by which people judge movies differently.", "tokens": [50364, 639, 307, 264, 700, 881, 1021, 10139, 538, 597, 561, 6995, 6233, 7614, 13, 50588], "temperature": 0.0, "avg_logprob": -0.2627735319591704, "compression_ratio": 2.1690140845070425, "no_speech_prob": 0.05500048026442528}, {"id": 243, "seek": 123552, "start": 1240.0, "end": 1243.6399999999999, "text": " This is the second most important one by which people judge movies differently.", "tokens": [50588, 639, 307, 264, 1150, 881, 1021, 472, 538, 597, 561, 6995, 6233, 7614, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2627735319591704, "compression_ratio": 2.1690140845070425, "no_speech_prob": 0.05500048026442528}, {"id": 244, "seek": 123552, "start": 1243.6399999999999, "end": 1247.26, "text": " And then the third most important one by which people judge movies differently is something", "tokens": [50770, 400, 550, 264, 2636, 881, 1021, 472, 538, 597, 561, 6995, 6233, 7614, 307, 746, 50951], "temperature": 0.0, "avg_logprob": -0.2627735319591704, "compression_ratio": 2.1690140845070425, "no_speech_prob": 0.05500048026442528}, {"id": 245, "seek": 123552, "start": 1247.26, "end": 1254.28, "text": " where at one end, we have a bunch of violent and scary movies, and at the other end, we", "tokens": [50951, 689, 412, 472, 917, 11, 321, 362, 257, 3840, 295, 11867, 293, 6958, 6233, 11, 293, 412, 264, 661, 917, 11, 321, 51302], "temperature": 0.0, "avg_logprob": -0.2627735319591704, "compression_ratio": 2.1690140845070425, "no_speech_prob": 0.05500048026442528}, {"id": 246, "seek": 123552, "start": 1254.28, "end": 1257.28, "text": " have a bunch of very happy movies.", "tokens": [51302, 362, 257, 3840, 295, 588, 2055, 6233, 13, 51452], "temperature": 0.0, "avg_logprob": -0.2627735319591704, "compression_ratio": 2.1690140845070425, "no_speech_prob": 0.05500048026442528}, {"id": 247, "seek": 123552, "start": 1257.28, "end": 1262.48, "text": " And for those of you who haven't seen Babe, Australian movie, happiest movie ever.", "tokens": [51452, 400, 337, 729, 295, 291, 567, 2378, 380, 1612, 44127, 11, 13337, 3169, 11, 37584, 3169, 1562, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2627735319591704, "compression_ratio": 2.1690140845070425, "no_speech_prob": 0.05500048026442528}, {"id": 248, "seek": 126248, "start": 1262.48, "end": 1267.96, "text": " It's about a small pig and its adventures and its path to success.", "tokens": [50364, 467, 311, 466, 257, 1359, 8120, 293, 1080, 20905, 293, 1080, 3100, 281, 2245, 13, 50638], "temperature": 0.0, "avg_logprob": -0.26696569779339957, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0646476298570633}, {"id": 249, "seek": 126248, "start": 1267.96, "end": 1270.72, "text": " Happiest movie ever, according to MovieLens.", "tokens": [50638, 7412, 6495, 3169, 1562, 11, 4650, 281, 28766, 43, 694, 13, 50776], "temperature": 0.0, "avg_logprob": -0.26696569779339957, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0646476298570633}, {"id": 250, "seek": 126248, "start": 1270.72, "end": 1271.72, "text": " So that's interesting.", "tokens": [50776, 407, 300, 311, 1880, 13, 50826], "temperature": 0.0, "avg_logprob": -0.26696569779339957, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0646476298570633}, {"id": 251, "seek": 126248, "start": 1271.72, "end": 1276.4, "text": " It's not saying that these factors are good or bad or anything like that.", "tokens": [50826, 467, 311, 406, 1566, 300, 613, 6771, 366, 665, 420, 1578, 420, 1340, 411, 300, 13, 51060], "temperature": 0.0, "avg_logprob": -0.26696569779339957, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0646476298570633}, {"id": 252, "seek": 126248, "start": 1276.4, "end": 1282.96, "text": " It's just saying that these are the things that when we've done this matrix decomposition", "tokens": [51060, 467, 311, 445, 1566, 300, 613, 366, 264, 721, 300, 562, 321, 600, 1096, 341, 8141, 48356, 51388], "temperature": 0.0, "avg_logprob": -0.26696569779339957, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0646476298570633}, {"id": 253, "seek": 126248, "start": 1282.96, "end": 1288.9, "text": " have popped out as being the ways in which people are differing in their ratings for", "tokens": [51388, 362, 21545, 484, 382, 885, 264, 2098, 294, 597, 561, 366, 743, 278, 294, 641, 24603, 337, 51685], "temperature": 0.0, "avg_logprob": -0.26696569779339957, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0646476298570633}, {"id": 254, "seek": 126248, "start": 1288.9, "end": 1290.3600000000001, "text": " different kinds of movies.", "tokens": [51685, 819, 3685, 295, 6233, 13, 51758], "temperature": 0.0, "avg_logprob": -0.26696569779339957, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.0646476298570633}, {"id": 255, "seek": 129036, "start": 1290.36, "end": 1299.24, "text": " So one of the reasons I wanted to show you this is to say that these kinds of SGD-learned", "tokens": [50364, 407, 472, 295, 264, 4112, 286, 1415, 281, 855, 291, 341, 307, 281, 584, 300, 613, 3685, 295, 34520, 35, 12, 306, 1083, 292, 50808], "temperature": 0.0, "avg_logprob": -0.25928483850815714, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.013848396018147469}, {"id": 256, "seek": 129036, "start": 1299.24, "end": 1305.3999999999999, "text": " many-parameter networks are not inscrutable.", "tokens": [50808, 867, 12, 2181, 335, 2398, 9590, 366, 406, 1028, 10757, 32148, 13, 51116], "temperature": 0.0, "avg_logprob": -0.25928483850815714, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.013848396018147469}, {"id": 257, "seek": 129036, "start": 1305.3999999999999, "end": 1310.12, "text": " Indeed it's not great to go in and look at every one of those 50 latent factor coefficients", "tokens": [51116, 15061, 309, 311, 406, 869, 281, 352, 294, 293, 574, 412, 633, 472, 295, 729, 2625, 48994, 5952, 31994, 51352], "temperature": 0.0, "avg_logprob": -0.25928483850815714, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.013848396018147469}, {"id": 258, "seek": 129036, "start": 1310.12, "end": 1316.52, "text": " in detail, but you have to think about how to visualize them, how to look at them.", "tokens": [51352, 294, 2607, 11, 457, 291, 362, 281, 519, 466, 577, 281, 23273, 552, 11, 577, 281, 574, 412, 552, 13, 51672], "temperature": 0.0, "avg_logprob": -0.25928483850815714, "compression_ratio": 1.5073170731707317, "no_speech_prob": 0.013848396018147469}, {"id": 259, "seek": 131652, "start": 1317.0, "end": 1321.4, "text": " In this case, I went a step further and I grabbed a couple of principal components and", "tokens": [50388, 682, 341, 1389, 11, 286, 1437, 257, 1823, 3052, 293, 286, 18607, 257, 1916, 295, 9716, 6677, 293, 50608], "temperature": 0.0, "avg_logprob": -0.3121521659519361, "compression_ratio": 1.8786610878661087, "no_speech_prob": 0.04672127962112427}, {"id": 260, "seek": 131652, "start": 1321.4, "end": 1324.72, "text": " tried drawing a picture.", "tokens": [50608, 3031, 6316, 257, 3036, 13, 50774], "temperature": 0.0, "avg_logprob": -0.3121521659519361, "compression_ratio": 1.8786610878661087, "no_speech_prob": 0.04672127962112427}, {"id": 261, "seek": 131652, "start": 1324.72, "end": 1329.96, "text": " And so with pictures, of course, you can start to see things in multiple dimensions.", "tokens": [50774, 400, 370, 365, 5242, 11, 295, 1164, 11, 291, 393, 722, 281, 536, 721, 294, 3866, 12819, 13, 51036], "temperature": 0.0, "avg_logprob": -0.3121521659519361, "compression_ratio": 1.8786610878661087, "no_speech_prob": 0.04672127962112427}, {"id": 262, "seek": 131652, "start": 1329.96, "end": 1333.76, "text": " And so here I've got the first and third principal components.", "tokens": [51036, 400, 370, 510, 286, 600, 658, 264, 700, 293, 2636, 9716, 6677, 13, 51226], "temperature": 0.0, "avg_logprob": -0.3121521659519361, "compression_ratio": 1.8786610878661087, "no_speech_prob": 0.04672127962112427}, {"id": 263, "seek": 131652, "start": 1333.76, "end": 1340.4, "text": " And so you can see at the far right-hand side here, we have more of the Hollywood-type movies,", "tokens": [51226, 400, 370, 291, 393, 536, 412, 264, 1400, 558, 12, 5543, 1252, 510, 11, 321, 362, 544, 295, 264, 11628, 12, 20467, 6233, 11, 51558], "temperature": 0.0, "avg_logprob": -0.3121521659519361, "compression_ratio": 1.8786610878661087, "no_speech_prob": 0.04672127962112427}, {"id": 264, "seek": 131652, "start": 1340.4, "end": 1344.2, "text": " and at the far left, some of the more classic movies, and at the top, some of the more violent", "tokens": [51558, 293, 412, 264, 1400, 1411, 11, 512, 295, 264, 544, 7230, 6233, 11, 293, 412, 264, 1192, 11, 512, 295, 264, 544, 11867, 51748], "temperature": 0.0, "avg_logprob": -0.3121521659519361, "compression_ratio": 1.8786610878661087, "no_speech_prob": 0.04672127962112427}, {"id": 265, "seek": 134420, "start": 1344.2, "end": 1351.24, "text": " movies, and at the bottom, some of the happier movies.", "tokens": [50364, 6233, 11, 293, 412, 264, 2767, 11, 512, 295, 264, 20423, 6233, 13, 50716], "temperature": 0.0, "avg_logprob": -0.3985451263131447, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.20943579077720642}, {"id": 266, "seek": 134420, "start": 1351.24, "end": 1358.1200000000001, "text": " And so if you wanted to find a movie that was violent and classic, you would go into", "tokens": [50716, 400, 370, 498, 291, 1415, 281, 915, 257, 3169, 300, 390, 11867, 293, 7230, 11, 291, 576, 352, 666, 51060], "temperature": 0.0, "avg_logprob": -0.3985451263131447, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.20943579077720642}, {"id": 267, "seek": 134420, "start": 1358.1200000000001, "end": 1362.4, "text": " the top left, and Hubrix O'Clockwork Orange would probably be the one that most people", "tokens": [51060, 264, 1192, 1411, 11, 293, 18986, 6579, 422, 6, 34, 4102, 1902, 17106, 576, 1391, 312, 264, 472, 300, 881, 561, 51274], "temperature": 0.0, "avg_logprob": -0.3985451263131447, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.20943579077720642}, {"id": 268, "seek": 134420, "start": 1362.4, "end": 1363.4, "text": " would come up with first.", "tokens": [51274, 576, 808, 493, 365, 700, 13, 51324], "temperature": 0.0, "avg_logprob": -0.3985451263131447, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.20943579077720642}, {"id": 269, "seek": 134420, "start": 1363.4, "end": 1370.64, "text": " Or if you wanted to come up with something that was very Hollywood and very non-violent,", "tokens": [51324, 1610, 498, 291, 1415, 281, 808, 493, 365, 746, 300, 390, 588, 11628, 293, 588, 2107, 12, 48473, 11, 51686], "temperature": 0.0, "avg_logprob": -0.3985451263131447, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.20943579077720642}, {"id": 270, "seek": 134420, "start": 1370.64, "end": 1373.24, "text": " you would be down here in sleepless in Seattle.", "tokens": [51686, 291, 576, 312, 760, 510, 294, 12931, 20434, 294, 15721, 13, 51816], "temperature": 0.0, "avg_logprob": -0.3985451263131447, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.20943579077720642}, {"id": 271, "seek": 137324, "start": 1373.28, "end": 1381.08, "text": " So you can really learn a lot by looking at these kinds of models, but you don't do it", "tokens": [50366, 407, 291, 393, 534, 1466, 257, 688, 538, 1237, 412, 613, 3685, 295, 5245, 11, 457, 291, 500, 380, 360, 309, 50756], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 272, "seek": 137324, "start": 1381.08, "end": 1386.08, "text": " by looking at the coefficients, you do it by visualizations, you do it by interrogating", "tokens": [50756, 538, 1237, 412, 264, 31994, 11, 291, 360, 309, 538, 5056, 14455, 11, 291, 360, 309, 538, 24871, 990, 51006], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 273, "seek": 137324, "start": 1386.08, "end": 1387.08, "text": " it.", "tokens": [51006, 309, 13, 51056], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 274, "seek": 137324, "start": 1387.08, "end": 1389.6, "text": " And so I think this is a big difference.", "tokens": [51056, 400, 370, 286, 519, 341, 307, 257, 955, 2649, 13, 51182], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 275, "seek": 137324, "start": 1389.6, "end": 1393.48, "text": " For any of you that have done much statistics before or have a background in the social", "tokens": [51182, 1171, 604, 295, 291, 300, 362, 1096, 709, 12523, 949, 420, 362, 257, 3678, 294, 264, 2093, 51376], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 276, "seek": 137324, "start": 1393.48, "end": 1398.2, "text": " sciences, you've spent most of your time doing regressions and looking at coefficients and", "tokens": [51376, 17677, 11, 291, 600, 4418, 881, 295, 428, 565, 884, 1121, 735, 626, 293, 1237, 412, 31994, 293, 51612], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 277, "seek": 137324, "start": 1398.2, "end": 1399.64, "text": " t-tests and stuff.", "tokens": [51612, 256, 12, 83, 4409, 293, 1507, 13, 51684], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 278, "seek": 137324, "start": 1399.64, "end": 1400.96, "text": " This is a very different world.", "tokens": [51684, 639, 307, 257, 588, 819, 1002, 13, 51750], "temperature": 0.0, "avg_logprob": -0.27333309269752826, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.006692381110042334}, {"id": 279, "seek": 140096, "start": 1400.96, "end": 1408.72, "text": " This is a world where you're asking the model questions and getting the modeled results,", "tokens": [50364, 639, 307, 257, 1002, 689, 291, 434, 3365, 264, 2316, 1651, 293, 1242, 264, 37140, 3542, 11, 50752], "temperature": 0.0, "avg_logprob": -0.26167389198585794, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.0373244434595108}, {"id": 280, "seek": 140096, "start": 1408.72, "end": 1412.04, "text": " which is kind of what we're doing here.", "tokens": [50752, 597, 307, 733, 295, 437, 321, 434, 884, 510, 13, 50918], "temperature": 0.0, "avg_logprob": -0.26167389198585794, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.0373244434595108}, {"id": 281, "seek": 140096, "start": 1412.04, "end": 1417.8, "text": " I mentioned I would talk briefly about this syntax.", "tokens": [50918, 286, 2835, 286, 576, 751, 10515, 466, 341, 28431, 13, 51206], "temperature": 0.0, "avg_logprob": -0.26167389198585794, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.0373244434595108}, {"id": 282, "seek": 140096, "start": 1417.8, "end": 1422.44, "text": " And this syntax is something that we're going to be using a lot more of, and it's part of", "tokens": [51206, 400, 341, 28431, 307, 746, 300, 321, 434, 516, 281, 312, 1228, 257, 688, 544, 295, 11, 293, 309, 311, 644, 295, 51438], "temperature": 0.0, "avg_logprob": -0.26167389198585794, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.0373244434595108}, {"id": 283, "seek": 140096, "start": 1422.44, "end": 1426.48, "text": " what's called the Keras Functional API.", "tokens": [51438, 437, 311, 1219, 264, 591, 6985, 11166, 41048, 9362, 13, 51640], "temperature": 0.0, "avg_logprob": -0.26167389198585794, "compression_ratio": 1.5422885572139304, "no_speech_prob": 0.0373244434595108}, {"id": 284, "seek": 142648, "start": 1426.48, "end": 1431.1200000000001, "text": " The Keras Functional API is a way of doing exactly the same things that you've already", "tokens": [50364, 440, 591, 6985, 11166, 41048, 9362, 307, 257, 636, 295, 884, 2293, 264, 912, 721, 300, 291, 600, 1217, 50596], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 285, "seek": 142648, "start": 1431.1200000000001, "end": 1435.32, "text": " learned how to do using a different API.", "tokens": [50596, 3264, 577, 281, 360, 1228, 257, 819, 9362, 13, 50806], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 286, "seek": 142648, "start": 1435.32, "end": 1437.72, "text": " And that is not such a dumb idea.", "tokens": [50806, 400, 300, 307, 406, 1270, 257, 10316, 1558, 13, 50926], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 287, "seek": 142648, "start": 1437.72, "end": 1441.1200000000001, "text": " The API you've learned so far is the Sequential API.", "tokens": [50926, 440, 9362, 291, 600, 3264, 370, 1400, 307, 264, 46859, 2549, 9362, 13, 51096], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 288, "seek": 142648, "start": 1441.1200000000001, "end": 1445.48, "text": " It's where you use the word sequential and then you write in order the layers of your", "tokens": [51096, 467, 311, 689, 291, 764, 264, 1349, 42881, 293, 550, 291, 2464, 294, 1668, 264, 7914, 295, 428, 51314], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 289, "seek": 142648, "start": 1445.48, "end": 1447.1200000000001, "text": " neural network.", "tokens": [51314, 18161, 3209, 13, 51396], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 290, "seek": 142648, "start": 1447.1200000000001, "end": 1451.68, "text": " That's all very well, but what if you want to do something like what we wanted to do", "tokens": [51396, 663, 311, 439, 588, 731, 11, 457, 437, 498, 291, 528, 281, 360, 746, 411, 437, 321, 1415, 281, 360, 51624], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 291, "seek": 142648, "start": 1451.68, "end": 1454.76, "text": " just now, where we had two different things coming in.", "tokens": [51624, 445, 586, 11, 689, 321, 632, 732, 819, 721, 1348, 294, 13, 51778], "temperature": 0.0, "avg_logprob": -0.27700780442923556, "compression_ratio": 1.695167286245353, "no_speech_prob": 0.08631788194179535}, {"id": 292, "seek": 145476, "start": 1454.84, "end": 1459.28, "text": " We had a user ID coming in and a movie ID coming in, and each one went through its own", "tokens": [50368, 492, 632, 257, 4195, 7348, 1348, 294, 293, 257, 3169, 7348, 1348, 294, 11, 293, 1184, 472, 1437, 807, 1080, 1065, 50590], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 293, "seek": 145476, "start": 1459.28, "end": 1462.0, "text": " embedding and then they got multiplied together.", "tokens": [50590, 12240, 3584, 293, 550, 436, 658, 17207, 1214, 13, 50726], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 294, "seek": 145476, "start": 1462.0, "end": 1464.2, "text": " How do you express that as a sequence?", "tokens": [50726, 1012, 360, 291, 5109, 300, 382, 257, 8310, 30, 50836], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 295, "seek": 145476, "start": 1464.2, "end": 1466.48, "text": " It's not very easy to do that.", "tokens": [50836, 467, 311, 406, 588, 1858, 281, 360, 300, 13, 50950], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 296, "seek": 145476, "start": 1466.48, "end": 1471.4, "text": " So the Functional API was designed to answer this question.", "tokens": [50950, 407, 264, 11166, 41048, 9362, 390, 4761, 281, 1867, 341, 1168, 13, 51196], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 297, "seek": 145476, "start": 1471.4, "end": 1475.24, "text": " The first thing to note about the Functional API is that you can do everything you can", "tokens": [51196, 440, 700, 551, 281, 3637, 466, 264, 11166, 41048, 9362, 307, 300, 291, 393, 360, 1203, 291, 393, 51388], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 298, "seek": 145476, "start": 1475.24, "end": 1477.52, "text": " do in the Sequential API.", "tokens": [51388, 360, 294, 264, 46859, 2549, 9362, 13, 51502], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 299, "seek": 145476, "start": 1477.52, "end": 1481.76, "text": " And here's an example of something you can do perfectly well with the Sequential API,", "tokens": [51502, 400, 510, 311, 364, 1365, 295, 746, 291, 393, 360, 6239, 731, 365, 264, 46859, 2549, 9362, 11, 51714], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 300, "seek": 145476, "start": 1481.76, "end": 1484.48, "text": " which is something with two dense layers.", "tokens": [51714, 597, 307, 746, 365, 732, 18011, 7914, 13, 51850], "temperature": 0.0, "avg_logprob": -0.24450322507901956, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005442065885290504}, {"id": 301, "seek": 148448, "start": 1485.2, "end": 1486.1200000000001, "text": " But it looks a bit different.", "tokens": [50400, 583, 309, 1542, 257, 857, 819, 13, 50446], "temperature": 0.0, "avg_logprob": -0.2898086834979314, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.004755101166665554}, {"id": 302, "seek": 148448, "start": 1486.1200000000001, "end": 1492.92, "text": " Every Functional API model starts with an input layer and then you assign that to some", "tokens": [50446, 2048, 11166, 41048, 9362, 2316, 3719, 365, 364, 4846, 4583, 293, 550, 291, 6269, 300, 281, 512, 50786], "temperature": 0.0, "avg_logprob": -0.2898086834979314, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.004755101166665554}, {"id": 303, "seek": 148448, "start": 1492.92, "end": 1494.28, "text": " variable.", "tokens": [50786, 7006, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2898086834979314, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.004755101166665554}, {"id": 304, "seek": 148448, "start": 1494.28, "end": 1500.32, "text": " And then you list each of the layers in order and for each of them, after you've provided", "tokens": [50854, 400, 550, 291, 1329, 1184, 295, 264, 7914, 294, 1668, 293, 337, 1184, 295, 552, 11, 934, 291, 600, 5649, 51156], "temperature": 0.0, "avg_logprob": -0.2898086834979314, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.004755101166665554}, {"id": 305, "seek": 148448, "start": 1500.32, "end": 1507.04, "text": " the details for that layer, you then immediately call the layer passing in the output of the", "tokens": [51156, 264, 4365, 337, 300, 4583, 11, 291, 550, 4258, 818, 264, 4583, 8437, 294, 264, 5598, 295, 264, 51492], "temperature": 0.0, "avg_logprob": -0.2898086834979314, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.004755101166665554}, {"id": 306, "seek": 148448, "start": 1507.04, "end": 1508.72, "text": " previous layer.", "tokens": [51492, 3894, 4583, 13, 51576], "temperature": 0.0, "avg_logprob": -0.2898086834979314, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.004755101166665554}, {"id": 307, "seek": 148448, "start": 1508.72, "end": 1511.72, "text": " So this passes in inputs and calls it x.", "tokens": [51576, 407, 341, 11335, 294, 15743, 293, 5498, 309, 2031, 13, 51726], "temperature": 0.0, "avg_logprob": -0.2898086834979314, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.004755101166665554}, {"id": 308, "seek": 151172, "start": 1511.88, "end": 1515.8, "text": " And then this passes in our x, and this is our new version of x.", "tokens": [50372, 400, 550, 341, 11335, 294, 527, 2031, 11, 293, 341, 307, 527, 777, 3037, 295, 2031, 13, 50568], "temperature": 0.0, "avg_logprob": -0.28391611944768846, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.023688530549407005}, {"id": 309, "seek": 151172, "start": 1515.8, "end": 1520.3600000000001, "text": " And then this next dense layer gets the next version of x and returns predictions.", "tokens": [50568, 400, 550, 341, 958, 18011, 4583, 2170, 264, 958, 3037, 295, 2031, 293, 11247, 21264, 13, 50796], "temperature": 0.0, "avg_logprob": -0.28391611944768846, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.023688530549407005}, {"id": 310, "seek": 151172, "start": 1520.3600000000001, "end": 1527.52, "text": " So you can see that each layer is saying what its previous layer is.", "tokens": [50796, 407, 291, 393, 536, 300, 1184, 4583, 307, 1566, 437, 1080, 3894, 4583, 307, 13, 51154], "temperature": 0.0, "avg_logprob": -0.28391611944768846, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.023688530549407005}, {"id": 311, "seek": 151172, "start": 1527.52, "end": 1533.1200000000001, "text": " So it's doing exactly the same thing as the Sequential API, just in a different way.", "tokens": [51154, 407, 309, 311, 884, 2293, 264, 912, 551, 382, 264, 46859, 2549, 9362, 11, 445, 294, 257, 819, 636, 13, 51434], "temperature": 0.0, "avg_logprob": -0.28391611944768846, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.023688530549407005}, {"id": 312, "seek": 151172, "start": 1533.1200000000001, "end": 1539.0, "text": " Now as the docs note here, the sequential model is probably a better choice to implement", "tokens": [51434, 823, 382, 264, 45623, 3637, 510, 11, 264, 42881, 2316, 307, 1391, 257, 1101, 3922, 281, 4445, 51728], "temperature": 0.0, "avg_logprob": -0.28391611944768846, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.023688530549407005}, {"id": 313, "seek": 153900, "start": 1539.0, "end": 1542.2, "text": " this particular network because it's easier.", "tokens": [50364, 341, 1729, 3209, 570, 309, 311, 3571, 13, 50524], "temperature": 0.0, "avg_logprob": -0.27858000023420465, "compression_ratio": 1.481818181818182, "no_speech_prob": 0.03021419420838356}, {"id": 314, "seek": 153900, "start": 1542.2, "end": 1547.48, "text": " This is just showing that you can do it.", "tokens": [50524, 639, 307, 445, 4099, 300, 291, 393, 360, 309, 13, 50788], "temperature": 0.0, "avg_logprob": -0.27858000023420465, "compression_ratio": 1.481818181818182, "no_speech_prob": 0.03021419420838356}, {"id": 315, "seek": 153900, "start": 1547.48, "end": 1554.44, "text": " On the other hand, the model that we just looked at would be quite difficult, if not", "tokens": [50788, 1282, 264, 661, 1011, 11, 264, 2316, 300, 321, 445, 2956, 412, 576, 312, 1596, 2252, 11, 498, 406, 51136], "temperature": 0.0, "avg_logprob": -0.27858000023420465, "compression_ratio": 1.481818181818182, "no_speech_prob": 0.03021419420838356}, {"id": 316, "seek": 153900, "start": 1554.44, "end": 1557.92, "text": " impossible to do with the Sequential API.", "tokens": [51136, 6243, 281, 360, 365, 264, 46859, 2549, 9362, 13, 51310], "temperature": 0.0, "avg_logprob": -0.27858000023420465, "compression_ratio": 1.481818181818182, "no_speech_prob": 0.03021419420838356}, {"id": 317, "seek": 153900, "start": 1557.92, "end": 1561.38, "text": " But with the Functional API, it was very easy.", "tokens": [51310, 583, 365, 264, 11166, 41048, 9362, 11, 309, 390, 588, 1858, 13, 51483], "temperature": 0.0, "avg_logprob": -0.27858000023420465, "compression_ratio": 1.481818181818182, "no_speech_prob": 0.03021419420838356}, {"id": 318, "seek": 153900, "start": 1561.38, "end": 1568.56, "text": " We created a whole separate model which gave an output u for user.", "tokens": [51483, 492, 2942, 257, 1379, 4994, 2316, 597, 2729, 364, 5598, 344, 337, 4195, 13, 51842], "temperature": 0.0, "avg_logprob": -0.27858000023420465, "compression_ratio": 1.481818181818182, "no_speech_prob": 0.03021419420838356}, {"id": 319, "seek": 156856, "start": 1569.12, "end": 1574.24, "text": " And that was the result of creating an embedding, where we said an embedding has its own input", "tokens": [50392, 400, 300, 390, 264, 1874, 295, 4084, 364, 12240, 3584, 11, 689, 321, 848, 364, 12240, 3584, 575, 1080, 1065, 4846, 50648], "temperature": 0.0, "avg_logprob": -0.27188968658447266, "compression_ratio": 2.0235849056603774, "no_speech_prob": 0.02096378244459629}, {"id": 320, "seek": 156856, "start": 1574.24, "end": 1576.44, "text": " and then goes through an embedding layer.", "tokens": [50648, 293, 550, 1709, 807, 364, 12240, 3584, 4583, 13, 50758], "temperature": 0.0, "avg_logprob": -0.27188968658447266, "compression_ratio": 2.0235849056603774, "no_speech_prob": 0.02096378244459629}, {"id": 321, "seek": 156856, "start": 1576.44, "end": 1581.08, "text": " And then we returned the input to that and the embedding layer, like so.", "tokens": [50758, 400, 550, 321, 8752, 264, 4846, 281, 300, 293, 264, 12240, 3584, 4583, 11, 411, 370, 13, 50990], "temperature": 0.0, "avg_logprob": -0.27188968658447266, "compression_ratio": 2.0235849056603774, "no_speech_prob": 0.02096378244459629}, {"id": 322, "seek": 156856, "start": 1581.08, "end": 1584.96, "text": " So that gave us our user input and our user embedding, and our movie input and our movie", "tokens": [50990, 407, 300, 2729, 505, 527, 4195, 4846, 293, 527, 4195, 12240, 3584, 11, 293, 527, 3169, 4846, 293, 527, 3169, 51184], "temperature": 0.0, "avg_logprob": -0.27188968658447266, "compression_ratio": 2.0235849056603774, "no_speech_prob": 0.02096378244459629}, {"id": 323, "seek": 156856, "start": 1584.96, "end": 1585.96, "text": " embedding.", "tokens": [51184, 12240, 3584, 13, 51234], "temperature": 0.0, "avg_logprob": -0.27188968658447266, "compression_ratio": 2.0235849056603774, "no_speech_prob": 0.02096378244459629}, {"id": 324, "seek": 156856, "start": 1585.96, "end": 1589.6799999999998, "text": " So there's like 2 separate little models.", "tokens": [51234, 407, 456, 311, 411, 568, 4994, 707, 5245, 13, 51420], "temperature": 0.0, "avg_logprob": -0.27188968658447266, "compression_ratio": 2.0235849056603774, "no_speech_prob": 0.02096378244459629}, {"id": 325, "seek": 156856, "start": 1589.6799999999998, "end": 1593.36, "text": " And then we did a similar thing to create 2 little models for our bias terms.", "tokens": [51420, 400, 550, 321, 630, 257, 2531, 551, 281, 1884, 568, 707, 5245, 337, 527, 12577, 2115, 13, 51604], "temperature": 0.0, "avg_logprob": -0.27188968658447266, "compression_ratio": 2.0235849056603774, "no_speech_prob": 0.02096378244459629}, {"id": 326, "seek": 159336, "start": 1593.36, "end": 1598.6799999999998, "text": " They were both things that grabbed an embedding, returning a single output, and then flattened", "tokens": [50364, 814, 645, 1293, 721, 300, 18607, 364, 12240, 3584, 11, 12678, 257, 2167, 5598, 11, 293, 550, 24183, 292, 50630], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 327, "seek": 159336, "start": 1598.6799999999998, "end": 1599.6799999999998, "text": " it.", "tokens": [50630, 309, 13, 50680], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 328, "seek": 159336, "start": 1599.6799999999998, "end": 1601.76, "text": " And that grabbed our biases.", "tokens": [50680, 400, 300, 18607, 527, 32152, 13, 50784], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 329, "seek": 159336, "start": 1601.76, "end": 1604.56, "text": " And so now we've got 4 separate models.", "tokens": [50784, 400, 370, 586, 321, 600, 658, 1017, 4994, 5245, 13, 50924], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 330, "seek": 159336, "start": 1604.56, "end": 1605.6799999999998, "text": " And so we can merge them.", "tokens": [50924, 400, 370, 321, 393, 22183, 552, 13, 50980], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 331, "seek": 159336, "start": 1605.6799999999998, "end": 1608.4799999999998, "text": " There's this function called merge.", "tokens": [50980, 821, 311, 341, 2445, 1219, 22183, 13, 51120], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 332, "seek": 159336, "start": 1608.4799999999998, "end": 1609.4799999999998, "text": " It's pretty confusing.", "tokens": [51120, 467, 311, 1238, 13181, 13, 51170], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 333, "seek": 159336, "start": 1609.4799999999998, "end": 1612.56, "text": " There's a small mmerge and a big mmerge.", "tokens": [51170, 821, 311, 257, 1359, 275, 936, 432, 293, 257, 955, 275, 936, 432, 13, 51324], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 334, "seek": 159336, "start": 1612.56, "end": 1615.1599999999999, "text": " In general, you will be using the small mmerge.", "tokens": [51324, 682, 2674, 11, 291, 486, 312, 1228, 264, 1359, 275, 936, 432, 13, 51454], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 335, "seek": 159336, "start": 1615.1599999999999, "end": 1618.28, "text": " I'm not going to go into the details of why they're both there.", "tokens": [51454, 286, 478, 406, 516, 281, 352, 666, 264, 4365, 295, 983, 436, 434, 1293, 456, 13, 51610], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 336, "seek": 159336, "start": 1618.28, "end": 1620.3999999999999, "text": " They are there for a reason.", "tokens": [51610, 814, 366, 456, 337, 257, 1778, 13, 51716], "temperature": 0.0, "avg_logprob": -0.26498753474308895, "compression_ratio": 1.7290836653386454, "no_speech_prob": 0.18009081482887268}, {"id": 337, "seek": 162040, "start": 1620.4, "end": 1625.52, "text": " If something weird happens to you with merge, try remembering to use the small mmerge.", "tokens": [50364, 759, 746, 3657, 2314, 281, 291, 365, 22183, 11, 853, 20719, 281, 764, 264, 1359, 275, 936, 432, 13, 50620], "temperature": 0.0, "avg_logprob": -0.27242838038076267, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.035677485167980194}, {"id": 338, "seek": 162040, "start": 1625.52, "end": 1631.3200000000002, "text": " The small mmerge takes 2 previous outputs that you've just created using the functional", "tokens": [50620, 440, 1359, 275, 936, 432, 2516, 568, 3894, 23930, 300, 291, 600, 445, 2942, 1228, 264, 11745, 50910], "temperature": 0.0, "avg_logprob": -0.27242838038076267, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.035677485167980194}, {"id": 339, "seek": 162040, "start": 1631.3200000000002, "end": 1639.0, "text": " API and combines them in whatever way you want, in this case, the dot product.", "tokens": [50910, 9362, 293, 29520, 552, 294, 2035, 636, 291, 528, 11, 294, 341, 1389, 11, 264, 5893, 1674, 13, 51294], "temperature": 0.0, "avg_logprob": -0.27242838038076267, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.035677485167980194}, {"id": 340, "seek": 162040, "start": 1639.0, "end": 1643.2800000000002, "text": " So that grabs our user and movie embeddings and takes the dot product.", "tokens": [51294, 407, 300, 30028, 527, 4195, 293, 3169, 12240, 29432, 293, 2516, 264, 5893, 1674, 13, 51508], "temperature": 0.0, "avg_logprob": -0.27242838038076267, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.035677485167980194}, {"id": 341, "seek": 162040, "start": 1643.2800000000002, "end": 1650.0400000000002, "text": " We grab the output of that and take our user bias and the sum and the output of that and", "tokens": [51508, 492, 4444, 264, 5598, 295, 300, 293, 747, 527, 4195, 12577, 293, 264, 2408, 293, 264, 5598, 295, 300, 293, 51846], "temperature": 0.0, "avg_logprob": -0.27242838038076267, "compression_ratio": 1.7725321888412018, "no_speech_prob": 0.035677485167980194}, {"id": 342, "seek": 165004, "start": 1650.08, "end": 1653.48, "text": " the movie bias and the sum.", "tokens": [50366, 264, 3169, 12577, 293, 264, 2408, 13, 50536], "temperature": 0.0, "avg_logprob": -0.2502221001519097, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.01115789171308279}, {"id": 343, "seek": 165004, "start": 1653.48, "end": 1658.6399999999999, "text": " So that's a functional API to creating that model.", "tokens": [50536, 407, 300, 311, 257, 11745, 9362, 281, 4084, 300, 2316, 13, 50794], "temperature": 0.0, "avg_logprob": -0.2502221001519097, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.01115789171308279}, {"id": 344, "seek": 165004, "start": 1658.6399999999999, "end": 1664.68, "text": " At the end of which, we then use the model function to actually create our model saying", "tokens": [50794, 1711, 264, 917, 295, 597, 11, 321, 550, 764, 264, 2316, 2445, 281, 767, 1884, 527, 2316, 1566, 51096], "temperature": 0.0, "avg_logprob": -0.2502221001519097, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.01115789171308279}, {"id": 345, "seek": 165004, "start": 1664.68, "end": 1669.84, "text": " what are the inputs to the model and what is the output of the model.", "tokens": [51096, 437, 366, 264, 15743, 281, 264, 2316, 293, 437, 307, 264, 5598, 295, 264, 2316, 13, 51354], "temperature": 0.0, "avg_logprob": -0.2502221001519097, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.01115789171308279}, {"id": 346, "seek": 165004, "start": 1669.84, "end": 1674.74, "text": " So you can see this is different to usual because we've now got multiple inputs.", "tokens": [51354, 407, 291, 393, 536, 341, 307, 819, 281, 7713, 570, 321, 600, 586, 658, 3866, 15743, 13, 51599], "temperature": 0.0, "avg_logprob": -0.2502221001519097, "compression_ratio": 1.6510416666666667, "no_speech_prob": 0.01115789171308279}, {"id": 347, "seek": 167474, "start": 1674.74, "end": 1681.74, "text": " So then when we call fit, we now have to pass in an array of inputs, a user ID and", "tokens": [50364, 407, 550, 562, 321, 818, 3318, 11, 321, 586, 362, 281, 1320, 294, 364, 10225, 295, 15743, 11, 257, 4195, 7348, 293, 50714], "temperature": 0.0, "avg_logprob": -0.2910213269685444, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.18240465223789215}, {"id": 348, "seek": 167474, "start": 1681.74, "end": 1683.6200000000001, "text": " a movie ID.", "tokens": [50714, 257, 3169, 7348, 13, 50808], "temperature": 0.0, "avg_logprob": -0.2910213269685444, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.18240465223789215}, {"id": 349, "seek": 167474, "start": 1683.6200000000001, "end": 1690.44, "text": " So the functional API is something that we're going to be using increasingly from now on.", "tokens": [50808, 407, 264, 11745, 9362, 307, 746, 300, 321, 434, 516, 281, 312, 1228, 12980, 490, 586, 322, 13, 51149], "temperature": 0.0, "avg_logprob": -0.2910213269685444, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.18240465223789215}, {"id": 350, "seek": 167474, "start": 1690.44, "end": 1697.7, "text": " Now that we've learned all the basic architectures, we're going to be starting to build more exotic", "tokens": [51149, 823, 300, 321, 600, 3264, 439, 264, 3875, 6331, 1303, 11, 321, 434, 516, 281, 312, 2891, 281, 1322, 544, 27063, 51512], "temperature": 0.0, "avg_logprob": -0.2910213269685444, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.18240465223789215}, {"id": 351, "seek": 167474, "start": 1697.7, "end": 1702.3, "text": " architectures for more special cases and we'll be using the functional API more and more.", "tokens": [51512, 6331, 1303, 337, 544, 2121, 3331, 293, 321, 603, 312, 1228, 264, 11745, 9362, 544, 293, 544, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2910213269685444, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.18240465223789215}, {"id": 352, "seek": 170230, "start": 1702.86, "end": 1703.86, "text": " Question?", "tokens": [50392, 14464, 30, 50442], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 353, "seek": 170230, "start": 1703.86, "end": 1707.18, "text": " Is the only reason to use an embedding layer so that you can provide a list of integers", "tokens": [50442, 1119, 264, 787, 1778, 281, 764, 364, 12240, 3584, 4583, 370, 300, 291, 393, 2893, 257, 1329, 295, 41674, 50608], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 354, "seek": 170230, "start": 1707.18, "end": 1708.18, "text": " as input?", "tokens": [50608, 382, 4846, 30, 50658], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 355, "seek": 170230, "start": 1708.18, "end": 1709.18, "text": " Answer?", "tokens": [50658, 24545, 30, 50708], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 356, "seek": 170230, "start": 1709.18, "end": 1710.54, "text": " That's a great question.", "tokens": [50708, 663, 311, 257, 869, 1168, 13, 50776], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 357, "seek": 170230, "start": 1710.54, "end": 1714.86, "text": " Is the only reason to use an embedding layer so that you can use integers as input?", "tokens": [50776, 1119, 264, 787, 1778, 281, 764, 364, 12240, 3584, 4583, 370, 300, 291, 393, 764, 41674, 382, 4846, 30, 50992], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 358, "seek": 170230, "start": 1714.86, "end": 1716.06, "text": " Absolutely yes.", "tokens": [50992, 7021, 2086, 13, 51052], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 359, "seek": 170230, "start": 1716.06, "end": 1720.1399999999999, "text": " So instead of using an embedding layer, we could have one-hot encoded all of those user", "tokens": [51052, 407, 2602, 295, 1228, 364, 12240, 3584, 4583, 11, 321, 727, 362, 472, 12, 12194, 2058, 12340, 439, 295, 729, 4195, 51256], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 360, "seek": 170230, "start": 1720.1399999999999, "end": 1725.74, "text": " IDs and one-hot encoded all of those movie IDs and created dense layers on top of them", "tokens": [51256, 48212, 293, 472, 12, 12194, 2058, 12340, 439, 295, 729, 3169, 48212, 293, 2942, 18011, 7914, 322, 1192, 295, 552, 51536], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 361, "seek": 170230, "start": 1725.74, "end": 1729.02, "text": " and it would have done exactly the same thing.", "tokens": [51536, 293, 309, 576, 362, 1096, 2293, 264, 912, 551, 13, 51700], "temperature": 0.0, "avg_logprob": -0.3269853066271684, "compression_ratio": 2.017467248908297, "no_speech_prob": 0.0695352703332901}, {"id": 362, "seek": 172902, "start": 1729.02, "end": 1740.06, "text": " Why choose 50 latent factors and then reduce them down with a principal component analysis?", "tokens": [50364, 1545, 2826, 2625, 48994, 6771, 293, 550, 5407, 552, 760, 365, 257, 9716, 6542, 5215, 30, 50916], "temperature": 0.0, "avg_logprob": -0.40630412101745605, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.37711024284362793}, {"id": 363, "seek": 172902, "start": 1740.06, "end": 1743.34, "text": " Why not just have 3 latent factors to begin with?", "tokens": [50916, 1545, 406, 445, 362, 805, 48994, 6771, 281, 1841, 365, 30, 51080], "temperature": 0.0, "avg_logprob": -0.40630412101745605, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.37711024284362793}, {"id": 364, "seek": 172902, "start": 1743.34, "end": 1745.26, "text": " I'm not quite sure why use both.", "tokens": [51080, 286, 478, 406, 1596, 988, 983, 764, 1293, 13, 51176], "temperature": 0.0, "avg_logprob": -0.40630412101745605, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.37711024284362793}, {"id": 365, "seek": 172902, "start": 1745.26, "end": 1755.58, "text": " If we only use 3 latent factors, then our predictive model would have been less accurate.", "tokens": [51176, 759, 321, 787, 764, 805, 48994, 6771, 11, 550, 527, 35521, 2316, 576, 362, 668, 1570, 8559, 13, 51692], "temperature": 0.0, "avg_logprob": -0.40630412101745605, "compression_ratio": 1.5260115606936415, "no_speech_prob": 0.37711024284362793}, {"id": 366, "seek": 175558, "start": 1755.58, "end": 1761.26, "text": " So we want an accurate predictive model so that when people come to our website, we can", "tokens": [50364, 407, 321, 528, 364, 8559, 35521, 2316, 370, 300, 562, 561, 808, 281, 527, 3144, 11, 321, 393, 50648], "temperature": 0.0, "avg_logprob": -0.25472517013549806, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.024049846455454826}, {"id": 367, "seek": 175558, "start": 1761.26, "end": 1764.02, "text": " do a good job of telling them what movie to watch.", "tokens": [50648, 360, 257, 665, 1691, 295, 3585, 552, 437, 3169, 281, 1159, 13, 50786], "temperature": 0.0, "avg_logprob": -0.25472517013549806, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.024049846455454826}, {"id": 368, "seek": 175558, "start": 1764.02, "end": 1766.3799999999999, "text": " So 50 latent factors for that.", "tokens": [50786, 407, 2625, 48994, 6771, 337, 300, 13, 50904], "temperature": 0.0, "avg_logprob": -0.25472517013549806, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.024049846455454826}, {"id": 369, "seek": 175558, "start": 1766.3799999999999, "end": 1770.6, "text": " But then for the purpose of our visualization of understanding what those factors are doing,", "tokens": [50904, 583, 550, 337, 264, 4334, 295, 527, 25801, 295, 3701, 437, 729, 6771, 366, 884, 11, 51115], "temperature": 0.0, "avg_logprob": -0.25472517013549806, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.024049846455454826}, {"id": 370, "seek": 175558, "start": 1770.6, "end": 1778.9399999999998, "text": " we want a small number so that we can interpret them more easily.", "tokens": [51115, 321, 528, 257, 1359, 1230, 370, 300, 321, 393, 7302, 552, 544, 3612, 13, 51532], "temperature": 0.0, "avg_logprob": -0.25472517013549806, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.024049846455454826}, {"id": 371, "seek": 175558, "start": 1778.9399999999998, "end": 1783.4199999999998, "text": " One thing you might want to try during the week is taking one or two of your models and", "tokens": [51532, 1485, 551, 291, 1062, 528, 281, 853, 1830, 264, 1243, 307, 1940, 472, 420, 732, 295, 428, 5245, 293, 51756], "temperature": 0.0, "avg_logprob": -0.25472517013549806, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.024049846455454826}, {"id": 372, "seek": 178342, "start": 1783.42, "end": 1785.98, "text": " converting them to use the Functional API.", "tokens": [50364, 29942, 552, 281, 764, 264, 11166, 41048, 9362, 13, 50492], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 373, "seek": 178342, "start": 1785.98, "end": 1791.3400000000001, "text": " Just to start to get the hang of how this API looks.", "tokens": [50492, 1449, 281, 722, 281, 483, 264, 3967, 295, 577, 341, 9362, 1542, 13, 50760], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 374, "seek": 178342, "start": 1791.3400000000001, "end": 1792.3400000000001, "text": " Question.", "tokens": [50760, 14464, 13, 50810], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 375, "seek": 178342, "start": 1792.3400000000001, "end": 1797.8200000000002, "text": " Are these Functional Models how we would add additional information to images in CNNs,", "tokens": [50810, 2014, 613, 11166, 41048, 6583, 1625, 577, 321, 576, 909, 4497, 1589, 281, 5267, 294, 24859, 82, 11, 51084], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 376, "seek": 178342, "start": 1797.8200000000002, "end": 1800.42, "text": " say driving speed or turning radius?", "tokens": [51084, 584, 4840, 3073, 420, 6246, 15845, 30, 51214], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 377, "seek": 178342, "start": 1800.42, "end": 1801.42, "text": " Answer.", "tokens": [51214, 24545, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 378, "seek": 178342, "start": 1801.42, "end": 1802.5, "text": " Yes, absolutely.", "tokens": [51264, 1079, 11, 3122, 13, 51318], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 379, "seek": 178342, "start": 1802.5, "end": 1808.0600000000002, "text": " In general, the idea of adding additional information to a CNN is basically like adding", "tokens": [51318, 682, 2674, 11, 264, 1558, 295, 5127, 4497, 1589, 281, 257, 24859, 307, 1936, 411, 5127, 51596], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 380, "seek": 178342, "start": 1808.0600000000002, "end": 1810.0600000000002, "text": " metadata.", "tokens": [51596, 26603, 13, 51696], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 381, "seek": 178342, "start": 1810.0600000000002, "end": 1812.74, "text": " This happens in collaborative filtering a lot.", "tokens": [51696, 639, 2314, 294, 16555, 30822, 257, 688, 13, 51830], "temperature": 0.0, "avg_logprob": -0.3997589777974249, "compression_ratio": 1.6352459016393444, "no_speech_prob": 0.060972049832344055}, {"id": 382, "seek": 181274, "start": 1813.06, "end": 1818.86, "text": " You might have a collaborative filtering model that as well as having the ratings table,", "tokens": [50380, 509, 1062, 362, 257, 16555, 30822, 2316, 300, 382, 731, 382, 1419, 264, 24603, 3199, 11, 50670], "temperature": 0.0, "avg_logprob": -0.2785188547770182, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0005192877724766731}, {"id": 383, "seek": 181274, "start": 1818.86, "end": 1824.26, "text": " you also have information about what genre the movie is in, maybe the demographic information", "tokens": [50670, 291, 611, 362, 1589, 466, 437, 11022, 264, 3169, 307, 294, 11, 1310, 264, 26331, 1589, 50940], "temperature": 0.0, "avg_logprob": -0.2785188547770182, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0005192877724766731}, {"id": 384, "seek": 181274, "start": 1824.26, "end": 1825.86, "text": " about the user.", "tokens": [50940, 466, 264, 4195, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2785188547770182, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0005192877724766731}, {"id": 385, "seek": 181274, "start": 1825.86, "end": 1831.3, "text": " So you can incorporate all that stuff by having additional inputs.", "tokens": [51020, 407, 291, 393, 16091, 439, 300, 1507, 538, 1419, 4497, 15743, 13, 51292], "temperature": 0.0, "avg_logprob": -0.2785188547770182, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0005192877724766731}, {"id": 386, "seek": 181274, "start": 1831.3, "end": 1836.3, "text": " With a CNN, for example, I'll give you a good example.", "tokens": [51292, 2022, 257, 24859, 11, 337, 1365, 11, 286, 603, 976, 291, 257, 665, 1365, 13, 51542], "temperature": 0.0, "avg_logprob": -0.2785188547770182, "compression_ratio": 1.5686274509803921, "no_speech_prob": 0.0005192877724766731}, {"id": 387, "seek": 183630, "start": 1836.3, "end": 1843.18, "text": " The new Kaggle fish recognition competition, one of the things that turns out is a useful", "tokens": [50364, 440, 777, 48751, 22631, 3506, 11150, 6211, 11, 472, 295, 264, 721, 300, 4523, 484, 307, 257, 4420, 50708], "temperature": 0.0, "avg_logprob": -0.3043101093556621, "compression_ratio": 1.7224489795918367, "no_speech_prob": 0.06278563290834427}, {"id": 388, "seek": 183630, "start": 1843.18, "end": 1847.3799999999999, "text": " predictor, this is a leakage problem, is the size of the image.", "tokens": [50708, 6069, 284, 11, 341, 307, 257, 47799, 1154, 11, 307, 264, 2744, 295, 264, 3256, 13, 50918], "temperature": 0.0, "avg_logprob": -0.3043101093556621, "compression_ratio": 1.7224489795918367, "no_speech_prob": 0.06278563290834427}, {"id": 389, "seek": 183630, "start": 1847.3799999999999, "end": 1852.26, "text": " So you could have another input, which is the height and width of the image, just as", "tokens": [50918, 407, 291, 727, 362, 1071, 4846, 11, 597, 307, 264, 6681, 293, 11402, 295, 264, 3256, 11, 445, 382, 51162], "temperature": 0.0, "avg_logprob": -0.3043101093556621, "compression_ratio": 1.7224489795918367, "no_speech_prob": 0.06278563290834427}, {"id": 390, "seek": 183630, "start": 1852.26, "end": 1856.1399999999999, "text": " integers, and have that as a separate input which is concatenated to the output of your", "tokens": [51162, 41674, 11, 293, 362, 300, 382, 257, 4994, 4846, 597, 307, 1588, 7186, 770, 281, 264, 5598, 295, 428, 51356], "temperature": 0.0, "avg_logprob": -0.3043101093556621, "compression_ratio": 1.7224489795918367, "no_speech_prob": 0.06278563290834427}, {"id": 391, "seek": 183630, "start": 1856.1399999999999, "end": 1861.22, "text": " convolutional layer after the first flattened layer, and then your dense layers can incorporate", "tokens": [51356, 45216, 304, 4583, 934, 264, 700, 24183, 292, 4583, 11, 293, 550, 428, 18011, 7914, 393, 16091, 51610], "temperature": 0.0, "avg_logprob": -0.3043101093556621, "compression_ratio": 1.7224489795918367, "no_speech_prob": 0.06278563290834427}, {"id": 392, "seek": 186122, "start": 1861.54, "end": 1866.54, "text": " the convolutional outputs and your metadata.", "tokens": [50380, 264, 45216, 304, 23930, 293, 428, 26603, 13, 50630], "temperature": 0.0, "avg_logprob": -0.4841522216796875, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.39197275042533875}, {"id": 393, "seek": 186122, "start": 1866.54, "end": 1875.46, "text": " You might remember from last week that this whole thing about collaborative filtering", "tokens": [50630, 509, 1062, 1604, 490, 1036, 1243, 300, 341, 1379, 551, 466, 16555, 30822, 51076], "temperature": 0.0, "avg_logprob": -0.4841522216796875, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.39197275042533875}, {"id": 394, "seek": 186122, "start": 1875.46, "end": 1879.78, "text": " was a journey to somewhere else.", "tokens": [51076, 390, 257, 4671, 281, 4079, 1646, 13, 51292], "temperature": 0.0, "avg_logprob": -0.4841522216796875, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.39197275042533875}, {"id": 395, "seek": 186122, "start": 1879.78, "end": 1885.22, "text": " And the journey is to NLP, natural language processing.", "tokens": [51292, 400, 264, 4671, 307, 281, 426, 45196, 11, 3303, 2856, 9007, 13, 51564], "temperature": 0.0, "avg_logprob": -0.4841522216796875, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.39197275042533875}, {"id": 396, "seek": 188522, "start": 1885.22, "end": 1893.38, "text": " This is a question about collaborative filtering.", "tokens": [50364, 639, 307, 257, 1168, 466, 16555, 30822, 13, 50772], "temperature": 0.0, "avg_logprob": -0.3763253650968037, "compression_ratio": 1.4076433121019107, "no_speech_prob": 0.05653079226613045}, {"id": 397, "seek": 188522, "start": 1893.38, "end": 1902.5, "text": " So if we need to predict the missing values, the NANs or the 0.0, so if a user hasn't watched", "tokens": [50772, 407, 498, 321, 643, 281, 6069, 264, 5361, 4190, 11, 264, 426, 1770, 82, 420, 264, 1958, 13, 15, 11, 370, 498, 257, 4195, 6132, 380, 6337, 51228], "temperature": 0.0, "avg_logprob": -0.3763253650968037, "compression_ratio": 1.4076433121019107, "no_speech_prob": 0.05653079226613045}, {"id": 398, "seek": 188522, "start": 1902.5, "end": 1904.74, "text": " a movie, what would be the prediction?", "tokens": [51228, 257, 3169, 11, 437, 576, 312, 264, 17630, 30, 51340], "temperature": 0.0, "avg_logprob": -0.3763253650968037, "compression_ratio": 1.4076433121019107, "no_speech_prob": 0.05653079226613045}, {"id": 399, "seek": 188522, "start": 1904.74, "end": 1913.94, "text": " Or how do we go about predicting that?", "tokens": [51340, 1610, 577, 360, 321, 352, 466, 32884, 300, 30, 51800], "temperature": 0.0, "avg_logprob": -0.3763253650968037, "compression_ratio": 1.4076433121019107, "no_speech_prob": 0.05653079226613045}, {"id": 400, "seek": 191394, "start": 1914.66, "end": 1920.9, "text": " So this is really the key purpose of creating this model, so that you can make predictions", "tokens": [50400, 407, 341, 307, 534, 264, 2141, 4334, 295, 4084, 341, 2316, 11, 370, 300, 291, 393, 652, 21264, 50712], "temperature": 0.0, "avg_logprob": -0.34650489611503404, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0010649432661011815}, {"id": 401, "seek": 191394, "start": 1920.9, "end": 1927.42, "text": " for movie-user combinations you haven't seen before.", "tokens": [50712, 337, 3169, 12, 18088, 21267, 291, 2378, 380, 1612, 949, 13, 51038], "temperature": 0.0, "avg_logprob": -0.34650489611503404, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0010649432661011815}, {"id": 402, "seek": 191394, "start": 1927.42, "end": 1934.3600000000001, "text": " The way you do that is to simply do something like this.", "tokens": [51038, 440, 636, 291, 360, 300, 307, 281, 2935, 360, 746, 411, 341, 13, 51385], "temperature": 0.0, "avg_logprob": -0.34650489611503404, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0010649432661011815}, {"id": 403, "seek": 191394, "start": 1934.3600000000001, "end": 1941.6200000000001, "text": " You just call model.predict and pass in a movieID-userID pair that you haven't seen", "tokens": [51385, 509, 445, 818, 2316, 13, 79, 24945, 293, 1320, 294, 257, 3169, 2777, 12, 18088, 2777, 6119, 300, 291, 2378, 380, 1612, 51748], "temperature": 0.0, "avg_logprob": -0.34650489611503404, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0010649432661011815}, {"id": 404, "seek": 191394, "start": 1941.6200000000001, "end": 1942.6200000000001, "text": " before.", "tokens": [51748, 949, 13, 51798], "temperature": 0.0, "avg_logprob": -0.34650489611503404, "compression_ratio": 1.5956284153005464, "no_speech_prob": 0.0010649432661011815}, {"id": 405, "seek": 194262, "start": 1942.86, "end": 1948.5, "text": " And all that's going to do is it's going to take the dot product of that movie's latent", "tokens": [50376, 400, 439, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 747, 264, 5893, 1674, 295, 300, 3169, 311, 48994, 50658], "temperature": 0.0, "avg_logprob": -0.29768114673848056, "compression_ratio": 1.8125, "no_speech_prob": 0.0011158802080899477}, {"id": 406, "seek": 194262, "start": 1948.5, "end": 1954.3, "text": " factors and that user's latent factors and add on those biases and return you back the", "tokens": [50658, 6771, 293, 300, 4195, 311, 48994, 6771, 293, 909, 322, 729, 32152, 293, 2736, 291, 646, 264, 50948], "temperature": 0.0, "avg_logprob": -0.29768114673848056, "compression_ratio": 1.8125, "no_speech_prob": 0.0011158802080899477}, {"id": 407, "seek": 194262, "start": 1954.3, "end": 1955.3, "text": " answer.", "tokens": [50948, 1867, 13, 50998], "temperature": 0.0, "avg_logprob": -0.29768114673848056, "compression_ratio": 1.8125, "no_speech_prob": 0.0011158802080899477}, {"id": 408, "seek": 194262, "start": 1955.3, "end": 1958.6999999999998, "text": " It's that easy.", "tokens": [50998, 467, 311, 300, 1858, 13, 51168], "temperature": 0.0, "avg_logprob": -0.29768114673848056, "compression_ratio": 1.8125, "no_speech_prob": 0.0011158802080899477}, {"id": 409, "seek": 194262, "start": 1958.6999999999998, "end": 1965.06, "text": " And so if this was a Kaggle competition, that would be how we would generate our submission", "tokens": [51168, 400, 370, 498, 341, 390, 257, 48751, 22631, 6211, 11, 300, 576, 312, 577, 321, 576, 8460, 527, 23689, 51486], "temperature": 0.0, "avg_logprob": -0.29768114673848056, "compression_ratio": 1.8125, "no_speech_prob": 0.0011158802080899477}, {"id": 410, "seek": 194262, "start": 1965.06, "end": 1968.02, "text": " for the Kaggle competition, would be to take their test set, which would be a bunch of", "tokens": [51486, 337, 264, 48751, 22631, 6211, 11, 576, 312, 281, 747, 641, 1500, 992, 11, 597, 576, 312, 257, 3840, 295, 51634], "temperature": 0.0, "avg_logprob": -0.29768114673848056, "compression_ratio": 1.8125, "no_speech_prob": 0.0011158802080899477}, {"id": 411, "seek": 196802, "start": 1968.02, "end": 1976.1, "text": " movie-user pairs that we haven't seen before.", "tokens": [50364, 3169, 12, 18088, 15494, 300, 321, 2378, 380, 1612, 949, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3559432029724121, "compression_ratio": 1.3417721518987342, "no_speech_prob": 0.012240955606102943}, {"id": 412, "seek": 196802, "start": 1976.1, "end": 1977.18, "text": " Natural Language Processing.", "tokens": [50768, 20137, 24445, 31093, 278, 13, 50822], "temperature": 0.0, "avg_logprob": -0.3559432029724121, "compression_ratio": 1.3417721518987342, "no_speech_prob": 0.012240955606102943}, {"id": 413, "seek": 196802, "start": 1977.18, "end": 1984.86, "text": " So collaborative filtering is extremely useful of itself.", "tokens": [50822, 407, 16555, 30822, 307, 4664, 4420, 295, 2564, 13, 51206], "temperature": 0.0, "avg_logprob": -0.3559432029724121, "compression_ratio": 1.3417721518987342, "no_speech_prob": 0.012240955606102943}, {"id": 414, "seek": 196802, "start": 1984.86, "end": 1990.54, "text": " Without any doubt, it is far more commercially important right now than NLP is.", "tokens": [51206, 9129, 604, 6385, 11, 309, 307, 1400, 544, 41751, 1021, 558, 586, 813, 426, 45196, 307, 13, 51490], "temperature": 0.0, "avg_logprob": -0.3559432029724121, "compression_ratio": 1.3417721518987342, "no_speech_prob": 0.012240955606102943}, {"id": 415, "seek": 199054, "start": 1990.54, "end": 1998.26, "text": " Having said that, Fast.ai's mission is to impact society in as positive a way as possible", "tokens": [50364, 10222, 848, 300, 11, 15968, 13, 1301, 311, 4447, 307, 281, 2712, 4086, 294, 382, 3353, 257, 636, 382, 1944, 50750], "temperature": 0.0, "avg_logprob": -0.26832255195168886, "compression_ratio": 1.556420233463035, "no_speech_prob": 0.07368405908346176}, {"id": 416, "seek": 199054, "start": 1998.26, "end": 2003.18, "text": " and doing a better job at predicting movies is not necessarily the best way to do that.", "tokens": [50750, 293, 884, 257, 1101, 1691, 412, 32884, 6233, 307, 406, 4725, 264, 1151, 636, 281, 360, 300, 13, 50996], "temperature": 0.0, "avg_logprob": -0.26832255195168886, "compression_ratio": 1.556420233463035, "no_speech_prob": 0.07368405908346176}, {"id": 417, "seek": 199054, "start": 2003.18, "end": 2008.22, "text": " So we're maybe less excited about collaborative filtering than some people in industry are.", "tokens": [50996, 407, 321, 434, 1310, 1570, 2919, 466, 16555, 30822, 813, 512, 561, 294, 3518, 366, 13, 51248], "temperature": 0.0, "avg_logprob": -0.26832255195168886, "compression_ratio": 1.556420233463035, "no_speech_prob": 0.07368405908346176}, {"id": 418, "seek": 199054, "start": 2008.22, "end": 2010.62, "text": " So that's why it's not our main destination.", "tokens": [51248, 407, 300, 311, 983, 309, 311, 406, 527, 2135, 12236, 13, 51368], "temperature": 0.0, "avg_logprob": -0.26832255195168886, "compression_ratio": 1.556420233463035, "no_speech_prob": 0.07368405908346176}, {"id": 419, "seek": 199054, "start": 2010.62, "end": 2016.82, "text": " NLP, on the other hand, can be a very big deal if you can do a good job, for example,", "tokens": [51368, 426, 45196, 11, 322, 264, 661, 1011, 11, 393, 312, 257, 588, 955, 2028, 498, 291, 393, 360, 257, 665, 1691, 11, 337, 1365, 11, 51678], "temperature": 0.0, "avg_logprob": -0.26832255195168886, "compression_ratio": 1.556420233463035, "no_speech_prob": 0.07368405908346176}, {"id": 420, "seek": 201682, "start": 2016.82, "end": 2025.26, "text": " of reading through lots of medical journal articles or family histories and patient notes.", "tokens": [50364, 295, 3760, 807, 3195, 295, 4625, 6708, 11290, 420, 1605, 30631, 293, 4537, 5570, 13, 50786], "temperature": 0.0, "avg_logprob": -0.263942118441121, "compression_ratio": 1.503968253968254, "no_speech_prob": 0.02479490078985691}, {"id": 421, "seek": 201682, "start": 2025.26, "end": 2030.22, "text": " You could be a long way towards creating a fantastic diagnostic tool to use in the developing", "tokens": [50786, 509, 727, 312, 257, 938, 636, 3030, 4084, 257, 5456, 27897, 2290, 281, 764, 294, 264, 6416, 51034], "temperature": 0.0, "avg_logprob": -0.263942118441121, "compression_ratio": 1.503968253968254, "no_speech_prob": 0.02479490078985691}, {"id": 422, "seek": 201682, "start": 2030.22, "end": 2035.28, "text": " world to help bring medicine to people who don't currently have it, which is almost as", "tokens": [51034, 1002, 281, 854, 1565, 7195, 281, 561, 567, 500, 380, 4362, 362, 309, 11, 597, 307, 1920, 382, 51287], "temperature": 0.0, "avg_logprob": -0.263942118441121, "compression_ratio": 1.503968253968254, "no_speech_prob": 0.02479490078985691}, {"id": 423, "seek": 201682, "start": 2035.28, "end": 2038.3799999999999, "text": " good as telling them not to watch Battlefield Earth.", "tokens": [51287, 665, 382, 3585, 552, 406, 281, 1159, 41091, 4755, 13, 51442], "temperature": 0.0, "avg_logprob": -0.263942118441121, "compression_ratio": 1.503968253968254, "no_speech_prob": 0.02479490078985691}, {"id": 424, "seek": 201682, "start": 2038.3799999999999, "end": 2040.8999999999999, "text": " They're both important.", "tokens": [51442, 814, 434, 1293, 1021, 13, 51568], "temperature": 0.0, "avg_logprob": -0.263942118441121, "compression_ratio": 1.503968253968254, "no_speech_prob": 0.02479490078985691}, {"id": 425, "seek": 201682, "start": 2040.8999999999999, "end": 2044.48, "text": " So let's talk a bit about NLP.", "tokens": [51568, 407, 718, 311, 751, 257, 857, 466, 426, 45196, 13, 51747], "temperature": 0.0, "avg_logprob": -0.263942118441121, "compression_ratio": 1.503968253968254, "no_speech_prob": 0.02479490078985691}, {"id": 426, "seek": 204448, "start": 2044.48, "end": 2049.06, "text": " In order to do this, we're going to look at a particular dataset.", "tokens": [50364, 682, 1668, 281, 360, 341, 11, 321, 434, 516, 281, 574, 412, 257, 1729, 28872, 13, 50593], "temperature": 0.0, "avg_logprob": -0.2926575072268222, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0024343596305698156}, {"id": 427, "seek": 204448, "start": 2049.06, "end": 2055.18, "text": " This dataset is like a really classic example of what people do with natural language processing", "tokens": [50593, 639, 28872, 307, 411, 257, 534, 7230, 1365, 295, 437, 561, 360, 365, 3303, 2856, 9007, 50899], "temperature": 0.0, "avg_logprob": -0.2926575072268222, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0024343596305698156}, {"id": 428, "seek": 204448, "start": 2055.18, "end": 2057.9, "text": " and it's called sentiment analysis.", "tokens": [50899, 293, 309, 311, 1219, 16149, 5215, 13, 51035], "temperature": 0.0, "avg_logprob": -0.2926575072268222, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0024343596305698156}, {"id": 429, "seek": 204448, "start": 2057.9, "end": 2063.3, "text": " Sentiment analysis means that you take a piece of text, it could be a phrase, a sentence,", "tokens": [51035, 23652, 2328, 5215, 1355, 300, 291, 747, 257, 2522, 295, 2487, 11, 309, 727, 312, 257, 9535, 11, 257, 8174, 11, 51305], "temperature": 0.0, "avg_logprob": -0.2926575072268222, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0024343596305698156}, {"id": 430, "seek": 204448, "start": 2063.3, "end": 2070.16, "text": " a paragraph or a whole document, and decide whether or not that is a positive or negative", "tokens": [51305, 257, 18865, 420, 257, 1379, 4166, 11, 293, 4536, 1968, 420, 406, 300, 307, 257, 3353, 420, 3671, 51648], "temperature": 0.0, "avg_logprob": -0.2926575072268222, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0024343596305698156}, {"id": 431, "seek": 204448, "start": 2070.16, "end": 2073.62, "text": " sentiment.", "tokens": [51648, 16149, 13, 51821], "temperature": 0.0, "avg_logprob": -0.2926575072268222, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0024343596305698156}, {"id": 432, "seek": 207362, "start": 2073.7599999999998, "end": 2081.3599999999997, "text": " Keras actually comes with such a dataset, which is called the IMDB sentiment dataset.", "tokens": [50371, 591, 6985, 767, 1487, 365, 1270, 257, 28872, 11, 597, 307, 1219, 264, 21463, 27735, 16149, 28872, 13, 50751], "temperature": 0.0, "avg_logprob": -0.3516373634338379, "compression_ratio": 1.3414634146341464, "no_speech_prob": 0.002714975271373987}, {"id": 433, "seek": 207362, "start": 2081.3599999999997, "end": 2090.72, "text": " The IMDB sentiment dataset was originally developed from the Stanford AI group.", "tokens": [50751, 440, 21463, 27735, 16149, 28872, 390, 7993, 4743, 490, 264, 20374, 7318, 1594, 13, 51219], "temperature": 0.0, "avg_logprob": -0.3516373634338379, "compression_ratio": 1.3414634146341464, "no_speech_prob": 0.002714975271373987}, {"id": 434, "seek": 209072, "start": 2090.72, "end": 2103.72, "text": " The paper about it was actually published in 2012.", "tokens": [50364, 440, 3035, 466, 309, 390, 767, 6572, 294, 9125, 13, 51014], "temperature": 0.0, "avg_logprob": -0.33558399272414874, "compression_ratio": 1.4, "no_speech_prob": 0.00912549439817667}, {"id": 435, "seek": 209072, "start": 2103.72, "end": 2109.7799999999997, "text": " They talk about all the details about what people try to do with sentiment analysis.", "tokens": [51014, 814, 751, 466, 439, 264, 4365, 466, 437, 561, 853, 281, 360, 365, 16149, 5215, 13, 51317], "temperature": 0.0, "avg_logprob": -0.33558399272414874, "compression_ratio": 1.4, "no_speech_prob": 0.00912549439817667}, {"id": 436, "seek": 209072, "start": 2109.7799999999997, "end": 2115.72, "text": " In general, although I think academic papers tend to be way more math-y than they should", "tokens": [51317, 682, 2674, 11, 4878, 286, 519, 7778, 10577, 3928, 281, 312, 636, 544, 5221, 12, 88, 813, 436, 820, 51614], "temperature": 0.0, "avg_logprob": -0.33558399272414874, "compression_ratio": 1.4, "no_speech_prob": 0.00912549439817667}, {"id": 437, "seek": 211572, "start": 2115.72, "end": 2122.54, "text": " be, the introductory sections often do a great job of capturing why this is an interesting", "tokens": [50364, 312, 11, 264, 39048, 10863, 2049, 360, 257, 869, 1691, 295, 23384, 983, 341, 307, 364, 1880, 50705], "temperature": 0.0, "avg_logprob": -0.31036437076071033, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.5038635730743408}, {"id": 438, "seek": 211572, "start": 2122.54, "end": 2126.14, "text": " problem, what kind of approaches people have taken, and so forth.", "tokens": [50705, 1154, 11, 437, 733, 295, 11587, 561, 362, 2726, 11, 293, 370, 5220, 13, 50885], "temperature": 0.0, "avg_logprob": -0.31036437076071033, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.5038635730743408}, {"id": 439, "seek": 211572, "start": 2126.14, "end": 2129.9399999999996, "text": " The other reason papers are super-helpful is that you can skip down to the experiment", "tokens": [50885, 440, 661, 1778, 10577, 366, 1687, 12, 37451, 906, 307, 300, 291, 393, 10023, 760, 281, 264, 5120, 51075], "temperature": 0.0, "avg_logprob": -0.31036437076071033, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.5038635730743408}, {"id": 440, "seek": 211572, "start": 2129.9399999999996, "end": 2130.9399999999996, "text": " section.", "tokens": [51075, 3541, 13, 51125], "temperature": 0.0, "avg_logprob": -0.31036437076071033, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.5038635730743408}, {"id": 441, "seek": 211572, "start": 2130.9399999999996, "end": 2136.1, "text": " Every machine learning paper pretty much has an experiment section and find out what the", "tokens": [51125, 2048, 3479, 2539, 3035, 1238, 709, 575, 364, 5120, 3541, 293, 915, 484, 437, 264, 51383], "temperature": 0.0, "avg_logprob": -0.31036437076071033, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.5038635730743408}, {"id": 442, "seek": 211572, "start": 2136.1, "end": 2137.1, "text": " score is.", "tokens": [51383, 6175, 307, 13, 51433], "temperature": 0.0, "avg_logprob": -0.31036437076071033, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.5038635730743408}, {"id": 443, "seek": 211572, "start": 2137.1, "end": 2139.5, "text": " So here's their score section.", "tokens": [51433, 407, 510, 311, 641, 6175, 3541, 13, 51553], "temperature": 0.0, "avg_logprob": -0.31036437076071033, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.5038635730743408}, {"id": 444, "seek": 213950, "start": 2139.5, "end": 2146.04, "text": " So here they showed that using this dataset they created of IMDB movie reviews along with", "tokens": [50364, 407, 510, 436, 4712, 300, 1228, 341, 28872, 436, 2942, 295, 21463, 27735, 3169, 10229, 2051, 365, 50691], "temperature": 0.0, "avg_logprob": -0.262606324089898, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.021946175023913383}, {"id": 445, "seek": 213950, "start": 2146.04, "end": 2155.96, "text": " their sentiment, their full model plus an additional model got a score of 88.33% accuracy", "tokens": [50691, 641, 16149, 11, 641, 1577, 2316, 1804, 364, 4497, 2316, 658, 257, 6175, 295, 24587, 13, 10191, 4, 14170, 51187], "temperature": 0.0, "avg_logprob": -0.262606324089898, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.021946175023913383}, {"id": 446, "seek": 213950, "start": 2155.96, "end": 2157.76, "text": " in predicting sentiment.", "tokens": [51187, 294, 32884, 16149, 13, 51277], "temperature": 0.0, "avg_logprob": -0.262606324089898, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.021946175023913383}, {"id": 447, "seek": 213950, "start": 2157.76, "end": 2161.76, "text": " They had another one here where they also added in some unlabeled data.", "tokens": [51277, 814, 632, 1071, 472, 510, 689, 436, 611, 3869, 294, 512, 32118, 18657, 292, 1412, 13, 51477], "temperature": 0.0, "avg_logprob": -0.262606324089898, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.021946175023913383}, {"id": 448, "seek": 213950, "start": 2161.76, "end": 2165.4, "text": " We're not going to be looking at that today, that would be a semi-supervised learning problem.", "tokens": [51477, 492, 434, 406, 516, 281, 312, 1237, 412, 300, 965, 11, 300, 576, 312, 257, 12909, 12, 48172, 24420, 2539, 1154, 13, 51659], "temperature": 0.0, "avg_logprob": -0.262606324089898, "compression_ratio": 1.5523012552301256, "no_speech_prob": 0.021946175023913383}, {"id": 449, "seek": 216540, "start": 2165.54, "end": 2172.7000000000003, "text": " Today our goal is to beat 88.33 as being the academic state-of-the-art for this dataset,", "tokens": [50371, 2692, 527, 3387, 307, 281, 4224, 24587, 13, 10191, 382, 885, 264, 7778, 1785, 12, 2670, 12, 3322, 12, 446, 337, 341, 28872, 11, 50729], "temperature": 0.0, "avg_logprob": -0.2982043849611745, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.028869740664958954}, {"id": 450, "seek": 216540, "start": 2172.7000000000003, "end": 2179.5, "text": " at least as of this time.", "tokens": [50729, 412, 1935, 382, 295, 341, 565, 13, 51069], "temperature": 0.0, "avg_logprob": -0.2982043849611745, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.028869740664958954}, {"id": 451, "seek": 216540, "start": 2179.5, "end": 2184.14, "text": " To grab it, we can just say from keras.datasets import imdb.", "tokens": [51069, 1407, 4444, 309, 11, 321, 393, 445, 584, 490, 350, 6985, 13, 20367, 296, 1385, 974, 566, 67, 65, 13, 51301], "temperature": 0.0, "avg_logprob": -0.2982043849611745, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.028869740664958954}, {"id": 452, "seek": 216540, "start": 2184.14, "end": 2187.4, "text": " Keras actually kind of fiddles around with it in ways that I don't really like.", "tokens": [51301, 591, 6985, 767, 733, 295, 283, 14273, 904, 926, 365, 309, 294, 2098, 300, 286, 500, 380, 534, 411, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2982043849611745, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.028869740664958954}, {"id": 453, "seek": 216540, "start": 2187.4, "end": 2193.1600000000003, "text": " So I actually copied and pasted from the keras file these 3 lines to import it directly without", "tokens": [51464, 407, 286, 767, 25365, 293, 1791, 292, 490, 264, 350, 6985, 3991, 613, 805, 3876, 281, 974, 309, 3838, 1553, 51752], "temperature": 0.0, "avg_logprob": -0.2982043849611745, "compression_ratio": 1.5530973451327434, "no_speech_prob": 0.028869740664958954}, {"id": 454, "seek": 219316, "start": 2193.18, "end": 2194.74, "text": " screwing with it.", "tokens": [50365, 5630, 278, 365, 309, 13, 50443], "temperature": 0.0, "avg_logprob": -0.4355750560760498, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.16237446665763855}, {"id": 455, "seek": 219316, "start": 2194.74, "end": 2203.2599999999998, "text": " So that's why rather than using the keras dataset directly, I'm using these 3 lines.", "tokens": [50443, 407, 300, 311, 983, 2831, 813, 1228, 264, 350, 6985, 28872, 3838, 11, 286, 478, 1228, 613, 805, 3876, 13, 50869], "temperature": 0.0, "avg_logprob": -0.4355750560760498, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.16237446665763855}, {"id": 456, "seek": 219316, "start": 2203.2599999999998, "end": 2212.66, "text": " There are 25,000 movie reviews in the training set.", "tokens": [50869, 821, 366, 3552, 11, 1360, 3169, 10229, 294, 264, 3097, 992, 13, 51339], "temperature": 0.0, "avg_logprob": -0.4355750560760498, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.16237446665763855}, {"id": 457, "seek": 219316, "start": 2212.66, "end": 2213.66, "text": " Here's an example of one.", "tokens": [51339, 1692, 311, 364, 1365, 295, 472, 13, 51389], "temperature": 0.0, "avg_logprob": -0.4355750560760498, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.16237446665763855}, {"id": 458, "seek": 219316, "start": 2213.66, "end": 2219.8599999999997, "text": " Realmware Ohio is a cartoon comedy, around at the same time as some other programs, blah", "tokens": [51389, 44723, 3039, 14469, 307, 257, 18569, 13394, 11, 926, 412, 264, 912, 565, 382, 512, 661, 4268, 11, 12288, 51699], "temperature": 0.0, "avg_logprob": -0.4355750560760498, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.16237446665763855}, {"id": 459, "seek": 219316, "start": 2219.8599999999997, "end": 2221.62, "text": " blah blah.", "tokens": [51699, 12288, 12288, 13, 51787], "temperature": 0.0, "avg_logprob": -0.4355750560760498, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.16237446665763855}, {"id": 460, "seek": 222162, "start": 2221.64, "end": 2228.24, "text": " So the dataset actually does not quite come to us in this format, it actually comes to", "tokens": [50365, 407, 264, 28872, 767, 775, 406, 1596, 808, 281, 505, 294, 341, 7877, 11, 309, 767, 1487, 281, 50695], "temperature": 0.0, "avg_logprob": -0.30799405328158674, "compression_ratio": 1.5460992907801419, "no_speech_prob": 0.0013250091578811407}, {"id": 461, "seek": 222162, "start": 2228.24, "end": 2233.88, "text": " us in this format, which is a list of IDs.", "tokens": [50695, 505, 294, 341, 7877, 11, 597, 307, 257, 1329, 295, 48212, 13, 50977], "temperature": 0.0, "avg_logprob": -0.30799405328158674, "compression_ratio": 1.5460992907801419, "no_speech_prob": 0.0013250091578811407}, {"id": 462, "seek": 222162, "start": 2233.88, "end": 2242.7999999999997, "text": " And so these IDs we can look up in the word index, which is something that they provide.", "tokens": [50977, 400, 370, 613, 48212, 321, 393, 574, 493, 294, 264, 1349, 8186, 11, 597, 307, 746, 300, 436, 2893, 13, 51423], "temperature": 0.0, "avg_logprob": -0.30799405328158674, "compression_ratio": 1.5460992907801419, "no_speech_prob": 0.0013250091578811407}, {"id": 463, "seek": 224280, "start": 2242.8, "end": 2261.0600000000004, "text": " And so for example, if we look at IDXArray, the word index as you can see basically maps", "tokens": [50364, 400, 370, 337, 1365, 11, 498, 321, 574, 412, 7348, 55, 10683, 3458, 11, 264, 1349, 8186, 382, 291, 393, 536, 1936, 11317, 51277], "temperature": 0.0, "avg_logprob": -0.3360303347228003, "compression_ratio": 1.3625, "no_speech_prob": 0.002714959206059575}, {"id": 464, "seek": 224280, "start": 2261.0600000000004, "end": 2264.52, "text": " an integer to every word.", "tokens": [51277, 364, 24922, 281, 633, 1349, 13, 51450], "temperature": 0.0, "avg_logprob": -0.3360303347228003, "compression_ratio": 1.3625, "no_speech_prob": 0.002714959206059575}, {"id": 465, "seek": 224280, "start": 2264.52, "end": 2269.7000000000003, "text": " It's in order of how frequently those words appeared in this particular corpus, which", "tokens": [51450, 467, 311, 294, 1668, 295, 577, 10374, 729, 2283, 8516, 294, 341, 1729, 1181, 31624, 11, 597, 51709], "temperature": 0.0, "avg_logprob": -0.3360303347228003, "compression_ratio": 1.3625, "no_speech_prob": 0.002714959206059575}, {"id": 466, "seek": 224280, "start": 2269.7000000000003, "end": 2271.54, "text": " is kind of handy.", "tokens": [51709, 307, 733, 295, 13239, 13, 51801], "temperature": 0.0, "avg_logprob": -0.3360303347228003, "compression_ratio": 1.3625, "no_speech_prob": 0.002714959206059575}, {"id": 467, "seek": 227154, "start": 2271.68, "end": 2279.56, "text": " Then I also create a reverse index, so it goes from word to ID.", "tokens": [50371, 1396, 286, 611, 1884, 257, 9943, 8186, 11, 370, 309, 1709, 490, 1349, 281, 7348, 13, 50765], "temperature": 0.0, "avg_logprob": -0.2986205057664351, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0007916135946288705}, {"id": 468, "seek": 227154, "start": 2279.56, "end": 2285.7599999999998, "text": " I can see that in the very first training example, the very first word is word number", "tokens": [50765, 286, 393, 536, 300, 294, 264, 588, 700, 3097, 1365, 11, 264, 588, 700, 1349, 307, 1349, 1230, 51075], "temperature": 0.0, "avg_logprob": -0.2986205057664351, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0007916135946288705}, {"id": 469, "seek": 227154, "start": 2285.7599999999998, "end": 2287.7599999999998, "text": " 23022.", "tokens": [51075, 35311, 7490, 13, 51175], "temperature": 0.0, "avg_logprob": -0.2986205057664351, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0007916135946288705}, {"id": 470, "seek": 227154, "start": 2287.7599999999998, "end": 2293.52, "text": " So if I look up index to word 23022, it is the word Bromwell.", "tokens": [51175, 407, 498, 286, 574, 493, 8186, 281, 1349, 35311, 7490, 11, 309, 307, 264, 1349, 1603, 298, 6326, 13, 51463], "temperature": 0.0, "avg_logprob": -0.2986205057664351, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0007916135946288705}, {"id": 471, "seek": 227154, "start": 2293.52, "end": 2300.32, "text": " And so then I just go through and I map everything in that first review to index to word and", "tokens": [51463, 400, 370, 550, 286, 445, 352, 807, 293, 286, 4471, 1203, 294, 300, 700, 3131, 281, 8186, 281, 1349, 293, 51803], "temperature": 0.0, "avg_logprob": -0.2986205057664351, "compression_ratio": 1.594871794871795, "no_speech_prob": 0.0007916135946288705}, {"id": 472, "seek": 230032, "start": 2300.34, "end": 2304.6200000000003, "text": " then join it together with a space and that's how we can turn the data that they give us", "tokens": [50365, 550, 3917, 309, 1214, 365, 257, 1901, 293, 300, 311, 577, 321, 393, 1261, 264, 1412, 300, 436, 976, 505, 50579], "temperature": 0.0, "avg_logprob": -0.27919013590752323, "compression_ratio": 1.5808080808080809, "no_speech_prob": 0.0060035064816474915}, {"id": 473, "seek": 230032, "start": 2304.6200000000003, "end": 2309.5800000000004, "text": " back into the movie review.", "tokens": [50579, 646, 666, 264, 3169, 3131, 13, 50827], "temperature": 0.0, "avg_logprob": -0.27919013590752323, "compression_ratio": 1.5808080808080809, "no_speech_prob": 0.0060035064816474915}, {"id": 474, "seek": 230032, "start": 2309.5800000000004, "end": 2314.1400000000003, "text": " As well as providing the reviews, they also provide labels.", "tokens": [50827, 1018, 731, 382, 6530, 264, 10229, 11, 436, 611, 2893, 16949, 13, 51055], "temperature": 0.0, "avg_logprob": -0.27919013590752323, "compression_ratio": 1.5808080808080809, "no_speech_prob": 0.0060035064816474915}, {"id": 475, "seek": 230032, "start": 2314.1400000000003, "end": 2319.7000000000003, "text": " 1 is positive sentiment, 0 is negative sentiment.", "tokens": [51055, 502, 307, 3353, 16149, 11, 1958, 307, 3671, 16149, 13, 51333], "temperature": 0.0, "avg_logprob": -0.27919013590752323, "compression_ratio": 1.5808080808080809, "no_speech_prob": 0.0060035064816474915}, {"id": 476, "seek": 230032, "start": 2319.7000000000003, "end": 2326.2200000000003, "text": " So our goal is to take these 25,000 reviews that look like this and predict whether it", "tokens": [51333, 407, 527, 3387, 307, 281, 747, 613, 3552, 11, 1360, 10229, 300, 574, 411, 341, 293, 6069, 1968, 309, 51659], "temperature": 0.0, "avg_logprob": -0.27919013590752323, "compression_ratio": 1.5808080808080809, "no_speech_prob": 0.0060035064816474915}, {"id": 477, "seek": 232622, "start": 2326.22, "end": 2331.16, "text": " will be positive or negative in sentiment and the data is actually provided to us as", "tokens": [50364, 486, 312, 3353, 420, 3671, 294, 16149, 293, 264, 1412, 307, 767, 5649, 281, 505, 382, 50611], "temperature": 0.0, "avg_logprob": -0.2728984139182351, "compression_ratio": 1.5078740157480315, "no_speech_prob": 0.24507425725460052}, {"id": 478, "seek": 232622, "start": 2331.16, "end": 2335.0, "text": " a list of word IDs for each review.", "tokens": [50611, 257, 1329, 295, 1349, 48212, 337, 1184, 3131, 13, 50803], "temperature": 0.0, "avg_logprob": -0.2728984139182351, "compression_ratio": 1.5078740157480315, "no_speech_prob": 0.24507425725460052}, {"id": 479, "seek": 232622, "start": 2335.0, "end": 2343.12, "text": " Is everybody clear on the problem we are trying to solve and how it's laid out?", "tokens": [50803, 1119, 2201, 1850, 322, 264, 1154, 321, 366, 1382, 281, 5039, 293, 577, 309, 311, 9897, 484, 30, 51209], "temperature": 0.0, "avg_logprob": -0.2728984139182351, "compression_ratio": 1.5078740157480315, "no_speech_prob": 0.24507425725460052}, {"id": 480, "seek": 232622, "start": 2343.12, "end": 2345.54, "text": " So there's a couple of things we can do to make it simpler.", "tokens": [51209, 407, 456, 311, 257, 1916, 295, 721, 321, 393, 360, 281, 652, 309, 18587, 13, 51330], "temperature": 0.0, "avg_logprob": -0.2728984139182351, "compression_ratio": 1.5078740157480315, "no_speech_prob": 0.24507425725460052}, {"id": 481, "seek": 232622, "start": 2345.54, "end": 2348.4399999999996, "text": " One is we can reduce the vocabulary.", "tokens": [51330, 1485, 307, 321, 393, 5407, 264, 19864, 13, 51475], "temperature": 0.0, "avg_logprob": -0.2728984139182351, "compression_ratio": 1.5078740157480315, "no_speech_prob": 0.24507425725460052}, {"id": 482, "seek": 232622, "start": 2348.4399999999996, "end": 2355.7999999999997, "text": " So currently there are some pretty unusual words, like word number 23022 is Bromwell.", "tokens": [51475, 407, 4362, 456, 366, 512, 1238, 10901, 2283, 11, 411, 1349, 1230, 35311, 7490, 307, 1603, 298, 6326, 13, 51843], "temperature": 0.0, "avg_logprob": -0.2728984139182351, "compression_ratio": 1.5078740157480315, "no_speech_prob": 0.24507425725460052}, {"id": 483, "seek": 235580, "start": 2356.38, "end": 2361.46, "text": " And if we're trying to figure out how to deal with all these different words, having to", "tokens": [50393, 400, 498, 321, 434, 1382, 281, 2573, 484, 577, 281, 2028, 365, 439, 613, 819, 2283, 11, 1419, 281, 50647], "temperature": 0.0, "avg_logprob": -0.24537605151795505, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00038596487138420343}, {"id": 484, "seek": 235580, "start": 2361.46, "end": 2365.78, "text": " figure out the various ways in which the word Bromwell is used is probably not going to", "tokens": [50647, 2573, 484, 264, 3683, 2098, 294, 597, 264, 1349, 1603, 298, 6326, 307, 1143, 307, 1391, 406, 516, 281, 50863], "temperature": 0.0, "avg_logprob": -0.24537605151795505, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00038596487138420343}, {"id": 485, "seek": 235580, "start": 2365.78, "end": 2368.9, "text": " net as much for a lot of computation and memory cost.", "tokens": [50863, 2533, 382, 709, 337, 257, 688, 295, 24903, 293, 4675, 2063, 13, 51019], "temperature": 0.0, "avg_logprob": -0.24537605151795505, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00038596487138420343}, {"id": 486, "seek": 235580, "start": 2368.9, "end": 2373.46, "text": " So we're going to truncate the vocabulary down to 5000.", "tokens": [51019, 407, 321, 434, 516, 281, 504, 409, 66, 473, 264, 19864, 760, 281, 23777, 13, 51247], "temperature": 0.0, "avg_logprob": -0.24537605151795505, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00038596487138420343}, {"id": 487, "seek": 235580, "start": 2373.46, "end": 2377.34, "text": " It's very easy to do that because the words are already ordered by frequency.", "tokens": [51247, 467, 311, 588, 1858, 281, 360, 300, 570, 264, 2283, 366, 1217, 8866, 538, 7893, 13, 51441], "temperature": 0.0, "avg_logprob": -0.24537605151795505, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00038596487138420343}, {"id": 488, "seek": 235580, "start": 2377.34, "end": 2385.46, "text": " I simply go through everything in our training set and I just say if the word ID is less", "tokens": [51441, 286, 2935, 352, 807, 1203, 294, 527, 3097, 992, 293, 286, 445, 584, 498, 264, 1349, 7348, 307, 1570, 51847], "temperature": 0.0, "avg_logprob": -0.24537605151795505, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.00038596487138420343}, {"id": 489, "seek": 238546, "start": 2385.52, "end": 2389.16, "text": " than this vocab size of 5000, we'll leave it as it is.", "tokens": [50367, 813, 341, 2329, 455, 2744, 295, 23777, 11, 321, 603, 1856, 309, 382, 309, 307, 13, 50549], "temperature": 0.0, "avg_logprob": -0.25927402903732744, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.00030533235985785723}, {"id": 490, "seek": 238546, "start": 2389.16, "end": 2394.2, "text": " Otherwise we'll replace it with the number 5000.", "tokens": [50549, 10328, 321, 603, 7406, 309, 365, 264, 1230, 23777, 13, 50801], "temperature": 0.0, "avg_logprob": -0.25927402903732744, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.00030533235985785723}, {"id": 491, "seek": 238546, "start": 2394.2, "end": 2401.2, "text": " So at the end of this, we now have replaced all of our rare words with a single ID.", "tokens": [50801, 407, 412, 264, 917, 295, 341, 11, 321, 586, 362, 10772, 439, 295, 527, 5892, 2283, 365, 257, 2167, 7348, 13, 51151], "temperature": 0.0, "avg_logprob": -0.25927402903732744, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.00030533235985785723}, {"id": 492, "seek": 238546, "start": 2401.2, "end": 2404.76, "text": " Here's a quick look at the sentences.", "tokens": [51151, 1692, 311, 257, 1702, 574, 412, 264, 16579, 13, 51329], "temperature": 0.0, "avg_logprob": -0.25927402903732744, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.00030533235985785723}, {"id": 493, "seek": 238546, "start": 2404.76, "end": 2409.56, "text": " The reviews are sometimes up to 2493 words long.", "tokens": [51329, 440, 10229, 366, 2171, 493, 281, 4022, 26372, 2283, 938, 13, 51569], "temperature": 0.0, "avg_logprob": -0.25927402903732744, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.00030533235985785723}, {"id": 494, "seek": 238546, "start": 2409.56, "end": 2412.76, "text": " Some people spend far too much time in IMDb.", "tokens": [51569, 2188, 561, 3496, 1400, 886, 709, 565, 294, 21463, 35, 65, 13, 51729], "temperature": 0.0, "avg_logprob": -0.25927402903732744, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.00030533235985785723}, {"id": 495, "seek": 238546, "start": 2412.76, "end": 2414.52, "text": " Some are as short as 10 words.", "tokens": [51729, 2188, 366, 382, 2099, 382, 1266, 2283, 13, 51817], "temperature": 0.0, "avg_logprob": -0.25927402903732744, "compression_ratio": 1.5151515151515151, "no_speech_prob": 0.00030533235985785723}, {"id": 496, "seek": 241452, "start": 2414.58, "end": 2418.02, "text": " I've averaged there 237 words.", "tokens": [50367, 286, 600, 18247, 2980, 456, 6673, 22, 2283, 13, 50539], "temperature": 0.0, "avg_logprob": -0.2658773461977641, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.014956609345972538}, {"id": 497, "seek": 241452, "start": 2418.02, "end": 2424.54, "text": " As you'll see, we actually need to make all of our reviews the same length.", "tokens": [50539, 1018, 291, 603, 536, 11, 321, 767, 643, 281, 652, 439, 295, 527, 10229, 264, 912, 4641, 13, 50865], "temperature": 0.0, "avg_logprob": -0.2658773461977641, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.014956609345972538}, {"id": 498, "seek": 241452, "start": 2424.54, "end": 2432.9, "text": " So allowing this 2493 word review would use up a lot of memory and time.", "tokens": [50865, 407, 8293, 341, 4022, 26372, 1349, 3131, 576, 764, 493, 257, 688, 295, 4675, 293, 565, 13, 51283], "temperature": 0.0, "avg_logprob": -0.2658773461977641, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.014956609345972538}, {"id": 499, "seek": 241452, "start": 2432.9, "end": 2437.46, "text": " So we're going to decide to truncate every review at 500 words.", "tokens": [51283, 407, 321, 434, 516, 281, 4536, 281, 504, 409, 66, 473, 633, 3131, 412, 5923, 2283, 13, 51511], "temperature": 0.0, "avg_logprob": -0.2658773461977641, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.014956609345972538}, {"id": 500, "seek": 241452, "start": 2437.46, "end": 2439.86, "text": " That's more than twice as big as the mean.", "tokens": [51511, 663, 311, 544, 813, 6091, 382, 955, 382, 264, 914, 13, 51631], "temperature": 0.0, "avg_logprob": -0.2658773461977641, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.014956609345972538}, {"id": 501, "seek": 241452, "start": 2439.86, "end": 2443.3, "text": " So we're not going to lose too much.", "tokens": [51631, 407, 321, 434, 406, 516, 281, 3624, 886, 709, 13, 51803], "temperature": 0.0, "avg_logprob": -0.2658773461977641, "compression_ratio": 1.516431924882629, "no_speech_prob": 0.014956609345972538}, {"id": 502, "seek": 244330, "start": 2443.7200000000003, "end": 2446.6000000000004, "text": " What we now need to do is we have to create a rectangular...", "tokens": [50385, 708, 321, 586, 643, 281, 360, 307, 321, 362, 281, 1884, 257, 31167, 485, 50529], "temperature": 0.0, "avg_logprob": -0.6099276308153496, "compression_ratio": 1.3897058823529411, "no_speech_prob": 0.2598121464252472}, {"id": 503, "seek": 244330, "start": 2446.6000000000004, "end": 2447.6000000000004, "text": " Question?", "tokens": [50529, 14464, 30, 50579], "temperature": 0.0, "avg_logprob": -0.6099276308153496, "compression_ratio": 1.3897058823529411, "no_speech_prob": 0.2598121464252472}, {"id": 504, "seek": 244330, "start": 2447.6000000000004, "end": 2452.88, "text": " What if the word 5000 gives a bias?", "tokens": [50579, 708, 498, 264, 1349, 23777, 2709, 257, 12577, 30, 50843], "temperature": 0.0, "avg_logprob": -0.6099276308153496, "compression_ratio": 1.3897058823529411, "no_speech_prob": 0.2598121464252472}, {"id": 505, "seek": 244330, "start": 2452.88, "end": 2459.88, "text": " So whatever the word 5000 is, let's find out what the word 5000 is.", "tokens": [50843, 407, 2035, 264, 1349, 23777, 307, 11, 718, 311, 915, 484, 437, 264, 1349, 23777, 307, 13, 51193], "temperature": 0.0, "avg_logprob": -0.6099276308153496, "compression_ratio": 1.3897058823529411, "no_speech_prob": 0.2598121464252472}, {"id": 506, "seek": 244330, "start": 2459.88, "end": 2462.88, "text": " IDX2WORD 5000.", "tokens": [51193, 7348, 55, 17, 54, 13748, 23777, 13, 51343], "temperature": 0.0, "avg_logprob": -0.6099276308153496, "compression_ratio": 1.3897058823529411, "no_speech_prob": 0.2598121464252472}, {"id": 507, "seek": 246288, "start": 2462.88, "end": 2473.6600000000003, "text": " Okay, it's the year, 1987.", "tokens": [50364, 1033, 11, 309, 311, 264, 1064, 11, 29008, 13, 50903], "temperature": 0.0, "avg_logprob": -0.3984429265411807, "compression_ratio": 1.4303030303030304, "no_speech_prob": 0.006589686963707209}, {"id": 508, "seek": 246288, "start": 2473.6600000000003, "end": 2474.6600000000003, "text": " That's fine.", "tokens": [50903, 663, 311, 2489, 13, 50953], "temperature": 0.0, "avg_logprob": -0.3984429265411807, "compression_ratio": 1.4303030303030304, "no_speech_prob": 0.006589686963707209}, {"id": 509, "seek": 246288, "start": 2474.6600000000003, "end": 2477.62, "text": " We're about to learn a machine learning model.", "tokens": [50953, 492, 434, 466, 281, 1466, 257, 3479, 2539, 2316, 13, 51101], "temperature": 0.0, "avg_logprob": -0.3984429265411807, "compression_ratio": 1.4303030303030304, "no_speech_prob": 0.006589686963707209}, {"id": 510, "seek": 246288, "start": 2477.62, "end": 2484.26, "text": " And so the vast majority of the time, it comes across the word 5000.", "tokens": [51101, 400, 370, 264, 8369, 6286, 295, 264, 565, 11, 309, 1487, 2108, 264, 1349, 23777, 13, 51433], "temperature": 0.0, "avg_logprob": -0.3984429265411807, "compression_ratio": 1.4303030303030304, "no_speech_prob": 0.006589686963707209}, {"id": 511, "seek": 246288, "start": 2484.26, "end": 2486.7000000000003, "text": " It's actually going to mean rare word.", "tokens": [51433, 467, 311, 767, 516, 281, 914, 5892, 1349, 13, 51555], "temperature": 0.0, "avg_logprob": -0.3984429265411807, "compression_ratio": 1.4303030303030304, "no_speech_prob": 0.006589686963707209}, {"id": 512, "seek": 246288, "start": 2486.7000000000003, "end": 2490.34, "text": " It's not going to specifically mean 1987.", "tokens": [51555, 467, 311, 406, 516, 281, 4682, 914, 29008, 13, 51737], "temperature": 0.0, "avg_logprob": -0.3984429265411807, "compression_ratio": 1.4303030303030304, "no_speech_prob": 0.006589686963707209}, {"id": 513, "seek": 249034, "start": 2490.34, "end": 2494.32, "text": " It's going to learn to deal with that as best as it can.", "tokens": [50364, 467, 311, 516, 281, 1466, 281, 2028, 365, 300, 382, 1151, 382, 309, 393, 13, 50563], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 514, "seek": 249034, "start": 2494.32, "end": 2498.9, "text": " The idea is the rare words don't appear too often, so hopefully this is not going to cause", "tokens": [50563, 440, 1558, 307, 264, 5892, 2283, 500, 380, 4204, 886, 2049, 11, 370, 4696, 341, 307, 406, 516, 281, 3082, 50792], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 515, "seek": 249034, "start": 2498.9, "end": 2499.9, "text": " too much problem.", "tokens": [50792, 886, 709, 1154, 13, 50842], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 516, "seek": 249034, "start": 2499.9, "end": 2500.9, "text": " Question?", "tokens": [50842, 14464, 30, 50892], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 517, "seek": 249034, "start": 2500.9, "end": 2506.76, "text": " And doesn't just using frequencies favor stop words?", "tokens": [50892, 400, 1177, 380, 445, 1228, 20250, 2294, 1590, 2283, 30, 51185], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 518, "seek": 249034, "start": 2506.76, "end": 2508.44, "text": " We're not just using frequencies.", "tokens": [51185, 492, 434, 406, 445, 1228, 20250, 13, 51269], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 519, "seek": 249034, "start": 2508.44, "end": 2515.92, "text": " All we're doing is we're just truncating our vocabulary at this point.", "tokens": [51269, 1057, 321, 434, 884, 307, 321, 434, 445, 504, 409, 66, 990, 527, 19864, 412, 341, 935, 13, 51643], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 520, "seek": 249034, "start": 2515.92, "end": 2517.88, "text": " Can you put that close to your mouth?", "tokens": [51643, 1664, 291, 829, 300, 1998, 281, 428, 4525, 30, 51741], "temperature": 0.0, "avg_logprob": -0.32912726541167325, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.2172960340976715}, {"id": 521, "seek": 251788, "start": 2517.88, "end": 2523.7000000000003, "text": " So the 5000 word, can we just replace it with some neutral word to take care of that", "tokens": [50364, 407, 264, 23777, 1349, 11, 393, 321, 445, 7406, 309, 365, 512, 10598, 1349, 281, 747, 1127, 295, 300, 50655], "temperature": 0.0, "avg_logprob": -0.359095538104022, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.11593637615442276}, {"id": 522, "seek": 251788, "start": 2523.7000000000003, "end": 2524.7000000000003, "text": " bias thing?", "tokens": [50655, 12577, 551, 30, 50705], "temperature": 0.0, "avg_logprob": -0.359095538104022, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.11593637615442276}, {"id": 523, "seek": 251788, "start": 2524.7000000000003, "end": 2528.86, "text": " There's really not going to be a bias here.", "tokens": [50705, 821, 311, 534, 406, 516, 281, 312, 257, 12577, 510, 13, 50913], "temperature": 0.0, "avg_logprob": -0.359095538104022, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.11593637615442276}, {"id": 524, "seek": 251788, "start": 2528.86, "end": 2531.84, "text": " We're just replacing it with a random ID.", "tokens": [50913, 492, 434, 445, 19139, 309, 365, 257, 4974, 7348, 13, 51062], "temperature": 0.0, "avg_logprob": -0.359095538104022, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.11593637615442276}, {"id": 525, "seek": 251788, "start": 2531.84, "end": 2540.7200000000003, "text": " The fact that occasionally the word 1987 actually pops up, it's totally insignificant.", "tokens": [51062, 440, 1186, 300, 16895, 264, 1349, 29008, 767, 16795, 493, 11, 309, 311, 3879, 43685, 13, 51506], "temperature": 0.0, "avg_logprob": -0.359095538104022, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.11593637615442276}, {"id": 526, "seek": 251788, "start": 2540.7200000000003, "end": 2544.48, "text": " We could replace it with minus 1.", "tokens": [51506, 492, 727, 7406, 309, 365, 3175, 502, 13, 51694], "temperature": 0.0, "avg_logprob": -0.359095538104022, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.11593637615442276}, {"id": 527, "seek": 254448, "start": 2544.48, "end": 2547.14, "text": " It's just a sentinel value which has no meaning.", "tokens": [50364, 467, 311, 445, 257, 2279, 40952, 2158, 597, 575, 572, 3620, 13, 50497], "temperature": 0.0, "avg_logprob": -0.40166964213053385, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.4571112096309662}, {"id": 528, "seek": 254448, "start": 2547.14, "end": 2554.1, "text": " Also, we're getting rid of all the words that are 5000 and beyond.", "tokens": [50497, 2743, 11, 321, 434, 1242, 3973, 295, 439, 264, 2283, 300, 366, 23777, 293, 4399, 13, 50845], "temperature": 0.0, "avg_logprob": -0.40166964213053385, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.4571112096309662}, {"id": 529, "seek": 254448, "start": 2554.1, "end": 2556.68, "text": " It's not just a single word.", "tokens": [50845, 467, 311, 406, 445, 257, 2167, 1349, 13, 50974], "temperature": 0.0, "avg_logprob": -0.40166964213053385, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.4571112096309662}, {"id": 530, "seek": 254448, "start": 2556.68, "end": 2565.54, "text": " It represents all of the less common words.", "tokens": [50974, 467, 8855, 439, 295, 264, 1570, 2689, 2283, 13, 51417], "temperature": 0.0, "avg_logprob": -0.40166964213053385, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.4571112096309662}, {"id": 531, "seek": 254448, "start": 2565.54, "end": 2572.6, "text": " It's one of these design decisions which it's not worth spending a lot of time thinking", "tokens": [51417, 467, 311, 472, 295, 613, 1715, 5327, 597, 309, 311, 406, 3163, 6434, 257, 688, 295, 565, 1953, 51770], "temperature": 0.0, "avg_logprob": -0.40166964213053385, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.4571112096309662}, {"id": 532, "seek": 257260, "start": 2572.6, "end": 2574.04, "text": " about because it's not significant.", "tokens": [50364, 466, 570, 309, 311, 406, 4776, 13, 50436], "temperature": 0.0, "avg_logprob": -0.29474445099526264, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.07807173579931259}, {"id": 533, "seek": 257260, "start": 2574.04, "end": 2578.44, "text": " So I just picked whatever happened to be easiest at the time.", "tokens": [50436, 407, 286, 445, 6183, 2035, 2011, 281, 312, 12889, 412, 264, 565, 13, 50656], "temperature": 0.0, "avg_logprob": -0.29474445099526264, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.07807173579931259}, {"id": 534, "seek": 257260, "start": 2578.44, "end": 2583.56, "text": " As I said, I could have used minus 1, it's just not important.", "tokens": [50656, 1018, 286, 848, 11, 286, 727, 362, 1143, 3175, 502, 11, 309, 311, 445, 406, 1021, 13, 50912], "temperature": 0.0, "avg_logprob": -0.29474445099526264, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.07807173579931259}, {"id": 535, "seek": 257260, "start": 2583.56, "end": 2594.94, "text": " So what is important is that we have to create a rectangular matrix which we can pass to", "tokens": [50912, 407, 437, 307, 1021, 307, 300, 321, 362, 281, 1884, 257, 31167, 8141, 597, 321, 393, 1320, 281, 51481], "temperature": 0.0, "avg_logprob": -0.29474445099526264, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.07807173579931259}, {"id": 536, "seek": 257260, "start": 2594.94, "end": 2597.52, "text": " our machine learning model.", "tokens": [51481, 527, 3479, 2539, 2316, 13, 51610], "temperature": 0.0, "avg_logprob": -0.29474445099526264, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.07807173579931259}, {"id": 537, "seek": 257260, "start": 2597.52, "end": 2602.3199999999997, "text": " So quite conveniently, Keras comes with something called pad-sequences that does that for us.", "tokens": [51610, 407, 1596, 44375, 11, 591, 6985, 1487, 365, 746, 1219, 6887, 12, 11834, 2667, 300, 775, 300, 337, 505, 13, 51850], "temperature": 0.0, "avg_logprob": -0.29474445099526264, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.07807173579931259}, {"id": 538, "seek": 260232, "start": 2603.04, "end": 2608.56, "text": " It takes everything greater than this length and truncates it, and everything less than", "tokens": [50400, 467, 2516, 1203, 5044, 813, 341, 4641, 293, 504, 409, 66, 1024, 309, 11, 293, 1203, 1570, 813, 50676], "temperature": 0.0, "avg_logprob": -0.2393167720121496, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0037071884144097567}, {"id": 539, "seek": 260232, "start": 2608.56, "end": 2614.96, "text": " that length and pads it with whatever we asked for, which in this case is 0s.", "tokens": [50676, 300, 4641, 293, 19179, 309, 365, 2035, 321, 2351, 337, 11, 597, 294, 341, 1389, 307, 1958, 82, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2393167720121496, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0037071884144097567}, {"id": 540, "seek": 260232, "start": 2614.96, "end": 2621.6000000000004, "text": " So at the end of this, the shape of our training set is now a numpy array of 25,000 rows by", "tokens": [50996, 407, 412, 264, 917, 295, 341, 11, 264, 3909, 295, 527, 3097, 992, 307, 586, 257, 1031, 8200, 10225, 295, 3552, 11, 1360, 13241, 538, 51328], "temperature": 0.0, "avg_logprob": -0.2393167720121496, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0037071884144097567}, {"id": 541, "seek": 260232, "start": 2621.6000000000004, "end": 2623.52, "text": " 500 columns.", "tokens": [51328, 5923, 13766, 13, 51424], "temperature": 0.0, "avg_logprob": -0.2393167720121496, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0037071884144097567}, {"id": 542, "seek": 260232, "start": 2623.52, "end": 2631.2400000000002, "text": " And as you can see, it's padded the front with 0s, such that it has 500 words in it.", "tokens": [51424, 400, 382, 291, 393, 536, 11, 309, 311, 6887, 9207, 264, 1868, 365, 1958, 82, 11, 1270, 300, 309, 575, 5923, 2283, 294, 309, 13, 51810], "temperature": 0.0, "avg_logprob": -0.2393167720121496, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0037071884144097567}, {"id": 543, "seek": 263124, "start": 2631.24, "end": 2633.0, "text": " Other than that, it's exactly the same as before.", "tokens": [50364, 5358, 813, 300, 11, 309, 311, 2293, 264, 912, 382, 949, 13, 50452], "temperature": 0.0, "avg_logprob": -0.3110223050977363, "compression_ratio": 1.573943661971831, "no_speech_prob": 0.1992909163236618}, {"id": 544, "seek": 263124, "start": 2633.0, "end": 2639.3599999999997, "text": " And you can see Bromwell has now been not replaced with 5000, but with 4999.", "tokens": [50452, 400, 291, 393, 536, 1603, 298, 6326, 575, 586, 668, 406, 10772, 365, 23777, 11, 457, 365, 1017, 49017, 13, 50770], "temperature": 0.0, "avg_logprob": -0.3110223050977363, "compression_ratio": 1.573943661971831, "no_speech_prob": 0.1992909163236618}, {"id": 545, "seek": 263124, "start": 2639.3599999999997, "end": 2643.12, "text": " So this is our same movie review again after going through that padding process.", "tokens": [50770, 407, 341, 307, 527, 912, 3169, 3131, 797, 934, 516, 807, 300, 39562, 1399, 13, 50958], "temperature": 0.0, "avg_logprob": -0.3110223050977363, "compression_ratio": 1.573943661971831, "no_speech_prob": 0.1992909163236618}, {"id": 546, "seek": 263124, "start": 2643.12, "end": 2648.3599999999997, "text": " Question, does it matter if we pad from the left or the right?", "tokens": [50958, 14464, 11, 775, 309, 1871, 498, 321, 6887, 490, 264, 1411, 420, 264, 558, 30, 51220], "temperature": 0.0, "avg_logprob": -0.3110223050977363, "compression_ratio": 1.573943661971831, "no_speech_prob": 0.1992909163236618}, {"id": 547, "seek": 263124, "start": 2648.3599999999997, "end": 2653.7599999999998, "text": " I know that there's some reason that Keras decided to pad the front rather than the back.", "tokens": [51220, 286, 458, 300, 456, 311, 512, 1778, 300, 591, 6985, 3047, 281, 6887, 264, 1868, 2831, 813, 264, 646, 13, 51490], "temperature": 0.0, "avg_logprob": -0.3110223050977363, "compression_ratio": 1.573943661971831, "no_speech_prob": 0.1992909163236618}, {"id": 548, "seek": 263124, "start": 2653.7599999999998, "end": 2656.7999999999997, "text": " I don't recall what it is.", "tokens": [51490, 286, 500, 380, 9901, 437, 309, 307, 13, 51642], "temperature": 0.0, "avg_logprob": -0.3110223050977363, "compression_ratio": 1.573943661971831, "no_speech_prob": 0.1992909163236618}, {"id": 549, "seek": 263124, "start": 2656.7999999999997, "end": 2660.0, "text": " Since it's what it does by default, I don't worry about it.", "tokens": [51642, 4162, 309, 311, 437, 309, 775, 538, 7576, 11, 286, 500, 380, 3292, 466, 309, 13, 51802], "temperature": 0.0, "avg_logprob": -0.3110223050977363, "compression_ratio": 1.573943661971831, "no_speech_prob": 0.1992909163236618}, {"id": 550, "seek": 266000, "start": 2660.0, "end": 2663.92, "text": " I don't think it's important.", "tokens": [50364, 286, 500, 380, 519, 309, 311, 1021, 13, 50560], "temperature": 0.0, "avg_logprob": -0.26549222447850684, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.05664866045117378}, {"id": 551, "seek": 266000, "start": 2663.92, "end": 2669.92, "text": " So now that we have a rectangular matrix of numbers and we have some labels, we can use", "tokens": [50560, 407, 586, 300, 321, 362, 257, 31167, 8141, 295, 3547, 293, 321, 362, 512, 16949, 11, 321, 393, 764, 50860], "temperature": 0.0, "avg_logprob": -0.26549222447850684, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.05664866045117378}, {"id": 552, "seek": 266000, "start": 2669.92, "end": 2674.92, "text": " the exact techniques we've already learned to create a model.", "tokens": [50860, 264, 1900, 7512, 321, 600, 1217, 3264, 281, 1884, 257, 2316, 13, 51110], "temperature": 0.0, "avg_logprob": -0.26549222447850684, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.05664866045117378}, {"id": 553, "seek": 266000, "start": 2674.92, "end": 2679.0, "text": " And as per usual, we should try to create the simplest possible model we can to start", "tokens": [51110, 400, 382, 680, 7713, 11, 321, 820, 853, 281, 1884, 264, 22811, 1944, 2316, 321, 393, 281, 722, 51314], "temperature": 0.0, "avg_logprob": -0.26549222447850684, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.05664866045117378}, {"id": 554, "seek": 266000, "start": 2679.0, "end": 2680.0, "text": " with.", "tokens": [51314, 365, 13, 51364], "temperature": 0.0, "avg_logprob": -0.26549222447850684, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.05664866045117378}, {"id": 555, "seek": 266000, "start": 2680.0, "end": 2685.28, "text": " And we know that the simplest model we can is one with one hidden layer in the middle.", "tokens": [51364, 400, 321, 458, 300, 264, 22811, 2316, 321, 393, 307, 472, 365, 472, 7633, 4583, 294, 264, 2808, 13, 51628], "temperature": 0.0, "avg_logprob": -0.26549222447850684, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.05664866045117378}, {"id": 556, "seek": 266000, "start": 2685.28, "end": 2689.4, "text": " Or at least this is the simplest model that we generally think ought to be pretty useful", "tokens": [51628, 1610, 412, 1935, 341, 307, 264, 22811, 2316, 300, 321, 5101, 519, 13416, 281, 312, 1238, 4420, 51834], "temperature": 0.0, "avg_logprob": -0.26549222447850684, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.05664866045117378}, {"id": 557, "seek": 268940, "start": 2689.8, "end": 2691.8, "text": " for just about everything.", "tokens": [50384, 337, 445, 466, 1203, 13, 50484], "temperature": 0.0, "avg_logprob": -0.26218267587515026, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.05920884385704994}, {"id": 558, "seek": 268940, "start": 2691.8, "end": 2694.92, "text": " Now here is why we started with collaborative filtering.", "tokens": [50484, 823, 510, 307, 983, 321, 1409, 365, 16555, 30822, 13, 50640], "temperature": 0.0, "avg_logprob": -0.26218267587515026, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.05920884385704994}, {"id": 559, "seek": 268940, "start": 2694.92, "end": 2697.64, "text": " And that's because we're starting with an embedding.", "tokens": [50640, 400, 300, 311, 570, 321, 434, 2891, 365, 364, 12240, 3584, 13, 50776], "temperature": 0.0, "avg_logprob": -0.26218267587515026, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.05920884385704994}, {"id": 560, "seek": 268940, "start": 2697.64, "end": 2705.32, "text": " So if you think about it, our input are word IDs and we want to convert that into a vector.", "tokens": [50776, 407, 498, 291, 519, 466, 309, 11, 527, 4846, 366, 1349, 48212, 293, 321, 528, 281, 7620, 300, 666, 257, 8062, 13, 51160], "temperature": 0.0, "avg_logprob": -0.26218267587515026, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.05920884385704994}, {"id": 561, "seek": 268940, "start": 2705.32, "end": 2708.12, "text": " And that is what an embedding does.", "tokens": [51160, 400, 300, 307, 437, 364, 12240, 3584, 775, 13, 51300], "temperature": 0.0, "avg_logprob": -0.26218267587515026, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.05920884385704994}, {"id": 562, "seek": 268940, "start": 2708.12, "end": 2717.64, "text": " So again, rather than one-hot encoding this into a 5000 column long huge input thing and", "tokens": [51300, 407, 797, 11, 2831, 813, 472, 12, 12194, 43430, 341, 666, 257, 23777, 7738, 938, 2603, 4846, 551, 293, 51776], "temperature": 0.0, "avg_logprob": -0.26218267587515026, "compression_ratio": 1.5972850678733033, "no_speech_prob": 0.05920884385704994}, {"id": 563, "seek": 271764, "start": 2717.72, "end": 2725.48, "text": " then doing a matrix product, an embedding just says look up that movie ID and grab that", "tokens": [50368, 550, 884, 257, 8141, 1674, 11, 364, 12240, 3584, 445, 1619, 574, 493, 300, 3169, 7348, 293, 4444, 300, 50756], "temperature": 0.0, "avg_logprob": -0.2731244370744035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.011157638393342495}, {"id": 564, "seek": 271764, "start": 2725.48, "end": 2727.2, "text": " vector directly.", "tokens": [50756, 8062, 3838, 13, 50842], "temperature": 0.0, "avg_logprob": -0.2731244370744035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.011157638393342495}, {"id": 565, "seek": 271764, "start": 2727.2, "end": 2734.4, "text": " So it's just a computational and memory shortcut to creating a one-hot encoding followed by", "tokens": [50842, 407, 309, 311, 445, 257, 28270, 293, 4675, 24822, 281, 4084, 257, 472, 12, 12194, 43430, 6263, 538, 51202], "temperature": 0.0, "avg_logprob": -0.2731244370744035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.011157638393342495}, {"id": 566, "seek": 271764, "start": 2734.4, "end": 2736.4, "text": " a matrix product.", "tokens": [51202, 257, 8141, 1674, 13, 51302], "temperature": 0.0, "avg_logprob": -0.2731244370744035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.011157638393342495}, {"id": 567, "seek": 271764, "start": 2736.4, "end": 2743.64, "text": " So we're creating an embedding where we are going to have 5000 latent factors or 5000", "tokens": [51302, 407, 321, 434, 4084, 364, 12240, 3584, 689, 321, 366, 516, 281, 362, 23777, 48994, 6771, 420, 23777, 51664], "temperature": 0.0, "avg_logprob": -0.2731244370744035, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.011157638393342495}, {"id": 568, "seek": 274364, "start": 2743.64, "end": 2753.44, "text": " things, each one we're going to have 32 items rather than 50.", "tokens": [50364, 721, 11, 1184, 472, 321, 434, 516, 281, 362, 8858, 4754, 2831, 813, 2625, 13, 50854], "temperature": 0.0, "avg_logprob": -0.275824100413221, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.47259315848350525}, {"id": 569, "seek": 274364, "start": 2753.44, "end": 2757.96, "text": " So then we're going to flatten that, have our single dense layer, a bit of dropout,", "tokens": [50854, 407, 550, 321, 434, 516, 281, 24183, 300, 11, 362, 527, 2167, 18011, 4583, 11, 257, 857, 295, 3270, 346, 11, 51080], "temperature": 0.0, "avg_logprob": -0.275824100413221, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.47259315848350525}, {"id": 570, "seek": 274364, "start": 2757.96, "end": 2760.62, "text": " and then our output to a sigmoid.", "tokens": [51080, 293, 550, 527, 5598, 281, 257, 4556, 3280, 327, 13, 51213], "temperature": 0.0, "avg_logprob": -0.275824100413221, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.47259315848350525}, {"id": 571, "seek": 274364, "start": 2760.62, "end": 2764.56, "text": " So that's a pretty simple model.", "tokens": [51213, 407, 300, 311, 257, 1238, 2199, 2316, 13, 51410], "temperature": 0.0, "avg_logprob": -0.275824100413221, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.47259315848350525}, {"id": 572, "seek": 274364, "start": 2764.56, "end": 2768.08, "text": " You can see it's a good idea to go through and make sure you understand why all these", "tokens": [51410, 509, 393, 536, 309, 311, 257, 665, 1558, 281, 352, 807, 293, 652, 988, 291, 1223, 983, 439, 613, 51586], "temperature": 0.0, "avg_logprob": -0.275824100413221, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.47259315848350525}, {"id": 573, "seek": 274364, "start": 2768.08, "end": 2770.04, "text": " parameter counts are what they are.", "tokens": [51586, 13075, 14893, 366, 437, 436, 366, 13, 51684], "temperature": 0.0, "avg_logprob": -0.275824100413221, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.47259315848350525}, {"id": 574, "seek": 277004, "start": 2770.04, "end": 2774.08, "text": " That's something you can do during the week and double-check that you're comfortable with", "tokens": [50364, 663, 311, 746, 291, 393, 360, 1830, 264, 1243, 293, 3834, 12, 15723, 300, 291, 434, 4619, 365, 50566], "temperature": 0.0, "avg_logprob": -0.29228728925678094, "compression_ratio": 1.4157303370786516, "no_speech_prob": 0.25979742407798767}, {"id": 575, "seek": 277004, "start": 2774.08, "end": 2775.08, "text": " all of those.", "tokens": [50566, 439, 295, 729, 13, 50616], "temperature": 0.0, "avg_logprob": -0.29228728925678094, "compression_ratio": 1.4157303370786516, "no_speech_prob": 0.25979742407798767}, {"id": 576, "seek": 277004, "start": 2775.08, "end": 2781.24, "text": " So this is the size of each of the weight matrices at each point.", "tokens": [50616, 407, 341, 307, 264, 2744, 295, 1184, 295, 264, 3364, 32284, 412, 1184, 935, 13, 50924], "temperature": 0.0, "avg_logprob": -0.29228728925678094, "compression_ratio": 1.4157303370786516, "no_speech_prob": 0.25979742407798767}, {"id": 577, "seek": 277004, "start": 2781.24, "end": 2783.24, "text": " And we can fit it.", "tokens": [50924, 400, 321, 393, 3318, 309, 13, 51024], "temperature": 0.0, "avg_logprob": -0.29228728925678094, "compression_ratio": 1.4157303370786516, "no_speech_prob": 0.25979742407798767}, {"id": 578, "seek": 277004, "start": 2783.24, "end": 2797.02, "text": " And after 2 epochs, we have 88% accuracy on the validation set.", "tokens": [51024, 400, 934, 568, 30992, 28346, 11, 321, 362, 24587, 4, 14170, 322, 264, 24071, 992, 13, 51713], "temperature": 0.0, "avg_logprob": -0.29228728925678094, "compression_ratio": 1.4157303370786516, "no_speech_prob": 0.25979742407798767}, {"id": 579, "seek": 279702, "start": 2797.02, "end": 2812.38, "text": " And so let's just compare that to Stanford where they had 88.3 and we have 88.04.", "tokens": [50364, 400, 370, 718, 311, 445, 6794, 300, 281, 20374, 689, 436, 632, 24587, 13, 18, 293, 321, 362, 24587, 13, 14565, 13, 51132], "temperature": 0.0, "avg_logprob": -0.4098544614068393, "compression_ratio": 1.2444444444444445, "no_speech_prob": 0.03676794096827507}, {"id": 580, "seek": 279702, "start": 2812.38, "end": 2815.1, "text": " So we're not yet there, but we're well on the right track.", "tokens": [51132, 407, 321, 434, 406, 1939, 456, 11, 457, 321, 434, 731, 322, 264, 558, 2837, 13, 51268], "temperature": 0.0, "avg_logprob": -0.4098544614068393, "compression_ratio": 1.2444444444444445, "no_speech_prob": 0.03676794096827507}, {"id": 581, "seek": 279702, "start": 2815.1, "end": 2816.1, "text": " Question 2.", "tokens": [51268, 14464, 568, 13, 51318], "temperature": 0.0, "avg_logprob": -0.4098544614068393, "compression_ratio": 1.2444444444444445, "no_speech_prob": 0.03676794096827507}, {"id": 582, "seek": 279702, "start": 2816.1, "end": 2817.1, "text": " Why 32?", "tokens": [51318, 1545, 8858, 30, 51368], "temperature": 0.0, "avg_logprob": -0.4098544614068393, "compression_ratio": 1.2444444444444445, "no_speech_prob": 0.03676794096827507}, {"id": 583, "seek": 279702, "start": 2817.1, "end": 2818.1, "text": " Answer.", "tokens": [51368, 24545, 13, 51418], "temperature": 0.0, "avg_logprob": -0.4098544614068393, "compression_ratio": 1.2444444444444445, "no_speech_prob": 0.03676794096827507}, {"id": 584, "seek": 281810, "start": 2818.1, "end": 2826.9, "text": " This is always the question about why have x number of filters in your convolutional", "tokens": [50364, 639, 307, 1009, 264, 1168, 466, 983, 362, 2031, 1230, 295, 15995, 294, 428, 45216, 304, 50804], "temperature": 0.0, "avg_logprob": -0.30845396859305246, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.0726320892572403}, {"id": 585, "seek": 281810, "start": 2826.9, "end": 2831.3399999999997, "text": " layer or why have x number of outputs in your dense layer.", "tokens": [50804, 4583, 420, 983, 362, 2031, 1230, 295, 23930, 294, 428, 18011, 4583, 13, 51026], "temperature": 0.0, "avg_logprob": -0.30845396859305246, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.0726320892572403}, {"id": 586, "seek": 281810, "start": 2831.3399999999997, "end": 2837.66, "text": " It's just a case of trying things and seeing what works and also getting some intuition", "tokens": [51026, 467, 311, 445, 257, 1389, 295, 1382, 721, 293, 2577, 437, 1985, 293, 611, 1242, 512, 24002, 51342], "temperature": 0.0, "avg_logprob": -0.30845396859305246, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.0726320892572403}, {"id": 587, "seek": 281810, "start": 2837.66, "end": 2839.46, "text": " by looking at other models.", "tokens": [51342, 538, 1237, 412, 661, 5245, 13, 51432], "temperature": 0.0, "avg_logprob": -0.30845396859305246, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.0726320892572403}, {"id": 588, "seek": 281810, "start": 2839.46, "end": 2842.7799999999997, "text": " In this case, I think 32 was the first I tried.", "tokens": [51432, 682, 341, 1389, 11, 286, 519, 8858, 390, 264, 700, 286, 3031, 13, 51598], "temperature": 0.0, "avg_logprob": -0.30845396859305246, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.0726320892572403}, {"id": 589, "seek": 284278, "start": 2842.78, "end": 2849.46, "text": " I kind of felt like from my understanding of really big embedding models, which we'll", "tokens": [50364, 286, 733, 295, 2762, 411, 490, 452, 3701, 295, 534, 955, 12240, 3584, 5245, 11, 597, 321, 603, 50698], "temperature": 0.0, "avg_logprob": -0.278271972492177, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.053401608020067215}, {"id": 590, "seek": 284278, "start": 2849.46, "end": 2856.5, "text": " learn about shortly, even 50 dimensions is enough to capture vocabularies of size 100,000", "tokens": [50698, 1466, 466, 13392, 11, 754, 2625, 12819, 307, 1547, 281, 7983, 2329, 455, 1040, 530, 295, 2744, 2319, 11, 1360, 51050], "temperature": 0.0, "avg_logprob": -0.278271972492177, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.053401608020067215}, {"id": 591, "seek": 284278, "start": 2856.5, "end": 2857.5800000000004, "text": " or more.", "tokens": [51050, 420, 544, 13, 51104], "temperature": 0.0, "avg_logprob": -0.278271972492177, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.053401608020067215}, {"id": 592, "seek": 284278, "start": 2857.5800000000004, "end": 2862.34, "text": " So I felt like 32 was likely to be more than enough to capture a vocabulary of size 5,000.", "tokens": [51104, 407, 286, 2762, 411, 8858, 390, 3700, 281, 312, 544, 813, 1547, 281, 7983, 257, 19864, 295, 2744, 1025, 11, 1360, 13, 51342], "temperature": 0.0, "avg_logprob": -0.278271972492177, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.053401608020067215}, {"id": 593, "seek": 284278, "start": 2862.34, "end": 2866.94, "text": " I tried it and I got a pretty good result, so I basically left it there.", "tokens": [51342, 286, 3031, 309, 293, 286, 658, 257, 1238, 665, 1874, 11, 370, 286, 1936, 1411, 309, 456, 13, 51572], "temperature": 0.0, "avg_logprob": -0.278271972492177, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.053401608020067215}, {"id": 594, "seek": 286694, "start": 2866.94, "end": 2872.46, "text": " If at some point I discovered that I wasn't getting great results, I would try increasing", "tokens": [50364, 759, 412, 512, 935, 286, 6941, 300, 286, 2067, 380, 1242, 869, 3542, 11, 286, 576, 853, 5662, 50640], "temperature": 0.0, "avg_logprob": -0.2861109415690104, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.1666693389415741}, {"id": 595, "seek": 286694, "start": 2872.46, "end": 2873.46, "text": " it.", "tokens": [50640, 309, 13, 50690], "temperature": 0.0, "avg_logprob": -0.2861109415690104, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.1666693389415741}, {"id": 596, "seek": 286694, "start": 2873.46, "end": 2874.46, "text": " Question 3.", "tokens": [50690, 14464, 805, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2861109415690104, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.1666693389415741}, {"id": 597, "seek": 286694, "start": 2874.46, "end": 2878.3, "text": " Why sigmoid in the final layer?", "tokens": [50740, 1545, 4556, 3280, 327, 294, 264, 2572, 4583, 30, 50932], "temperature": 0.0, "avg_logprob": -0.2861109415690104, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.1666693389415741}, {"id": 598, "seek": 286694, "start": 2878.3, "end": 2884.26, "text": " You can always use a softmax instead of a sigmoid, it just means that you would have", "tokens": [50932, 509, 393, 1009, 764, 257, 2787, 41167, 2602, 295, 257, 4556, 3280, 327, 11, 309, 445, 1355, 300, 291, 576, 362, 51230], "temperature": 0.0, "avg_logprob": -0.2861109415690104, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.1666693389415741}, {"id": 599, "seek": 286694, "start": 2884.26, "end": 2885.9, "text": " to change your labels.", "tokens": [51230, 281, 1319, 428, 16949, 13, 51312], "temperature": 0.0, "avg_logprob": -0.2861109415690104, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.1666693389415741}, {"id": 600, "seek": 286694, "start": 2885.9, "end": 2895.54, "text": " Remember our labels were just ones or zeros, they were just a single column.", "tokens": [51312, 5459, 527, 16949, 645, 445, 2306, 420, 35193, 11, 436, 645, 445, 257, 2167, 7738, 13, 51794], "temperature": 0.0, "avg_logprob": -0.2861109415690104, "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.1666693389415741}, {"id": 601, "seek": 289554, "start": 2895.54, "end": 2898.86, "text": " If I wanted to use a softmax, I would have to create two columns, it wouldn't just be", "tokens": [50364, 759, 286, 1415, 281, 764, 257, 2787, 41167, 11, 286, 576, 362, 281, 1884, 732, 13766, 11, 309, 2759, 380, 445, 312, 50530], "temperature": 0.0, "avg_logprob": -0.26274425752701297, "compression_ratio": 1.7620967741935485, "no_speech_prob": 0.022975800558924675}, {"id": 602, "seek": 289554, "start": 2898.86, "end": 2905.38, "text": " one, it would be 1, 0, 1, 0, 1, 0.", "tokens": [50530, 472, 11, 309, 576, 312, 502, 11, 1958, 11, 502, 11, 1958, 11, 502, 11, 1958, 13, 50856], "temperature": 0.0, "avg_logprob": -0.26274425752701297, "compression_ratio": 1.7620967741935485, "no_speech_prob": 0.022975800558924675}, {"id": 603, "seek": 289554, "start": 2905.38, "end": 2910.42, "text": " In the past, I've generally stuck to using softmax and then categorical cross-entropy", "tokens": [50856, 682, 264, 1791, 11, 286, 600, 5101, 5541, 281, 1228, 2787, 41167, 293, 550, 19250, 804, 3278, 12, 317, 27514, 51108], "temperature": 0.0, "avg_logprob": -0.26274425752701297, "compression_ratio": 1.7620967741935485, "no_speech_prob": 0.022975800558924675}, {"id": 604, "seek": 289554, "start": 2910.42, "end": 2915.12, "text": " loss just to be consistent, because then regardless of whether you have two classes or more than", "tokens": [51108, 4470, 445, 281, 312, 8398, 11, 570, 550, 10060, 295, 1968, 291, 362, 732, 5359, 420, 544, 813, 51343], "temperature": 0.0, "avg_logprob": -0.26274425752701297, "compression_ratio": 1.7620967741935485, "no_speech_prob": 0.022975800558924675}, {"id": 605, "seek": 289554, "start": 2915.12, "end": 2918.62, "text": " two classes, you can always do the same thing.", "tokens": [51343, 732, 5359, 11, 291, 393, 1009, 360, 264, 912, 551, 13, 51518], "temperature": 0.0, "avg_logprob": -0.26274425752701297, "compression_ratio": 1.7620967741935485, "no_speech_prob": 0.022975800558924675}, {"id": 606, "seek": 289554, "start": 2918.62, "end": 2923.62, "text": " In this case, I thought I want to show the other way you can do this, which is to just", "tokens": [51518, 682, 341, 1389, 11, 286, 1194, 286, 528, 281, 855, 264, 661, 636, 291, 393, 360, 341, 11, 597, 307, 281, 445, 51768], "temperature": 0.0, "avg_logprob": -0.26274425752701297, "compression_ratio": 1.7620967741935485, "no_speech_prob": 0.022975800558924675}, {"id": 607, "seek": 292362, "start": 2923.7, "end": 2925.38, "text": " have a single column output.", "tokens": [50368, 362, 257, 2167, 7738, 5598, 13, 50452], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 608, "seek": 292362, "start": 2925.38, "end": 2929.98, "text": " And remember a sigmoid is exactly the same thing as a softmax if you just have a binary", "tokens": [50452, 400, 1604, 257, 4556, 3280, 327, 307, 2293, 264, 912, 551, 382, 257, 2787, 41167, 498, 291, 445, 362, 257, 17434, 50682], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 609, "seek": 292362, "start": 2929.98, "end": 2930.98, "text": " output.", "tokens": [50682, 5598, 13, 50732], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 610, "seek": 292362, "start": 2930.98, "end": 2936.98, "text": " So rather than using categorical cross-entropy, we use binary cross-entropy.", "tokens": [50732, 407, 2831, 813, 1228, 19250, 804, 3278, 12, 317, 27514, 11, 321, 764, 17434, 3278, 12, 317, 27514, 13, 51032], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 611, "seek": 292362, "start": 2936.98, "end": 2941.22, "text": " Again, it's exactly the same thing, it just means I didn't have to worry about one-hot", "tokens": [51032, 3764, 11, 309, 311, 2293, 264, 912, 551, 11, 309, 445, 1355, 286, 994, 380, 362, 281, 3292, 466, 472, 12, 12194, 51244], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 612, "seek": 292362, "start": 2941.22, "end": 2944.18, "text": " encoding the output because it's just a binary output.", "tokens": [51244, 43430, 264, 5598, 570, 309, 311, 445, 257, 17434, 5598, 13, 51392], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 613, "seek": 292362, "start": 2944.18, "end": 2945.18, "text": " Question 4.", "tokens": [51392, 14464, 1017, 13, 51442], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 614, "seek": 292362, "start": 2945.18, "end": 2951.1, "text": " Do we know what the inter-rater agreement is for this data set?", "tokens": [51442, 1144, 321, 458, 437, 264, 728, 12, 81, 771, 8106, 307, 337, 341, 1412, 992, 30, 51738], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 615, "seek": 292362, "start": 2951.1, "end": 2952.5, "text": " No we don't.", "tokens": [51738, 883, 321, 500, 380, 13, 51808], "temperature": 0.0, "avg_logprob": -0.3741045227050781, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.4648806154727936}, {"id": 616, "seek": 295250, "start": 2952.5, "end": 2956.5, "text": " It's not something I have looked at.", "tokens": [50364, 467, 311, 406, 746, 286, 362, 2956, 412, 13, 50564], "temperature": 0.0, "avg_logprob": -0.30493449273510514, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.06754201650619507}, {"id": 617, "seek": 295250, "start": 2956.5, "end": 2961.7, "text": " The important thing as far as I'm concerned is what is the benchmark that the Stanford", "tokens": [50564, 440, 1021, 551, 382, 1400, 382, 286, 478, 5922, 307, 437, 307, 264, 18927, 300, 264, 20374, 50824], "temperature": 0.0, "avg_logprob": -0.30493449273510514, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.06754201650619507}, {"id": 618, "seek": 295250, "start": 2961.7, "end": 2964.98, "text": " people got and they compared it to a range of other previous benchmarks and they found", "tokens": [50824, 561, 658, 293, 436, 5347, 309, 281, 257, 3613, 295, 661, 3894, 43751, 293, 436, 1352, 50988], "temperature": 0.0, "avg_logprob": -0.30493449273510514, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.06754201650619507}, {"id": 619, "seek": 295250, "start": 2964.98, "end": 2966.34, "text": " that their technique was the best.", "tokens": [50988, 300, 641, 6532, 390, 264, 1151, 13, 51056], "temperature": 0.0, "avg_logprob": -0.30493449273510514, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.06754201650619507}, {"id": 620, "seek": 295250, "start": 2966.34, "end": 2969.1, "text": " So that's my goal here.", "tokens": [51056, 407, 300, 311, 452, 3387, 510, 13, 51194], "temperature": 0.0, "avg_logprob": -0.30493449273510514, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.06754201650619507}, {"id": 621, "seek": 295250, "start": 2969.1, "end": 2972.46, "text": " I'm sure there have been other techniques that have come out since that are probably", "tokens": [51194, 286, 478, 988, 456, 362, 668, 661, 7512, 300, 362, 808, 484, 1670, 300, 366, 1391, 51362], "temperature": 0.0, "avg_logprob": -0.30493449273510514, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.06754201650619507}, {"id": 622, "seek": 295250, "start": 2972.46, "end": 2981.3, "text": " better, but I haven't seen them in any papers yet, so this is my target.", "tokens": [51362, 1101, 11, 457, 286, 2378, 380, 1612, 552, 294, 604, 10577, 1939, 11, 370, 341, 307, 452, 3779, 13, 51804], "temperature": 0.0, "avg_logprob": -0.30493449273510514, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.06754201650619507}, {"id": 623, "seek": 298130, "start": 2981.3, "end": 2990.0600000000004, "text": " You can see that we can in one second of training get an accuracy which is pretty competitive", "tokens": [50364, 509, 393, 536, 300, 321, 393, 294, 472, 1150, 295, 3097, 483, 364, 14170, 597, 307, 1238, 10043, 50802], "temperature": 0.0, "avg_logprob": -0.23925803810037594, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.08631869405508041}, {"id": 624, "seek": 298130, "start": 2990.0600000000004, "end": 2991.78, "text": " and it's just a simple neural net.", "tokens": [50802, 293, 309, 311, 445, 257, 2199, 18161, 2533, 13, 50888], "temperature": 0.0, "avg_logprob": -0.23925803810037594, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.08631869405508041}, {"id": 625, "seek": 298130, "start": 2991.78, "end": 2997.84, "text": " So hopefully you're starting to get a sense that a neural net with one hidden layer is", "tokens": [50888, 407, 4696, 291, 434, 2891, 281, 483, 257, 2020, 300, 257, 18161, 2533, 365, 472, 7633, 4583, 307, 51191], "temperature": 0.0, "avg_logprob": -0.23925803810037594, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.08631869405508041}, {"id": 626, "seek": 298130, "start": 2997.84, "end": 3000.5800000000004, "text": " a great starting point for nearly everything.", "tokens": [51191, 257, 869, 2891, 935, 337, 6217, 1203, 13, 51328], "temperature": 0.0, "avg_logprob": -0.23925803810037594, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.08631869405508041}, {"id": 627, "seek": 298130, "start": 3000.5800000000004, "end": 3005.82, "text": " You now know how to create a pretty good sentiment analysis model and before today you didn't,", "tokens": [51328, 509, 586, 458, 577, 281, 1884, 257, 1238, 665, 16149, 5215, 2316, 293, 949, 965, 291, 994, 380, 11, 51590], "temperature": 0.0, "avg_logprob": -0.23925803810037594, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.08631869405508041}, {"id": 628, "seek": 298130, "start": 3005.82, "end": 3006.82, "text": " so that's a good step.", "tokens": [51590, 370, 300, 311, 257, 665, 1823, 13, 51640], "temperature": 0.0, "avg_logprob": -0.23925803810037594, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.08631869405508041}, {"id": 629, "seek": 300682, "start": 3007.82, "end": 3008.82, "text": " Question 2.", "tokens": [50414, 14464, 568, 13, 50464], "temperature": 0.0, "avg_logprob": -0.41754051208496096, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.13659602403640747}, {"id": 630, "seek": 300682, "start": 3008.82, "end": 3013.82, "text": " Just to confirm, whenever we use binary cross-entropy, we use sigmoid for the final layer?", "tokens": [50464, 1449, 281, 9064, 11, 5699, 321, 764, 17434, 3278, 12, 317, 27514, 11, 321, 764, 4556, 3280, 327, 337, 264, 2572, 4583, 30, 50714], "temperature": 0.0, "avg_logprob": -0.41754051208496096, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.13659602403640747}, {"id": 631, "seek": 300682, "start": 3013.82, "end": 3014.82, "text": " Yes.", "tokens": [50714, 1079, 13, 50764], "temperature": 0.0, "avg_logprob": -0.41754051208496096, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.13659602403640747}, {"id": 632, "seek": 300682, "start": 3014.82, "end": 3016.38, "text": " And could you explain embedding again?", "tokens": [50764, 400, 727, 291, 2903, 12240, 3584, 797, 30, 50842], "temperature": 0.0, "avg_logprob": -0.41754051208496096, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.13659602403640747}, {"id": 633, "seek": 300682, "start": 3016.38, "end": 3024.2200000000003, "text": " What is the actual input into the dense layer for Word 2.3.4.5?", "tokens": [50842, 708, 307, 264, 3539, 4846, 666, 264, 18011, 4583, 337, 8725, 568, 13, 18, 13, 19, 13, 20, 30, 51234], "temperature": 0.0, "avg_logprob": -0.41754051208496096, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.13659602403640747}, {"id": 634, "seek": 300682, "start": 3024.2200000000003, "end": 3026.38, "text": " This is an embedding.", "tokens": [51234, 639, 307, 364, 12240, 3584, 13, 51342], "temperature": 0.0, "avg_logprob": -0.41754051208496096, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.13659602403640747}, {"id": 635, "seek": 302638, "start": 3026.38, "end": 3030.82, "text": " So an embedding is something that I think would be particularly helpful if we go back", "tokens": [50364, 407, 364, 12240, 3584, 307, 746, 300, 286, 519, 576, 312, 4098, 4961, 498, 321, 352, 646, 50586], "temperature": 0.0, "avg_logprob": -0.33169448158957743, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.012241072952747345}, {"id": 636, "seek": 302638, "start": 3030.82, "end": 3041.58, "text": " to our MovieLens recommendation dataset.", "tokens": [50586, 281, 527, 28766, 43, 694, 11879, 28872, 13, 51124], "temperature": 0.0, "avg_logprob": -0.33169448158957743, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.012241072952747345}, {"id": 637, "seek": 302638, "start": 3041.58, "end": 3049.34, "text": " And remember that the actual data coming in does not look like this, but it looks like", "tokens": [51124, 400, 1604, 300, 264, 3539, 1412, 1348, 294, 775, 406, 574, 411, 341, 11, 457, 309, 1542, 411, 51512], "temperature": 0.0, "avg_logprob": -0.33169448158957743, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.012241072952747345}, {"id": 638, "seek": 302638, "start": 3049.34, "end": 3051.58, "text": " this.", "tokens": [51512, 341, 13, 51624], "temperature": 0.0, "avg_logprob": -0.33169448158957743, "compression_ratio": 1.4407894736842106, "no_speech_prob": 0.012241072952747345}, {"id": 639, "seek": 305158, "start": 3051.58, "end": 3057.38, "text": " So when we then come along and say what do we predict the rating would be for user ID", "tokens": [50364, 407, 562, 321, 550, 808, 2051, 293, 584, 437, 360, 321, 6069, 264, 10990, 576, 312, 337, 4195, 7348, 50654], "temperature": 0.0, "avg_logprob": -0.338443866995878, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.037325162440538406}, {"id": 640, "seek": 305158, "start": 3057.38, "end": 3068.62, "text": " 1 for movie ID 1172, we actually have to go through our list of movie IDs and find movie", "tokens": [50654, 502, 337, 3169, 7348, 2975, 28890, 11, 321, 767, 362, 281, 352, 807, 527, 1329, 295, 3169, 48212, 293, 915, 3169, 51216], "temperature": 0.0, "avg_logprob": -0.338443866995878, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.037325162440538406}, {"id": 641, "seek": 305158, "start": 3068.62, "end": 3075.46, "text": " ID number 31, and then having found 31, look up its latent factor.", "tokens": [51216, 7348, 1230, 10353, 11, 293, 550, 1419, 1352, 10353, 11, 574, 493, 1080, 48994, 5952, 13, 51558], "temperature": 0.0, "avg_logprob": -0.338443866995878, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.037325162440538406}, {"id": 642, "seek": 305158, "start": 3075.46, "end": 3079.98, "text": " And then we have to do the same thing for user ID number 1 and find its latent factor,", "tokens": [51558, 400, 550, 321, 362, 281, 360, 264, 912, 551, 337, 4195, 7348, 1230, 502, 293, 915, 1080, 48994, 5952, 11, 51784], "temperature": 0.0, "avg_logprob": -0.338443866995878, "compression_ratio": 1.690721649484536, "no_speech_prob": 0.037325162440538406}, {"id": 643, "seek": 307998, "start": 3079.98, "end": 3081.94, "text": " and then we have to multiply the two together.", "tokens": [50364, 293, 550, 321, 362, 281, 12972, 264, 732, 1214, 13, 50462], "temperature": 0.0, "avg_logprob": -0.22069725782974906, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.02297704480588436}, {"id": 644, "seek": 307998, "start": 3081.94, "end": 3087.66, "text": " So that step of taking an ID and finding it in a list and returning the vector that it's", "tokens": [50462, 407, 300, 1823, 295, 1940, 364, 7348, 293, 5006, 309, 294, 257, 1329, 293, 12678, 264, 8062, 300, 309, 311, 50748], "temperature": 0.0, "avg_logprob": -0.22069725782974906, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.02297704480588436}, {"id": 645, "seek": 307998, "start": 3087.66, "end": 3091.3, "text": " attached to, that is what an embedding is.", "tokens": [50748, 8570, 281, 11, 300, 307, 437, 364, 12240, 3584, 307, 13, 50930], "temperature": 0.0, "avg_logprob": -0.22069725782974906, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.02297704480588436}, {"id": 646, "seek": 307998, "start": 3091.3, "end": 3100.2, "text": " So an embedding returns a vector which is of length, in this case, 32.", "tokens": [50930, 407, 364, 12240, 3584, 11247, 257, 8062, 597, 307, 295, 4641, 11, 294, 341, 1389, 11, 8858, 13, 51375], "temperature": 0.0, "avg_logprob": -0.22069725782974906, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.02297704480588436}, {"id": 647, "seek": 307998, "start": 3100.2, "end": 3106.86, "text": " So the output of this is that for each, the none always means your mini-batch size.", "tokens": [51375, 407, 264, 5598, 295, 341, 307, 300, 337, 1184, 11, 264, 6022, 1009, 1355, 428, 8382, 12, 65, 852, 2744, 13, 51708], "temperature": 0.0, "avg_logprob": -0.22069725782974906, "compression_ratio": 1.6009615384615385, "no_speech_prob": 0.02297704480588436}, {"id": 648, "seek": 310686, "start": 3106.86, "end": 3117.1, "text": " So for each movie review, for each of the 500 words in that sequence, you're getting", "tokens": [50364, 407, 337, 1184, 3169, 3131, 11, 337, 1184, 295, 264, 5923, 2283, 294, 300, 8310, 11, 291, 434, 1242, 50876], "temperature": 0.0, "avg_logprob": -0.26316376674322434, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.05261621251702309}, {"id": 649, "seek": 310686, "start": 3117.1, "end": 3119.54, "text": " a 32-element vector.", "tokens": [50876, 257, 8858, 12, 68, 3054, 8062, 13, 50998], "temperature": 0.0, "avg_logprob": -0.26316376674322434, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.05261621251702309}, {"id": 650, "seek": 310686, "start": 3119.54, "end": 3127.54, "text": " And so therefore you have a mini-batch size of 500x32 tensor coming out of this layer.", "tokens": [50998, 400, 370, 4412, 291, 362, 257, 8382, 12, 65, 852, 2744, 295, 5923, 87, 11440, 40863, 1348, 484, 295, 341, 4583, 13, 51398], "temperature": 0.0, "avg_logprob": -0.26316376674322434, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.05261621251702309}, {"id": 651, "seek": 310686, "start": 3127.54, "end": 3136.08, "text": " That gets flattened, so 500x32 is 16,000, and that is the input into your first dense", "tokens": [51398, 663, 2170, 24183, 292, 11, 370, 5923, 87, 11440, 307, 3165, 11, 1360, 11, 293, 300, 307, 264, 4846, 666, 428, 700, 18011, 51825], "temperature": 0.0, "avg_logprob": -0.26316376674322434, "compression_ratio": 1.4404145077720207, "no_speech_prob": 0.05261621251702309}, {"id": 652, "seek": 313608, "start": 3136.08, "end": 3137.08, "text": " layer.", "tokens": [50364, 4583, 13, 50414], "temperature": 0.0, "avg_logprob": -0.44211269247120827, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.25680842995643616}, {"id": 653, "seek": 313608, "start": 3137.08, "end": 3138.08, "text": " Question For Josie", "tokens": [50414, 14464, 220, 12587, 220, 41, 329, 414, 50464], "temperature": 0.0, "avg_logprob": -0.44211269247120827, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.25680842995643616}, {"id": 654, "seek": 313608, "start": 3138.08, "end": 3144.96, "text": " I also think it might be helpful to show that for a review, instead of having that in words,", "tokens": [50464, 286, 611, 519, 309, 1062, 312, 4961, 281, 855, 300, 337, 257, 3131, 11, 2602, 295, 1419, 300, 294, 2283, 11, 50808], "temperature": 0.0, "avg_logprob": -0.44211269247120827, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.25680842995643616}, {"id": 655, "seek": 313608, "start": 3144.96, "end": 3152.2, "text": " that's being entered as a sequence of numbers where the number is how that word rangs.", "tokens": [50808, 300, 311, 885, 9065, 382, 257, 8310, 295, 3547, 689, 264, 1230, 307, 577, 300, 1349, 5872, 21559, 13, 51170], "temperature": 0.0, "avg_logprob": -0.44211269247120827, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.25680842995643616}, {"id": 656, "seek": 313608, "start": 3152.2, "end": 3160.2, "text": " So we look at this first review, and remember this has now been truncated to 4999, and it's", "tokens": [51170, 407, 321, 574, 412, 341, 700, 3131, 11, 293, 1604, 341, 575, 586, 668, 504, 409, 66, 770, 281, 16513, 8494, 11, 293, 309, 311, 51570], "temperature": 0.0, "avg_logprob": -0.44211269247120827, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.25680842995643616}, {"id": 657, "seek": 313608, "start": 3160.2, "end": 3165.52, "text": " still 309, so it's going to take 309 and it's going to look up the 309th vector in the embedding", "tokens": [51570, 920, 2217, 24, 11, 370, 309, 311, 516, 281, 747, 2217, 24, 293, 309, 311, 516, 281, 574, 493, 264, 2217, 24, 392, 8062, 294, 264, 12240, 3584, 51836], "temperature": 0.0, "avg_logprob": -0.44211269247120827, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.25680842995643616}, {"id": 658, "seek": 316552, "start": 3165.96, "end": 3173.92, "text": " and it's going to return it and it's going to concatenate it to create this tensor.", "tokens": [50386, 293, 309, 311, 516, 281, 2736, 309, 293, 309, 311, 516, 281, 1588, 7186, 473, 309, 281, 1884, 341, 40863, 13, 50784], "temperature": 0.0, "avg_logprob": -0.42968519907149055, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412442333996296}, {"id": 659, "seek": 316552, "start": 3173.92, "end": 3174.92, "text": " So that's what embedding is.", "tokens": [50784, 407, 300, 311, 437, 12240, 3584, 307, 13, 50834], "temperature": 0.0, "avg_logprob": -0.42968519907149055, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412442333996296}, {"id": 660, "seek": 316552, "start": 3174.92, "end": 3181.2, "text": " An embedding is a shortcut to a one-hot encoding followed by a matrix product.", "tokens": [50834, 1107, 12240, 3584, 307, 257, 24822, 281, 257, 472, 12, 12194, 43430, 6263, 538, 257, 8141, 1674, 13, 51148], "temperature": 0.0, "avg_logprob": -0.42968519907149055, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412442333996296}, {"id": 661, "seek": 316552, "start": 3181.2, "end": 3184.2, "text": " Question For Josie", "tokens": [51148, 14464, 220, 12587, 508, 329, 414, 51298], "temperature": 0.0, "avg_logprob": -0.42968519907149055, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.009412442333996296}, {"id": 662, "seek": 318420, "start": 3184.2, "end": 3209.4399999999996, "text": " The interesting thing about NLP is that we're trying to capture something which is very", "tokens": [50364, 440, 1880, 551, 466, 426, 45196, 307, 300, 321, 434, 1382, 281, 7983, 746, 597, 307, 588, 51626], "temperature": 0.0, "avg_logprob": -0.5203669357299805, "compression_ratio": 1.1123595505617978, "no_speech_prob": 0.36282768845558167}, {"id": 663, "seek": 318420, "start": 3209.4399999999996, "end": 3211.58, "text": " subjective.", "tokens": [51626, 25972, 13, 51733], "temperature": 0.0, "avg_logprob": -0.5203669357299805, "compression_ratio": 1.1123595505617978, "no_speech_prob": 0.36282768845558167}, {"id": 664, "seek": 321158, "start": 3211.7, "end": 3217.7799999999997, "text": " In this case, you would have to read the original paper to find out how they got these particular", "tokens": [50370, 682, 341, 1389, 11, 291, 576, 362, 281, 1401, 264, 3380, 3035, 281, 915, 484, 577, 436, 658, 613, 1729, 50674], "temperature": 0.0, "avg_logprob": -0.33049642122708833, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.017440838739275932}, {"id": 665, "seek": 321158, "start": 3217.7799999999997, "end": 3220.9, "text": " labels.", "tokens": [50674, 16949, 13, 50830], "temperature": 0.0, "avg_logprob": -0.33049642122708833, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.017440838739275932}, {"id": 666, "seek": 321158, "start": 3220.9, "end": 3226.38, "text": " The way that people tend to get labels is either, in this case it's the IMDB dataset.", "tokens": [50830, 440, 636, 300, 561, 3928, 281, 483, 16949, 307, 2139, 11, 294, 341, 1389, 309, 311, 264, 21463, 27735, 28872, 13, 51104], "temperature": 0.0, "avg_logprob": -0.33049642122708833, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.017440838739275932}, {"id": 667, "seek": 321158, "start": 3226.38, "end": 3231.58, "text": " IMDB has ratings, so you can just say anything higher than 8 is very positive and anything", "tokens": [51104, 21463, 27735, 575, 24603, 11, 370, 291, 393, 445, 584, 1340, 2946, 813, 1649, 307, 588, 3353, 293, 1340, 51364], "temperature": 0.0, "avg_logprob": -0.33049642122708833, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.017440838739275932}, {"id": 668, "seek": 321158, "start": 3231.58, "end": 3237.1, "text": " lower than 2 is very negative, and we'll throw away everything in the middle.", "tokens": [51364, 3126, 813, 568, 307, 588, 3671, 11, 293, 321, 603, 3507, 1314, 1203, 294, 264, 2808, 13, 51640], "temperature": 0.0, "avg_logprob": -0.33049642122708833, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.017440838739275932}, {"id": 669, "seek": 323710, "start": 3237.1, "end": 3242.42, "text": " The other way that people tend to label academic datasets is to send it off to Amazon Mechanical", "tokens": [50364, 440, 661, 636, 300, 561, 3928, 281, 7645, 7778, 42856, 307, 281, 2845, 309, 766, 281, 6795, 30175, 804, 50630], "temperature": 0.0, "avg_logprob": -0.361337559563773, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.03514038398861885}, {"id": 670, "seek": 323710, "start": 3242.42, "end": 3247.16, "text": " Turk and pay them a few cents to label each thing.", "tokens": [50630, 15714, 293, 1689, 552, 257, 1326, 14941, 281, 7645, 1184, 551, 13, 50867], "temperature": 0.0, "avg_logprob": -0.361337559563773, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.03514038398861885}, {"id": 671, "seek": 323710, "start": 3247.16, "end": 3250.94, "text": " So that's the kind of ways that you can label stuff.", "tokens": [50867, 407, 300, 311, 264, 733, 295, 2098, 300, 291, 393, 7645, 1507, 13, 51056], "temperature": 0.0, "avg_logprob": -0.361337559563773, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.03514038398861885}, {"id": 672, "seek": 323710, "start": 3250.94, "end": 3256.3399999999997, "text": " Question And there are places where people don't just use Mechanical Turk, but they specifically", "tokens": [51056, 14464, 220, 5289, 456, 366, 3190, 689, 561, 500, 380, 445, 764, 30175, 804, 15714, 11, 457, 436, 4682, 51326], "temperature": 0.0, "avg_logprob": -0.361337559563773, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.03514038398861885}, {"id": 673, "seek": 323710, "start": 3256.3399999999997, "end": 3258.58, "text": " try to hire linguistics PhDs.", "tokens": [51326, 853, 281, 11158, 21766, 6006, 14476, 82, 13, 51438], "temperature": 0.0, "avg_logprob": -0.361337559563773, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.03514038398861885}, {"id": 674, "seek": 323710, "start": 3258.58, "end": 3263.06, "text": " Answer You certainly wouldn't do that for this, because the whole purpose here is to", "tokens": [51438, 24545, 509, 3297, 2759, 380, 360, 300, 337, 341, 11, 570, 264, 1379, 4334, 510, 307, 281, 51662], "temperature": 0.0, "avg_logprob": -0.361337559563773, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.03514038398861885}, {"id": 675, "seek": 323710, "start": 3263.06, "end": 3267.06, "text": " kind of capture normal people's sentiment.", "tokens": [51662, 733, 295, 7983, 2710, 561, 311, 16149, 13, 51862], "temperature": 0.0, "avg_logprob": -0.361337559563773, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.03514038398861885}, {"id": 676, "seek": 326706, "start": 3267.06, "end": 3271.54, "text": " We know of a team at Google that does that.", "tokens": [50364, 492, 458, 295, 257, 1469, 412, 3329, 300, 775, 300, 13, 50588], "temperature": 0.0, "avg_logprob": -0.3437648773193359, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.12408410757780075}, {"id": 677, "seek": 326706, "start": 3271.54, "end": 3277.7, "text": " And I know when I was in medicine, we went through all these radiology reports and tried", "tokens": [50588, 400, 286, 458, 562, 286, 390, 294, 7195, 11, 321, 1437, 807, 439, 613, 16335, 1793, 7122, 293, 3031, 50896], "temperature": 0.0, "avg_logprob": -0.3437648773193359, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.12408410757780075}, {"id": 678, "seek": 326706, "start": 3277.7, "end": 3281.62, "text": " to capture which ones were critical findings and which ones weren't critical findings,", "tokens": [50896, 281, 7983, 597, 2306, 645, 4924, 16483, 293, 597, 2306, 4999, 380, 4924, 16483, 11, 51092], "temperature": 0.0, "avg_logprob": -0.3437648773193359, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.12408410757780075}, {"id": 679, "seek": 326706, "start": 3281.62, "end": 3285.9, "text": " and we used good radiologists rather than Mechanical Turk for that purpose.", "tokens": [51092, 293, 321, 1143, 665, 16335, 12256, 2831, 813, 30175, 804, 15714, 337, 300, 4334, 13, 51306], "temperature": 0.0, "avg_logprob": -0.3437648773193359, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.12408410757780075}, {"id": 680, "seek": 328590, "start": 3285.9, "end": 3294.9, "text": " Question So we are not considering any sentence construction or bigrams, just a bag of words", "tokens": [50364, 14464, 407, 321, 366, 406, 8079, 604, 8174, 6435, 420, 955, 2356, 82, 11, 445, 257, 3411, 295, 2283, 50814], "temperature": 0.0, "avg_logprob": -0.3219254515891851, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0863119587302208}, {"id": 681, "seek": 328590, "start": 3294.9, "end": 3298.78, "text": " and the literal set of words that are being used in a comment?", "tokens": [50814, 293, 264, 20411, 992, 295, 2283, 300, 366, 885, 1143, 294, 257, 2871, 30, 51008], "temperature": 0.0, "avg_logprob": -0.3219254515891851, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0863119587302208}, {"id": 682, "seek": 328590, "start": 3298.78, "end": 3301.1, "text": " Answer It's not actually just a bag of words.", "tokens": [51008, 24545, 467, 311, 406, 767, 445, 257, 3411, 295, 2283, 13, 51124], "temperature": 0.0, "avg_logprob": -0.3219254515891851, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0863119587302208}, {"id": 683, "seek": 328590, "start": 3301.1, "end": 3306.52, "text": " If you think about it, this dense layer here has 1.6 million parameters.", "tokens": [51124, 759, 291, 519, 466, 309, 11, 341, 18011, 4583, 510, 575, 502, 13, 21, 2459, 9834, 13, 51395], "temperature": 0.0, "avg_logprob": -0.3219254515891851, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0863119587302208}, {"id": 684, "seek": 328590, "start": 3306.52, "end": 3312.9, "text": " It's connecting every one of those 500 inputs to our output.", "tokens": [51395, 467, 311, 11015, 633, 472, 295, 729, 5923, 15743, 281, 527, 5598, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3219254515891851, "compression_ratio": 1.536697247706422, "no_speech_prob": 0.0863119587302208}, {"id": 685, "seek": 331290, "start": 3312.9, "end": 3322.2000000000003, "text": " So it's actually doing that for every one of the incoming factors.", "tokens": [50364, 407, 309, 311, 767, 884, 300, 337, 633, 472, 295, 264, 22341, 6771, 13, 50829], "temperature": 0.0, "avg_logprob": -0.2914664908631207, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.024421267211437225}, {"id": 686, "seek": 331290, "start": 3322.2000000000003, "end": 3331.1600000000003, "text": " So it's creating a pretty complex, big Cartesian product of all of these weights.", "tokens": [50829, 407, 309, 311, 4084, 257, 1238, 3997, 11, 955, 22478, 42434, 1674, 295, 439, 295, 613, 17443, 13, 51277], "temperature": 0.0, "avg_logprob": -0.2914664908631207, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.024421267211437225}, {"id": 687, "seek": 331290, "start": 3331.1600000000003, "end": 3335.86, "text": " It's taking account of the position of a word in the overall sentence.", "tokens": [51277, 467, 311, 1940, 2696, 295, 264, 2535, 295, 257, 1349, 294, 264, 4787, 8174, 13, 51512], "temperature": 0.0, "avg_logprob": -0.2914664908631207, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.024421267211437225}, {"id": 688, "seek": 331290, "start": 3335.86, "end": 3340.7000000000003, "text": " It's not terribly sophisticated and it's not taking account of its position compared to", "tokens": [51512, 467, 311, 406, 22903, 16950, 293, 309, 311, 406, 1940, 2696, 295, 1080, 2535, 5347, 281, 51754], "temperature": 0.0, "avg_logprob": -0.2914664908631207, "compression_ratio": 1.6594594594594594, "no_speech_prob": 0.024421267211437225}, {"id": 689, "seek": 334070, "start": 3340.7, "end": 3346.9399999999996, "text": " other words, but it is taking account of whereabouts it occurs in the whole review.", "tokens": [50364, 661, 2283, 11, 457, 309, 307, 1940, 2696, 295, 689, 41620, 309, 11843, 294, 264, 1379, 3131, 13, 50676], "temperature": 0.0, "avg_logprob": -0.280252190046413, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.11278233677148819}, {"id": 690, "seek": 334070, "start": 3346.9399999999996, "end": 3356.14, "text": " So it's not like the dumbest kind of model I could come up with, it's a good starting", "tokens": [50676, 407, 309, 311, 406, 411, 264, 10316, 377, 733, 295, 2316, 286, 727, 808, 493, 365, 11, 309, 311, 257, 665, 2891, 51136], "temperature": 0.0, "avg_logprob": -0.280252190046413, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.11278233677148819}, {"id": 691, "seek": 334070, "start": 3356.14, "end": 3357.2999999999997, "text": " point.", "tokens": [51136, 935, 13, 51194], "temperature": 0.0, "avg_logprob": -0.280252190046413, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.11278233677148819}, {"id": 692, "seek": 334070, "start": 3357.2999999999997, "end": 3361.3399999999997, "text": " But we would expect that with a little bit of thought, which we're about to use, we could", "tokens": [51194, 583, 321, 576, 2066, 300, 365, 257, 707, 857, 295, 1194, 11, 597, 321, 434, 466, 281, 764, 11, 321, 727, 51396], "temperature": 0.0, "avg_logprob": -0.280252190046413, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.11278233677148819}, {"id": 693, "seek": 334070, "start": 3361.3399999999997, "end": 3364.8999999999996, "text": " do a lot better.", "tokens": [51396, 360, 257, 688, 1101, 13, 51574], "temperature": 0.0, "avg_logprob": -0.280252190046413, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.11278233677148819}, {"id": 694, "seek": 334070, "start": 3364.8999999999996, "end": 3366.9399999999996, "text": " So why don't we go ahead and do that.", "tokens": [51574, 407, 983, 500, 380, 321, 352, 2286, 293, 360, 300, 13, 51676], "temperature": 0.0, "avg_logprob": -0.280252190046413, "compression_ratio": 1.558252427184466, "no_speech_prob": 0.11278233677148819}, {"id": 695, "seek": 336694, "start": 3366.94, "end": 3370.94, "text": " So the slightly better, hopefully you guys have all predicted what that would be, it's", "tokens": [50364, 407, 264, 4748, 1101, 11, 4696, 291, 1074, 362, 439, 19147, 437, 300, 576, 312, 11, 309, 311, 50564], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 696, "seek": 336694, "start": 3370.94, "end": 3372.98, "text": " a convolutional neural network.", "tokens": [50564, 257, 45216, 304, 18161, 3209, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 697, "seek": 336694, "start": 3372.98, "end": 3376.38, "text": " And the reason I hope you predicted that is because a, we've already talked about how", "tokens": [50666, 400, 264, 1778, 286, 1454, 291, 19147, 300, 307, 570, 257, 11, 321, 600, 1217, 2825, 466, 577, 50836], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 698, "seek": 336694, "start": 3376.38, "end": 3381.86, "text": " CNNs are taking over the world, and b, specifically they're taking over the world any time we", "tokens": [50836, 24859, 82, 366, 1940, 670, 264, 1002, 11, 293, 272, 11, 4682, 436, 434, 1940, 670, 264, 1002, 604, 565, 321, 51110], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 699, "seek": 336694, "start": 3381.86, "end": 3384.02, "text": " have some kind of ordered data.", "tokens": [51110, 362, 512, 733, 295, 8866, 1412, 13, 51218], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 700, "seek": 336694, "start": 3384.02, "end": 3386.26, "text": " And clearly a sentence is ordered.", "tokens": [51218, 400, 4448, 257, 8174, 307, 8866, 13, 51330], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 701, "seek": 336694, "start": 3386.26, "end": 3390.0, "text": " One word comes after another word, it has a specific ordering.", "tokens": [51330, 1485, 1349, 1487, 934, 1071, 1349, 11, 309, 575, 257, 2685, 21739, 13, 51517], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 702, "seek": 336694, "start": 3390.0, "end": 3394.62, "text": " So therefore we can use a convolution.", "tokens": [51517, 407, 4412, 321, 393, 764, 257, 45216, 13, 51748], "temperature": 0.0, "avg_logprob": -0.2798563200851967, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.31067290902137756}, {"id": 703, "seek": 339462, "start": 3394.62, "end": 3399.02, "text": " We can't use a 2D convolution because the sentence is not in 2D, the sentence is in", "tokens": [50364, 492, 393, 380, 764, 257, 568, 35, 45216, 570, 264, 8174, 307, 406, 294, 568, 35, 11, 264, 8174, 307, 294, 50584], "temperature": 0.0, "avg_logprob": -0.21330473340790848, "compression_ratio": 2.0485436893203883, "no_speech_prob": 0.13660405576229095}, {"id": 704, "seek": 339462, "start": 3399.02, "end": 3402.06, "text": " 1D, so we're going to use a 1D convolution.", "tokens": [50584, 502, 35, 11, 370, 321, 434, 516, 281, 764, 257, 502, 35, 45216, 13, 50736], "temperature": 0.0, "avg_logprob": -0.21330473340790848, "compression_ratio": 2.0485436893203883, "no_speech_prob": 0.13660405576229095}, {"id": 705, "seek": 339462, "start": 3402.06, "end": 3405.46, "text": " So a 1D convolution is even simpler than a 2D convolution.", "tokens": [50736, 407, 257, 502, 35, 45216, 307, 754, 18587, 813, 257, 568, 35, 45216, 13, 50906], "temperature": 0.0, "avg_logprob": -0.21330473340790848, "compression_ratio": 2.0485436893203883, "no_speech_prob": 0.13660405576229095}, {"id": 706, "seek": 339462, "start": 3405.46, "end": 3411.64, "text": " We're just going to grab a string of a few words and we're going to take their embeddings", "tokens": [50906, 492, 434, 445, 516, 281, 4444, 257, 6798, 295, 257, 1326, 2283, 293, 321, 434, 516, 281, 747, 641, 12240, 29432, 51215], "temperature": 0.0, "avg_logprob": -0.21330473340790848, "compression_ratio": 2.0485436893203883, "no_speech_prob": 0.13660405576229095}, {"id": 707, "seek": 339462, "start": 3411.64, "end": 3416.2599999999998, "text": " and we're going to take that string and we're going to multiply it by some filter.", "tokens": [51215, 293, 321, 434, 516, 281, 747, 300, 6798, 293, 321, 434, 516, 281, 12972, 309, 538, 512, 6608, 13, 51446], "temperature": 0.0, "avg_logprob": -0.21330473340790848, "compression_ratio": 2.0485436893203883, "no_speech_prob": 0.13660405576229095}, {"id": 708, "seek": 339462, "start": 3416.2599999999998, "end": 3421.54, "text": " And then we're going to move that sequence along our sentence.", "tokens": [51446, 400, 550, 321, 434, 516, 281, 1286, 300, 8310, 2051, 527, 8174, 13, 51710], "temperature": 0.0, "avg_logprob": -0.21330473340790848, "compression_ratio": 2.0485436893203883, "no_speech_prob": 0.13660405576229095}, {"id": 709, "seek": 342154, "start": 3421.54, "end": 3430.94, "text": " So this is our normal next place we go as we try to gradually increase the complexity,", "tokens": [50364, 407, 341, 307, 527, 2710, 958, 1081, 321, 352, 382, 321, 853, 281, 13145, 3488, 264, 14024, 11, 50834], "temperature": 0.0, "avg_logprob": -0.27046150759042026, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.00898510031402111}, {"id": 710, "seek": 342154, "start": 3430.94, "end": 3438.98, "text": " which is to grab our simplest possible CNN, which is a convolution, dropout, max-polling.", "tokens": [50834, 597, 307, 281, 4444, 527, 22811, 1944, 24859, 11, 597, 307, 257, 45216, 11, 3270, 346, 11, 11469, 12, 79, 1833, 278, 13, 51236], "temperature": 0.0, "avg_logprob": -0.27046150759042026, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.00898510031402111}, {"id": 711, "seek": 342154, "start": 3438.98, "end": 3442.94, "text": " And then flatten that and then we have our dense layer and our output.", "tokens": [51236, 400, 550, 24183, 300, 293, 550, 321, 362, 527, 18011, 4583, 293, 527, 5598, 13, 51434], "temperature": 0.0, "avg_logprob": -0.27046150759042026, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.00898510031402111}, {"id": 712, "seek": 342154, "start": 3442.94, "end": 3448.1, "text": " So this is exactly like what we did back when we were looking at gradually improving our", "tokens": [51434, 407, 341, 307, 2293, 411, 437, 321, 630, 646, 562, 321, 645, 1237, 412, 13145, 11470, 527, 51692], "temperature": 0.0, "avg_logprob": -0.27046150759042026, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.00898510031402111}, {"id": 713, "seek": 344810, "start": 3448.22, "end": 3450.14, "text": " state farm result.", "tokens": [50370, 1785, 5421, 1874, 13, 50466], "temperature": 0.0, "avg_logprob": -0.2928315709146221, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.29420119524002075}, {"id": 714, "seek": 344810, "start": 3450.14, "end": 3453.42, "text": " But rather than having convolution 2D, we have convolution 1D.", "tokens": [50466, 583, 2831, 813, 1419, 45216, 568, 35, 11, 321, 362, 45216, 502, 35, 13, 50630], "temperature": 0.0, "avg_logprob": -0.2928315709146221, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.29420119524002075}, {"id": 715, "seek": 344810, "start": 3453.42, "end": 3457.2, "text": " The parameters are exactly the same.", "tokens": [50630, 440, 9834, 366, 2293, 264, 912, 13, 50819], "temperature": 0.0, "avg_logprob": -0.2928315709146221, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.29420119524002075}, {"id": 716, "seek": 344810, "start": 3457.2, "end": 3462.5, "text": " How many filters do you want to create and what is the size of your convolution.", "tokens": [50819, 1012, 867, 15995, 360, 291, 528, 281, 1884, 293, 437, 307, 264, 2744, 295, 428, 45216, 13, 51084], "temperature": 0.0, "avg_logprob": -0.2928315709146221, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.29420119524002075}, {"id": 717, "seek": 344810, "start": 3462.5, "end": 3466.96, "text": " Originally I tried 3 here, 5 turned out to be better.", "tokens": [51084, 28696, 286, 3031, 805, 510, 11, 1025, 3574, 484, 281, 312, 1101, 13, 51307], "temperature": 0.0, "avg_logprob": -0.2928315709146221, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.29420119524002075}, {"id": 718, "seek": 344810, "start": 3466.96, "end": 3474.48, "text": " So I'm looking at 5 words at a time and multiplying them by each one of 64 filters.", "tokens": [51307, 407, 286, 478, 1237, 412, 1025, 2283, 412, 257, 565, 293, 30955, 552, 538, 1184, 472, 295, 12145, 15995, 13, 51683], "temperature": 0.0, "avg_logprob": -0.2928315709146221, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.29420119524002075}, {"id": 719, "seek": 347448, "start": 3474.48, "end": 3482.08, "text": " So that is going to return, we're going to start with the same embedding as before.", "tokens": [50364, 407, 300, 307, 516, 281, 2736, 11, 321, 434, 516, 281, 722, 365, 264, 912, 12240, 3584, 382, 949, 13, 50744], "temperature": 0.0, "avg_logprob": -0.27766218303162377, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.007695566397160292}, {"id": 720, "seek": 347448, "start": 3482.08, "end": 3490.68, "text": " So we take our sentences and we turn them into a 500x32 matrix for each of our inputs.", "tokens": [50744, 407, 321, 747, 527, 16579, 293, 321, 1261, 552, 666, 257, 5923, 87, 11440, 8141, 337, 1184, 295, 527, 15743, 13, 51174], "temperature": 0.0, "avg_logprob": -0.27766218303162377, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.007695566397160292}, {"id": 721, "seek": 347448, "start": 3490.68, "end": 3496.2400000000002, "text": " We then put it through our convolution and because our convolution has a border mode", "tokens": [51174, 492, 550, 829, 309, 807, 527, 45216, 293, 570, 527, 45216, 575, 257, 7838, 4391, 51452], "temperature": 0.0, "avg_logprob": -0.27766218303162377, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.007695566397160292}, {"id": 722, "seek": 347448, "start": 3496.2400000000002, "end": 3501.34, "text": " of same, we get back exactly the same shape that we gave it.", "tokens": [51452, 295, 912, 11, 321, 483, 646, 2293, 264, 912, 3909, 300, 321, 2729, 309, 13, 51707], "temperature": 0.0, "avg_logprob": -0.27766218303162377, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.007695566397160292}, {"id": 723, "seek": 350134, "start": 3501.34, "end": 3505.1800000000003, "text": " We then put it through our 1D max-polling and that will halve its size.", "tokens": [50364, 492, 550, 829, 309, 807, 527, 502, 35, 11469, 12, 79, 1833, 278, 293, 300, 486, 7523, 303, 1080, 2744, 13, 50556], "temperature": 0.0, "avg_logprob": -0.28738005119457577, "compression_ratio": 1.3733333333333333, "no_speech_prob": 0.08035572618246078}, {"id": 724, "seek": 350134, "start": 3505.1800000000003, "end": 3509.58, "text": " And then we stick it through the same dense layers as we had before.", "tokens": [50556, 400, 550, 321, 2897, 309, 807, 264, 912, 18011, 7914, 382, 321, 632, 949, 13, 50776], "temperature": 0.0, "avg_logprob": -0.28738005119457577, "compression_ratio": 1.3733333333333333, "no_speech_prob": 0.08035572618246078}, {"id": 725, "seek": 350134, "start": 3509.58, "end": 3514.7400000000002, "text": " So that's a really simple convolutional neural network for words.", "tokens": [50776, 407, 300, 311, 257, 534, 2199, 45216, 304, 18161, 3209, 337, 2283, 13, 51034], "temperature": 0.0, "avg_logprob": -0.28738005119457577, "compression_ratio": 1.3733333333333333, "no_speech_prob": 0.08035572618246078}, {"id": 726, "seek": 351474, "start": 3514.74, "end": 3535.74, "text": " So we compile it, run it, and we get 89.47 compared to 88.33.", "tokens": [50364, 407, 321, 31413, 309, 11, 1190, 309, 11, 293, 321, 483, 31877, 13, 14060, 5347, 281, 24587, 13, 10191, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3009466727574666, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.10668128728866577}, {"id": 727, "seek": 351474, "start": 3535.74, "end": 3540.2999999999997, "text": " So we have already broken the academic state-of-the-art as at when this paper was written.", "tokens": [51414, 407, 321, 362, 1217, 5463, 264, 7778, 1785, 12, 2670, 12, 3322, 12, 446, 382, 412, 562, 341, 3035, 390, 3720, 13, 51642], "temperature": 0.0, "avg_logprob": -0.3009466727574666, "compression_ratio": 1.2666666666666666, "no_speech_prob": 0.10668128728866577}, {"id": 728, "seek": 354030, "start": 3540.3, "end": 3545.34, "text": " And again, simple convolutional neural network gets us a very, very long way.", "tokens": [50364, 400, 797, 11, 2199, 45216, 304, 18161, 3209, 2170, 505, 257, 588, 11, 588, 938, 636, 13, 50616], "temperature": 0.0, "avg_logprob": -0.43639516255941735, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.10520608723163605}, {"id": 729, "seek": 354030, "start": 3545.34, "end": 3546.34, "text": " Question 2.", "tokens": [50616, 14464, 568, 13, 50666], "temperature": 0.0, "avg_logprob": -0.43639516255941735, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.10520608723163605}, {"id": 730, "seek": 354030, "start": 3546.34, "end": 3558.02, "text": " Convolution 2D for images is easier to understand, element-wise multiplication and addition,", "tokens": [50666, 2656, 85, 3386, 568, 35, 337, 5267, 307, 3571, 281, 1223, 11, 4478, 12, 3711, 27290, 293, 4500, 11, 51250], "temperature": 0.0, "avg_logprob": -0.43639516255941735, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.10520608723163605}, {"id": 731, "seek": 354030, "start": 3558.02, "end": 3563.38, "text": " but what does it mean for a sequence of words?", "tokens": [51250, 457, 437, 775, 309, 914, 337, 257, 8310, 295, 2283, 30, 51518], "temperature": 0.0, "avg_logprob": -0.43639516255941735, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.10520608723163605}, {"id": 732, "seek": 354030, "start": 3563.38, "end": 3567.46, "text": " Don't think of it as a sequence of words, because remember it's been through an embedding.", "tokens": [51518, 1468, 380, 519, 295, 309, 382, 257, 8310, 295, 2283, 11, 570, 1604, 309, 311, 668, 807, 364, 12240, 3584, 13, 51722], "temperature": 0.0, "avg_logprob": -0.43639516255941735, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.10520608723163605}, {"id": 733, "seek": 356746, "start": 3567.62, "end": 3571.86, "text": " So it's a sequence of 32 element vectors.", "tokens": [50372, 407, 309, 311, 257, 8310, 295, 8858, 4478, 18875, 13, 50584], "temperature": 0.0, "avg_logprob": -0.2202335573592276, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.1242046058177948}, {"id": 734, "seek": 356746, "start": 3571.86, "end": 3577.7, "text": " So it's doing exactly the same thing as we were doing in a 2D convolution, but rather", "tokens": [50584, 407, 309, 311, 884, 2293, 264, 912, 551, 382, 321, 645, 884, 294, 257, 568, 35, 45216, 11, 457, 2831, 50876], "temperature": 0.0, "avg_logprob": -0.2202335573592276, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.1242046058177948}, {"id": 735, "seek": 356746, "start": 3577.7, "end": 3585.8, "text": " than having 3 channels of color, we have 32 channels of embedding.", "tokens": [50876, 813, 1419, 805, 9235, 295, 2017, 11, 321, 362, 8858, 9235, 295, 12240, 3584, 13, 51281], "temperature": 0.0, "avg_logprob": -0.2202335573592276, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.1242046058177948}, {"id": 736, "seek": 358580, "start": 3585.8, "end": 3598.88, "text": " So we're just going through and we're just like in our convolution spreadsheet, remember", "tokens": [50364, 407, 321, 434, 445, 516, 807, 293, 321, 434, 445, 411, 294, 527, 45216, 27733, 11, 1604, 51018], "temperature": 0.0, "avg_logprob": -0.2610533667392418, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.017985481768846512}, {"id": 737, "seek": 358580, "start": 3598.88, "end": 3611.28, "text": " how in the second one, once we had two filters already, our filter had to be a 3x3x2 tensor", "tokens": [51018, 577, 294, 264, 1150, 472, 11, 1564, 321, 632, 732, 15995, 1217, 11, 527, 6608, 632, 281, 312, 257, 805, 87, 18, 87, 17, 40863, 51638], "temperature": 0.0, "avg_logprob": -0.2610533667392418, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.017985481768846512}, {"id": 738, "seek": 358580, "start": 3611.28, "end": 3614.48, "text": " in order to allow us to create the second layer.", "tokens": [51638, 294, 1668, 281, 2089, 505, 281, 1884, 264, 1150, 4583, 13, 51798], "temperature": 0.0, "avg_logprob": -0.2610533667392418, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.017985481768846512}, {"id": 739, "seek": 361448, "start": 3614.48, "end": 3624.88, "text": " For us, we now don't have a 3x3x2 tensor, we have a 5x1x32, or more conveniently, a", "tokens": [50364, 1171, 505, 11, 321, 586, 500, 380, 362, 257, 805, 87, 18, 87, 17, 40863, 11, 321, 362, 257, 1025, 87, 16, 87, 11440, 11, 420, 544, 44375, 11, 257, 50884], "temperature": 0.0, "avg_logprob": -0.22611964478784677, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.016152724623680115}, {"id": 740, "seek": 361448, "start": 3624.88, "end": 3626.6, "text": " 5x32 matrix.", "tokens": [50884, 1025, 87, 11440, 8141, 13, 50970], "temperature": 0.0, "avg_logprob": -0.22611964478784677, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.016152724623680115}, {"id": 741, "seek": 361448, "start": 3626.6, "end": 3632.6, "text": " So each convolution is going to go through each of the 5 words and each of the 32 embeddings,", "tokens": [50970, 407, 1184, 45216, 307, 516, 281, 352, 807, 1184, 295, 264, 1025, 2283, 293, 1184, 295, 264, 8858, 12240, 29432, 11, 51270], "temperature": 0.0, "avg_logprob": -0.22611964478784677, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.016152724623680115}, {"id": 742, "seek": 361448, "start": 3632.6, "end": 3636.92, "text": " do an element-wise multiplication and add them all up.", "tokens": [51270, 360, 364, 4478, 12, 3711, 27290, 293, 909, 552, 439, 493, 13, 51486], "temperature": 0.0, "avg_logprob": -0.22611964478784677, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.016152724623680115}, {"id": 743, "seek": 361448, "start": 3636.92, "end": 3642.48, "text": " So the important thing to remember is that once we've done the embedding layer, which", "tokens": [51486, 407, 264, 1021, 551, 281, 1604, 307, 300, 1564, 321, 600, 1096, 264, 12240, 3584, 4583, 11, 597, 51764], "temperature": 0.0, "avg_logprob": -0.22611964478784677, "compression_ratio": 1.5467289719626167, "no_speech_prob": 0.016152724623680115}, {"id": 744, "seek": 364248, "start": 3642.48, "end": 3647.08, "text": " is always going to be our first step for every NLP model, is that we don't have words", "tokens": [50364, 307, 1009, 516, 281, 312, 527, 700, 1823, 337, 633, 426, 45196, 2316, 11, 307, 300, 321, 500, 380, 362, 2283, 50594], "temperature": 0.0, "avg_logprob": -0.2595481671785053, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0006771541084162891}, {"id": 745, "seek": 364248, "start": 3647.08, "end": 3648.12, "text": " anymore.", "tokens": [50594, 3602, 13, 50646], "temperature": 0.0, "avg_logprob": -0.2595481671785053, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0006771541084162891}, {"id": 746, "seek": 364248, "start": 3648.12, "end": 3654.84, "text": " We now have vectors which are attempting to capture the information in that word in some", "tokens": [50646, 492, 586, 362, 18875, 597, 366, 22001, 281, 7983, 264, 1589, 294, 300, 1349, 294, 512, 50982], "temperature": 0.0, "avg_logprob": -0.2595481671785053, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0006771541084162891}, {"id": 747, "seek": 364248, "start": 3654.84, "end": 3661.52, "text": " way, just like our latent factors captured information about a movie and a user in our", "tokens": [50982, 636, 11, 445, 411, 527, 48994, 6771, 11828, 1589, 466, 257, 3169, 293, 257, 4195, 294, 527, 51316], "temperature": 0.0, "avg_logprob": -0.2595481671785053, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0006771541084162891}, {"id": 748, "seek": 364248, "start": 3661.52, "end": 3663.76, "text": " collaborative filtering.", "tokens": [51316, 16555, 30822, 13, 51428], "temperature": 0.0, "avg_logprob": -0.2595481671785053, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0006771541084162891}, {"id": 749, "seek": 364248, "start": 3663.76, "end": 3668.94, "text": " We haven't yet looked at what they do, we will in a moment, just like we did with the", "tokens": [51428, 492, 2378, 380, 1939, 2956, 412, 437, 436, 360, 11, 321, 486, 294, 257, 1623, 11, 445, 411, 321, 630, 365, 264, 51687], "temperature": 0.0, "avg_logprob": -0.2595481671785053, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0006771541084162891}, {"id": 750, "seek": 366894, "start": 3669.34, "end": 3670.5, "text": " movie vectors.", "tokens": [50384, 3169, 18875, 13, 50442], "temperature": 0.0, "avg_logprob": -0.24480099148220485, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.047421958297491074}, {"id": 751, "seek": 366894, "start": 3670.5, "end": 3677.58, "text": " But we do know from our experience that SGD is going to try to fill out those 32 places", "tokens": [50442, 583, 321, 360, 458, 490, 527, 1752, 300, 34520, 35, 307, 516, 281, 853, 281, 2836, 484, 729, 8858, 3190, 50796], "temperature": 0.0, "avg_logprob": -0.24480099148220485, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.047421958297491074}, {"id": 752, "seek": 366894, "start": 3677.58, "end": 3686.82, "text": " with information about how that word is being used, which allow us to make these predictions.", "tokens": [50796, 365, 1589, 466, 577, 300, 1349, 307, 885, 1143, 11, 597, 2089, 505, 281, 652, 613, 21264, 13, 51258], "temperature": 0.0, "avg_logprob": -0.24480099148220485, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.047421958297491074}, {"id": 753, "seek": 366894, "start": 3686.82, "end": 3691.86, "text": " Just like when you first learned about 2D convolutions, it took you probably a few days", "tokens": [51258, 1449, 411, 562, 291, 700, 3264, 466, 568, 35, 3754, 15892, 11, 309, 1890, 291, 1391, 257, 1326, 1708, 51510], "temperature": 0.0, "avg_logprob": -0.24480099148220485, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.047421958297491074}, {"id": 754, "seek": 366894, "start": 3691.86, "end": 3697.7000000000003, "text": " of fiddling around with spreadsheets and pieces of paper and Python and checking inputs and", "tokens": [51510, 295, 283, 14273, 1688, 926, 365, 23651, 1385, 293, 3755, 295, 3035, 293, 15329, 293, 8568, 15743, 293, 51802], "temperature": 0.0, "avg_logprob": -0.24480099148220485, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.047421958297491074}, {"id": 755, "seek": 369770, "start": 3697.74, "end": 3702.5, "text": " outputs to get a really intuitive understanding of what a 2D convolution is doing.", "tokens": [50366, 23930, 281, 483, 257, 534, 21769, 3701, 295, 437, 257, 568, 35, 45216, 307, 884, 13, 50604], "temperature": 0.0, "avg_logprob": -0.2597217121343503, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.1732698231935501}, {"id": 756, "seek": 369770, "start": 3702.5, "end": 3707.2999999999997, "text": " You may find it's the same with a 1D convolution, but it will take you probably a fifth of the", "tokens": [50604, 509, 815, 915, 309, 311, 264, 912, 365, 257, 502, 35, 45216, 11, 457, 309, 486, 747, 291, 1391, 257, 9266, 295, 264, 50844], "temperature": 0.0, "avg_logprob": -0.2597217121343503, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.1732698231935501}, {"id": 757, "seek": 369770, "start": 3707.2999999999997, "end": 3715.1, "text": " time to get there because you've really done all the hard work already.", "tokens": [50844, 565, 281, 483, 456, 570, 291, 600, 534, 1096, 439, 264, 1152, 589, 1217, 13, 51234], "temperature": 0.0, "avg_logprob": -0.2597217121343503, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.1732698231935501}, {"id": 758, "seek": 369770, "start": 3715.1, "end": 3724.62, "text": " I think now is a great time to have a break, so let's come back here at 7.57.", "tokens": [51234, 286, 519, 586, 307, 257, 869, 565, 281, 362, 257, 1821, 11, 370, 718, 311, 808, 646, 510, 412, 1614, 13, 19004, 13, 51710], "temperature": 0.0, "avg_logprob": -0.2597217121343503, "compression_ratio": 1.52803738317757, "no_speech_prob": 0.1732698231935501}, {"id": 759, "seek": 372462, "start": 3724.62, "end": 3736.06, "text": " So there's a couple of concepts that we come across from time to time in this class.", "tokens": [50364, 407, 456, 311, 257, 1916, 295, 10392, 300, 321, 808, 2108, 490, 565, 281, 565, 294, 341, 1508, 13, 50936], "temperature": 0.0, "avg_logprob": -0.28512996822208553, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.0007915538153611124}, {"id": 760, "seek": 372462, "start": 3736.06, "end": 3740.94, "text": " There is no way that me lecturing to you is going to be enough to get an intuitive understanding", "tokens": [50936, 821, 307, 572, 636, 300, 385, 5899, 1345, 281, 291, 307, 516, 281, 312, 1547, 281, 483, 364, 21769, 3701, 51180], "temperature": 0.0, "avg_logprob": -0.28512996822208553, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.0007915538153611124}, {"id": 761, "seek": 372462, "start": 3740.94, "end": 3741.94, "text": " of it.", "tokens": [51180, 295, 309, 13, 51230], "temperature": 0.0, "avg_logprob": -0.28512996822208553, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.0007915538153611124}, {"id": 762, "seek": 372462, "start": 3741.94, "end": 3746.94, "text": " The first clearly is the 2D convolution.", "tokens": [51230, 440, 700, 4448, 307, 264, 568, 35, 45216, 13, 51480], "temperature": 0.0, "avg_logprob": -0.28512996822208553, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.0007915538153611124}, {"id": 763, "seek": 372462, "start": 3746.94, "end": 3750.98, "text": " Hopefully you've had lots of opportunities to experiment and practice and read.", "tokens": [51480, 10429, 291, 600, 632, 3195, 295, 4786, 281, 5120, 293, 3124, 293, 1401, 13, 51682], "temperature": 0.0, "avg_logprob": -0.28512996822208553, "compression_ratio": 1.522167487684729, "no_speech_prob": 0.0007915538153611124}, {"id": 764, "seek": 375098, "start": 3750.98, "end": 3754.18, "text": " These are things you have to tackle from many, many different directions to understand", "tokens": [50364, 1981, 366, 721, 291, 362, 281, 14896, 490, 867, 11, 867, 819, 11095, 281, 1223, 50524], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 765, "seek": 375098, "start": 3754.18, "end": 3756.9, "text": " a 2D convolution.", "tokens": [50524, 257, 568, 35, 45216, 13, 50660], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 766, "seek": 375098, "start": 3756.9, "end": 3760.7400000000002, "text": " And 2D convolutions in a sense are really 3D, because if it's in full color, you've", "tokens": [50660, 400, 568, 35, 3754, 15892, 294, 257, 2020, 366, 534, 805, 35, 11, 570, 498, 309, 311, 294, 1577, 2017, 11, 291, 600, 50852], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 767, "seek": 375098, "start": 3760.7400000000002, "end": 3761.7400000000002, "text": " got 3 channels.", "tokens": [50852, 658, 805, 9235, 13, 50902], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 768, "seek": 375098, "start": 3761.7400000000002, "end": 3764.3, "text": " Hopefully that's something you've all played with.", "tokens": [50902, 10429, 300, 311, 746, 291, 600, 439, 3737, 365, 13, 51030], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 769, "seek": 375098, "start": 3764.3, "end": 3770.34, "text": " And once you have multiple filters, later on in your image models, you still have 3D", "tokens": [51030, 400, 1564, 291, 362, 3866, 15995, 11, 1780, 322, 294, 428, 3256, 5245, 11, 291, 920, 362, 805, 35, 51332], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 770, "seek": 375098, "start": 3770.34, "end": 3771.78, "text": " and you have more than 3 channels.", "tokens": [51332, 293, 291, 362, 544, 813, 805, 9235, 13, 51404], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 771, "seek": 375098, "start": 3771.78, "end": 3775.7, "text": " You might have 32 filters or 64 filters.", "tokens": [51404, 509, 1062, 362, 8858, 15995, 420, 12145, 15995, 13, 51600], "temperature": 0.0, "avg_logprob": -0.31439085264463684, "compression_ratio": 1.6910569105691058, "no_speech_prob": 0.030212976038455963}, {"id": 772, "seek": 377570, "start": 3775.7, "end": 3783.3399999999997, "text": " In this lesson, we've introduced one much simpler concept, but it's still new, which", "tokens": [50364, 682, 341, 6898, 11, 321, 600, 7268, 472, 709, 18587, 3410, 11, 457, 309, 311, 920, 777, 11, 597, 50746], "temperature": 0.0, "avg_logprob": -0.2546985371907552, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.002889552153646946}, {"id": 773, "seek": 377570, "start": 3783.3399999999997, "end": 3790.7, "text": " is the 1D convolution, which is really a 2D convolution because just like with images", "tokens": [50746, 307, 264, 502, 35, 45216, 11, 597, 307, 534, 257, 568, 35, 45216, 570, 445, 411, 365, 5267, 51114], "temperature": 0.0, "avg_logprob": -0.2546985371907552, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.002889552153646946}, {"id": 774, "seek": 377570, "start": 3790.7, "end": 3798.1, "text": " we had red, green, blue, now we have the 32 embedding factors.", "tokens": [51114, 321, 632, 2182, 11, 3092, 11, 3344, 11, 586, 321, 362, 264, 8858, 12240, 3584, 6771, 13, 51484], "temperature": 0.0, "avg_logprob": -0.2546985371907552, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.002889552153646946}, {"id": 775, "seek": 377570, "start": 3798.1, "end": 3803.22, "text": " So that's something you definitely need to experiment with.", "tokens": [51484, 407, 300, 311, 746, 291, 2138, 643, 281, 5120, 365, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2546985371907552, "compression_ratio": 1.47979797979798, "no_speech_prob": 0.002889552153646946}, {"id": 776, "seek": 380322, "start": 3803.22, "end": 3807.18, "text": " Create a model with just an embedding layer, look at what the output is, what is its shape,", "tokens": [50364, 20248, 257, 2316, 365, 445, 364, 12240, 3584, 4583, 11, 574, 412, 437, 264, 5598, 307, 11, 437, 307, 1080, 3909, 11, 50562], "temperature": 0.0, "avg_logprob": -0.24153860182989212, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.29741212725639343}, {"id": 777, "seek": 380322, "start": 3807.18, "end": 3816.3799999999997, "text": " what does it look like, and then how does a 1D convolution modify that.", "tokens": [50562, 437, 775, 309, 574, 411, 11, 293, 550, 577, 775, 257, 502, 35, 45216, 16927, 300, 13, 51022], "temperature": 0.0, "avg_logprob": -0.24153860182989212, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.29741212725639343}, {"id": 778, "seek": 380322, "start": 3816.3799999999997, "end": 3823.9399999999996, "text": " And then trying to understand what an embedding is is kind of your next big task if you're", "tokens": [51022, 400, 550, 1382, 281, 1223, 437, 364, 12240, 3584, 307, 307, 733, 295, 428, 958, 955, 5633, 498, 291, 434, 51400], "temperature": 0.0, "avg_logprob": -0.24153860182989212, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.29741212725639343}, {"id": 779, "seek": 380322, "start": 3823.9399999999996, "end": 3825.62, "text": " not already feeling comfortable with it.", "tokens": [51400, 406, 1217, 2633, 4619, 365, 309, 13, 51484], "temperature": 0.0, "avg_logprob": -0.24153860182989212, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.29741212725639343}, {"id": 780, "seek": 380322, "start": 3825.62, "end": 3831.2999999999997, "text": " And if you haven't seen them before today, I'm sure you won't, because this is a big", "tokens": [51484, 400, 498, 291, 2378, 380, 1612, 552, 949, 965, 11, 286, 478, 988, 291, 1582, 380, 11, 570, 341, 307, 257, 955, 51768], "temperature": 0.0, "avg_logprob": -0.24153860182989212, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.29741212725639343}, {"id": 781, "seek": 380322, "start": 3831.2999999999997, "end": 3832.66, "text": " new concept.", "tokens": [51768, 777, 3410, 13, 51836], "temperature": 0.0, "avg_logprob": -0.24153860182989212, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.29741212725639343}, {"id": 782, "seek": 383266, "start": 3833.1, "end": 3838.62, "text": " It's not in any way mathematically challenging, it's literally looking up an array and returning", "tokens": [50386, 467, 311, 406, 294, 604, 636, 44003, 7595, 11, 309, 311, 3736, 1237, 493, 364, 10225, 293, 12678, 50662], "temperature": 0.0, "avg_logprob": -0.2679093340609936, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.007937883958220482}, {"id": 783, "seek": 383266, "start": 3838.62, "end": 3840.7799999999997, "text": " the thing at that ID.", "tokens": [50662, 264, 551, 412, 300, 7348, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2679093340609936, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.007937883958220482}, {"id": 784, "seek": 383266, "start": 3840.7799999999997, "end": 3849.2999999999997, "text": " So an embedding looking at movie ID 3 is go to the 3rd column of the matrix and return", "tokens": [50770, 407, 364, 12240, 3584, 1237, 412, 3169, 7348, 805, 307, 352, 281, 264, 805, 7800, 7738, 295, 264, 8141, 293, 2736, 51196], "temperature": 0.0, "avg_logprob": -0.2679093340609936, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.007937883958220482}, {"id": 785, "seek": 383266, "start": 3849.2999999999997, "end": 3850.2999999999997, "text": " what you see.", "tokens": [51196, 437, 291, 536, 13, 51246], "temperature": 0.0, "avg_logprob": -0.2679093340609936, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.007937883958220482}, {"id": 786, "seek": 383266, "start": 3850.2999999999997, "end": 3853.06, "text": " That's all an embedding does.", "tokens": [51246, 663, 311, 439, 364, 12240, 3584, 775, 13, 51384], "temperature": 0.0, "avg_logprob": -0.2679093340609936, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.007937883958220482}, {"id": 787, "seek": 383266, "start": 3853.06, "end": 3857.2999999999997, "text": " It couldn't be mathematically simpler, it's the simplest possible operation.", "tokens": [51384, 467, 2809, 380, 312, 44003, 18587, 11, 309, 311, 264, 22811, 1944, 6916, 13, 51596], "temperature": 0.0, "avg_logprob": -0.2679093340609936, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.007937883958220482}, {"id": 788, "seek": 383266, "start": 3857.2999999999997, "end": 3860.7, "text": " Return the thing at this index.", "tokens": [51596, 24350, 264, 551, 412, 341, 8186, 13, 51766], "temperature": 0.0, "avg_logprob": -0.2679093340609936, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.007937883958220482}, {"id": 789, "seek": 386070, "start": 3860.74, "end": 3868.02, "text": " But the intuitive understanding of what happens when you put an embedding into an SGD and", "tokens": [50366, 583, 264, 21769, 3701, 295, 437, 2314, 562, 291, 829, 364, 12240, 3584, 666, 364, 34520, 35, 293, 50730], "temperature": 0.0, "avg_logprob": -0.2794562706580529, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.004829647950828075}, {"id": 790, "seek": 386070, "start": 3868.02, "end": 3877.2999999999997, "text": " learn a vector which it turns out to be useful is something which is kind of mind-blowing.", "tokens": [50730, 1466, 257, 8062, 597, 309, 4523, 484, 281, 312, 4420, 307, 746, 597, 307, 733, 295, 1575, 12, 43788, 13, 51194], "temperature": 0.0, "avg_logprob": -0.2794562706580529, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.004829647950828075}, {"id": 791, "seek": 386070, "start": 3877.2999999999997, "end": 3886.3799999999997, "text": " As we saw from the movie lens example, with just a dot product and this simple lookup", "tokens": [51194, 1018, 321, 1866, 490, 264, 3169, 6765, 1365, 11, 365, 445, 257, 5893, 1674, 293, 341, 2199, 574, 1010, 51648], "temperature": 0.0, "avg_logprob": -0.2794562706580529, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.004829647950828075}, {"id": 792, "seek": 388638, "start": 3886.38, "end": 3893.46, "text": " something in an index operation, we ended up with vectors which captured all kinds of", "tokens": [50364, 746, 294, 364, 8186, 6916, 11, 321, 4590, 493, 365, 18875, 597, 11828, 439, 3685, 295, 50718], "temperature": 0.0, "avg_logprob": -0.29332477168033, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.1066882461309433}, {"id": 793, "seek": 388638, "start": 3893.46, "end": 3898.26, "text": " interesting features about movies without us in any way asking it to.", "tokens": [50718, 1880, 4122, 466, 6233, 1553, 505, 294, 604, 636, 3365, 309, 281, 13, 50958], "temperature": 0.0, "avg_logprob": -0.29332477168033, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.1066882461309433}, {"id": 794, "seek": 388638, "start": 3898.26, "end": 3905.94, "text": " So I kind of wanted to make sure that you guys really felt like after this class you're", "tokens": [50958, 407, 286, 733, 295, 1415, 281, 652, 988, 300, 291, 1074, 534, 2762, 411, 934, 341, 1508, 291, 434, 51342], "temperature": 0.0, "avg_logprob": -0.29332477168033, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.1066882461309433}, {"id": 795, "seek": 388638, "start": 3905.94, "end": 3912.58, "text": " going to go away and try to find a dozen different ways of looking at these concepts.", "tokens": [51342, 516, 281, 352, 1314, 293, 853, 281, 915, 257, 16654, 819, 2098, 295, 1237, 412, 613, 10392, 13, 51674], "temperature": 0.0, "avg_logprob": -0.29332477168033, "compression_ratio": 1.5592417061611374, "no_speech_prob": 0.1066882461309433}, {"id": 796, "seek": 391258, "start": 3912.58, "end": 3916.46, "text": " One of those ways is to look at how other people explain them.", "tokens": [50364, 1485, 295, 729, 2098, 307, 281, 574, 412, 577, 661, 561, 2903, 552, 13, 50558], "temperature": 0.0, "avg_logprob": -0.2858781469873635, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.21467778086662292}, {"id": 797, "seek": 391258, "start": 3916.46, "end": 3924.14, "text": " Chris Ola has one of the very, very best technical blogs I've come across and I quite often refer", "tokens": [50558, 6688, 422, 875, 575, 472, 295, 264, 588, 11, 588, 1151, 6191, 31038, 286, 600, 808, 2108, 293, 286, 1596, 2049, 2864, 50942], "temperature": 0.0, "avg_logprob": -0.2858781469873635, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.21467778086662292}, {"id": 798, "seek": 391258, "start": 3924.14, "end": 3925.34, "text": " to him in this class.", "tokens": [50942, 281, 796, 294, 341, 1508, 13, 51002], "temperature": 0.0, "avg_logprob": -0.2858781469873635, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.21467778086662292}, {"id": 799, "seek": 391258, "start": 3925.34, "end": 3930.94, "text": " In his Understanding Convolutions post, he actually has a very interesting example of", "tokens": [51002, 682, 702, 36858, 2656, 85, 15892, 2183, 11, 415, 767, 575, 257, 588, 1880, 1365, 295, 51282], "temperature": 0.0, "avg_logprob": -0.2858781469873635, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.21467778086662292}, {"id": 800, "seek": 391258, "start": 3930.94, "end": 3936.9, "text": " thinking about what a dropped ball does as a convolutional operation.", "tokens": [51282, 1953, 466, 437, 257, 8119, 2594, 775, 382, 257, 45216, 304, 6916, 13, 51580], "temperature": 0.0, "avg_logprob": -0.2858781469873635, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.21467778086662292}, {"id": 801, "seek": 393690, "start": 3936.9, "end": 3944.54, "text": " He shows how you can think about a 1D convolution using this dropped ball analogy.", "tokens": [50364, 634, 3110, 577, 291, 393, 519, 466, 257, 502, 35, 45216, 1228, 341, 8119, 2594, 21663, 13, 50746], "temperature": 0.0, "avg_logprob": -0.3185564041137695, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.381073534488678}, {"id": 802, "seek": 393690, "start": 3944.54, "end": 3949.82, "text": " Particularly if you have some background in electrical or mechanical engineering, I suspect", "tokens": [50746, 32281, 498, 291, 362, 512, 3678, 294, 12147, 420, 12070, 7043, 11, 286, 9091, 51010], "temperature": 0.0, "avg_logprob": -0.3185564041137695, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.381073534488678}, {"id": 803, "seek": 393690, "start": 3949.82, "end": 3952.9, "text": " you'll find this a very helpful example.", "tokens": [51010, 291, 603, 915, 341, 257, 588, 4961, 1365, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3185564041137695, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.381073534488678}, {"id": 804, "seek": 393690, "start": 3952.9, "end": 3958.5, "text": " There are many resources out there for thinking about convolutions and I hope some of you", "tokens": [51164, 821, 366, 867, 3593, 484, 456, 337, 1953, 466, 3754, 15892, 293, 286, 1454, 512, 295, 291, 51444], "temperature": 0.0, "avg_logprob": -0.3185564041137695, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.381073534488678}, {"id": 805, "seek": 393690, "start": 3958.5, "end": 3962.14, "text": " will share on the forums any that you come across.", "tokens": [51444, 486, 2073, 322, 264, 26998, 604, 300, 291, 808, 2108, 13, 51626], "temperature": 0.0, "avg_logprob": -0.3185564041137695, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.381073534488678}, {"id": 806, "seek": 396214, "start": 3962.14, "end": 3969.14, "text": " Question from audience 1.", "tokens": [50364, 14464, 490, 4034, 502, 13, 50714], "temperature": 0.4, "avg_logprob": -0.6868375661421795, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.07806944102048874}, {"id": 807, "seek": 396214, "start": 3969.14, "end": 3971.9, "text": " Essentially, are we training the input too?", "tokens": [50714, 23596, 11, 366, 321, 3097, 264, 4846, 886, 30, 50852], "temperature": 0.4, "avg_logprob": -0.6868375661421795, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.07806944102048874}, {"id": 808, "seek": 396214, "start": 3971.9, "end": 3972.9, "text": " Answer.", "tokens": [50852, 24545, 13, 50902], "temperature": 0.4, "avg_logprob": -0.6868375661421795, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.07806944102048874}, {"id": 809, "seek": 396214, "start": 3972.9, "end": 3975.02, "text": " We are absolutely training the input.", "tokens": [50902, 492, 366, 3122, 3097, 264, 4846, 13, 51008], "temperature": 0.4, "avg_logprob": -0.6868375661421795, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.07806944102048874}, {"id": 810, "seek": 396214, "start": 3975.02, "end": 3984.24, "text": " The only input we have is 25,000 sequences of 500 integers.", "tokens": [51008, 440, 787, 4846, 321, 362, 307, 3552, 11, 1360, 22978, 295, 5923, 41674, 13, 51469], "temperature": 0.4, "avg_logprob": -0.6868375661421795, "compression_ratio": 1.3059701492537314, "no_speech_prob": 0.07806944102048874}, {"id": 811, "seek": 398424, "start": 3984.24, "end": 3992.4399999999996, "text": " So we take each of those integers and replace them with a lookup into a 500-column matrix.", "tokens": [50364, 407, 321, 747, 1184, 295, 729, 41674, 293, 7406, 552, 365, 257, 574, 1010, 666, 257, 5923, 12, 8768, 16449, 8141, 13, 50774], "temperature": 0.0, "avg_logprob": -0.2960906255812872, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.12084463983774185}, {"id": 812, "seek": 398424, "start": 3992.4399999999996, "end": 3997.8799999999997, "text": " Initially that matrix is random, just like in our Excel example.", "tokens": [50774, 29446, 300, 8141, 307, 4974, 11, 445, 411, 294, 527, 19060, 1365, 13, 51046], "temperature": 0.0, "avg_logprob": -0.2960906255812872, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.12084463983774185}, {"id": 813, "seek": 398424, "start": 3997.8799999999997, "end": 4005.8799999999997, "text": " We started with a random matrix, these are all random numbers, and then we created this", "tokens": [51046, 492, 1409, 365, 257, 4974, 8141, 11, 613, 366, 439, 4974, 3547, 11, 293, 550, 321, 2942, 341, 51446], "temperature": 0.0, "avg_logprob": -0.2960906255812872, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.12084463983774185}, {"id": 814, "seek": 398424, "start": 4005.8799999999997, "end": 4011.12, "text": " loss function which was the sum of the squares of differences between the dot product and", "tokens": [51446, 4470, 2445, 597, 390, 264, 2408, 295, 264, 19368, 295, 7300, 1296, 264, 5893, 1674, 293, 51708], "temperature": 0.0, "avg_logprob": -0.2960906255812872, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.12084463983774185}, {"id": 815, "seek": 398424, "start": 4011.12, "end": 4012.4799999999996, "text": " the rating.", "tokens": [51708, 264, 10990, 13, 51776], "temperature": 0.0, "avg_logprob": -0.2960906255812872, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.12084463983774185}, {"id": 816, "seek": 401248, "start": 4012.52, "end": 4021.2400000000002, "text": " If we then use the gradient descent solver in Excel to solve that, it attempts to modify", "tokens": [50366, 759, 321, 550, 764, 264, 16235, 23475, 1404, 331, 294, 19060, 281, 5039, 300, 11, 309, 15257, 281, 16927, 50802], "temperature": 0.0, "avg_logprob": -0.28316403817439423, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0035380583722144365}, {"id": 817, "seek": 401248, "start": 4021.2400000000002, "end": 4030.48, "text": " the two embedding matrices, and as you can see the objective is going down, to try and", "tokens": [50802, 264, 732, 12240, 3584, 32284, 11, 293, 382, 291, 393, 536, 264, 10024, 307, 516, 760, 11, 281, 853, 293, 51264], "temperature": 0.0, "avg_logprob": -0.28316403817439423, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0035380583722144365}, {"id": 818, "seek": 401248, "start": 4030.48, "end": 4036.48, "text": " come up with the two embedding matrices which give us the best approximation of the original", "tokens": [51264, 808, 493, 365, 264, 732, 12240, 3584, 32284, 597, 976, 505, 264, 1151, 28023, 295, 264, 3380, 51564], "temperature": 0.0, "avg_logprob": -0.28316403817439423, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0035380583722144365}, {"id": 819, "seek": 401248, "start": 4036.48, "end": 4038.72, "text": " rating matrix.", "tokens": [51564, 10990, 8141, 13, 51676], "temperature": 0.0, "avg_logprob": -0.28316403817439423, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0035380583722144365}, {"id": 820, "seek": 403872, "start": 4038.7599999999998, "end": 4047.3199999999997, "text": " So this Excel spreadsheet is something which you can play with and do exactly what our", "tokens": [50366, 407, 341, 19060, 27733, 307, 746, 597, 291, 393, 862, 365, 293, 360, 2293, 437, 527, 50794], "temperature": 0.0, "avg_logprob": -0.3004207992553711, "compression_ratio": 1.3866666666666667, "no_speech_prob": 0.015188510529696941}, {"id": 821, "seek": 403872, "start": 4047.3199999999997, "end": 4053.48, "text": " first MovieLens example is doing in Python.", "tokens": [50794, 700, 28766, 43, 694, 1365, 307, 884, 294, 15329, 13, 51102], "temperature": 0.0, "avg_logprob": -0.3004207992553711, "compression_ratio": 1.3866666666666667, "no_speech_prob": 0.015188510529696941}, {"id": 822, "seek": 403872, "start": 4053.48, "end": 4065.8199999999997, "text": " The only difference is that our version in Python also has L2 regularization.", "tokens": [51102, 440, 787, 2649, 307, 300, 527, 3037, 294, 15329, 611, 575, 441, 17, 3890, 2144, 13, 51719], "temperature": 0.0, "avg_logprob": -0.3004207992553711, "compression_ratio": 1.3866666666666667, "no_speech_prob": 0.015188510529696941}, {"id": 823, "seek": 406582, "start": 4065.98, "end": 4070.1800000000003, "text": " This one's just finished here, so you can see it's come up with, these are no longer", "tokens": [50372, 639, 472, 311, 445, 4335, 510, 11, 370, 291, 393, 536, 309, 311, 808, 493, 365, 11, 613, 366, 572, 2854, 50582], "temperature": 0.0, "avg_logprob": -0.32732874033402426, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0488533079624176}, {"id": 824, "seek": 406582, "start": 4070.1800000000003, "end": 4074.94, "text": " random, we've now got two embedding matrices which have got the loss function down from", "tokens": [50582, 4974, 11, 321, 600, 586, 658, 732, 12240, 3584, 32284, 597, 362, 658, 264, 4470, 2445, 760, 490, 50820], "temperature": 0.0, "avg_logprob": -0.32732874033402426, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0488533079624176}, {"id": 825, "seek": 406582, "start": 4074.94, "end": 4077.1400000000003, "text": " 40 to 5.6.", "tokens": [50820, 3356, 281, 1025, 13, 21, 13, 50930], "temperature": 0.0, "avg_logprob": -0.32732874033402426, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0488533079624176}, {"id": 826, "seek": 406582, "start": 4077.1400000000003, "end": 4082.54, "text": " So you can see, for example, these ratings are now very close to what they're meant to", "tokens": [50930, 407, 291, 393, 536, 11, 337, 1365, 11, 613, 24603, 366, 586, 588, 1998, 281, 437, 436, 434, 4140, 281, 51200], "temperature": 0.0, "avg_logprob": -0.32732874033402426, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0488533079624176}, {"id": 827, "seek": 406582, "start": 4082.54, "end": 4083.54, "text": " be.", "tokens": [51200, 312, 13, 51250], "temperature": 0.0, "avg_logprob": -0.32732874033402426, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0488533079624176}, {"id": 828, "seek": 406582, "start": 4083.54, "end": 4088.1400000000003, "text": " So this is exactly what Keras and SGD are doing in our Python example.", "tokens": [51250, 407, 341, 307, 2293, 437, 591, 6985, 293, 34520, 35, 366, 884, 294, 527, 15329, 1365, 13, 51480], "temperature": 0.0, "avg_logprob": -0.32732874033402426, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.0488533079624176}, {"id": 829, "seek": 408814, "start": 4088.46, "end": 4098.46, "text": " So my question is, is it that we got an embedding in which each word is a vector of 32 elements?", "tokens": [50380, 407, 452, 1168, 307, 11, 307, 309, 300, 321, 658, 364, 12240, 3584, 294, 597, 1184, 1349, 307, 257, 8062, 295, 8858, 4959, 30, 50880], "temperature": 0.0, "avg_logprob": -0.3388439780787418, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.1685234010219574}, {"id": 830, "seek": 408814, "start": 4098.46, "end": 4100.46, "text": " It's more clear that way, no?", "tokens": [50880, 467, 311, 544, 1850, 300, 636, 11, 572, 30, 50980], "temperature": 0.0, "avg_logprob": -0.3388439780787418, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.1685234010219574}, {"id": 831, "seek": 408814, "start": 4100.46, "end": 4101.46, "text": " Yes, exactly.", "tokens": [50980, 1079, 11, 2293, 13, 51030], "temperature": 0.0, "avg_logprob": -0.3388439780787418, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.1685234010219574}, {"id": 832, "seek": 408814, "start": 4101.46, "end": 4107.46, "text": " Each word in our vocabulary of 5,000 is being converted into a vector of 32 elements.", "tokens": [51030, 6947, 1349, 294, 527, 19864, 295, 1025, 11, 1360, 307, 885, 16424, 666, 257, 8062, 295, 8858, 4959, 13, 51330], "temperature": 0.0, "avg_logprob": -0.3388439780787418, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.1685234010219574}, {"id": 833, "seek": 408814, "start": 4107.46, "end": 4109.46, "text": " Exactly right.", "tokens": [51330, 7587, 558, 13, 51430], "temperature": 0.0, "avg_logprob": -0.3388439780787418, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.1685234010219574}, {"id": 834, "seek": 408814, "start": 4109.46, "end": 4115.46, "text": " Another question is, what would be the equivalent dense network if we didn't use a 2D embedding?", "tokens": [51430, 3996, 1168, 307, 11, 437, 576, 312, 264, 10344, 18011, 3209, 498, 321, 994, 380, 764, 257, 568, 35, 12240, 3584, 30, 51730], "temperature": 0.0, "avg_logprob": -0.3388439780787418, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.1685234010219574}, {"id": 835, "seek": 411546, "start": 4115.78, "end": 4118.78, "text": " This is in the initial model, the simple one.", "tokens": [50380, 639, 307, 294, 264, 5883, 2316, 11, 264, 2199, 472, 13, 50530], "temperature": 0.0, "avg_logprob": -0.31757288803288963, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.023658689111471176}, {"id": 836, "seek": 411546, "start": 4118.78, "end": 4124.78, "text": " A dense layer with input of size embedding size of half size?", "tokens": [50530, 316, 18011, 4583, 365, 4846, 295, 2744, 12240, 3584, 2744, 295, 1922, 2744, 30, 50830], "temperature": 0.0, "avg_logprob": -0.31757288803288963, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.023658689111471176}, {"id": 837, "seek": 411546, "start": 4124.78, "end": 4129.78, "text": " I actually don't know what that means, sorry.", "tokens": [50830, 286, 767, 500, 380, 458, 437, 300, 1355, 11, 2597, 13, 51080], "temperature": 0.0, "avg_logprob": -0.31757288803288963, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.023658689111471176}, {"id": 838, "seek": 411546, "start": 4129.78, "end": 4137.78, "text": " Next question is, does it matter that encoded values which are close by are close in color", "tokens": [51080, 3087, 1168, 307, 11, 775, 309, 1871, 300, 2058, 12340, 4190, 597, 366, 1998, 538, 366, 1998, 294, 2017, 51480], "temperature": 0.0, "avg_logprob": -0.31757288803288963, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.023658689111471176}, {"id": 839, "seek": 411546, "start": 4137.78, "end": 4140.78, "text": " in the case of pictures, which is not true for word vectors?", "tokens": [51480, 294, 264, 1389, 295, 5242, 11, 597, 307, 406, 2074, 337, 1349, 18875, 30, 51630], "temperature": 0.0, "avg_logprob": -0.31757288803288963, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.023658689111471176}, {"id": 840, "seek": 414078, "start": 4141.099999999999, "end": 4150.099999999999, "text": " For example, 254 and 255 are close as colors, but for words they have no relation.", "tokens": [50380, 1171, 1365, 11, 3552, 19, 293, 3552, 20, 366, 1998, 382, 4577, 11, 457, 337, 2283, 436, 362, 572, 9721, 13, 50830], "temperature": 0.0, "avg_logprob": -0.24207487451024803, "compression_ratio": 1.56, "no_speech_prob": 0.05920056253671646}, {"id": 841, "seek": 414078, "start": 4150.099999999999, "end": 4156.42, "text": " The important thing to realize is that the word IDs are not used mathematically in any", "tokens": [50830, 440, 1021, 551, 281, 4325, 307, 300, 264, 1349, 48212, 366, 406, 1143, 44003, 294, 604, 51146], "temperature": 0.0, "avg_logprob": -0.24207487451024803, "compression_ratio": 1.56, "no_speech_prob": 0.05920056253671646}, {"id": 842, "seek": 414078, "start": 4156.42, "end": 4160.74, "text": " way at all, other than as an index to look up into an integer.", "tokens": [51146, 636, 412, 439, 11, 661, 813, 382, 364, 8186, 281, 574, 493, 666, 364, 24922, 13, 51362], "temperature": 0.0, "avg_logprob": -0.24207487451024803, "compression_ratio": 1.56, "no_speech_prob": 0.05920056253671646}, {"id": 843, "seek": 414078, "start": 4160.74, "end": 4166.179999999999, "text": " So the fact that this is movie number 27, the number 27 is not used in any way.", "tokens": [51362, 407, 264, 1186, 300, 341, 307, 3169, 1230, 7634, 11, 264, 1230, 7634, 307, 406, 1143, 294, 604, 636, 13, 51634], "temperature": 0.0, "avg_logprob": -0.24207487451024803, "compression_ratio": 1.56, "no_speech_prob": 0.05920056253671646}, {"id": 844, "seek": 416618, "start": 4166.18, "end": 4171.06, "text": " We just take the number 27 and find its vector.", "tokens": [50364, 492, 445, 747, 264, 1230, 7634, 293, 915, 1080, 8062, 13, 50608], "temperature": 0.0, "avg_logprob": -0.26287804047266644, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.11278203874826431}, {"id": 845, "seek": 416618, "start": 4171.06, "end": 4176.1, "text": " So what's important is the values of each latent factor as to whether they're close", "tokens": [50608, 407, 437, 311, 1021, 307, 264, 4190, 295, 1184, 48994, 5952, 382, 281, 1968, 436, 434, 1998, 50860], "temperature": 0.0, "avg_logprob": -0.26287804047266644, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.11278203874826431}, {"id": 846, "seek": 416618, "start": 4176.1, "end": 4177.1, "text": " together.", "tokens": [50860, 1214, 13, 50910], "temperature": 0.0, "avg_logprob": -0.26287804047266644, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.11278203874826431}, {"id": 847, "seek": 416618, "start": 4177.1, "end": 4180.54, "text": " So in the movie example, there were some latent factors that were something about is it a", "tokens": [50910, 407, 294, 264, 3169, 1365, 11, 456, 645, 512, 48994, 6771, 300, 645, 746, 466, 307, 309, 257, 51082], "temperature": 0.0, "avg_logprob": -0.26287804047266644, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.11278203874826431}, {"id": 848, "seek": 416618, "start": 4180.54, "end": 4185.54, "text": " Hollywood blockbuster, and there were some latent factors that were something about is", "tokens": [51082, 11628, 3461, 41148, 11, 293, 456, 645, 512, 48994, 6771, 300, 645, 746, 466, 307, 51332], "temperature": 0.0, "avg_logprob": -0.26287804047266644, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.11278203874826431}, {"id": 849, "seek": 416618, "start": 4185.54, "end": 4188.1, "text": " it a violent movie or not.", "tokens": [51332, 309, 257, 11867, 3169, 420, 406, 13, 51460], "temperature": 0.0, "avg_logprob": -0.26287804047266644, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.11278203874826431}, {"id": 850, "seek": 416618, "start": 4188.1, "end": 4191.820000000001, "text": " It's the similarity on those factors that matters.", "tokens": [51460, 467, 311, 264, 32194, 322, 729, 6771, 300, 7001, 13, 51646], "temperature": 0.0, "avg_logprob": -0.26287804047266644, "compression_ratio": 1.8857142857142857, "no_speech_prob": 0.11278203874826431}, {"id": 851, "seek": 419182, "start": 4191.82, "end": 4198.98, "text": " The ID is never ever used, other than as an index to simply index into a matrix to", "tokens": [50364, 440, 7348, 307, 1128, 1562, 1143, 11, 661, 813, 382, 364, 8186, 281, 2935, 8186, 666, 257, 8141, 281, 50722], "temperature": 0.0, "avg_logprob": -0.27600694314027446, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04885533079504967}, {"id": 852, "seek": 419182, "start": 4198.98, "end": 4201.099999999999, "text": " return the vector that we found.", "tokens": [50722, 2736, 264, 8062, 300, 321, 1352, 13, 50828], "temperature": 0.0, "avg_logprob": -0.27600694314027446, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04885533079504967}, {"id": 853, "seek": 419182, "start": 4201.099999999999, "end": 4206.66, "text": " So as Yannet was mentioning, in our case now for the word embeddings, we're looking up", "tokens": [50828, 407, 382, 398, 969, 302, 390, 18315, 11, 294, 527, 1389, 586, 337, 264, 1349, 12240, 29432, 11, 321, 434, 1237, 493, 51106], "temperature": 0.0, "avg_logprob": -0.27600694314027446, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04885533079504967}, {"id": 854, "seek": 419182, "start": 4206.66, "end": 4215.62, "text": " in our embeddings to return a 32-element vector of floats that are initially random and the", "tokens": [51106, 294, 527, 12240, 29432, 281, 2736, 257, 8858, 12, 68, 3054, 8062, 295, 37878, 300, 366, 9105, 4974, 293, 264, 51554], "temperature": 0.0, "avg_logprob": -0.27600694314027446, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04885533079504967}, {"id": 855, "seek": 421562, "start": 4215.62, "end": 4224.5, "text": " model is trying to learn the 32 floats for each of our words that is semantically useful.", "tokens": [50364, 2316, 307, 1382, 281, 1466, 264, 8858, 37878, 337, 1184, 295, 527, 2283, 300, 307, 4361, 49505, 4420, 13, 50808], "temperature": 0.0, "avg_logprob": -0.42258019142962516, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.3593260645866394}, {"id": 856, "seek": 421562, "start": 4224.5, "end": 4228.38, "text": " In a moment, we're going to look at some visualizations of that to try and understand what it's actually", "tokens": [50808, 682, 257, 1623, 11, 321, 434, 516, 281, 574, 412, 512, 5056, 14455, 295, 300, 281, 853, 293, 1223, 437, 309, 311, 767, 51002], "temperature": 0.0, "avg_logprob": -0.42258019142962516, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.3593260645866394}, {"id": 857, "seek": 421562, "start": 4228.38, "end": 4229.38, "text": " learned.", "tokens": [51002, 3264, 13, 51052], "temperature": 0.0, "avg_logprob": -0.42258019142962516, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.3593260645866394}, {"id": 858, "seek": 421562, "start": 4229.38, "end": 4235.62, "text": " Question, What is the significance of the dropout on the embedding as well as on the", "tokens": [51052, 14464, 11, 708, 307, 264, 17687, 295, 264, 3270, 346, 322, 264, 12240, 3584, 382, 731, 382, 322, 264, 51364], "temperature": 0.0, "avg_logprob": -0.42258019142962516, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.3593260645866394}, {"id": 859, "seek": 421562, "start": 4235.62, "end": 4236.62, "text": " next layer?", "tokens": [51364, 958, 4583, 30, 51414], "temperature": 0.0, "avg_logprob": -0.42258019142962516, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.3593260645866394}, {"id": 860, "seek": 421562, "start": 4236.62, "end": 4241.62, "text": " So what is the significance and what is the difference between the two?", "tokens": [51414, 407, 437, 307, 264, 17687, 293, 437, 307, 264, 2649, 1296, 264, 732, 30, 51664], "temperature": 0.0, "avg_logprob": -0.42258019142962516, "compression_ratio": 1.6533333333333333, "no_speech_prob": 0.3593260645866394}, {"id": 861, "seek": 424162, "start": 4242.62, "end": 4249.74, "text": " So, you can apply the dropout parameter to the embedding layer itself.", "tokens": [50414, 407, 11, 291, 393, 3079, 264, 3270, 346, 13075, 281, 264, 12240, 3584, 4583, 2564, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2596385262229226, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.0013250164920464158}, {"id": 862, "seek": 424162, "start": 4249.74, "end": 4259.22, "text": " What that does is it zeroes out at random 20% of each of these 32 embeddings for each", "tokens": [50770, 708, 300, 775, 307, 309, 4018, 279, 484, 412, 4974, 945, 4, 295, 1184, 295, 613, 8858, 12240, 29432, 337, 1184, 51244], "temperature": 0.0, "avg_logprob": -0.2596385262229226, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.0013250164920464158}, {"id": 863, "seek": 424162, "start": 4259.22, "end": 4260.22, "text": " word.", "tokens": [51244, 1349, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2596385262229226, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.0013250164920464158}, {"id": 864, "seek": 424162, "start": 4260.22, "end": 4266.099999999999, "text": " So it's basically avoiding overfitting the specifics of each word's embedding.", "tokens": [51294, 407, 309, 311, 1936, 20220, 670, 69, 2414, 264, 28454, 295, 1184, 1349, 311, 12240, 3584, 13, 51588], "temperature": 0.0, "avg_logprob": -0.2596385262229226, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.0013250164920464158}, {"id": 865, "seek": 426610, "start": 4266.1, "end": 4273.4400000000005, "text": " This dropout, on the other hand, is removing at random some of the words, effectively some", "tokens": [50364, 639, 3270, 346, 11, 322, 264, 661, 1011, 11, 307, 12720, 412, 4974, 512, 295, 264, 2283, 11, 8659, 512, 50731], "temperature": 0.0, "avg_logprob": -0.27119115193684895, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.07695885002613068}, {"id": 866, "seek": 426610, "start": 4273.4400000000005, "end": 4276.14, "text": " of the whole vectors.", "tokens": [50731, 295, 264, 1379, 18875, 13, 50866], "temperature": 0.0, "avg_logprob": -0.27119115193684895, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.07695885002613068}, {"id": 867, "seek": 426610, "start": 4276.14, "end": 4283.58, "text": " The significance of which one to use where is not something which I've seen anybody research", "tokens": [50866, 440, 17687, 295, 597, 472, 281, 764, 689, 307, 406, 746, 597, 286, 600, 1612, 4472, 2132, 51238], "temperature": 0.0, "avg_logprob": -0.27119115193684895, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.07695885002613068}, {"id": 868, "seek": 426610, "start": 4283.58, "end": 4290.9400000000005, "text": " in-depth, so I'm not sure that we have an answer that says use this amount in this place.", "tokens": [51238, 294, 12, 25478, 11, 370, 286, 478, 406, 988, 300, 321, 362, 364, 1867, 300, 1619, 764, 341, 2372, 294, 341, 1081, 13, 51606], "temperature": 0.0, "avg_logprob": -0.27119115193684895, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.07695885002613068}, {"id": 869, "seek": 429094, "start": 4290.94, "end": 4296.339999999999, "text": " I just tried a few different values in different places and it seems that putting the same", "tokens": [50364, 286, 445, 3031, 257, 1326, 819, 4190, 294, 819, 3190, 293, 309, 2544, 300, 3372, 264, 912, 50634], "temperature": 0.0, "avg_logprob": -0.2659694767799698, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.14800582826137543}, {"id": 870, "seek": 429094, "start": 4296.339999999999, "end": 4300.66, "text": " amount of dropout in all these different spots seems to work pretty well in my experiments,", "tokens": [50634, 2372, 295, 3270, 346, 294, 439, 613, 819, 10681, 2544, 281, 589, 1238, 731, 294, 452, 12050, 11, 50850], "temperature": 0.0, "avg_logprob": -0.2659694767799698, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.14800582826137543}, {"id": 871, "seek": 429094, "start": 4300.66, "end": 4304.12, "text": " so it's a reasonable rule of thumb.", "tokens": [50850, 370, 309, 311, 257, 10585, 4978, 295, 9298, 13, 51023], "temperature": 0.0, "avg_logprob": -0.2659694767799698, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.14800582826137543}, {"id": 872, "seek": 429094, "start": 4304.12, "end": 4308.94, "text": " If you find you're massively overfitting or massively underfitting, try playing around", "tokens": [51023, 759, 291, 915, 291, 434, 29379, 670, 69, 2414, 420, 29379, 833, 69, 2414, 11, 853, 2433, 926, 51264], "temperature": 0.0, "avg_logprob": -0.2659694767799698, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.14800582826137543}, {"id": 873, "seek": 429094, "start": 4308.94, "end": 4313.099999999999, "text": " with the various values and report back on the forum and tell us what you find.", "tokens": [51264, 365, 264, 3683, 4190, 293, 2275, 646, 322, 264, 17542, 293, 980, 505, 437, 291, 915, 13, 51472], "temperature": 0.0, "avg_logprob": -0.2659694767799698, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.14800582826137543}, {"id": 874, "seek": 429094, "start": 4313.099999999999, "end": 4316.46, "text": " Maybe you'll find some different, better configurations than I've come up with.", "tokens": [51472, 2704, 291, 603, 915, 512, 819, 11, 1101, 31493, 813, 286, 600, 808, 493, 365, 13, 51640], "temperature": 0.0, "avg_logprob": -0.2659694767799698, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.14800582826137543}, {"id": 875, "seek": 429094, "start": 4316.46, "end": 4320.299999999999, "text": " I'm sure some of you will.", "tokens": [51640, 286, 478, 988, 512, 295, 291, 486, 13, 51832], "temperature": 0.0, "avg_logprob": -0.2659694767799698, "compression_ratio": 1.7508896797153024, "no_speech_prob": 0.14800582826137543}, {"id": 876, "seek": 432030, "start": 4320.66, "end": 4323.02, "text": " Great question.", "tokens": [50382, 3769, 1168, 13, 50500], "temperature": 0.0, "avg_logprob": -0.3153028739126105, "compression_ratio": 1.485, "no_speech_prob": 0.004982230253517628}, {"id": 877, "seek": 432030, "start": 4323.02, "end": 4328.860000000001, "text": " So let's think about what's going on here.", "tokens": [50500, 407, 718, 311, 519, 466, 437, 311, 516, 322, 510, 13, 50792], "temperature": 0.0, "avg_logprob": -0.3153028739126105, "compression_ratio": 1.485, "no_speech_prob": 0.004982230253517628}, {"id": 878, "seek": 432030, "start": 4328.860000000001, "end": 4334.06, "text": " We are taking each of our 5000 words in our vocabulary and we're replacing them with a", "tokens": [50792, 492, 366, 1940, 1184, 295, 527, 23777, 2283, 294, 527, 19864, 293, 321, 434, 19139, 552, 365, 257, 51052], "temperature": 0.0, "avg_logprob": -0.3153028739126105, "compression_ratio": 1.485, "no_speech_prob": 0.004982230253517628}, {"id": 879, "seek": 432030, "start": 4334.06, "end": 4341.74, "text": " 32-element long vector, which we are training to hopefully capture all of the information", "tokens": [51052, 8858, 12, 68, 3054, 938, 8062, 11, 597, 321, 366, 3097, 281, 4696, 7983, 439, 295, 264, 1589, 51436], "temperature": 0.0, "avg_logprob": -0.3153028739126105, "compression_ratio": 1.485, "no_speech_prob": 0.004982230253517628}, {"id": 880, "seek": 432030, "start": 4341.74, "end": 4346.900000000001, "text": " about what this word means and what it does and how it works.", "tokens": [51436, 466, 437, 341, 1349, 1355, 293, 437, 309, 775, 293, 577, 309, 1985, 13, 51694], "temperature": 0.0, "avg_logprob": -0.3153028739126105, "compression_ratio": 1.485, "no_speech_prob": 0.004982230253517628}, {"id": 881, "seek": 434690, "start": 4346.9, "end": 4351.74, "text": " We might expect intuitively that somebody might have done this before.", "tokens": [50364, 492, 1062, 2066, 46506, 300, 2618, 1062, 362, 1096, 341, 949, 13, 50606], "temperature": 0.0, "avg_logprob": -0.2520191743201816, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.06187417358160019}, {"id": 882, "seek": 434690, "start": 4351.74, "end": 4357.94, "text": " Just like with ImageNet and VGG, you can get a pre-trained network that says, oh, if you've", "tokens": [50606, 1449, 411, 365, 29903, 31890, 293, 691, 27561, 11, 291, 393, 483, 257, 659, 12, 17227, 2001, 3209, 300, 1619, 11, 1954, 11, 498, 291, 600, 50916], "temperature": 0.0, "avg_logprob": -0.2520191743201816, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.06187417358160019}, {"id": 883, "seek": 434690, "start": 4357.94, "end": 4363.259999999999, "text": " got an image that looks a bit like a dog, well we've trained a network which has seen", "tokens": [50916, 658, 364, 3256, 300, 1542, 257, 857, 411, 257, 3000, 11, 731, 321, 600, 8895, 257, 3209, 597, 575, 1612, 51182], "temperature": 0.0, "avg_logprob": -0.2520191743201816, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.06187417358160019}, {"id": 884, "seek": 434690, "start": 4363.259999999999, "end": 4370.36, "text": " lots of dogs and so it will probably take your dog image and return some useful predictions", "tokens": [51182, 3195, 295, 7197, 293, 370, 309, 486, 1391, 747, 428, 3000, 3256, 293, 2736, 512, 4420, 21264, 51537], "temperature": 0.0, "avg_logprob": -0.2520191743201816, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.06187417358160019}, {"id": 885, "seek": 434690, "start": 4370.36, "end": 4373.679999999999, "text": " because we've done lots of dog images before.", "tokens": [51537, 570, 321, 600, 1096, 3195, 295, 3000, 5267, 949, 13, 51703], "temperature": 0.0, "avg_logprob": -0.2520191743201816, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.06187417358160019}, {"id": 886, "seek": 437368, "start": 4373.68, "end": 4378.96, "text": " The interesting thing here is your dog picture and the VGG author's dog pictures are not", "tokens": [50364, 440, 1880, 551, 510, 307, 428, 3000, 3036, 293, 264, 691, 27561, 3793, 311, 3000, 5242, 366, 406, 50628], "temperature": 0.0, "avg_logprob": -0.25632994515555246, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0033244022633880377}, {"id": 887, "seek": 437368, "start": 4378.96, "end": 4379.96, "text": " the same.", "tokens": [50628, 264, 912, 13, 50678], "temperature": 0.0, "avg_logprob": -0.25632994515555246, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0033244022633880377}, {"id": 888, "seek": 437368, "start": 4379.96, "end": 4383.4400000000005, "text": " They are going to be different in all kinds of ways.", "tokens": [50678, 814, 366, 516, 281, 312, 819, 294, 439, 3685, 295, 2098, 13, 50852], "temperature": 0.0, "avg_logprob": -0.25632994515555246, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0033244022633880377}, {"id": 889, "seek": 437368, "start": 4383.4400000000005, "end": 4389.200000000001, "text": " And so to get pre-trained weights for images, you have to give somebody a whole pre-trained", "tokens": [50852, 400, 370, 281, 483, 659, 12, 17227, 2001, 17443, 337, 5267, 11, 291, 362, 281, 976, 2618, 257, 1379, 659, 12, 17227, 2001, 51140], "temperature": 0.0, "avg_logprob": -0.25632994515555246, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0033244022633880377}, {"id": 890, "seek": 437368, "start": 4389.200000000001, "end": 4396.0, "text": " network, which is like 500 megabytes worth of weights in a whole architecture.", "tokens": [51140, 3209, 11, 597, 307, 411, 5923, 10816, 24538, 3163, 295, 17443, 294, 257, 1379, 9482, 13, 51480], "temperature": 0.0, "avg_logprob": -0.25632994515555246, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0033244022633880377}, {"id": 891, "seek": 437368, "start": 4396.0, "end": 4397.76, "text": " Words are much easier.", "tokens": [51480, 32857, 366, 709, 3571, 13, 51568], "temperature": 0.0, "avg_logprob": -0.25632994515555246, "compression_ratio": 1.5610859728506787, "no_speech_prob": 0.0033244022633880377}, {"id": 892, "seek": 439776, "start": 4397.76, "end": 4403.76, "text": " In a document, the word dog always appears the same way.", "tokens": [50364, 682, 257, 4166, 11, 264, 1349, 3000, 1009, 7038, 264, 912, 636, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22605165481567382, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.06278759986162186}, {"id": 893, "seek": 439776, "start": 4403.76, "end": 4404.76, "text": " It's the word dog.", "tokens": [50664, 467, 311, 264, 1349, 3000, 13, 50714], "temperature": 0.0, "avg_logprob": -0.22605165481567382, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.06278759986162186}, {"id": 894, "seek": 439776, "start": 4404.76, "end": 4409.16, "text": " It doesn't have different lighting conditions or facial expressions or whatever, it's just", "tokens": [50714, 467, 1177, 380, 362, 819, 9577, 4487, 420, 15642, 15277, 420, 2035, 11, 309, 311, 445, 50934], "temperature": 0.0, "avg_logprob": -0.22605165481567382, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.06278759986162186}, {"id": 895, "seek": 439776, "start": 4409.16, "end": 4410.780000000001, "text": " the word dog.", "tokens": [50934, 264, 1349, 3000, 13, 51015], "temperature": 0.0, "avg_logprob": -0.22605165481567382, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.06278759986162186}, {"id": 896, "seek": 439776, "start": 4410.780000000001, "end": 4416.76, "text": " So the cool thing is in NLP, we don't have to pass around pre-trained networks, we can", "tokens": [51015, 407, 264, 1627, 551, 307, 294, 426, 45196, 11, 321, 500, 380, 362, 281, 1320, 926, 659, 12, 17227, 2001, 9590, 11, 321, 393, 51314], "temperature": 0.0, "avg_logprob": -0.22605165481567382, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.06278759986162186}, {"id": 897, "seek": 439776, "start": 4416.76, "end": 4423.4800000000005, "text": " pass around pre-trained embeddings, or as they're commonly known, pre-trained word vectors.", "tokens": [51314, 1320, 926, 659, 12, 17227, 2001, 12240, 29432, 11, 420, 382, 436, 434, 12719, 2570, 11, 659, 12, 17227, 2001, 1349, 18875, 13, 51650], "temperature": 0.0, "avg_logprob": -0.22605165481567382, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.06278759986162186}, {"id": 898, "seek": 442348, "start": 4423.48, "end": 4429.32, "text": " That is to say, other people have already created big models with big text corpuses", "tokens": [50364, 663, 307, 281, 584, 11, 661, 561, 362, 1217, 2942, 955, 5245, 365, 955, 2487, 1181, 79, 8355, 50656], "temperature": 0.0, "avg_logprob": -0.2771576380325576, "compression_ratio": 1.441860465116279, "no_speech_prob": 0.04813545569777489}, {"id": 899, "seek": 442348, "start": 4429.32, "end": 4437.12, "text": " where they've attempted to build a 32-element vector or however long vector which captures", "tokens": [50656, 689, 436, 600, 18997, 281, 1322, 257, 8858, 12, 68, 3054, 8062, 420, 4461, 938, 8062, 597, 27986, 51046], "temperature": 0.0, "avg_logprob": -0.2771576380325576, "compression_ratio": 1.441860465116279, "no_speech_prob": 0.04813545569777489}, {"id": 900, "seek": 442348, "start": 4437.12, "end": 4442.24, "text": " all of the useful information about what that word is and how it behaves.", "tokens": [51046, 439, 295, 264, 4420, 1589, 466, 437, 300, 1349, 307, 293, 577, 309, 36896, 13, 51302], "temperature": 0.0, "avg_logprob": -0.2771576380325576, "compression_ratio": 1.441860465116279, "no_speech_prob": 0.04813545569777489}, {"id": 901, "seek": 444224, "start": 4442.24, "end": 4464.44, "text": " So for example, if we type in word vector download, you can see that lots of questions", "tokens": [50364, 407, 337, 1365, 11, 498, 321, 2010, 294, 1349, 8062, 5484, 11, 291, 393, 536, 300, 3195, 295, 1651, 51474], "temperature": 0.0, "avg_logprob": -0.2924126127491827, "compression_ratio": 1.0886075949367089, "no_speech_prob": 0.03161797299981117}, {"id": 902, "seek": 446444, "start": 4464.44, "end": 4471.44, "text": " and answers and pages about where we can download pre-trained word embeddings.", "tokens": [50364, 293, 6338, 293, 7183, 466, 689, 321, 393, 5484, 659, 12, 17227, 2001, 1349, 12240, 29432, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5469769069126674, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.19680538773536682}, {"id": 903, "seek": 447144, "start": 4471.44, "end": 4487.679999999999, "text": " That's pretty cool, but I guess what is a little unintuitive to me is that I think this", "tokens": [50364, 663, 311, 1238, 1627, 11, 457, 286, 2041, 437, 307, 257, 707, 29466, 48314, 281, 385, 307, 300, 286, 519, 341, 51176], "temperature": 0.0, "avg_logprob": -0.2901295444421601, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.0048281727358698845}, {"id": 904, "seek": 447144, "start": 4487.679999999999, "end": 4492.799999999999, "text": " means that if I can train a corpus on the works of Shakespeare, somehow that tells me", "tokens": [51176, 1355, 300, 498, 286, 393, 3847, 257, 1181, 31624, 322, 264, 1985, 295, 22825, 11, 6063, 300, 5112, 385, 51432], "temperature": 0.0, "avg_logprob": -0.2901295444421601, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.0048281727358698845}, {"id": 905, "seek": 447144, "start": 4492.799999999999, "end": 4498.32, "text": " something about how I can understand movie reviews.", "tokens": [51432, 746, 466, 577, 286, 393, 1223, 3169, 10229, 13, 51708], "temperature": 0.0, "avg_logprob": -0.2901295444421601, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.0048281727358698845}, {"id": 906, "seek": 449832, "start": 4498.32, "end": 4503.36, "text": " I imagine that in some sense that's true about how language is structured and whatnot,", "tokens": [50364, 286, 3811, 300, 294, 512, 2020, 300, 311, 2074, 466, 577, 2856, 307, 18519, 293, 25882, 11, 50616], "temperature": 0.0, "avg_logprob": -0.3637083749922495, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.6547534465789795}, {"id": 907, "seek": 449832, "start": 4503.36, "end": 4520.88, "text": " but the meaning of the word dog in Shakespeare is probably going to be used pretty differently.", "tokens": [50616, 457, 264, 3620, 295, 264, 1349, 3000, 294, 22825, 307, 1391, 516, 281, 312, 1143, 1238, 7614, 13, 51492], "temperature": 0.0, "avg_logprob": -0.3637083749922495, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.6547534465789795}, {"id": 908, "seek": 449832, "start": 4520.88, "end": 4526.24, "text": " The word vectors that I'm going to be using, and I don't strongly recommend, but slightly", "tokens": [51492, 440, 1349, 18875, 300, 286, 478, 516, 281, 312, 1228, 11, 293, 286, 500, 380, 10613, 2748, 11, 457, 4748, 51760], "temperature": 0.0, "avg_logprob": -0.3637083749922495, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.6547534465789795}, {"id": 909, "seek": 452624, "start": 4526.24, "end": 4529.2, "text": " recommend are the GloVe word vectors.", "tokens": [50364, 2748, 366, 264, 10786, 53, 68, 1349, 18875, 13, 50512], "temperature": 0.0, "avg_logprob": -0.2500236602056594, "compression_ratio": 1.7819148936170213, "no_speech_prob": 0.04208340123295784}, {"id": 910, "seek": 452624, "start": 4529.2, "end": 4534.48, "text": " The other main competition to these is called the Word2Vec word vectors.", "tokens": [50512, 440, 661, 2135, 6211, 281, 613, 307, 1219, 264, 8725, 17, 53, 3045, 1349, 18875, 13, 50776], "temperature": 0.0, "avg_logprob": -0.2500236602056594, "compression_ratio": 1.7819148936170213, "no_speech_prob": 0.04208340123295784}, {"id": 911, "seek": 452624, "start": 4534.48, "end": 4539.08, "text": " The GloVe word vectors come from a researcher named Jeffrey Pennington from Stanford.", "tokens": [50776, 440, 10786, 53, 68, 1349, 18875, 808, 490, 257, 21751, 4926, 28721, 10571, 773, 1756, 490, 20374, 13, 51006], "temperature": 0.0, "avg_logprob": -0.2500236602056594, "compression_ratio": 1.7819148936170213, "no_speech_prob": 0.04208340123295784}, {"id": 912, "seek": 452624, "start": 4539.08, "end": 4544.679999999999, "text": " The Word2Vec word vectors come from Google.", "tokens": [51006, 440, 8725, 17, 53, 3045, 1349, 18875, 808, 490, 3329, 13, 51286], "temperature": 0.0, "avg_logprob": -0.2500236602056594, "compression_ratio": 1.7819148936170213, "no_speech_prob": 0.04208340123295784}, {"id": 913, "seek": 452624, "start": 4544.679999999999, "end": 4551.76, "text": " I will however mention that the TensorFlow documentation on the Word2Vec vectors is fantastic.", "tokens": [51286, 286, 486, 4461, 2152, 300, 264, 37624, 14333, 322, 264, 8725, 17, 53, 3045, 18875, 307, 5456, 13, 51640], "temperature": 0.0, "avg_logprob": -0.2500236602056594, "compression_ratio": 1.7819148936170213, "no_speech_prob": 0.04208340123295784}, {"id": 914, "seek": 455176, "start": 4551.76, "end": 4556.4400000000005, "text": " I would definitely highly recommend checking this out.", "tokens": [50364, 286, 576, 2138, 5405, 2748, 8568, 341, 484, 13, 50598], "temperature": 0.0, "avg_logprob": -0.2556827479395373, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.06278359889984131}, {"id": 915, "seek": 455176, "start": 4556.4400000000005, "end": 4567.280000000001, "text": " The GloVe word vectors have been pre-trained on a number of different corpuses.", "tokens": [50598, 440, 10786, 53, 68, 1349, 18875, 362, 668, 659, 12, 17227, 2001, 322, 257, 1230, 295, 819, 1181, 79, 8355, 13, 51140], "temperature": 0.0, "avg_logprob": -0.2556827479395373, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.06278359889984131}, {"id": 916, "seek": 455176, "start": 4567.280000000001, "end": 4573.400000000001, "text": " One of them has been pre-trained on all of Wikipedia and a huge database full of newspaper", "tokens": [51140, 1485, 295, 552, 575, 668, 659, 12, 17227, 2001, 322, 439, 295, 28999, 293, 257, 2603, 8149, 1577, 295, 13669, 51446], "temperature": 0.0, "avg_logprob": -0.2556827479395373, "compression_ratio": 1.4423076923076923, "no_speech_prob": 0.06278359889984131}, {"id": 917, "seek": 457340, "start": 4573.4, "end": 4581.96, "text": " articles, a total of 6 billion words covering 400,000-size vocabulary.", "tokens": [50364, 11290, 11, 257, 3217, 295, 1386, 5218, 2283, 10322, 8423, 11, 1360, 12, 27553, 19864, 13, 50792], "temperature": 0.0, "avg_logprob": -0.29217092958215163, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.11435125768184662}, {"id": 918, "seek": 457340, "start": 4581.96, "end": 4587.759999999999, "text": " They provide 50-dimensional, 100-dimensional, 200-dimensional and 300-dimensional pre-trained", "tokens": [50792, 814, 2893, 2625, 12, 18759, 11, 2319, 12, 18759, 11, 2331, 12, 18759, 293, 6641, 12, 18759, 659, 12, 17227, 2001, 51082], "temperature": 0.0, "avg_logprob": -0.29217092958215163, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.11435125768184662}, {"id": 919, "seek": 457340, "start": 4587.759999999999, "end": 4589.16, "text": " vectors.", "tokens": [51082, 18875, 13, 51152], "temperature": 0.0, "avg_logprob": -0.29217092958215163, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.11435125768184662}, {"id": 920, "seek": 457340, "start": 4589.16, "end": 4596.799999999999, "text": " They have another one which has been trained on 840 billion words of a huge dump of the", "tokens": [51152, 814, 362, 1071, 472, 597, 575, 668, 8895, 322, 1649, 5254, 5218, 2283, 295, 257, 2603, 11430, 295, 264, 51534], "temperature": 0.0, "avg_logprob": -0.29217092958215163, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.11435125768184662}, {"id": 921, "seek": 457340, "start": 4596.799999999999, "end": 4599.4, "text": " entire internet.", "tokens": [51534, 2302, 4705, 13, 51664], "temperature": 0.0, "avg_logprob": -0.29217092958215163, "compression_ratio": 1.5977011494252873, "no_speech_prob": 0.11435125768184662}, {"id": 922, "seek": 459940, "start": 4599.4, "end": 4603.599999999999, "text": " And then they have another one which has been trained on 2 billion tweets, which I", "tokens": [50364, 400, 550, 436, 362, 1071, 472, 597, 575, 668, 8895, 322, 568, 5218, 25671, 11, 597, 286, 50574], "temperature": 0.0, "avg_logprob": -0.29540694249819405, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.310624897480011}, {"id": 923, "seek": 459940, "start": 4603.599999999999, "end": 4610.54, "text": " believe all of the Donald Trump tweets have been carefully cleaned out prior to usage.", "tokens": [50574, 1697, 439, 295, 264, 8632, 3899, 25671, 362, 668, 7500, 16146, 484, 4059, 281, 14924, 13, 50921], "temperature": 0.0, "avg_logprob": -0.29540694249819405, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.310624897480011}, {"id": 924, "seek": 459940, "start": 4610.54, "end": 4618.74, "text": " So in my case, what I've done is I've downloaded the 6 billion token version and I will show", "tokens": [50921, 407, 294, 452, 1389, 11, 437, 286, 600, 1096, 307, 286, 600, 21748, 264, 1386, 5218, 14862, 3037, 293, 286, 486, 855, 51331], "temperature": 0.0, "avg_logprob": -0.29540694249819405, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.310624897480011}, {"id": 925, "seek": 459940, "start": 4618.74, "end": 4621.799999999999, "text": " you what one of these looks like.", "tokens": [51331, 291, 437, 472, 295, 613, 1542, 411, 13, 51484], "temperature": 0.0, "avg_logprob": -0.29540694249819405, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.310624897480011}, {"id": 926, "seek": 462180, "start": 4621.8, "end": 4622.8, "text": " So here is ...", "tokens": [50364, 407, 510, 307, 1097, 50414], "temperature": 0.0, "avg_logprob": -0.5247855752201404, "compression_ratio": 1.35625, "no_speech_prob": 0.0297594852745533}, {"id": 927, "seek": 462180, "start": 4622.8, "end": 4634.8, "text": " Question, Are you losing context because of capital letters, punctuation?", "tokens": [50414, 14464, 11, 2014, 291, 7027, 4319, 570, 295, 4238, 7825, 11, 27006, 16073, 30, 51014], "temperature": 0.0, "avg_logprob": -0.5247855752201404, "compression_ratio": 1.35625, "no_speech_prob": 0.0297594852745533}, {"id": 928, "seek": 462180, "start": 4634.8, "end": 4639.64, "text": " Answer, We'll look at that in a moment.", "tokens": [51014, 24545, 11, 492, 603, 574, 412, 300, 294, 257, 1623, 13, 51256], "temperature": 0.0, "avg_logprob": -0.5247855752201404, "compression_ratio": 1.35625, "no_speech_prob": 0.0297594852745533}, {"id": 929, "seek": 462180, "start": 4639.64, "end": 4643.88, "text": " Sometimes these are cased.", "tokens": [51256, 4803, 613, 366, 269, 1937, 13, 51468], "temperature": 0.0, "avg_logprob": -0.5247855752201404, "compression_ratio": 1.35625, "no_speech_prob": 0.0297594852745533}, {"id": 930, "seek": 462180, "start": 4643.88, "end": 4647.4800000000005, "text": " So you can see for example this particular one includes case.", "tokens": [51468, 407, 291, 393, 536, 337, 1365, 341, 1729, 472, 5974, 1389, 13, 51648], "temperature": 0.0, "avg_logprob": -0.5247855752201404, "compression_ratio": 1.35625, "no_speech_prob": 0.0297594852745533}, {"id": 931, "seek": 464748, "start": 4647.48, "end": 4652.12, "text": " There are 2.2 million items of vocabulary in this.", "tokens": [50364, 821, 366, 568, 13, 17, 2459, 4754, 295, 19864, 294, 341, 13, 50596], "temperature": 0.0, "avg_logprob": -0.266383311327766, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.06852680444717407}, {"id": 932, "seek": 464748, "start": 4652.12, "end": 4653.32, "text": " Sometimes they're uncased.", "tokens": [50596, 4803, 436, 434, 6219, 1937, 13, 50656], "temperature": 0.0, "avg_logprob": -0.266383311327766, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.06852680444717407}, {"id": 933, "seek": 464748, "start": 4653.32, "end": 4656.959999999999, "text": " So we'll look at punctuation in a moment.", "tokens": [50656, 407, 321, 603, 574, 412, 27006, 16073, 294, 257, 1623, 13, 50838], "temperature": 0.0, "avg_logprob": -0.266383311327766, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.06852680444717407}, {"id": 934, "seek": 464748, "start": 4656.959999999999, "end": 4664.919999999999, "text": " Here is the start of the glove 50-dimensional word vectors trained on a corpus of 6 billion.", "tokens": [50838, 1692, 307, 264, 722, 295, 264, 26928, 2625, 12, 18759, 1349, 18875, 8895, 322, 257, 1181, 31624, 295, 1386, 5218, 13, 51236], "temperature": 0.0, "avg_logprob": -0.266383311327766, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.06852680444717407}, {"id": 935, "seek": 464748, "start": 4664.919999999999, "end": 4667.44, "text": " Here is the word the.", "tokens": [51236, 1692, 307, 264, 1349, 264, 13, 51362], "temperature": 0.0, "avg_logprob": -0.266383311327766, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.06852680444717407}, {"id": 936, "seek": 466744, "start": 4667.44, "end": 4678.5199999999995, "text": " And here are the 50 floats which attempt to capture all of the information in the word", "tokens": [50364, 400, 510, 366, 264, 2625, 37878, 597, 5217, 281, 7983, 439, 295, 264, 1589, 294, 264, 1349, 50918], "temperature": 0.0, "avg_logprob": -0.30914539098739624, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.019718455150723457}, {"id": 937, "seek": 466744, "start": 4678.5199999999995, "end": 4679.5199999999995, "text": " the.", "tokens": [50918, 264, 13, 50968], "temperature": 0.0, "avg_logprob": -0.30914539098739624, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.019718455150723457}, {"id": 938, "seek": 466744, "start": 4679.5199999999995, "end": 4685.04, "text": " Punctuation, well here is the word fullstop.", "tokens": [50968, 22574, 349, 16073, 11, 731, 510, 307, 264, 1349, 1577, 13559, 13, 51244], "temperature": 0.0, "avg_logprob": -0.30914539098739624, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.019718455150723457}, {"id": 939, "seek": 466744, "start": 4685.04, "end": 4690.0, "text": " And so here are the 50 floats that attempt to capture all of the information captured", "tokens": [51244, 400, 370, 510, 366, 264, 2625, 37878, 300, 5217, 281, 7983, 439, 295, 264, 1589, 11828, 51492], "temperature": 0.0, "avg_logprob": -0.30914539098739624, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.019718455150723457}, {"id": 940, "seek": 466744, "start": 4690.0, "end": 4693.2, "text": " by a fullstop.", "tokens": [51492, 538, 257, 1577, 13559, 13, 51652], "temperature": 0.0, "avg_logprob": -0.30914539098739624, "compression_ratio": 1.8372093023255813, "no_speech_prob": 0.019718455150723457}, {"id": 941, "seek": 469320, "start": 4693.2, "end": 4699.639999999999, "text": " So here is the word in, here is the word doublequote, here is apostrophe s.", "tokens": [50364, 407, 510, 307, 264, 1349, 294, 11, 510, 307, 264, 1349, 3834, 25016, 11, 510, 307, 19484, 27194, 262, 13, 50686], "temperature": 0.0, "avg_logprob": -0.300591335118374, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.007460553199052811}, {"id": 942, "seek": 469320, "start": 4699.639999999999, "end": 4703.599999999999, "text": " So you can see that the glove authors have tokenized their text in a very particular", "tokens": [50686, 407, 291, 393, 536, 300, 264, 26928, 16552, 362, 14862, 1602, 641, 2487, 294, 257, 588, 1729, 50884], "temperature": 0.0, "avg_logprob": -0.300591335118374, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.007460553199052811}, {"id": 943, "seek": 469320, "start": 4703.599999999999, "end": 4704.599999999999, "text": " way.", "tokens": [50884, 636, 13, 50934], "temperature": 0.0, "avg_logprob": -0.300591335118374, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.007460553199052811}, {"id": 944, "seek": 469320, "start": 4704.599999999999, "end": 4710.36, "text": " The idea that apostrophe s should be treated as a thing, that makes a lot of sense.", "tokens": [50934, 440, 1558, 300, 19484, 27194, 262, 820, 312, 8668, 382, 257, 551, 11, 300, 1669, 257, 688, 295, 2020, 13, 51222], "temperature": 0.0, "avg_logprob": -0.300591335118374, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.007460553199052811}, {"id": 945, "seek": 469320, "start": 4710.36, "end": 4714.7, "text": " It certainly has that thinginess in the English language.", "tokens": [51222, 467, 3297, 575, 300, 551, 1324, 294, 264, 3669, 2856, 13, 51439], "temperature": 0.0, "avg_logprob": -0.300591335118374, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.007460553199052811}, {"id": 946, "seek": 469320, "start": 4714.7, "end": 4721.92, "text": " And so indeed the way the authors of a word-embedding corpus have chosen to tokenize their text", "tokens": [51439, 400, 370, 6451, 264, 636, 264, 16552, 295, 257, 1349, 12, 443, 2883, 3584, 1181, 31624, 362, 8614, 281, 14862, 1125, 641, 2487, 51800], "temperature": 0.0, "avg_logprob": -0.300591335118374, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.007460553199052811}, {"id": 947, "seek": 472192, "start": 4722.36, "end": 4723.36, "text": " definitely matters.", "tokens": [50386, 2138, 7001, 13, 50436], "temperature": 0.0, "avg_logprob": -0.3629088052889196, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0440179742872715}, {"id": 948, "seek": 472192, "start": 4723.36, "end": 4727.92, "text": " One of the things I quite like about Glove is that they've been pretty smart in my opinion", "tokens": [50436, 1485, 295, 264, 721, 286, 1596, 411, 466, 10786, 303, 307, 300, 436, 600, 668, 1238, 4069, 294, 452, 4800, 50664], "temperature": 0.0, "avg_logprob": -0.3629088052889196, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0440179742872715}, {"id": 949, "seek": 472192, "start": 4727.92, "end": 4730.56, "text": " about how they've done this.", "tokens": [50664, 466, 577, 436, 600, 1096, 341, 13, 50796], "temperature": 0.0, "avg_logprob": -0.3629088052889196, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0440179742872715}, {"id": 950, "seek": 472192, "start": 4730.56, "end": 4733.8, "text": " What was the target variable?", "tokens": [50796, 708, 390, 264, 3779, 7006, 30, 50958], "temperature": 0.0, "avg_logprob": -0.3629088052889196, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0440179742872715}, {"id": 951, "seek": 472192, "start": 4733.8, "end": 4740.96, "text": " So the question is how does one create word vectors in general, what is the model that", "tokens": [50958, 407, 264, 1168, 307, 577, 775, 472, 1884, 1349, 18875, 294, 2674, 11, 437, 307, 264, 2316, 300, 51316], "temperature": 0.0, "avg_logprob": -0.3629088052889196, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0440179742872715}, {"id": 952, "seek": 472192, "start": 4740.96, "end": 4750.24, "text": " you're creating, and what are the labels that you're building.", "tokens": [51316, 291, 434, 4084, 11, 293, 437, 366, 264, 16949, 300, 291, 434, 2390, 13, 51780], "temperature": 0.0, "avg_logprob": -0.3629088052889196, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0440179742872715}, {"id": 953, "seek": 475024, "start": 4750.24, "end": 4755.28, "text": " One of the things that we talked about getting to at some point is unsupervised learning.", "tokens": [50364, 1485, 295, 264, 721, 300, 321, 2825, 466, 1242, 281, 412, 512, 935, 307, 2693, 12879, 24420, 2539, 13, 50616], "temperature": 0.0, "avg_logprob": -0.2562803339075159, "compression_ratio": 1.7393162393162394, "no_speech_prob": 0.1022985652089119}, {"id": 954, "seek": 475024, "start": 4755.28, "end": 4757.48, "text": " And this is a great example of unsupervised learning.", "tokens": [50616, 400, 341, 307, 257, 869, 1365, 295, 2693, 12879, 24420, 2539, 13, 50726], "temperature": 0.0, "avg_logprob": -0.2562803339075159, "compression_ratio": 1.7393162393162394, "no_speech_prob": 0.1022985652089119}, {"id": 955, "seek": 475024, "start": 4757.48, "end": 4765.8, "text": " We want to take 840 billion tokens of an internet dump and build a model of something.", "tokens": [50726, 492, 528, 281, 747, 1649, 5254, 5218, 22667, 295, 364, 4705, 11430, 293, 1322, 257, 2316, 295, 746, 13, 51142], "temperature": 0.0, "avg_logprob": -0.2562803339075159, "compression_ratio": 1.7393162393162394, "no_speech_prob": 0.1022985652089119}, {"id": 956, "seek": 475024, "start": 4765.8, "end": 4767.639999999999, "text": " So what do we build a model of?", "tokens": [51142, 407, 437, 360, 321, 1322, 257, 2316, 295, 30, 51234], "temperature": 0.0, "avg_logprob": -0.2562803339075159, "compression_ratio": 1.7393162393162394, "no_speech_prob": 0.1022985652089119}, {"id": 957, "seek": 475024, "start": 4767.639999999999, "end": 4769.44, "text": " This is a case of unsupervised learning.", "tokens": [51234, 639, 307, 257, 1389, 295, 2693, 12879, 24420, 2539, 13, 51324], "temperature": 0.0, "avg_logprob": -0.2562803339075159, "compression_ratio": 1.7393162393162394, "no_speech_prob": 0.1022985652089119}, {"id": 958, "seek": 475024, "start": 4769.44, "end": 4774.719999999999, "text": " We're trying to capture some structure of this data, in this case how does English look,", "tokens": [51324, 492, 434, 1382, 281, 7983, 512, 3877, 295, 341, 1412, 11, 294, 341, 1389, 577, 775, 3669, 574, 11, 51588], "temperature": 0.0, "avg_logprob": -0.2562803339075159, "compression_ratio": 1.7393162393162394, "no_speech_prob": 0.1022985652089119}, {"id": 959, "seek": 475024, "start": 4774.719999999999, "end": 4778.12, "text": " work and feel.", "tokens": [51588, 589, 293, 841, 13, 51758], "temperature": 0.0, "avg_logprob": -0.2562803339075159, "compression_ratio": 1.7393162393162394, "no_speech_prob": 0.1022985652089119}, {"id": 960, "seek": 477812, "start": 4778.12, "end": 4782.68, "text": " The way that this is done, at least in the Word2Vec example, is quite cool.", "tokens": [50364, 440, 636, 300, 341, 307, 1096, 11, 412, 1935, 294, 264, 8725, 17, 53, 3045, 1365, 11, 307, 1596, 1627, 13, 50592], "temperature": 0.0, "avg_logprob": -0.30890850587324664, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.007937896996736526}, {"id": 961, "seek": 477812, "start": 4782.68, "end": 4792.84, "text": " What they do is they take every sentence of say 11 words long, not just every sentence,", "tokens": [50592, 708, 436, 360, 307, 436, 747, 633, 8174, 295, 584, 2975, 2283, 938, 11, 406, 445, 633, 8174, 11, 51100], "temperature": 0.0, "avg_logprob": -0.30890850587324664, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.007937896996736526}, {"id": 962, "seek": 477812, "start": 4792.84, "end": 4797.599999999999, "text": " but every 11 long string of words that appears in the corpus, and then they take the middle", "tokens": [51100, 457, 633, 2975, 938, 6798, 295, 2283, 300, 7038, 294, 264, 1181, 31624, 11, 293, 550, 436, 747, 264, 2808, 51338], "temperature": 0.0, "avg_logprob": -0.30890850587324664, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.007937896996736526}, {"id": 963, "seek": 477812, "start": 4797.599999999999, "end": 4806.68, "text": " word and the first thing they do is create a copy of it, an exact copy.", "tokens": [51338, 1349, 293, 264, 700, 551, 436, 360, 307, 1884, 257, 5055, 295, 309, 11, 364, 1900, 5055, 13, 51792], "temperature": 0.0, "avg_logprob": -0.30890850587324664, "compression_ratio": 1.676923076923077, "no_speech_prob": 0.007937896996736526}, {"id": 964, "seek": 480668, "start": 4806.72, "end": 4817.4800000000005, "text": " And then in the copy, they delete the middle word and replace it with some random word.", "tokens": [50366, 400, 550, 294, 264, 5055, 11, 436, 12097, 264, 2808, 1349, 293, 7406, 309, 365, 512, 4974, 1349, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2796325403101304, "compression_ratio": 1.6686746987951808, "no_speech_prob": 0.005384894087910652}, {"id": 965, "seek": 480668, "start": 4817.4800000000005, "end": 4824.68, "text": " So we now have two strings of 11 words, one of which makes sense because it's real, one", "tokens": [50904, 407, 321, 586, 362, 732, 13985, 295, 2975, 2283, 11, 472, 295, 597, 1669, 2020, 570, 309, 311, 957, 11, 472, 51264], "temperature": 0.0, "avg_logprob": -0.2796325403101304, "compression_ratio": 1.6686746987951808, "no_speech_prob": 0.005384894087910652}, {"id": 966, "seek": 480668, "start": 4824.68, "end": 4828.92, "text": " of which probably doesn't make sense because the middle word has been replaced with something", "tokens": [51264, 295, 597, 1391, 1177, 380, 652, 2020, 570, 264, 2808, 1349, 575, 668, 10772, 365, 746, 51476], "temperature": 0.0, "avg_logprob": -0.2796325403101304, "compression_ratio": 1.6686746987951808, "no_speech_prob": 0.005384894087910652}, {"id": 967, "seek": 480668, "start": 4828.92, "end": 4830.16, "text": " random.", "tokens": [51476, 4974, 13, 51538], "temperature": 0.0, "avg_logprob": -0.2796325403101304, "compression_ratio": 1.6686746987951808, "no_speech_prob": 0.005384894087910652}, {"id": 968, "seek": 483016, "start": 4830.16, "end": 4839.4, "text": " And so the model task that they create, the label is 1 if it's a real sentence or 0 if", "tokens": [50364, 400, 370, 264, 2316, 5633, 300, 436, 1884, 11, 264, 7645, 307, 502, 498, 309, 311, 257, 957, 8174, 420, 1958, 498, 50826], "temperature": 0.0, "avg_logprob": -0.2802450910527655, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0340995192527771}, {"id": 969, "seek": 483016, "start": 4839.4, "end": 4841.0, "text": " it's a fake sentence.", "tokens": [50826, 309, 311, 257, 7592, 8174, 13, 50906], "temperature": 0.0, "avg_logprob": -0.2802450910527655, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0340995192527771}, {"id": 970, "seek": 483016, "start": 4841.0, "end": 4843.5, "text": " And that's the task they give it.", "tokens": [50906, 400, 300, 311, 264, 5633, 436, 976, 309, 13, 51031], "temperature": 0.0, "avg_logprob": -0.2802450910527655, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0340995192527771}, {"id": 971, "seek": 483016, "start": 4843.5, "end": 4849.84, "text": " So you can see it's not a directly useful task in any way, unless somebody actually", "tokens": [51031, 407, 291, 393, 536, 309, 311, 406, 257, 3838, 4420, 5633, 294, 604, 636, 11, 5969, 2618, 767, 51348], "temperature": 0.0, "avg_logprob": -0.2802450910527655, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0340995192527771}, {"id": 972, "seek": 483016, "start": 4849.84, "end": 4854.24, "text": " comes along and says, I just found this corpus in which somebody's replaced half of the middle", "tokens": [51348, 1487, 2051, 293, 1619, 11, 286, 445, 1352, 341, 1181, 31624, 294, 597, 2618, 311, 10772, 1922, 295, 264, 2808, 51568], "temperature": 0.0, "avg_logprob": -0.2802450910527655, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0340995192527771}, {"id": 973, "seek": 483016, "start": 4854.24, "end": 4856.599999999999, "text": " words with random words.", "tokens": [51568, 2283, 365, 4974, 2283, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2802450910527655, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.0340995192527771}, {"id": 974, "seek": 485660, "start": 4856.6, "end": 4860.120000000001, "text": " But it is something where in order to be able to tackle this task, you're going to", "tokens": [50364, 583, 309, 307, 746, 689, 294, 1668, 281, 312, 1075, 281, 14896, 341, 5633, 11, 291, 434, 516, 281, 50540], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 975, "seek": 485660, "start": 4860.120000000001, "end": 4862.400000000001, "text": " have to know something about language.", "tokens": [50540, 362, 281, 458, 746, 466, 2856, 13, 50654], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 976, "seek": 485660, "start": 4862.400000000001, "end": 4865.6, "text": " You're going to have to be able to recognize that this sentence doesn't make sense and", "tokens": [50654, 509, 434, 516, 281, 362, 281, 312, 1075, 281, 5521, 300, 341, 8174, 1177, 380, 652, 2020, 293, 50814], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 977, "seek": 485660, "start": 4865.6, "end": 4867.5, "text": " this sentence does make sense.", "tokens": [50814, 341, 8174, 775, 652, 2020, 13, 50909], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 978, "seek": 485660, "start": 4867.5, "end": 4870.160000000001, "text": " So this is a great example of unsupervised learning.", "tokens": [50909, 407, 341, 307, 257, 869, 1365, 295, 2693, 12879, 24420, 2539, 13, 51042], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 979, "seek": 485660, "start": 4870.160000000001, "end": 4875.860000000001, "text": " Generally speaking in deep learning, unsupervised learning means coming up with a task which", "tokens": [51042, 21082, 4124, 294, 2452, 2539, 11, 2693, 12879, 24420, 2539, 1355, 1348, 493, 365, 257, 5633, 597, 51327], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 980, "seek": 485660, "start": 4875.860000000001, "end": 4880.200000000001, "text": " is as close to the task you're eventually going to be interested in as possible, but", "tokens": [51327, 307, 382, 1998, 281, 264, 5633, 291, 434, 4728, 516, 281, 312, 3102, 294, 382, 1944, 11, 457, 51544], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 981, "seek": 485660, "start": 4880.200000000001, "end": 4884.96, "text": " that doesn't require labels or where the labels are really cheap to generate.", "tokens": [51544, 300, 1177, 380, 3651, 16949, 420, 689, 264, 16949, 366, 534, 7084, 281, 8460, 13, 51782], "temperature": 0.0, "avg_logprob": -0.24161764291616586, "compression_ratio": 1.9027777777777777, "no_speech_prob": 0.37385720014572144}, {"id": 982, "seek": 488496, "start": 4884.96, "end": 4885.96, "text": " Question?", "tokens": [50364, 14464, 30, 50414], "temperature": 0.0, "avg_logprob": -0.5828449301523705, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.6401941180229187}, {"id": 983, "seek": 488496, "start": 4885.96, "end": 4890.16, "text": " I was just thinking about the language aspect.", "tokens": [50414, 286, 390, 445, 1953, 466, 264, 2856, 4171, 13, 50624], "temperature": 0.0, "avg_logprob": -0.5828449301523705, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.6401941180229187}, {"id": 984, "seek": 488496, "start": 4890.16, "end": 4899.0, "text": " I'm talking about just English, it's just a stream of tokens.", "tokens": [50624, 286, 478, 1417, 466, 445, 3669, 11, 309, 311, 445, 257, 4309, 295, 22667, 13, 51066], "temperature": 0.0, "avg_logprob": -0.5828449301523705, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.6401941180229187}, {"id": 985, "seek": 488496, "start": 4899.0, "end": 4904.2, "text": " So if it's a different language, we're still turning them into a vector of floats.", "tokens": [51066, 407, 498, 309, 311, 257, 819, 2856, 11, 321, 434, 920, 6246, 552, 666, 257, 8062, 295, 37878, 13, 51326], "temperature": 0.0, "avg_logprob": -0.5828449301523705, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.6401941180229187}, {"id": 986, "seek": 488496, "start": 4904.2, "end": 4910.56, "text": " So how does that change across languages or in a mixed-language text?", "tokens": [51326, 407, 577, 775, 300, 1319, 2108, 8650, 420, 294, 257, 7467, 12, 25241, 20473, 2487, 30, 51644], "temperature": 0.0, "avg_logprob": -0.5828449301523705, "compression_ratio": 1.5224719101123596, "no_speech_prob": 0.6401941180229187}, {"id": 987, "seek": 491056, "start": 4910.56, "end": 4919.6, "text": " So it turns out that the embeddings that is created when you look at, say, Hindu and", "tokens": [50364, 407, 309, 4523, 484, 300, 264, 12240, 29432, 300, 307, 2942, 562, 291, 574, 412, 11, 584, 11, 21231, 293, 50816], "temperature": 0.0, "avg_logprob": -0.3136151278460467, "compression_ratio": 1.4, "no_speech_prob": 0.0032223339658230543}, {"id": 988, "seek": 491056, "start": 4919.6, "end": 4925.400000000001, "text": " Japanese turn out to be nearly the same.", "tokens": [50816, 5433, 1261, 484, 281, 312, 6217, 264, 912, 13, 51106], "temperature": 0.0, "avg_logprob": -0.3136151278460467, "compression_ratio": 1.4, "no_speech_prob": 0.0032223339658230543}, {"id": 989, "seek": 491056, "start": 4925.400000000001, "end": 4934.72, "text": " And so one way to translate language is to create a bunch of word vectors in English", "tokens": [51106, 400, 370, 472, 636, 281, 13799, 2856, 307, 281, 1884, 257, 3840, 295, 1349, 18875, 294, 3669, 51572], "temperature": 0.0, "avg_logprob": -0.3136151278460467, "compression_ratio": 1.4, "no_speech_prob": 0.0032223339658230543}, {"id": 990, "seek": 493472, "start": 4935.08, "end": 4940.8, "text": " for various words, and then to create a bunch of word vectors in, say, Japanese for various", "tokens": [50382, 337, 3683, 2283, 11, 293, 550, 281, 1884, 257, 3840, 295, 1349, 18875, 294, 11, 584, 11, 5433, 337, 3683, 50668], "temperature": 0.0, "avg_logprob": -0.2867092604047797, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.12250966578722}, {"id": 991, "seek": 493472, "start": 4940.8, "end": 4942.12, "text": " words.", "tokens": [50668, 2283, 13, 50734], "temperature": 0.0, "avg_logprob": -0.2867092604047797, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.12250966578722}, {"id": 992, "seek": 493472, "start": 4942.12, "end": 4946.84, "text": " And then what you can do is you can say, I want to translate this word, which might be", "tokens": [50734, 400, 550, 437, 291, 393, 360, 307, 291, 393, 584, 11, 286, 528, 281, 13799, 341, 1349, 11, 597, 1062, 312, 50970], "temperature": 0.0, "avg_logprob": -0.2867092604047797, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.12250966578722}, {"id": 993, "seek": 493472, "start": 4946.84, "end": 4950.0, "text": " Queen, to Japanese.", "tokens": [50970, 10077, 11, 281, 5433, 13, 51128], "temperature": 0.0, "avg_logprob": -0.2867092604047797, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.12250966578722}, {"id": 994, "seek": 493472, "start": 4950.0, "end": 4955.84, "text": " You can basically look up and find the nearest word in the same vector space in the Japanese", "tokens": [51128, 509, 393, 1936, 574, 493, 293, 915, 264, 23831, 1349, 294, 264, 912, 8062, 1901, 294, 264, 5433, 51420], "temperature": 0.0, "avg_logprob": -0.2867092604047797, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.12250966578722}, {"id": 995, "seek": 493472, "start": 4955.84, "end": 4959.820000000001, "text": " corpus and it turns out it works.", "tokens": [51420, 1181, 31624, 293, 309, 4523, 484, 309, 1985, 13, 51619], "temperature": 0.0, "avg_logprob": -0.2867092604047797, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.12250966578722}, {"id": 996, "seek": 495982, "start": 4959.82, "end": 4964.099999999999, "text": " So it's a fascinating thing about language.", "tokens": [50364, 407, 309, 311, 257, 10343, 551, 466, 2856, 13, 50578], "temperature": 0.0, "avg_logprob": -0.24903220715730087, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0012643701629713178}, {"id": 997, "seek": 495982, "start": 4964.099999999999, "end": 4970.259999999999, "text": " In fact, Google has just announced that they've replaced Google Translate with a neural translation", "tokens": [50578, 682, 1186, 11, 3329, 575, 445, 7548, 300, 436, 600, 10772, 3329, 6531, 17593, 365, 257, 18161, 12853, 50886], "temperature": 0.0, "avg_logprob": -0.24903220715730087, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0012643701629713178}, {"id": 998, "seek": 495982, "start": 4970.259999999999, "end": 4974.66, "text": " system and part of what that is doing is basically doing this.", "tokens": [50886, 1185, 293, 644, 295, 437, 300, 307, 884, 307, 1936, 884, 341, 13, 51106], "temperature": 0.0, "avg_logprob": -0.24903220715730087, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0012643701629713178}, {"id": 999, "seek": 495982, "start": 4974.66, "end": 4980.82, "text": " In fact, here are some interesting examples of some word embeddings.", "tokens": [51106, 682, 1186, 11, 510, 366, 512, 1880, 5110, 295, 512, 1349, 12240, 29432, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24903220715730087, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0012643701629713178}, {"id": 1000, "seek": 495982, "start": 4980.82, "end": 4985.179999999999, "text": " The word embedding for King and Queen has the same distance and direction as the word", "tokens": [51414, 440, 1349, 12240, 3584, 337, 3819, 293, 10077, 575, 264, 912, 4560, 293, 3513, 382, 264, 1349, 51632], "temperature": 0.0, "avg_logprob": -0.24903220715730087, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0012643701629713178}, {"id": 1001, "seek": 495982, "start": 4985.179999999999, "end": 4987.299999999999, "text": " embeddings for Man and Woman.", "tokens": [51632, 12240, 29432, 337, 2458, 293, 15794, 13, 51738], "temperature": 0.0, "avg_logprob": -0.24903220715730087, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0012643701629713178}, {"id": 1002, "seek": 498730, "start": 4987.3, "end": 4990.66, "text": " Ditto for Walking vs. Walked and Swimming vs. Swam.", "tokens": [50364, 413, 34924, 337, 26964, 12041, 13, 10818, 292, 293, 3926, 40471, 12041, 13, 3926, 335, 13, 50532], "temperature": 0.0, "avg_logprob": -0.2807820564092592, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.048855166882276535}, {"id": 1003, "seek": 498730, "start": 4990.66, "end": 4995.04, "text": " And ditto for Spain vs. Madrid and Italy vs. Rome.", "tokens": [50532, 400, 274, 34924, 337, 12838, 12041, 13, 22091, 293, 10705, 12041, 13, 12043, 13, 50751], "temperature": 0.0, "avg_logprob": -0.2807820564092592, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.048855166882276535}, {"id": 1004, "seek": 498730, "start": 4995.04, "end": 5002.14, "text": " So the embeddings that have to get learned in order to solve this stupid meaningless", "tokens": [50751, 407, 264, 12240, 29432, 300, 362, 281, 483, 3264, 294, 1668, 281, 5039, 341, 6631, 33232, 51106], "temperature": 0.0, "avg_logprob": -0.2807820564092592, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.048855166882276535}, {"id": 1005, "seek": 498730, "start": 5002.14, "end": 5007.66, "text": " random sentence task are quite amazing.", "tokens": [51106, 4974, 8174, 5633, 366, 1596, 2243, 13, 51382], "temperature": 0.0, "avg_logprob": -0.2807820564092592, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.048855166882276535}, {"id": 1006, "seek": 498730, "start": 5007.66, "end": 5014.38, "text": " And so I've actually downloaded those GloVe embeddings and I've pre-processed them and", "tokens": [51382, 400, 370, 286, 600, 767, 21748, 729, 10786, 53, 68, 12240, 29432, 293, 286, 600, 659, 12, 41075, 292, 552, 293, 51718], "temperature": 0.0, "avg_logprob": -0.2807820564092592, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.048855166882276535}, {"id": 1007, "seek": 501438, "start": 5014.38, "end": 5019.26, "text": " I'm going to upload these for you shortly into a form that's going to be really easy", "tokens": [50364, 286, 478, 516, 281, 6580, 613, 337, 291, 13392, 666, 257, 1254, 300, 311, 516, 281, 312, 534, 1858, 50608], "temperature": 0.0, "avg_logprob": -0.2631856744939631, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.08035166561603546}, {"id": 1008, "seek": 501438, "start": 5019.26, "end": 5020.9400000000005, "text": " for you to use in Python.", "tokens": [50608, 337, 291, 281, 764, 294, 15329, 13, 50692], "temperature": 0.0, "avg_logprob": -0.2631856744939631, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.08035166561603546}, {"id": 1009, "seek": 501438, "start": 5020.9400000000005, "end": 5025.22, "text": " And I've created this little thing called LoadGloVe which loads the pre-processed stuff", "tokens": [50692, 400, 286, 600, 2942, 341, 707, 551, 1219, 48408, 38, 752, 53, 68, 597, 12668, 264, 659, 12, 41075, 292, 1507, 50906], "temperature": 0.0, "avg_logprob": -0.2631856744939631, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.08035166561603546}, {"id": 1010, "seek": 501438, "start": 5025.22, "end": 5026.74, "text": " that I've created for you.", "tokens": [50906, 300, 286, 600, 2942, 337, 291, 13, 50982], "temperature": 0.0, "avg_logprob": -0.2631856744939631, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.08035166561603546}, {"id": 1011, "seek": 501438, "start": 5026.74, "end": 5028.3, "text": " And it's going to give you 3 things.", "tokens": [50982, 400, 309, 311, 516, 281, 976, 291, 805, 721, 13, 51060], "temperature": 0.0, "avg_logprob": -0.2631856744939631, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.08035166561603546}, {"id": 1012, "seek": 501438, "start": 5028.3, "end": 5035.78, "text": " It's going to give you the word vectors, which is the 400,000 by 50 dimensional vectors,", "tokens": [51060, 467, 311, 516, 281, 976, 291, 264, 1349, 18875, 11, 597, 307, 264, 8423, 11, 1360, 538, 2625, 18795, 18875, 11, 51434], "temperature": 0.0, "avg_logprob": -0.2631856744939631, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.08035166561603546}, {"id": 1013, "seek": 503578, "start": 5035.78, "end": 5044.96, "text": " a list of the words, the, comma, dot, of, to, and a list of the word indexes.", "tokens": [50364, 257, 1329, 295, 264, 2283, 11, 264, 11, 22117, 11, 5893, 11, 295, 11, 281, 11, 293, 257, 1329, 295, 264, 1349, 8186, 279, 13, 50823], "temperature": 0.0, "avg_logprob": -0.2137748269473805, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.03514424338936806}, {"id": 1014, "seek": 503578, "start": 5044.96, "end": 5053.139999999999, "text": " So you can now take a word and call word2vec to get back its 50-dimensional array.", "tokens": [50823, 407, 291, 393, 586, 747, 257, 1349, 293, 818, 1349, 17, 303, 66, 281, 483, 646, 1080, 2625, 12, 18759, 10225, 13, 51232], "temperature": 0.0, "avg_logprob": -0.2137748269473805, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.03514424338936806}, {"id": 1015, "seek": 503578, "start": 5053.139999999999, "end": 5056.3, "text": " And so then I drew a picture.", "tokens": [51232, 400, 370, 550, 286, 12804, 257, 3036, 13, 51390], "temperature": 0.0, "avg_logprob": -0.2137748269473805, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.03514424338936806}, {"id": 1016, "seek": 503578, "start": 5056.3, "end": 5063.219999999999, "text": " In order to turn a 50-dimensional vector into something 2-dimensional that I can plot, we", "tokens": [51390, 682, 1668, 281, 1261, 257, 2625, 12, 18759, 8062, 666, 746, 568, 12, 18759, 300, 286, 393, 7542, 11, 321, 51736], "temperature": 0.0, "avg_logprob": -0.2137748269473805, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.03514424338936806}, {"id": 1017, "seek": 506322, "start": 5063.22, "end": 5065.9400000000005, "text": " have to do something called dimensionality reduction.", "tokens": [50364, 362, 281, 360, 746, 1219, 10139, 1860, 11004, 13, 50500], "temperature": 0.0, "avg_logprob": -0.2196870231628418, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.03161491081118584}, {"id": 1018, "seek": 506322, "start": 5065.9400000000005, "end": 5070.3, "text": " And there's a particular technique, the details don't really matter, called t-SNE, which attempts", "tokens": [50500, 400, 456, 311, 257, 1729, 6532, 11, 264, 4365, 500, 380, 534, 1871, 11, 1219, 256, 12, 50, 15988, 11, 597, 15257, 50718], "temperature": 0.0, "avg_logprob": -0.2196870231628418, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.03161491081118584}, {"id": 1019, "seek": 506322, "start": 5070.3, "end": 5076.860000000001, "text": " to find a way of taking your high-dimensional information and plot it on 2 dimensions such", "tokens": [50718, 281, 915, 257, 636, 295, 1940, 428, 1090, 12, 18759, 1589, 293, 7542, 309, 322, 568, 12819, 1270, 51046], "temperature": 0.0, "avg_logprob": -0.2196870231628418, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.03161491081118584}, {"id": 1020, "seek": 506322, "start": 5076.860000000001, "end": 5081.54, "text": " that things that were close in the 50 dimensions are still close in the 2 dimensions.", "tokens": [51046, 300, 721, 300, 645, 1998, 294, 264, 2625, 12819, 366, 920, 1998, 294, 264, 568, 12819, 13, 51280], "temperature": 0.0, "avg_logprob": -0.2196870231628418, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.03161491081118584}, {"id": 1021, "seek": 506322, "start": 5081.54, "end": 5090.780000000001, "text": " And so I used t-SNE to plot the first 350 most common words and here they all are.", "tokens": [51280, 400, 370, 286, 1143, 256, 12, 50, 15988, 281, 7542, 264, 700, 18065, 881, 2689, 2283, 293, 510, 436, 439, 366, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2196870231628418, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.03161491081118584}, {"id": 1022, "seek": 509078, "start": 5090.78, "end": 5095.86, "text": " And so you can see that bits of punctuation have appeared close to each other, numerals", "tokens": [50364, 400, 370, 291, 393, 536, 300, 9239, 295, 27006, 16073, 362, 8516, 1998, 281, 1184, 661, 11, 7866, 1124, 50618], "temperature": 0.0, "avg_logprob": -0.2883002524282418, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0031725869048386812}, {"id": 1023, "seek": 509078, "start": 5095.86, "end": 5099.62, "text": " have appeared close to each other, written versions of numerals are close to each other,", "tokens": [50618, 362, 8516, 1998, 281, 1184, 661, 11, 3720, 9606, 295, 7866, 1124, 366, 1998, 281, 1184, 661, 11, 50806], "temperature": 0.0, "avg_logprob": -0.2883002524282418, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0031725869048386812}, {"id": 1024, "seek": 509078, "start": 5099.62, "end": 5105.42, "text": " seasons, games, leagues, played are all close to each other, various things about politics,", "tokens": [50806, 15050, 11, 2813, 11, 48429, 11, 3737, 366, 439, 1998, 281, 1184, 661, 11, 3683, 721, 466, 7341, 11, 51096], "temperature": 0.0, "avg_logprob": -0.2883002524282418, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0031725869048386812}, {"id": 1025, "seek": 509078, "start": 5105.42, "end": 5111.679999999999, "text": " school and university, president, general, prime, minister, and Bush.", "tokens": [51096, 1395, 293, 5454, 11, 3868, 11, 2674, 11, 5835, 11, 10563, 11, 293, 15782, 13, 51409], "temperature": 0.0, "avg_logprob": -0.2883002524282418, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0031725869048386812}, {"id": 1026, "seek": 509078, "start": 5111.679999999999, "end": 5118.74, "text": " Now this is a great example of where this t-SNE 2-dimensional projection is misleading", "tokens": [51409, 823, 341, 307, 257, 869, 1365, 295, 689, 341, 256, 12, 50, 15988, 568, 12, 18759, 22743, 307, 36429, 51762], "temperature": 0.0, "avg_logprob": -0.2883002524282418, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.0031725869048386812}, {"id": 1027, "seek": 511874, "start": 5118.78, "end": 5122.5, "text": " about the level of complexity that's actually in these word vectors.", "tokens": [50366, 466, 264, 1496, 295, 14024, 300, 311, 767, 294, 613, 1349, 18875, 13, 50552], "temperature": 0.0, "avg_logprob": -0.24919463592825583, "compression_ratio": 1.6374501992031874, "no_speech_prob": 0.01450346503406763}, {"id": 1028, "seek": 511874, "start": 5122.5, "end": 5127.38, "text": " In a different projection, Bush would be very close to Tree.", "tokens": [50552, 682, 257, 819, 22743, 11, 15782, 576, 312, 588, 1998, 281, 22291, 13, 50796], "temperature": 0.0, "avg_logprob": -0.24919463592825583, "compression_ratio": 1.6374501992031874, "no_speech_prob": 0.01450346503406763}, {"id": 1029, "seek": 511874, "start": 5127.38, "end": 5131.5, "text": " The 2-dimensional projection is losing a lot of information.", "tokens": [50796, 440, 568, 12, 18759, 22743, 307, 7027, 257, 688, 295, 1589, 13, 51002], "temperature": 0.0, "avg_logprob": -0.24919463592825583, "compression_ratio": 1.6374501992031874, "no_speech_prob": 0.01450346503406763}, {"id": 1030, "seek": 511874, "start": 5131.5, "end": 5138.82, "text": " The true detail here is a lot more complex than us mere humans can see on a page.", "tokens": [51002, 440, 2074, 2607, 510, 307, 257, 688, 544, 3997, 813, 505, 8401, 6255, 393, 536, 322, 257, 3028, 13, 51368], "temperature": 0.0, "avg_logprob": -0.24919463592825583, "compression_ratio": 1.6374501992031874, "no_speech_prob": 0.01450346503406763}, {"id": 1031, "seek": 511874, "start": 5138.82, "end": 5141.94, "text": " But hopefully you kind of get a sense of this.", "tokens": [51368, 583, 4696, 291, 733, 295, 483, 257, 2020, 295, 341, 13, 51524], "temperature": 0.0, "avg_logprob": -0.24919463592825583, "compression_ratio": 1.6374501992031874, "no_speech_prob": 0.01450346503406763}, {"id": 1032, "seek": 511874, "start": 5141.94, "end": 5148.219999999999, "text": " So all I've done here is I've just taken those 50-dimensional word vectors and I've plotted", "tokens": [51524, 407, 439, 286, 600, 1096, 510, 307, 286, 600, 445, 2726, 729, 2625, 12, 18759, 1349, 18875, 293, 286, 600, 43288, 51838], "temperature": 0.0, "avg_logprob": -0.24919463592825583, "compression_ratio": 1.6374501992031874, "no_speech_prob": 0.01450346503406763}, {"id": 1033, "seek": 514822, "start": 5148.22, "end": 5149.9800000000005, "text": " them in 2-dimensions.", "tokens": [50364, 552, 294, 568, 12, 13595, 8302, 13, 50452], "temperature": 0.0, "avg_logprob": -0.2974767511541193, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.048130009323358536}, {"id": 1034, "seek": 514822, "start": 5149.9800000000005, "end": 5158.66, "text": " And so you can see that when you learn a word embedding, you end up with something.", "tokens": [50452, 400, 370, 291, 393, 536, 300, 562, 291, 1466, 257, 1349, 12240, 3584, 11, 291, 917, 493, 365, 746, 13, 50886], "temperature": 0.0, "avg_logprob": -0.2974767511541193, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.048130009323358536}, {"id": 1035, "seek": 514822, "start": 5158.66, "end": 5162.820000000001, "text": " We've now seen, not just a word embedding, we've seen for movies, we were able to plot", "tokens": [50886, 492, 600, 586, 1612, 11, 406, 445, 257, 1349, 12240, 3584, 11, 321, 600, 1612, 337, 6233, 11, 321, 645, 1075, 281, 7542, 51094], "temperature": 0.0, "avg_logprob": -0.2974767511541193, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.048130009323358536}, {"id": 1036, "seek": 514822, "start": 5162.820000000001, "end": 5166.3, "text": " some movies in 2-dimensions and see how they relate to each other.", "tokens": [51094, 512, 6233, 294, 568, 12, 13595, 8302, 293, 536, 577, 436, 10961, 281, 1184, 661, 13, 51268], "temperature": 0.0, "avg_logprob": -0.2974767511541193, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.048130009323358536}, {"id": 1037, "seek": 514822, "start": 5166.3, "end": 5168.06, "text": " We can do the same thing for words.", "tokens": [51268, 492, 393, 360, 264, 912, 551, 337, 2283, 13, 51356], "temperature": 0.0, "avg_logprob": -0.2974767511541193, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.048130009323358536}, {"id": 1038, "seek": 514822, "start": 5168.06, "end": 5174.3, "text": " In general, when you have some high-cardinality categorical variable, whether it be lots of", "tokens": [51356, 682, 2674, 11, 562, 291, 362, 512, 1090, 12, 22259, 259, 1860, 19250, 804, 7006, 11, 1968, 309, 312, 3195, 295, 51668], "temperature": 0.0, "avg_logprob": -0.2974767511541193, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.048130009323358536}, {"id": 1039, "seek": 517430, "start": 5174.3, "end": 5180.74, "text": " movies or lots of reviewers or lots of words, you can turn it into a useful lower-dimensional", "tokens": [50364, 6233, 420, 3195, 295, 45837, 420, 3195, 295, 2283, 11, 291, 393, 1261, 309, 666, 257, 4420, 3126, 12, 18759, 50686], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1040, "seek": 517430, "start": 5180.74, "end": 5184.5, "text": " space using this very simple technique of creating an embedding.", "tokens": [50686, 1901, 1228, 341, 588, 2199, 6532, 295, 4084, 364, 12240, 3584, 13, 50874], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1041, "seek": 517430, "start": 5184.5, "end": 5185.5, "text": " Question Time 5", "tokens": [50874, 14464, 220, 22233, 1025, 50924], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1042, "seek": 517430, "start": 5185.5, "end": 5189.9800000000005, "text": " The explanation on how unsupervised learning was used in Word2Vec was pretty smart.", "tokens": [50924, 440, 10835, 322, 577, 2693, 12879, 24420, 2539, 390, 1143, 294, 8725, 17, 53, 3045, 390, 1238, 4069, 13, 51148], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1043, "seek": 517430, "start": 5189.9800000000005, "end": 5191.34, "text": " How was it done in GloVe?", "tokens": [51148, 1012, 390, 309, 1096, 294, 10786, 53, 68, 30, 51216], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1044, "seek": 517430, "start": 5191.34, "end": 5193.9800000000005, "text": " I don't recall how it was done in GloVe.", "tokens": [51216, 286, 500, 380, 9901, 577, 309, 390, 1096, 294, 10786, 53, 68, 13, 51348], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1045, "seek": 517430, "start": 5193.9800000000005, "end": 5195.62, "text": " I believe it was something similar.", "tokens": [51348, 286, 1697, 309, 390, 746, 2531, 13, 51430], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1046, "seek": 517430, "start": 5195.62, "end": 5201.5, "text": " I should mention though that both GloVe and Word2Vec did not use deep learning.", "tokens": [51430, 286, 820, 2152, 1673, 300, 1293, 10786, 53, 68, 293, 8725, 17, 53, 3045, 630, 406, 764, 2452, 2539, 13, 51724], "temperature": 0.0, "avg_logprob": -0.3436088562011719, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.194356769323349}, {"id": 1047, "seek": 520150, "start": 5201.5, "end": 5205.94, "text": " They actually tried to create a linear model.", "tokens": [50364, 814, 767, 3031, 281, 1884, 257, 8213, 2316, 13, 50586], "temperature": 0.0, "avg_logprob": -0.25860790925867416, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.11278660595417023}, {"id": 1048, "seek": 520150, "start": 5205.94, "end": 5211.14, "text": " The reason they did that was that they specifically wanted to create representations which had", "tokens": [50586, 440, 1778, 436, 630, 300, 390, 300, 436, 4682, 1415, 281, 1884, 33358, 597, 632, 50846], "temperature": 0.0, "avg_logprob": -0.25860790925867416, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.11278660595417023}, {"id": 1049, "seek": 520150, "start": 5211.14, "end": 5216.94, "text": " these kinds of linear relationships because they felt that this would be a useful characteristic", "tokens": [50846, 613, 3685, 295, 8213, 6159, 570, 436, 2762, 300, 341, 576, 312, 257, 4420, 16282, 51136], "temperature": 0.0, "avg_logprob": -0.25860790925867416, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.11278660595417023}, {"id": 1050, "seek": 520150, "start": 5216.94, "end": 5219.46, "text": " of these representations.", "tokens": [51136, 295, 613, 33358, 13, 51262], "temperature": 0.0, "avg_logprob": -0.25860790925867416, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.11278660595417023}, {"id": 1051, "seek": 520150, "start": 5219.46, "end": 5227.54, "text": " I'm not even sure if anybody has tried to create a similarly useful representation using", "tokens": [51262, 286, 478, 406, 754, 988, 498, 4472, 575, 3031, 281, 1884, 257, 14138, 4420, 10290, 1228, 51666], "temperature": 0.0, "avg_logprob": -0.25860790925867416, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.11278660595417023}, {"id": 1052, "seek": 520150, "start": 5227.54, "end": 5230.62, "text": " a deeper model and whether that turns out to be better.", "tokens": [51666, 257, 7731, 2316, 293, 1968, 300, 4523, 484, 281, 312, 1101, 13, 51820], "temperature": 0.0, "avg_logprob": -0.25860790925867416, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.11278660595417023}, {"id": 1053, "seek": 523062, "start": 5230.74, "end": 5235.74, "text": " But obviously with these linear models, it saves a lot of computational time as well.", "tokens": [50370, 583, 2745, 365, 613, 8213, 5245, 11, 309, 19155, 257, 688, 295, 28270, 565, 382, 731, 13, 50620], "temperature": 0.0, "avg_logprob": -0.3905687003300108, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.15200144052505493}, {"id": 1054, "seek": 523062, "start": 5235.74, "end": 5243.3, "text": " The embeddings, even though they were built using linear models, we can now use them as", "tokens": [50620, 440, 12240, 29432, 11, 754, 1673, 436, 645, 3094, 1228, 8213, 5245, 11, 321, 393, 586, 764, 552, 382, 50998], "temperature": 0.0, "avg_logprob": -0.3905687003300108, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.15200144052505493}, {"id": 1055, "seek": 523062, "start": 5243.3, "end": 5249.42, "text": " inputs to deep models, which is what we're about to do.", "tokens": [50998, 15743, 281, 2452, 5245, 11, 597, 307, 437, 321, 434, 466, 281, 360, 13, 51304], "temperature": 0.0, "avg_logprob": -0.3905687003300108, "compression_ratio": 1.4967320261437909, "no_speech_prob": 0.15200144052505493}, {"id": 1056, "seek": 524942, "start": 5249.42, "end": 5259.14, "text": " Question Time 6", "tokens": [50364, 14464, 6161, 1386, 50850], "temperature": 0.0, "avg_logprob": -0.4844916752406529, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.21997669339179993}, {"id": 1057, "seek": 524942, "start": 5259.14, "end": 5261.14, "text": " I was mentioning Word2Vec.", "tokens": [50850, 286, 390, 18315, 8725, 17, 53, 3045, 13, 50950], "temperature": 0.0, "avg_logprob": -0.4844916752406529, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.21997669339179993}, {"id": 1058, "seek": 524942, "start": 5261.14, "end": 5268.62, "text": " Word2Vec has been around for 2.5 years, 2 years.", "tokens": [50950, 8725, 17, 53, 3045, 575, 668, 926, 337, 568, 13, 20, 924, 11, 568, 924, 13, 51324], "temperature": 0.0, "avg_logprob": -0.4844916752406529, "compression_ratio": 1.0833333333333333, "no_speech_prob": 0.21997669339179993}, {"id": 1059, "seek": 529862, "start": 5299.42, "end": 5303.0199999999995, "text": " All of these word vectors do all of these things.", "tokens": [50404, 1057, 295, 613, 1349, 18875, 360, 439, 295, 613, 721, 13, 50584], "temperature": 0.0, "avg_logprob": -0.2580117346013634, "compression_ratio": 1.675, "no_speech_prob": 0.07807373255491257}, {"id": 1060, "seek": 529862, "start": 5303.0199999999995, "end": 5309.46, "text": " In that high-dimensional space, for example, you can see here is information about tense.", "tokens": [50584, 682, 300, 1090, 12, 18759, 1901, 11, 337, 1365, 11, 291, 393, 536, 510, 307, 1589, 466, 18760, 13, 50906], "temperature": 0.0, "avg_logprob": -0.2580117346013634, "compression_ratio": 1.675, "no_speech_prob": 0.07807373255491257}, {"id": 1061, "seek": 529862, "start": 5309.46, "end": 5315.3, "text": " So it's very easy to take a word vector and use it to create a part-of-speech recognizer.", "tokens": [50906, 407, 309, 311, 588, 1858, 281, 747, 257, 1349, 8062, 293, 764, 309, 281, 1884, 257, 644, 12, 2670, 12, 7053, 5023, 3068, 6545, 13, 51198], "temperature": 0.0, "avg_logprob": -0.2580117346013634, "compression_ratio": 1.675, "no_speech_prob": 0.07807373255491257}, {"id": 1062, "seek": 529862, "start": 5315.3, "end": 5320.46, "text": " You just need a fairly small labeled corpus, and it's actually pretty easy to download", "tokens": [51198, 509, 445, 643, 257, 6457, 1359, 21335, 1181, 31624, 11, 293, 309, 311, 767, 1238, 1858, 281, 5484, 51456], "temperature": 0.0, "avg_logprob": -0.2580117346013634, "compression_ratio": 1.675, "no_speech_prob": 0.07807373255491257}, {"id": 1063, "seek": 529862, "start": 5320.46, "end": 5326.18, "text": " a rather large labeled corpus, and build a simple model that goes from word vector to", "tokens": [51456, 257, 2831, 2416, 21335, 1181, 31624, 11, 293, 1322, 257, 2199, 2316, 300, 1709, 490, 1349, 8062, 281, 51742], "temperature": 0.0, "avg_logprob": -0.2580117346013634, "compression_ratio": 1.675, "no_speech_prob": 0.07807373255491257}, {"id": 1064, "seek": 532618, "start": 5326.18, "end": 5327.18, "text": " part-of-speech.", "tokens": [50364, 644, 12, 2670, 12, 7053, 5023, 13, 50414], "temperature": 0.0, "avg_logprob": -0.28090245536204134, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.014281189069151878}, {"id": 1065, "seek": 532618, "start": 5327.18, "end": 5335.740000000001, "text": " There's a really interesting paper called Exploring the Limits of Language Modeling.", "tokens": [50414, 821, 311, 257, 534, 1880, 3035, 1219, 12514, 3662, 264, 16406, 1208, 295, 24445, 6583, 11031, 13, 50842], "temperature": 0.0, "avg_logprob": -0.28090245536204134, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.014281189069151878}, {"id": 1066, "seek": 532618, "start": 5335.740000000001, "end": 5341.820000000001, "text": " That Parsi-McParseface thing got far more PR than it deserved.", "tokens": [50842, 663, 430, 32742, 12, 46716, 47, 685, 68, 2868, 551, 658, 1400, 544, 11568, 813, 309, 27964, 13, 51146], "temperature": 0.0, "avg_logprob": -0.28090245536204134, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.014281189069151878}, {"id": 1067, "seek": 532618, "start": 5341.820000000001, "end": 5351.1, "text": " It was not really an advance over the state-of-the-art language models of the time.", "tokens": [51146, 467, 390, 406, 534, 364, 7295, 670, 264, 1785, 12, 2670, 12, 3322, 12, 446, 2856, 5245, 295, 264, 565, 13, 51610], "temperature": 0.0, "avg_logprob": -0.28090245536204134, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.014281189069151878}, {"id": 1068, "seek": 532618, "start": 5351.1, "end": 5355.700000000001, "text": " But since that time, there have been some much more interesting things.", "tokens": [51610, 583, 1670, 300, 565, 11, 456, 362, 668, 512, 709, 544, 1880, 721, 13, 51840], "temperature": 0.0, "avg_logprob": -0.28090245536204134, "compression_ratio": 1.5118483412322274, "no_speech_prob": 0.014281189069151878}, {"id": 1069, "seek": 535570, "start": 5356.22, "end": 5359.5, "text": " One of the interesting papers is Exploring the Limits of Language Modeling, which is", "tokens": [50390, 1485, 295, 264, 1880, 10577, 307, 12514, 3662, 264, 16406, 1208, 295, 24445, 6583, 11031, 11, 597, 307, 50554], "temperature": 0.0, "avg_logprob": -0.24205831639906938, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.01495515275746584}, {"id": 1070, "seek": 535570, "start": 5359.5, "end": 5368.7, "text": " looking at what happens when you take a very, very, very large dataset and spend shitloads", "tokens": [50554, 1237, 412, 437, 2314, 562, 291, 747, 257, 588, 11, 588, 11, 588, 2416, 28872, 293, 3496, 4611, 2907, 82, 51014], "temperature": 0.0, "avg_logprob": -0.24205831639906938, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.01495515275746584}, {"id": 1071, "seek": 535570, "start": 5368.7, "end": 5374.74, "text": " of Google's money on lots and lots of GPUs for a very long time.", "tokens": [51014, 295, 3329, 311, 1460, 322, 3195, 293, 3195, 295, 18407, 82, 337, 257, 588, 938, 565, 13, 51316], "temperature": 0.0, "avg_logprob": -0.24205831639906938, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.01495515275746584}, {"id": 1072, "seek": 535570, "start": 5374.74, "end": 5381.62, "text": " They have some genuine, massive improvements to the state-of-the-art in language modeling.", "tokens": [51316, 814, 362, 512, 16699, 11, 5994, 13797, 281, 264, 1785, 12, 2670, 12, 3322, 12, 446, 294, 2856, 15983, 13, 51660], "temperature": 0.0, "avg_logprob": -0.24205831639906938, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.01495515275746584}, {"id": 1073, "seek": 538162, "start": 5381.78, "end": 5386.22, "text": " In general, when we're talking about language modeling, we're talking about things like", "tokens": [50372, 682, 2674, 11, 562, 321, 434, 1417, 466, 2856, 15983, 11, 321, 434, 1417, 466, 721, 411, 50594], "temperature": 0.0, "avg_logprob": -0.28522954362162045, "compression_ratio": 1.703125, "no_speech_prob": 0.25380614399909973}, {"id": 1074, "seek": 538162, "start": 5386.22, "end": 5392.9, "text": " is this a noun or a verb, is this a happy sentence or a sad sentence, is this a formal", "tokens": [50594, 307, 341, 257, 23307, 420, 257, 9595, 11, 307, 341, 257, 2055, 8174, 420, 257, 4227, 8174, 11, 307, 341, 257, 9860, 50928], "temperature": 0.0, "avg_logprob": -0.28522954362162045, "compression_ratio": 1.703125, "no_speech_prob": 0.25380614399909973}, {"id": 1075, "seek": 538162, "start": 5392.9, "end": 5397.66, "text": " speech or an informal speech, and so on and so forth.", "tokens": [50928, 6218, 420, 364, 24342, 6218, 11, 293, 370, 322, 293, 370, 5220, 13, 51166], "temperature": 0.0, "avg_logprob": -0.28522954362162045, "compression_ratio": 1.703125, "no_speech_prob": 0.25380614399909973}, {"id": 1076, "seek": 538162, "start": 5397.66, "end": 5401.86, "text": " And all of these things that NLP researchers do, we can now do super-easily with these", "tokens": [51166, 400, 439, 295, 613, 721, 300, 426, 45196, 10309, 360, 11, 321, 393, 586, 360, 1687, 12, 68, 296, 953, 365, 613, 51376], "temperature": 0.0, "avg_logprob": -0.28522954362162045, "compression_ratio": 1.703125, "no_speech_prob": 0.25380614399909973}, {"id": 1077, "seek": 538162, "start": 5401.86, "end": 5402.86, "text": " embeddings.", "tokens": [51376, 12240, 29432, 13, 51426], "temperature": 0.0, "avg_logprob": -0.28522954362162045, "compression_ratio": 1.703125, "no_speech_prob": 0.25380614399909973}, {"id": 1078, "seek": 540286, "start": 5402.86, "end": 5409.54, "text": " Question, did it use the optimizer as Adam, which automatically did generate all of those", "tokens": [50364, 14464, 11, 630, 309, 764, 264, 5028, 6545, 382, 7938, 11, 597, 6772, 630, 8460, 439, 295, 729, 50698], "temperature": 0.0, "avg_logprob": -0.4265732055014752, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.10668434947729111}, {"id": 1079, "seek": 540286, "start": 5409.54, "end": 5415.259999999999, "text": " latent factors, which would translate into some of these verbs?", "tokens": [50698, 48994, 6771, 11, 597, 576, 13799, 666, 512, 295, 613, 30051, 30, 50984], "temperature": 0.0, "avg_logprob": -0.4265732055014752, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.10668434947729111}, {"id": 1080, "seek": 540286, "start": 5415.259999999999, "end": 5422.099999999999, "text": " This uses two techniques, one of which you know and one of which you're about to know,", "tokens": [50984, 639, 4960, 732, 7512, 11, 472, 295, 597, 291, 458, 293, 472, 295, 597, 291, 434, 466, 281, 458, 11, 51326], "temperature": 0.0, "avg_logprob": -0.4265732055014752, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.10668434947729111}, {"id": 1081, "seek": 540286, "start": 5422.099999999999, "end": 5426.0199999999995, "text": " convolutional neural networks and recurrent neural networks, specifically a type called", "tokens": [51326, 45216, 304, 18161, 9590, 293, 18680, 1753, 18161, 9590, 11, 4682, 257, 2010, 1219, 51522], "temperature": 0.0, "avg_logprob": -0.4265732055014752, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.10668434947729111}, {"id": 1082, "seek": 540286, "start": 5426.0199999999995, "end": 5429.339999999999, "text": " LSTM.", "tokens": [51522, 441, 6840, 44, 13, 51688], "temperature": 0.0, "avg_logprob": -0.4265732055014752, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.10668434947729111}, {"id": 1083, "seek": 540286, "start": 5429.339999999999, "end": 5432.219999999999, "text": " You can check out this paper to see how they compare.", "tokens": [51688, 509, 393, 1520, 484, 341, 3035, 281, 536, 577, 436, 6794, 13, 51832], "temperature": 0.0, "avg_logprob": -0.4265732055014752, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.10668434947729111}, {"id": 1084, "seek": 543222, "start": 5432.58, "end": 5436.780000000001, "text": " Since this time, there's been an even newer paper that has furthered the state-of-the-art", "tokens": [50382, 4162, 341, 565, 11, 456, 311, 668, 364, 754, 17628, 3035, 300, 575, 3052, 292, 264, 1785, 12, 2670, 12, 3322, 12, 446, 50592], "temperature": 0.0, "avg_logprob": -0.20515533054576202, "compression_ratio": 1.4873417721518987, "no_speech_prob": 0.004982092417776585}, {"id": 1085, "seek": 543222, "start": 5436.780000000001, "end": 5441.02, "text": " in language modeling and it's using a convolutional neural network.", "tokens": [50592, 294, 2856, 15983, 293, 309, 311, 1228, 257, 45216, 304, 18161, 3209, 13, 50804], "temperature": 0.0, "avg_logprob": -0.20515533054576202, "compression_ratio": 1.4873417721518987, "no_speech_prob": 0.004982092417776585}, {"id": 1086, "seek": 543222, "start": 5441.02, "end": 5456.3, "text": " So right now, CNNs with pre-trained word embeddings are the state-of-the-art.", "tokens": [50804, 407, 558, 586, 11, 24859, 82, 365, 659, 12, 17227, 2001, 1349, 12240, 29432, 366, 264, 1785, 12, 2670, 12, 3322, 12, 446, 13, 51568], "temperature": 0.0, "avg_logprob": -0.20515533054576202, "compression_ratio": 1.4873417721518987, "no_speech_prob": 0.004982092417776585}, {"id": 1087, "seek": 545630, "start": 5456.3, "end": 5464.3, "text": " So given that we can now download these pre-trained word embeddings, that leads to the question", "tokens": [50364, 407, 2212, 300, 321, 393, 586, 5484, 613, 659, 12, 17227, 2001, 1349, 12240, 29432, 11, 300, 6689, 281, 264, 1168, 50764], "temperature": 0.0, "avg_logprob": -0.2519216413621779, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.019419100135564804}, {"id": 1088, "seek": 545630, "start": 5464.3, "end": 5472.5, "text": " of why are we using randomly generated word embeddings when we do our sentiment analysis.", "tokens": [50764, 295, 983, 366, 321, 1228, 16979, 10833, 1349, 12240, 29432, 562, 321, 360, 527, 16149, 5215, 13, 51174], "temperature": 0.0, "avg_logprob": -0.2519216413621779, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.019419100135564804}, {"id": 1089, "seek": 545630, "start": 5472.5, "end": 5475.34, "text": " That doesn't seem like a very good idea.", "tokens": [51174, 663, 1177, 380, 1643, 411, 257, 588, 665, 1558, 13, 51316], "temperature": 0.0, "avg_logprob": -0.2519216413621779, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.019419100135564804}, {"id": 1090, "seek": 545630, "start": 5475.34, "end": 5478.58, "text": " And indeed, it's not a remotely good idea.", "tokens": [51316, 400, 6451, 11, 309, 311, 406, 257, 20824, 665, 1558, 13, 51478], "temperature": 0.0, "avg_logprob": -0.2519216413621779, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.019419100135564804}, {"id": 1091, "seek": 545630, "start": 5478.58, "end": 5481.9400000000005, "text": " You should never do that.", "tokens": [51478, 509, 820, 1128, 360, 300, 13, 51646], "temperature": 0.0, "avg_logprob": -0.2519216413621779, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.019419100135564804}, {"id": 1092, "seek": 548194, "start": 5481.94, "end": 5491.0599999999995, "text": " From now on, you should now always use pre-trained word embeddings anytime you do NLP.", "tokens": [50364, 3358, 586, 322, 11, 291, 820, 586, 1009, 764, 659, 12, 17227, 2001, 1349, 12240, 29432, 13038, 291, 360, 426, 45196, 13, 50820], "temperature": 0.0, "avg_logprob": -0.25152823130289714, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.031616706401109695}, {"id": 1093, "seek": 548194, "start": 5491.0599999999995, "end": 5495.62, "text": " And over the next few weeks, we will be gradually making this easier and easier.", "tokens": [50820, 400, 670, 264, 958, 1326, 3259, 11, 321, 486, 312, 13145, 1455, 341, 3571, 293, 3571, 13, 51048], "temperature": 0.0, "avg_logprob": -0.25152823130289714, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.031616706401109695}, {"id": 1094, "seek": 548194, "start": 5495.62, "end": 5499.74, "text": " At this stage, it requires slightly less than a screen of code.", "tokens": [51048, 1711, 341, 3233, 11, 309, 7029, 4748, 1570, 813, 257, 2568, 295, 3089, 13, 51254], "temperature": 0.0, "avg_logprob": -0.25152823130289714, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.031616706401109695}, {"id": 1095, "seek": 548194, "start": 5499.74, "end": 5506.54, "text": " You have to load the embeddings off-disk, creating your word vectors, your words and", "tokens": [51254, 509, 362, 281, 3677, 264, 12240, 29432, 766, 12, 67, 7797, 11, 4084, 428, 1349, 18875, 11, 428, 2283, 293, 51594], "temperature": 0.0, "avg_logprob": -0.25152823130289714, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.031616706401109695}, {"id": 1096, "seek": 548194, "start": 5506.54, "end": 5507.54, "text": " your word indexes.", "tokens": [51594, 428, 1349, 8186, 279, 13, 51644], "temperature": 0.0, "avg_logprob": -0.25152823130289714, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.031616706401109695}, {"id": 1097, "seek": 550754, "start": 5508.1, "end": 5514.06, "text": " The next thing you have to do is the word indexes that come from GloVe are going to", "tokens": [50392, 440, 958, 551, 291, 362, 281, 360, 307, 264, 1349, 8186, 279, 300, 808, 490, 10786, 53, 68, 366, 516, 281, 50690], "temperature": 0.0, "avg_logprob": -0.2880651392835252, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.035143885761499405}, {"id": 1098, "seek": 550754, "start": 5514.06, "end": 5521.06, "text": " be different to the word indexes in your vocabulary.", "tokens": [50690, 312, 819, 281, 264, 1349, 8186, 279, 294, 428, 19864, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2880651392835252, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.035143885761499405}, {"id": 1099, "seek": 550754, "start": 5521.06, "end": 5526.46, "text": " In our case, this was the word Bromwell, in the GloVe case it's probably not the word", "tokens": [51040, 682, 527, 1389, 11, 341, 390, 264, 1349, 1603, 298, 6326, 11, 294, 264, 10786, 53, 68, 1389, 309, 311, 1391, 406, 264, 1349, 51310], "temperature": 0.0, "avg_logprob": -0.2880651392835252, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.035143885761499405}, {"id": 1100, "seek": 550754, "start": 5526.46, "end": 5527.46, "text": " Bromwell.", "tokens": [51310, 1603, 298, 6326, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2880651392835252, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.035143885761499405}, {"id": 1101, "seek": 550754, "start": 5527.46, "end": 5534.42, "text": " So this little piece of code is simply something that is mapping from one index to the other", "tokens": [51360, 407, 341, 707, 2522, 295, 3089, 307, 2935, 746, 300, 307, 18350, 490, 472, 8186, 281, 264, 661, 51708], "temperature": 0.0, "avg_logprob": -0.2880651392835252, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.035143885761499405}, {"id": 1102, "seek": 550754, "start": 5534.42, "end": 5536.38, "text": " index.", "tokens": [51708, 8186, 13, 51806], "temperature": 0.0, "avg_logprob": -0.2880651392835252, "compression_ratio": 1.6852791878172588, "no_speech_prob": 0.035143885761499405}, {"id": 1103, "seek": 553638, "start": 5536.82, "end": 5548.18, "text": " This createEmbedding function is then going to create an embedding matrix where the indexes", "tokens": [50386, 639, 1884, 36, 2504, 292, 3584, 2445, 307, 550, 516, 281, 1884, 364, 12240, 3584, 8141, 689, 264, 8186, 279, 50954], "temperature": 0.0, "avg_logprob": -0.2407565411226249, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0005193000542931259}, {"id": 1104, "seek": 553638, "start": 5548.18, "end": 5555.42, "text": " are the indexes in the IMDB dataset and the embeddings are the embeddings from GloVe.", "tokens": [50954, 366, 264, 8186, 279, 294, 264, 21463, 27735, 28872, 293, 264, 12240, 29432, 366, 264, 12240, 29432, 490, 10786, 53, 68, 13, 51316], "temperature": 0.0, "avg_logprob": -0.2407565411226249, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0005193000542931259}, {"id": 1105, "seek": 553638, "start": 5555.42, "end": 5558.12, "text": " So that's what EMB now contains.", "tokens": [51316, 407, 300, 311, 437, 16237, 33, 586, 8306, 13, 51451], "temperature": 0.0, "avg_logprob": -0.2407565411226249, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0005193000542931259}, {"id": 1106, "seek": 553638, "start": 5558.12, "end": 5564.5, "text": " This embedding matrix are the GloVe word vectors indexed according to the IMDB dataset.", "tokens": [51451, 639, 12240, 3584, 8141, 366, 264, 10786, 53, 68, 1349, 18875, 8186, 292, 4650, 281, 264, 21463, 27735, 28872, 13, 51770], "temperature": 0.0, "avg_logprob": -0.2407565411226249, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0005193000542931259}, {"id": 1107, "seek": 556450, "start": 5564.62, "end": 5571.74, "text": " Now I have simply copied and pasted the previous code and I have added this, weights equals", "tokens": [50370, 823, 286, 362, 2935, 25365, 293, 1791, 292, 264, 3894, 3089, 293, 286, 362, 3869, 341, 11, 17443, 6915, 50726], "temperature": 0.0, "avg_logprob": -0.23986543927873885, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.0014325098600238562}, {"id": 1108, "seek": 556450, "start": 5571.74, "end": 5574.3, "text": " my pre-trained embeddings.", "tokens": [50726, 452, 659, 12, 17227, 2001, 12240, 29432, 13, 50854], "temperature": 0.0, "avg_logprob": -0.23986543927873885, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.0014325098600238562}, {"id": 1109, "seek": 556450, "start": 5574.3, "end": 5579.62, "text": " Since we think these embeddings are pretty good, I've set trainable to false.", "tokens": [50854, 4162, 321, 519, 613, 12240, 29432, 366, 1238, 665, 11, 286, 600, 992, 3847, 712, 281, 7908, 13, 51120], "temperature": 0.0, "avg_logprob": -0.23986543927873885, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.0014325098600238562}, {"id": 1110, "seek": 556450, "start": 5579.62, "end": 5583.78, "text": " I won't leave it at false because we're going to need to fine-tune them, but we'll start", "tokens": [51120, 286, 1582, 380, 1856, 309, 412, 7908, 570, 321, 434, 516, 281, 643, 281, 2489, 12, 83, 2613, 552, 11, 457, 321, 603, 722, 51328], "temperature": 0.0, "avg_logprob": -0.23986543927873885, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.0014325098600238562}, {"id": 1111, "seek": 556450, "start": 5583.78, "end": 5584.98, "text": " it at false.", "tokens": [51328, 309, 412, 7908, 13, 51388], "temperature": 0.0, "avg_logprob": -0.23986543927873885, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.0014325098600238562}, {"id": 1112, "seek": 556450, "start": 5584.98, "end": 5589.46, "text": " One particular reason that we can't leave it at false is that sometimes I have had to", "tokens": [51388, 1485, 1729, 1778, 300, 321, 393, 380, 1856, 309, 412, 7908, 307, 300, 2171, 286, 362, 632, 281, 51612], "temperature": 0.0, "avg_logprob": -0.23986543927873885, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.0014325098600238562}, {"id": 1113, "seek": 558946, "start": 5589.46, "end": 5595.74, "text": " create a random embedding because sometimes the word that I looked up in GloVe didn't", "tokens": [50364, 1884, 257, 4974, 12240, 3584, 570, 2171, 264, 1349, 300, 286, 2956, 493, 294, 10786, 53, 68, 994, 380, 50678], "temperature": 0.0, "avg_logprob": -0.2755983317339862, "compression_ratio": 1.6761133603238867, "no_speech_prob": 0.28138917684555054}, {"id": 1114, "seek": 558946, "start": 5595.74, "end": 5596.74, "text": " exist.", "tokens": [50678, 2514, 13, 50728], "temperature": 0.0, "avg_logprob": -0.2755983317339862, "compression_ratio": 1.6761133603238867, "no_speech_prob": 0.28138917684555054}, {"id": 1115, "seek": 558946, "start": 5596.74, "end": 5603.18, "text": " For example, anything that finishes with apostrophe S, in GloVe they tokenized that to have apostrophe", "tokens": [50728, 1171, 1365, 11, 1340, 300, 23615, 365, 19484, 27194, 318, 11, 294, 10786, 53, 68, 436, 14862, 1602, 300, 281, 362, 19484, 27194, 51050], "temperature": 0.0, "avg_logprob": -0.2755983317339862, "compression_ratio": 1.6761133603238867, "no_speech_prob": 0.28138917684555054}, {"id": 1116, "seek": 558946, "start": 5603.18, "end": 5609.22, "text": " S and the word as separate tokens, but in IMDB they were combined into one token.", "tokens": [51050, 318, 293, 264, 1349, 382, 4994, 22667, 11, 457, 294, 21463, 27735, 436, 645, 9354, 666, 472, 14862, 13, 51352], "temperature": 0.0, "avg_logprob": -0.2755983317339862, "compression_ratio": 1.6761133603238867, "no_speech_prob": 0.28138917684555054}, {"id": 1117, "seek": 558946, "start": 5609.22, "end": 5612.06, "text": " And so all of those things aren't vectors for them.", "tokens": [51352, 400, 370, 439, 295, 729, 721, 3212, 380, 18875, 337, 552, 13, 51494], "temperature": 0.0, "avg_logprob": -0.2755983317339862, "compression_ratio": 1.6761133603238867, "no_speech_prob": 0.28138917684555054}, {"id": 1118, "seek": 558946, "start": 5612.06, "end": 5618.82, "text": " So I just randomly created embeddings for anything that I couldn't find in the GloVe", "tokens": [51494, 407, 286, 445, 16979, 2942, 12240, 29432, 337, 1340, 300, 286, 2809, 380, 915, 294, 264, 10786, 53, 68, 51832], "temperature": 0.0, "avg_logprob": -0.2755983317339862, "compression_ratio": 1.6761133603238867, "no_speech_prob": 0.28138917684555054}, {"id": 1119, "seek": 561882, "start": 5619.179999999999, "end": 5620.179999999999, "text": " vocabulary.", "tokens": [50382, 19864, 13, 50432], "temperature": 0.0, "avg_logprob": -0.27115767342703684, "compression_ratio": 1.4404761904761905, "no_speech_prob": 0.08388561010360718}, {"id": 1120, "seek": 561882, "start": 5620.179999999999, "end": 5626.74, "text": " But for now, let's start using just the embeddings that we're given and we will set this to non-trainable", "tokens": [50432, 583, 337, 586, 11, 718, 311, 722, 1228, 445, 264, 12240, 29432, 300, 321, 434, 2212, 293, 321, 486, 992, 341, 281, 2107, 12, 83, 7146, 712, 50760], "temperature": 0.0, "avg_logprob": -0.27115767342703684, "compression_ratio": 1.4404761904761905, "no_speech_prob": 0.08388561010360718}, {"id": 1121, "seek": 561882, "start": 5626.74, "end": 5635.86, "text": " and we will train a convolutional neural network using those embeddings for the IMDB task.", "tokens": [50760, 293, 321, 486, 3847, 257, 45216, 304, 18161, 3209, 1228, 729, 12240, 29432, 337, 264, 21463, 27735, 5633, 13, 51216], "temperature": 0.0, "avg_logprob": -0.27115767342703684, "compression_ratio": 1.4404761904761905, "no_speech_prob": 0.08388561010360718}, {"id": 1122, "seek": 561882, "start": 5635.86, "end": 5641.2, "text": " And after 2 epochs, we have 89.8.", "tokens": [51216, 400, 934, 568, 30992, 28346, 11, 321, 362, 31877, 13, 23, 13, 51483], "temperature": 0.0, "avg_logprob": -0.27115767342703684, "compression_ratio": 1.4404761904761905, "no_speech_prob": 0.08388561010360718}, {"id": 1123, "seek": 564120, "start": 5641.2, "end": 5649.58, "text": " Similarly with random embeddings, we had 89.5 and the academic state-of-the-art was", "tokens": [50364, 13157, 365, 4974, 12240, 29432, 11, 321, 632, 31877, 13, 20, 293, 264, 7778, 1785, 12, 2670, 12, 3322, 12, 446, 390, 50783], "temperature": 0.0, "avg_logprob": -0.25937416974235983, "compression_ratio": 1.3775510204081634, "no_speech_prob": 0.14414161443710327}, {"id": 1124, "seek": 564120, "start": 5649.58, "end": 5651.82, "text": " 88.3.", "tokens": [50783, 24587, 13, 18, 13, 50895], "temperature": 0.0, "avg_logprob": -0.25937416974235983, "compression_ratio": 1.3775510204081634, "no_speech_prob": 0.14414161443710327}, {"id": 1125, "seek": 564120, "start": 5651.82, "end": 5656.679999999999, "text": " So we made significant improvements.", "tokens": [50895, 407, 321, 1027, 4776, 13797, 13, 51138], "temperature": 0.0, "avg_logprob": -0.25937416974235983, "compression_ratio": 1.3775510204081634, "no_speech_prob": 0.14414161443710327}, {"id": 1126, "seek": 564120, "start": 5656.679999999999, "end": 5663.72, "text": " Let's now go ahead and say first-layer trainable is true.", "tokens": [51138, 961, 311, 586, 352, 2286, 293, 584, 700, 12, 8376, 260, 3847, 712, 307, 2074, 13, 51490], "temperature": 0.0, "avg_logprob": -0.25937416974235983, "compression_ratio": 1.3775510204081634, "no_speech_prob": 0.14414161443710327}, {"id": 1127, "seek": 564120, "start": 5663.72, "end": 5669.9, "text": " Decrease the learning rate a bit and do just one more epoch and we're now up to 90.1.", "tokens": [51490, 12427, 265, 651, 264, 2539, 3314, 257, 857, 293, 360, 445, 472, 544, 30992, 339, 293, 321, 434, 586, 493, 281, 4289, 13, 16, 13, 51799], "temperature": 0.0, "avg_logprob": -0.25937416974235983, "compression_ratio": 1.3775510204081634, "no_speech_prob": 0.14414161443710327}, {"id": 1128, "seek": 566990, "start": 5669.9, "end": 5674.44, "text": " So we've got way beyond the academic state-of-the-art here.", "tokens": [50364, 407, 321, 600, 658, 636, 4399, 264, 7778, 1785, 12, 2670, 12, 3322, 12, 446, 510, 13, 50591], "temperature": 0.0, "avg_logprob": -0.26997121774925376, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.010488998144865036}, {"id": 1129, "seek": 566990, "start": 5674.44, "end": 5680.7, "text": " We're kind of cheating because we're now not just building a model, we're now using a pre-trained", "tokens": [50591, 492, 434, 733, 295, 18309, 570, 321, 434, 586, 406, 445, 2390, 257, 2316, 11, 321, 434, 586, 1228, 257, 659, 12, 17227, 2001, 50904], "temperature": 0.0, "avg_logprob": -0.26997121774925376, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.010488998144865036}, {"id": 1130, "seek": 566990, "start": 5680.7, "end": 5684.9, "text": " word embedding model that somebody else has provided for us.", "tokens": [50904, 1349, 12240, 3584, 2316, 300, 2618, 1646, 575, 5649, 337, 505, 13, 51114], "temperature": 0.0, "avg_logprob": -0.26997121774925376, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.010488998144865036}, {"id": 1131, "seek": 566990, "start": 5684.9, "end": 5688.82, "text": " But why would you ever not do that now that that exists?", "tokens": [51114, 583, 983, 576, 291, 1562, 406, 360, 300, 586, 300, 300, 8198, 30, 51310], "temperature": 0.0, "avg_logprob": -0.26997121774925376, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.010488998144865036}, {"id": 1132, "seek": 566990, "start": 5688.82, "end": 5693.66, "text": " So you can see that we've had a big jump and furthermore it's only taken us 12 seconds", "tokens": [51310, 407, 291, 393, 536, 300, 321, 600, 632, 257, 955, 3012, 293, 3052, 3138, 309, 311, 787, 2726, 505, 2272, 3949, 51552], "temperature": 0.0, "avg_logprob": -0.26997121774925376, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.010488998144865036}, {"id": 1133, "seek": 566990, "start": 5693.66, "end": 5696.0199999999995, "text": " to train this network.", "tokens": [51552, 281, 3847, 341, 3209, 13, 51670], "temperature": 0.0, "avg_logprob": -0.26997121774925376, "compression_ratio": 1.5587044534412955, "no_speech_prob": 0.010488998144865036}, {"id": 1134, "seek": 569602, "start": 5696.02, "end": 5699.26, "text": " So we started out with the pre-trained word embeddings.", "tokens": [50364, 407, 321, 1409, 484, 365, 264, 659, 12, 17227, 2001, 1349, 12240, 29432, 13, 50526], "temperature": 0.0, "avg_logprob": -0.2630108070373535, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0070114741101861}, {"id": 1135, "seek": 569602, "start": 5699.26, "end": 5710.1, "text": " We set them initially to non-trainable in order to just train the layers that used them.", "tokens": [50526, 492, 992, 552, 9105, 281, 2107, 12, 83, 7146, 712, 294, 1668, 281, 445, 3847, 264, 7914, 300, 1143, 552, 13, 51068], "temperature": 0.0, "avg_logprob": -0.2630108070373535, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0070114741101861}, {"id": 1136, "seek": 569602, "start": 5710.1, "end": 5716.22, "text": " Waited until that was stable, which took really 2 epochs, and then we set them to trainable", "tokens": [51068, 3802, 292, 1826, 300, 390, 8351, 11, 597, 1890, 534, 568, 30992, 28346, 11, 293, 550, 321, 992, 552, 281, 3847, 712, 51374], "temperature": 0.0, "avg_logprob": -0.2630108070373535, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0070114741101861}, {"id": 1137, "seek": 569602, "start": 5716.22, "end": 5719.14, "text": " and did one more little fine-tuning step.", "tokens": [51374, 293, 630, 472, 544, 707, 2489, 12, 83, 37726, 1823, 13, 51520], "temperature": 0.0, "avg_logprob": -0.2630108070373535, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0070114741101861}, {"id": 1138, "seek": 569602, "start": 5719.14, "end": 5725.22, "text": " And this kind of approach of these 3 epochs of training is likely to work for a lot of", "tokens": [51520, 400, 341, 733, 295, 3109, 295, 613, 805, 30992, 28346, 295, 3097, 307, 3700, 281, 589, 337, 257, 688, 295, 51824], "temperature": 0.0, "avg_logprob": -0.2630108070373535, "compression_ratio": 1.6441441441441442, "no_speech_prob": 0.0070114741101861}, {"id": 1139, "seek": 572522, "start": 5725.22, "end": 5728.42, "text": " the NLP stuff that you'll find in the wild.", "tokens": [50364, 264, 426, 45196, 1507, 300, 291, 603, 915, 294, 264, 4868, 13, 50524], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1140, "seek": 572522, "start": 5728.42, "end": 5730.42, "text": " Question from joseph.", "tokens": [50524, 14464, 490, 361, 541, 950, 13, 50624], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1141, "seek": 572522, "start": 5730.42, "end": 5736.3, "text": " Do you not need to compile the model after resetting the input layer to trainable equals", "tokens": [50624, 1144, 291, 406, 643, 281, 31413, 264, 2316, 934, 14322, 783, 264, 4846, 4583, 281, 3847, 712, 6915, 50918], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1142, "seek": 572522, "start": 5736.3, "end": 5737.3, "text": " true?", "tokens": [50918, 2074, 30, 50968], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1143, "seek": 572522, "start": 5737.3, "end": 5742.7, "text": " Answer No, you don't, because the architecture of", "tokens": [50968, 24545, 883, 11, 291, 500, 380, 11, 570, 264, 9482, 295, 51238], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1144, "seek": 572522, "start": 5742.7, "end": 5744.900000000001, "text": " the model has not changed in any way.", "tokens": [51238, 264, 2316, 575, 406, 3105, 294, 604, 636, 13, 51348], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1145, "seek": 572522, "start": 5744.900000000001, "end": 5748.1, "text": " It's just changed the metadata attached to it.", "tokens": [51348, 467, 311, 445, 3105, 264, 26603, 8570, 281, 309, 13, 51508], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1146, "seek": 572522, "start": 5748.1, "end": 5751.52, "text": " There's never any harm in compiling the model.", "tokens": [51508, 821, 311, 1128, 604, 6491, 294, 715, 4883, 264, 2316, 13, 51679], "temperature": 0.0, "avg_logprob": -0.44527415639346407, "compression_ratio": 1.5545454545454545, "no_speech_prob": 0.025563817471265793}, {"id": 1147, "seek": 575152, "start": 5751.52, "end": 5755.8, "text": " Sometimes if you forget to compile, it just continues to use the old model.", "tokens": [50364, 4803, 498, 291, 2870, 281, 31413, 11, 309, 445, 6515, 281, 764, 264, 1331, 2316, 13, 50578], "temperature": 0.0, "avg_logprob": -0.28801253863743376, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.3345710039138794}, {"id": 1148, "seek": 575152, "start": 5755.8, "end": 5761.6, "text": " So best to err on the side of using it.", "tokens": [50578, 407, 1151, 281, 45267, 322, 264, 1252, 295, 1228, 309, 13, 50868], "temperature": 0.0, "avg_logprob": -0.28801253863743376, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.3345710039138794}, {"id": 1149, "seek": 575152, "start": 5761.6, "end": 5766.280000000001, "text": " Something that I thought was pretty cool is that during the week, one of our students", "tokens": [50868, 6595, 300, 286, 1194, 390, 1238, 1627, 307, 300, 1830, 264, 1243, 11, 472, 295, 527, 1731, 51102], "temperature": 0.0, "avg_logprob": -0.28801253863743376, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.3345710039138794}, {"id": 1150, "seek": 575152, "start": 5766.280000000001, "end": 5771.280000000001, "text": " here had an extremely popular post appear all over the place, as I saw it on the front", "tokens": [51102, 510, 632, 364, 4664, 3743, 2183, 4204, 439, 670, 264, 1081, 11, 382, 286, 1866, 309, 322, 264, 1868, 51352], "temperature": 0.0, "avg_logprob": -0.28801253863743376, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.3345710039138794}, {"id": 1151, "seek": 575152, "start": 5771.280000000001, "end": 5776.92, "text": " page of Hacker News, talking about how his company, Quid, uses deep learning and very", "tokens": [51352, 3028, 295, 389, 23599, 7987, 11, 1417, 466, 577, 702, 2237, 11, 2326, 327, 11, 4960, 2452, 2539, 293, 588, 51634], "temperature": 0.0, "avg_logprob": -0.28801253863743376, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.3345710039138794}, {"id": 1152, "seek": 575152, "start": 5776.92, "end": 5781.240000000001, "text": " happy to see with small data, which is what we're all about.", "tokens": [51634, 2055, 281, 536, 365, 1359, 1412, 11, 597, 307, 437, 321, 434, 439, 466, 13, 51850], "temperature": 0.0, "avg_logprob": -0.28801253863743376, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.3345710039138794}, {"id": 1153, "seek": 578124, "start": 5781.96, "end": 5786.36, "text": " For those of you who don't know it, Quid is a company, quite a successful startup actually,", "tokens": [50400, 1171, 729, 295, 291, 567, 500, 380, 458, 309, 11, 2326, 327, 307, 257, 2237, 11, 1596, 257, 4406, 18578, 767, 11, 50620], "temperature": 0.0, "avg_logprob": -0.29479400634765623, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00024156166182365268}, {"id": 1154, "seek": 578124, "start": 5786.36, "end": 5792.0, "text": " that is processing millions and millions of documents, things like patents and stuff like", "tokens": [50620, 300, 307, 9007, 6803, 293, 6803, 295, 8512, 11, 721, 411, 38142, 293, 1507, 411, 50902], "temperature": 0.0, "avg_logprob": -0.29479400634765623, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00024156166182365268}, {"id": 1155, "seek": 578124, "start": 5792.0, "end": 5796.92, "text": " that, and providing enterprise customers with really cool visualizations and interactive", "tokens": [50902, 300, 11, 293, 6530, 14132, 4581, 365, 534, 1627, 5056, 14455, 293, 15141, 51148], "temperature": 0.0, "avg_logprob": -0.29479400634765623, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00024156166182365268}, {"id": 1156, "seek": 578124, "start": 5796.92, "end": 5800.88, "text": " tools that let them analyze huge datasets.", "tokens": [51148, 3873, 300, 718, 552, 12477, 2603, 42856, 13, 51346], "temperature": 0.0, "avg_logprob": -0.29479400634765623, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00024156166182365268}, {"id": 1157, "seek": 578124, "start": 5800.88, "end": 5803.639999999999, "text": " This is by Ben Bowles, one of our students here.", "tokens": [51346, 639, 307, 538, 3964, 12903, 904, 11, 472, 295, 527, 1731, 510, 13, 51484], "temperature": 0.0, "avg_logprob": -0.29479400634765623, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00024156166182365268}, {"id": 1158, "seek": 578124, "start": 5803.639999999999, "end": 5809.92, "text": " He talked about how he compared 3 different approaches to a particular NLP classification", "tokens": [51484, 634, 2825, 466, 577, 415, 5347, 805, 819, 11587, 281, 257, 1729, 426, 45196, 21538, 51798], "temperature": 0.0, "avg_logprob": -0.29479400634765623, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00024156166182365268}, {"id": 1159, "seek": 580992, "start": 5809.96, "end": 5816.96, "text": " task, one of which involved some pretty complex and slow-to-develop, carefully engineered", "tokens": [50366, 5633, 11, 472, 295, 597, 3288, 512, 1238, 3997, 293, 2964, 12, 1353, 12, 35464, 11, 7500, 38648, 50716], "temperature": 0.0, "avg_logprob": -0.3980471871115945, "compression_ratio": 1.4269662921348314, "no_speech_prob": 0.10229772329330444}, {"id": 1160, "seek": 580992, "start": 5820.56, "end": 5821.56, "text": " features.", "tokens": [50896, 4122, 13, 50946], "temperature": 0.0, "avg_logprob": -0.3980471871115945, "compression_ratio": 1.4269662921348314, "no_speech_prob": 0.10229772329330444}, {"id": 1161, "seek": 580992, "start": 5821.56, "end": 5827.72, "text": " But Model 3, in this example, was a convolutional neural network.", "tokens": [50946, 583, 17105, 805, 11, 294, 341, 1365, 11, 390, 257, 45216, 304, 18161, 3209, 13, 51254], "temperature": 0.0, "avg_logprob": -0.3980471871115945, "compression_ratio": 1.4269662921348314, "no_speech_prob": 0.10229772329330444}, {"id": 1162, "seek": 580992, "start": 5827.72, "end": 5834.72, "text": " So I think this is pretty cool and I was hoping to talk to Ben about this piece of work.", "tokens": [51254, 407, 286, 519, 341, 307, 1238, 1627, 293, 286, 390, 7159, 281, 751, 281, 3964, 466, 341, 2522, 295, 589, 13, 51604], "temperature": 0.0, "avg_logprob": -0.3980471871115945, "compression_ratio": 1.4269662921348314, "no_speech_prob": 0.10229772329330444}, {"id": 1163, "seek": 583472, "start": 5835.72, "end": 5842.72, "text": " Question, Could you give us a little bit of context on what you were doing in this project?", "tokens": [50414, 14464, 11, 7497, 291, 976, 505, 257, 707, 857, 295, 4319, 322, 437, 291, 645, 884, 294, 341, 1716, 30, 50764], "temperature": 0.0, "avg_logprob": -0.49344560469704113, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.018544916063547134}, {"id": 1164, "seek": 583472, "start": 5843.400000000001, "end": 5850.400000000001, "text": " Answer, Yes, so the task is about detecting marketing language from company descriptions.", "tokens": [50798, 24545, 11, 1079, 11, 370, 264, 5633, 307, 466, 40237, 6370, 2856, 490, 2237, 24406, 13, 51148], "temperature": 0.0, "avg_logprob": -0.49344560469704113, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.018544916063547134}, {"id": 1165, "seek": 583472, "start": 5851.0, "end": 5854.72, "text": " So it had the flavor of being very similar to sentiment analysis.", "tokens": [51178, 407, 309, 632, 264, 6813, 295, 885, 588, 2531, 281, 16149, 5215, 13, 51364], "temperature": 0.0, "avg_logprob": -0.49344560469704113, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.018544916063547134}, {"id": 1166, "seek": 583472, "start": 5854.72, "end": 5859.52, "text": " You have two classes of things, they're kind of different in some kind of semantic way.", "tokens": [51364, 509, 362, 732, 5359, 295, 721, 11, 436, 434, 733, 295, 819, 294, 512, 733, 295, 47982, 636, 13, 51604], "temperature": 0.0, "avg_logprob": -0.49344560469704113, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.018544916063547134}, {"id": 1167, "seek": 583472, "start": 5859.52, "end": 5861.52, "text": " You've got some examples here.", "tokens": [51604, 509, 600, 658, 512, 5110, 510, 13, 51704], "temperature": 0.0, "avg_logprob": -0.49344560469704113, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.018544916063547134}, {"id": 1168, "seek": 586152, "start": 5861.52, "end": 5866.080000000001, "text": " One was like, our patent-pending support system is engineered to bring comfort and style.", "tokens": [50364, 1485, 390, 411, 11, 527, 20495, 12, 79, 2029, 1406, 1185, 307, 38648, 281, 1565, 3400, 293, 3758, 13, 50592], "temperature": 0.0, "avg_logprob": -0.45443683589270356, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.1560727208852768}, {"id": 1169, "seek": 586152, "start": 5866.080000000001, "end": 5871.320000000001, "text": " With your more marketing, I guess, and your spatial scanning software for mobile devices", "tokens": [50592, 2022, 428, 544, 6370, 11, 286, 2041, 11, 293, 428, 23598, 27019, 4722, 337, 6013, 5759, 50854], "temperature": 0.0, "avg_logprob": -0.45443683589270356, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.1560727208852768}, {"id": 1170, "seek": 586152, "start": 5871.320000000001, "end": 5872.92, "text": " is more informative.", "tokens": [50854, 307, 544, 27759, 13, 50934], "temperature": 0.0, "avg_logprob": -0.45443683589270356, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.1560727208852768}, {"id": 1171, "seek": 586152, "start": 5872.92, "end": 5877.92, "text": " Answer, Yes, the semantics of the marketing language is like, oh, this is exciting.", "tokens": [50934, 24545, 11, 1079, 11, 264, 4361, 45298, 295, 264, 6370, 2856, 307, 411, 11, 1954, 11, 341, 307, 4670, 13, 51184], "temperature": 0.0, "avg_logprob": -0.45443683589270356, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.1560727208852768}, {"id": 1172, "seek": 586152, "start": 5877.92, "end": 5884.92, "text": " There are certain types of meanings and semantics around which the marketing tends to cluster.", "tokens": [51184, 821, 366, 1629, 3467, 295, 28138, 293, 4361, 45298, 926, 597, 264, 6370, 12258, 281, 13630, 13, 51534], "temperature": 0.0, "avg_logprob": -0.45443683589270356, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.1560727208852768}, {"id": 1173, "seek": 586152, "start": 5884.92, "end": 5889.080000000001, "text": " I sort of realized, hey, this would be kind of a nice task for deep learning.", "tokens": [51534, 286, 1333, 295, 5334, 11, 4177, 11, 341, 576, 312, 733, 295, 257, 1481, 5633, 337, 2452, 2539, 13, 51742], "temperature": 0.0, "avg_logprob": -0.45443683589270356, "compression_ratio": 1.6581818181818182, "no_speech_prob": 0.1560727208852768}, {"id": 1174, "seek": 588908, "start": 5889.48, "end": 5893.48, "text": " How were these labeled, your data set, in the first place?", "tokens": [50384, 1012, 645, 613, 21335, 11, 428, 1412, 992, 11, 294, 264, 700, 1081, 30, 50584], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1175, "seek": 588908, "start": 5893.48, "end": 5898.48, "text": " Basically by a couple of us in the company, we basically just found some good ones and", "tokens": [50584, 8537, 538, 257, 1916, 295, 505, 294, 264, 2237, 11, 321, 1936, 445, 1352, 512, 665, 2306, 293, 50834], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1176, "seek": 588908, "start": 5898.48, "end": 5901.48, "text": " found the bad ones and then literally tried it out.", "tokens": [50834, 1352, 264, 1578, 2306, 293, 550, 3736, 3031, 309, 484, 13, 50984], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1177, "seek": 588908, "start": 5901.48, "end": 5905.48, "text": " It was literally as hacky as you could possibly imagine.", "tokens": [50984, 467, 390, 3736, 382, 10339, 88, 382, 291, 727, 6264, 3811, 13, 51184], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1178, "seek": 588908, "start": 5905.48, "end": 5909.48, "text": " It was kind of super, super scrappy.", "tokens": [51184, 467, 390, 733, 295, 1687, 11, 1687, 13943, 7966, 13, 51384], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1179, "seek": 588908, "start": 5909.48, "end": 5912.48, "text": " But it actually ended up being very useful for us.", "tokens": [51384, 583, 309, 767, 4590, 493, 885, 588, 4420, 337, 505, 13, 51534], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1180, "seek": 588908, "start": 5912.48, "end": 5914.48, "text": " I think that's kind of a nice lesson.", "tokens": [51534, 286, 519, 300, 311, 733, 295, 257, 1481, 6898, 13, 51634], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1181, "seek": 588908, "start": 5914.48, "end": 5917.48, "text": " Sometimes scrappy gets you most of the way you need it.", "tokens": [51634, 4803, 13943, 7966, 2170, 291, 881, 295, 264, 636, 291, 643, 309, 13, 51784], "temperature": 0.0, "avg_logprob": -0.3673693012987447, "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.270271897315979}, {"id": 1182, "seek": 591748, "start": 5917.879999999999, "end": 5919.879999999999, "text": " Think about how you get your data for your project.", "tokens": [50384, 6557, 466, 577, 291, 483, 428, 1412, 337, 428, 1716, 13, 50484], "temperature": 0.0, "avg_logprob": -0.3695476980770335, "compression_ratio": 1.5253456221198156, "no_speech_prob": 0.05339999496936798}, {"id": 1183, "seek": 591748, "start": 5919.879999999999, "end": 5922.879999999999, "text": " Well, you can actually just create it.", "tokens": [50484, 1042, 11, 291, 393, 767, 445, 1884, 309, 13, 50634], "temperature": 0.0, "avg_logprob": -0.3695476980770335, "compression_ratio": 1.5253456221198156, "no_speech_prob": 0.05339999496936798}, {"id": 1184, "seek": 591748, "start": 5922.879999999999, "end": 5927.879999999999, "text": " I love this lesson, and so is startup.", "tokens": [50634, 286, 959, 341, 6898, 11, 293, 370, 307, 18578, 13, 50884], "temperature": 0.0, "avg_logprob": -0.3695476980770335, "compression_ratio": 1.5253456221198156, "no_speech_prob": 0.05339999496936798}, {"id": 1185, "seek": 591748, "start": 5927.879999999999, "end": 5935.879999999999, "text": " When I talk to big enterprise executives, they're all about their 5-year metadata and", "tokens": [50884, 1133, 286, 751, 281, 955, 14132, 28485, 11, 436, 434, 439, 466, 641, 1025, 12, 5294, 26603, 293, 51284], "temperature": 0.0, "avg_logprob": -0.3695476980770335, "compression_ratio": 1.5253456221198156, "no_speech_prob": 0.05339999496936798}, {"id": 1186, "seek": 591748, "start": 5935.879999999999, "end": 5940.879999999999, "text": " data repository infrastructure program, at the end of which maybe they'll actually try", "tokens": [51284, 1412, 25841, 6896, 1461, 11, 412, 264, 917, 295, 597, 1310, 436, 603, 767, 853, 51534], "temperature": 0.0, "avg_logprob": -0.3695476980770335, "compression_ratio": 1.5253456221198156, "no_speech_prob": 0.05339999496936798}, {"id": 1187, "seek": 591748, "start": 5940.879999999999, "end": 5942.879999999999, "text": " to get some value out of it.", "tokens": [51534, 281, 483, 512, 2158, 484, 295, 309, 13, 51634], "temperature": 0.0, "avg_logprob": -0.3695476980770335, "compression_ratio": 1.5253456221198156, "no_speech_prob": 0.05339999496936798}, {"id": 1188, "seek": 594288, "start": 5943.28, "end": 5949.28, "text": " They're just like, okay, what have we got that we can do by Monday, let's throw it together", "tokens": [50384, 814, 434, 445, 411, 11, 1392, 11, 437, 362, 321, 658, 300, 321, 393, 360, 538, 8138, 11, 718, 311, 3507, 309, 1214, 50684], "temperature": 0.0, "avg_logprob": -0.3210639953613281, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.3701758086681366}, {"id": 1189, "seek": 594288, "start": 5949.28, "end": 5950.28, "text": " and see if it works.", "tokens": [50684, 293, 536, 498, 309, 1985, 13, 50734], "temperature": 0.0, "avg_logprob": -0.3210639953613281, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.3701758086681366}, {"id": 1190, "seek": 594288, "start": 5950.28, "end": 5956.24, "text": " The latter approach is so much better because by Monday you know whether it looks good,", "tokens": [50734, 440, 18481, 3109, 307, 370, 709, 1101, 570, 538, 8138, 291, 458, 1968, 309, 1542, 665, 11, 51032], "temperature": 0.0, "avg_logprob": -0.3210639953613281, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.3701758086681366}, {"id": 1191, "seek": 594288, "start": 5956.24, "end": 5960.76, "text": " you know which kind of things are important, and you can decide on how much it's worth", "tokens": [51032, 291, 458, 597, 733, 295, 721, 366, 1021, 11, 293, 291, 393, 4536, 322, 577, 709, 309, 311, 3163, 51258], "temperature": 0.0, "avg_logprob": -0.3210639953613281, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.3701758086681366}, {"id": 1192, "seek": 594288, "start": 5960.76, "end": 5961.76, "text": " investing in.", "tokens": [51258, 10978, 294, 13, 51308], "temperature": 0.0, "avg_logprob": -0.3210639953613281, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.3701758086681366}, {"id": 1193, "seek": 594288, "start": 5961.76, "end": 5968.28, "text": " So one of the things I wanted to show is your convolutional neural network did something", "tokens": [51308, 407, 472, 295, 264, 721, 286, 1415, 281, 855, 307, 428, 45216, 304, 18161, 3209, 630, 746, 51634], "temperature": 0.0, "avg_logprob": -0.3210639953613281, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.3701758086681366}, {"id": 1194, "seek": 594288, "start": 5968.28, "end": 5969.84, "text": " pretty neat.", "tokens": [51634, 1238, 10654, 13, 51712], "temperature": 0.0, "avg_logprob": -0.3210639953613281, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.3701758086681366}, {"id": 1195, "seek": 596984, "start": 5969.84, "end": 5975.400000000001, "text": " I wanted to use this same neat trick for our convolutional neural network, and it's a multi-size", "tokens": [50364, 286, 1415, 281, 764, 341, 912, 10654, 4282, 337, 527, 45216, 304, 18161, 3209, 11, 293, 309, 311, 257, 4825, 12, 27553, 50642], "temperature": 0.0, "avg_logprob": -0.22833708774896316, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04146003723144531}, {"id": 1196, "seek": 596984, "start": 5975.400000000001, "end": 5977.72, "text": " CNN.", "tokens": [50642, 24859, 13, 50758], "temperature": 0.0, "avg_logprob": -0.22833708774896316, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04146003723144531}, {"id": 1197, "seek": 596984, "start": 5977.72, "end": 5987.0, "text": " I mentioned earlier that when I built this CNN, I tried using a filter size of 5 and", "tokens": [50758, 286, 2835, 3071, 300, 562, 286, 3094, 341, 24859, 11, 286, 3031, 1228, 257, 6608, 2744, 295, 1025, 293, 51222], "temperature": 0.0, "avg_logprob": -0.22833708774896316, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04146003723144531}, {"id": 1198, "seek": 596984, "start": 5987.0, "end": 5990.92, "text": " I found it better than 3.", "tokens": [51222, 286, 1352, 309, 1101, 813, 805, 13, 51418], "temperature": 0.0, "avg_logprob": -0.22833708774896316, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04146003723144531}, {"id": 1199, "seek": 596984, "start": 5990.92, "end": 5996.4400000000005, "text": " What Ben in his blog post points out is that there's a neat paper in which they describe", "tokens": [51418, 708, 3964, 294, 702, 6968, 2183, 2793, 484, 307, 300, 456, 311, 257, 10654, 3035, 294, 597, 436, 6786, 51694], "temperature": 0.0, "avg_logprob": -0.22833708774896316, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04146003723144531}, {"id": 1200, "seek": 599644, "start": 5996.48, "end": 6001.44, "text": " doing something interesting, which is not just using one size convolution, but trying", "tokens": [50366, 884, 746, 1880, 11, 597, 307, 406, 445, 1228, 472, 2744, 45216, 11, 457, 1382, 50614], "temperature": 0.0, "avg_logprob": -0.31744036831698574, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.23090767860412598}, {"id": 1201, "seek": 599644, "start": 6001.44, "end": 6004.5599999999995, "text": " a few size convolutions.", "tokens": [50614, 257, 1326, 2744, 3754, 15892, 13, 50770], "temperature": 0.0, "avg_logprob": -0.31744036831698574, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.23090767860412598}, {"id": 1202, "seek": 599644, "start": 6004.5599999999995, "end": 6010.0, "text": " You can see here this is a great use of the functional API.", "tokens": [50770, 509, 393, 536, 510, 341, 307, 257, 869, 764, 295, 264, 11745, 9362, 13, 51042], "temperature": 0.0, "avg_logprob": -0.31744036831698574, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.23090767860412598}, {"id": 1203, "seek": 599644, "start": 6010.0, "end": 6013.2, "text": " I haven't exactly used your code, I've kind of rewritten it a little bit, but basically", "tokens": [51042, 286, 2378, 380, 2293, 1143, 428, 3089, 11, 286, 600, 733, 295, 319, 26859, 309, 257, 707, 857, 11, 457, 1936, 51202], "temperature": 0.0, "avg_logprob": -0.31744036831698574, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.23090767860412598}, {"id": 1204, "seek": 599644, "start": 6013.2, "end": 6014.759999999999, "text": " it's the same concept.", "tokens": [51202, 309, 311, 264, 912, 3410, 13, 51280], "temperature": 0.0, "avg_logprob": -0.31744036831698574, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.23090767860412598}, {"id": 1205, "seek": 599644, "start": 6014.759999999999, "end": 6021.12, "text": " Let's try size 3 and size 4 and size 5 convolutional filters.", "tokens": [51280, 961, 311, 853, 2744, 805, 293, 2744, 1017, 293, 2744, 1025, 45216, 304, 15995, 13, 51598], "temperature": 0.0, "avg_logprob": -0.31744036831698574, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.23090767860412598}, {"id": 1206, "seek": 602112, "start": 6021.12, "end": 6028.2, "text": " And so let's create a 1D convolutional filter of size 3, then size 4, then size 5, and then", "tokens": [50364, 400, 370, 718, 311, 1884, 257, 502, 35, 45216, 304, 6608, 295, 2744, 805, 11, 550, 2744, 1017, 11, 550, 2744, 1025, 11, 293, 550, 50718], "temperature": 0.0, "avg_logprob": -0.2949960882013494, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0390462800860405}, {"id": 1207, "seek": 602112, "start": 6028.2, "end": 6033.5599999999995, "text": " for each one, using the functional API, we'll add max pooling and we'll flatten it and we'll", "tokens": [50718, 337, 1184, 472, 11, 1228, 264, 11745, 9362, 11, 321, 603, 909, 11469, 7005, 278, 293, 321, 603, 24183, 309, 293, 321, 603, 50986], "temperature": 0.0, "avg_logprob": -0.2949960882013494, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0390462800860405}, {"id": 1208, "seek": 602112, "start": 6033.5599999999995, "end": 6037.0, "text": " add it to a list of these different convolutions.", "tokens": [50986, 909, 309, 281, 257, 1329, 295, 613, 819, 3754, 15892, 13, 51158], "temperature": 0.0, "avg_logprob": -0.2949960882013494, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0390462800860405}, {"id": 1209, "seek": 602112, "start": 6037.0, "end": 6042.4, "text": " And then at the end, we will merge them all together by simply concatenating them.", "tokens": [51158, 400, 550, 412, 264, 917, 11, 321, 486, 22183, 552, 439, 1214, 538, 2935, 1588, 7186, 990, 552, 13, 51428], "temperature": 0.0, "avg_logprob": -0.2949960882013494, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0390462800860405}, {"id": 1210, "seek": 602112, "start": 6042.4, "end": 6048.04, "text": " So we're now going to have a single vector containing the result of the 3 and 4 and 5", "tokens": [51428, 407, 321, 434, 586, 516, 281, 362, 257, 2167, 8062, 19273, 264, 1874, 295, 264, 805, 293, 1017, 293, 1025, 51710], "temperature": 0.0, "avg_logprob": -0.2949960882013494, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.0390462800860405}, {"id": 1211, "seek": 604804, "start": 6048.04, "end": 6051.68, "text": " size convolutions, like y-set of a 1.", "tokens": [50364, 2744, 3754, 15892, 11, 411, 288, 12, 3854, 295, 257, 502, 13, 50546], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1212, "seek": 604804, "start": 6051.68, "end": 6057.96, "text": " And then let's return that whole model as a little sub-model, which in Ben's code he", "tokens": [50546, 400, 550, 718, 311, 2736, 300, 1379, 2316, 382, 257, 707, 1422, 12, 8014, 338, 11, 597, 294, 3964, 311, 3089, 415, 50860], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1213, "seek": 604804, "start": 6057.96, "end": 6059.32, "text": " called graph.", "tokens": [50860, 1219, 4295, 13, 50928], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1214, "seek": 604804, "start": 6059.32, "end": 6064.16, "text": " The reason I assume you call this graph is because people tend to think of these things", "tokens": [50928, 440, 1778, 286, 6552, 291, 818, 341, 4295, 307, 570, 561, 3928, 281, 519, 295, 613, 721, 51170], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1215, "seek": 604804, "start": 6064.16, "end": 6066.6, "text": " as a computational graph.", "tokens": [51170, 382, 257, 28270, 4295, 13, 51292], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1216, "seek": 604804, "start": 6066.6, "end": 6073.54, "text": " A computational graph basically is saying this is a computation being expressed as various", "tokens": [51292, 316, 28270, 4295, 1936, 307, 1566, 341, 307, 257, 24903, 885, 12675, 382, 3683, 51639], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1217, "seek": 604804, "start": 6073.54, "end": 6074.54, "text": " inputs and outputs.", "tokens": [51639, 15743, 293, 23930, 13, 51689], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1218, "seek": 604804, "start": 6074.54, "end": 6076.68, "text": " You can think of it as a graph.", "tokens": [51689, 509, 393, 519, 295, 309, 382, 257, 4295, 13, 51796], "temperature": 0.0, "avg_logprob": -0.369022956261268, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.7121478915214539}, {"id": 1219, "seek": 607668, "start": 6077.320000000001, "end": 6084.84, "text": " Once you've got this little multi-layer convolution module, you can stick it inside a standard", "tokens": [50396, 3443, 291, 600, 658, 341, 707, 4825, 12, 8376, 260, 45216, 10088, 11, 291, 393, 2897, 309, 1854, 257, 3832, 50772], "temperature": 0.0, "avg_logprob": -0.2553798460191296, "compression_ratio": 1.5, "no_speech_prob": 0.0049821482971310616}, {"id": 1220, "seek": 607668, "start": 6084.84, "end": 6094.320000000001, "text": " sequential model by simply replacing the convolution 1D and max pooling piece with graph, where", "tokens": [50772, 42881, 2316, 538, 2935, 19139, 264, 45216, 502, 35, 293, 11469, 7005, 278, 2522, 365, 4295, 11, 689, 51246], "temperature": 0.0, "avg_logprob": -0.2553798460191296, "compression_ratio": 1.5, "no_speech_prob": 0.0049821482971310616}, {"id": 1221, "seek": 607668, "start": 6094.320000000001, "end": 6101.280000000001, "text": " graph is the concatenated version of all of these different scales of convolution.", "tokens": [51246, 4295, 307, 264, 1588, 7186, 770, 3037, 295, 439, 295, 613, 819, 17408, 295, 45216, 13, 51594], "temperature": 0.0, "avg_logprob": -0.2553798460191296, "compression_ratio": 1.5, "no_speech_prob": 0.0049821482971310616}, {"id": 1222, "seek": 610128, "start": 6101.28, "end": 6110.86, "text": " And so trying this out, I got a slightly better answer again, which is 90.36%.", "tokens": [50364, 400, 370, 1382, 341, 484, 11, 286, 658, 257, 4748, 1101, 1867, 797, 11, 597, 307, 4289, 13, 11309, 6856, 50843], "temperature": 0.0, "avg_logprob": -0.38808778188761, "compression_ratio": 1.4806201550387597, "no_speech_prob": 0.13477042317390442}, {"id": 1223, "seek": 610128, "start": 6110.86, "end": 6115.38, "text": " And I hadn't seen that paper before, so thank you for giving that great idea.", "tokens": [50843, 400, 286, 8782, 380, 1612, 300, 3035, 949, 11, 370, 1309, 291, 337, 2902, 300, 869, 1558, 13, 51069], "temperature": 0.0, "avg_logprob": -0.38808778188761, "compression_ratio": 1.4806201550387597, "no_speech_prob": 0.13477042317390442}, {"id": 1224, "seek": 610128, "start": 6115.38, "end": 6119.34, "text": " Did you have anything to add about this multi-scale convolution idea?", "tokens": [51069, 2589, 291, 362, 1340, 281, 909, 466, 341, 4825, 12, 20033, 45216, 1558, 30, 51267], "temperature": 0.0, "avg_logprob": -0.38808778188761, "compression_ratio": 1.4806201550387597, "no_speech_prob": 0.13477042317390442}, {"id": 1225, "seek": 610128, "start": 6119.34, "end": 6123.139999999999, "text": " Answer Question 2 Not really, other than I think it's super", "tokens": [51267, 24545, 220, 8547, 31342, 568, 1726, 534, 11, 661, 813, 286, 519, 309, 311, 1687, 51457], "temperature": 0.0, "avg_logprob": -0.38808778188761, "compression_ratio": 1.4806201550387597, "no_speech_prob": 0.13477042317390442}, {"id": 1226, "seek": 610128, "start": 6123.139999999999, "end": 6124.139999999999, "text": " cool.", "tokens": [51457, 1627, 13, 51507], "temperature": 0.0, "avg_logprob": -0.38808778188761, "compression_ratio": 1.4806201550387597, "no_speech_prob": 0.13477042317390442}, {"id": 1227, "seek": 610128, "start": 6124.139999999999, "end": 6130.46, "text": " But actually I'm still trying to figure out all the ins and outs of exactly how it works.", "tokens": [51507, 583, 767, 286, 478, 920, 1382, 281, 2573, 484, 439, 264, 1028, 293, 14758, 295, 2293, 577, 309, 1985, 13, 51823], "temperature": 0.0, "avg_logprob": -0.38808778188761, "compression_ratio": 1.4806201550387597, "no_speech_prob": 0.13477042317390442}, {"id": 1228, "seek": 613046, "start": 6130.64, "end": 6132.64, "text": " In some ways, implementation is easier than understanding.", "tokens": [50373, 682, 512, 2098, 11, 11420, 307, 3571, 813, 3701, 13, 50473], "temperature": 0.0, "avg_logprob": -0.39378491569967833, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5038599967956543}, {"id": 1229, "seek": 613046, "start": 6132.64, "end": 6134.8, "text": " Answer Question 3 Well, that's exactly right.", "tokens": [50473, 24545, 14464, 805, 1042, 11, 300, 311, 2293, 558, 13, 50581], "temperature": 0.0, "avg_logprob": -0.39378491569967833, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5038599967956543}, {"id": 1230, "seek": 613046, "start": 6134.8, "end": 6143.4800000000005, "text": " In a lot of these things, the math is kind of ridiculously simple.", "tokens": [50581, 682, 257, 688, 295, 613, 721, 11, 264, 5221, 307, 733, 295, 41358, 2199, 13, 51015], "temperature": 0.0, "avg_logprob": -0.39378491569967833, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5038599967956543}, {"id": 1231, "seek": 613046, "start": 6143.4800000000005, "end": 6148.36, "text": " And then you throw it at an SGD and let it do billions and billions of calculations in", "tokens": [51015, 400, 550, 291, 3507, 309, 412, 364, 34520, 35, 293, 718, 309, 360, 17375, 293, 17375, 295, 20448, 294, 51259], "temperature": 0.0, "avg_logprob": -0.39378491569967833, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5038599967956543}, {"id": 1232, "seek": 613046, "start": 6148.36, "end": 6154.28, "text": " a fraction of a second, and what it comes up with is kind of hard to grasp.", "tokens": [51259, 257, 14135, 295, 257, 1150, 11, 293, 437, 309, 1487, 493, 365, 307, 733, 295, 1152, 281, 21743, 13, 51555], "temperature": 0.0, "avg_logprob": -0.39378491569967833, "compression_ratio": 1.5251141552511416, "no_speech_prob": 0.5038599967956543}, {"id": 1233, "seek": 615428, "start": 6154.28, "end": 6157.78, "text": " Answer Question 4 And you are using capital M merge in this example.", "tokens": [50364, 24545, 14464, 1017, 400, 291, 366, 1228, 4238, 376, 22183, 294, 341, 1365, 13, 50539], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1234, "seek": 615428, "start": 6157.78, "end": 6158.78, "text": " Do you want to talk about that?", "tokens": [50539, 1144, 291, 528, 281, 751, 466, 300, 30, 50589], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1235, "seek": 615428, "start": 6158.78, "end": 6159.78, "text": " Answer Question 3 Not really.", "tokens": [50589, 24545, 14464, 805, 1726, 534, 13, 50639], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1236, "seek": 615428, "start": 6159.78, "end": 6163.78, "text": " Ben used capital M merge and I just did the same thing.", "tokens": [50639, 3964, 1143, 4238, 376, 22183, 293, 286, 445, 630, 264, 912, 551, 13, 50839], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1237, "seek": 615428, "start": 6163.78, "end": 6170.78, "text": " Where were at me, I would have used small m merge, so we'll have to agree to disagree", "tokens": [50839, 2305, 645, 412, 385, 11, 286, 576, 362, 1143, 1359, 275, 22183, 11, 370, 321, 603, 362, 281, 3986, 281, 14091, 51189], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1238, "seek": 615428, "start": 6170.78, "end": 6171.78, "text": " here.", "tokens": [51189, 510, 13, 51239], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1239, "seek": 615428, "start": 6171.78, "end": 6178.78, "text": " Let's not go there.", "tokens": [51239, 961, 311, 406, 352, 456, 13, 51589], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1240, "seek": 615428, "start": 6178.78, "end": 6181.3, "text": " So I think that's super fun.", "tokens": [51589, 407, 286, 519, 300, 311, 1687, 1019, 13, 51715], "temperature": 0.0, "avg_logprob": -0.49015724658966064, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.554428219795227}, {"id": 1241, "seek": 618130, "start": 6181.320000000001, "end": 6190.76, "text": " So we have a few minutes to talk about something enormous, so we're going to do a brief introduction", "tokens": [50365, 407, 321, 362, 257, 1326, 2077, 281, 751, 466, 746, 11322, 11, 370, 321, 434, 516, 281, 360, 257, 5353, 9339, 50837], "temperature": 0.0, "avg_logprob": -0.2678752605731671, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.021286962553858757}, {"id": 1242, "seek": 618130, "start": 6190.76, "end": 6191.76, "text": " to RNNs.", "tokens": [50837, 281, 45702, 45, 82, 13, 50887], "temperature": 0.0, "avg_logprob": -0.2678752605731671, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.021286962553858757}, {"id": 1243, "seek": 618130, "start": 6191.76, "end": 6197.0, "text": " And then next week, we will do a deep dive.", "tokens": [50887, 400, 550, 958, 1243, 11, 321, 486, 360, 257, 2452, 9192, 13, 51149], "temperature": 0.0, "avg_logprob": -0.2678752605731671, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.021286962553858757}, {"id": 1244, "seek": 618130, "start": 6197.0, "end": 6204.84, "text": " So everything we've learned so far about convolutional neural networks does not necessarily do a", "tokens": [51149, 407, 1203, 321, 600, 3264, 370, 1400, 466, 45216, 304, 18161, 9590, 775, 406, 4725, 360, 257, 51541], "temperature": 0.0, "avg_logprob": -0.2678752605731671, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.021286962553858757}, {"id": 1245, "seek": 620484, "start": 6204.84, "end": 6212.0, "text": " great job of solving a problem like how would you model this.", "tokens": [50364, 869, 1691, 295, 12606, 257, 1154, 411, 577, 576, 291, 2316, 341, 13, 50722], "temperature": 0.0, "avg_logprob": -0.33613414299197314, "compression_ratio": 1.598984771573604, "no_speech_prob": 0.7121935486793518}, {"id": 1246, "seek": 620484, "start": 6212.0, "end": 6221.16, "text": " Now notice this markup here, it has to recognize when you have a start tag and know to close", "tokens": [50722, 823, 3449, 341, 1491, 1010, 510, 11, 309, 575, 281, 5521, 562, 291, 362, 257, 722, 6162, 293, 458, 281, 1998, 51180], "temperature": 0.0, "avg_logprob": -0.33613414299197314, "compression_ratio": 1.598984771573604, "no_speech_prob": 0.7121935486793518}, {"id": 1247, "seek": 620484, "start": 6221.16, "end": 6227.34, "text": " that tag, but then over a longer period of time, that it's inside a weird XMLE comment", "tokens": [51180, 300, 6162, 11, 457, 550, 670, 257, 2854, 2896, 295, 565, 11, 300, 309, 311, 1854, 257, 3657, 43484, 36, 2871, 51489], "temperature": 0.0, "avg_logprob": -0.33613414299197314, "compression_ratio": 1.598984771573604, "no_speech_prob": 0.7121935486793518}, {"id": 1248, "seek": 620484, "start": 6227.34, "end": 6232.3, "text": " thing and to know that it has to finish off the weird XMLE comment thing.", "tokens": [51489, 551, 293, 281, 458, 300, 309, 575, 281, 2413, 766, 264, 3657, 43484, 36, 2871, 551, 13, 51737], "temperature": 0.0, "avg_logprob": -0.33613414299197314, "compression_ratio": 1.598984771573604, "no_speech_prob": 0.7121935486793518}, {"id": 1249, "seek": 623230, "start": 6232.360000000001, "end": 6239.08, "text": " Which means it has to keep memory about what happened in the distant past if you're going", "tokens": [50367, 3013, 1355, 309, 575, 281, 1066, 4675, 466, 437, 2011, 294, 264, 17275, 1791, 498, 291, 434, 516, 50703], "temperature": 0.0, "avg_logprob": -0.2959575933568618, "compression_ratio": 1.515625, "no_speech_prob": 0.05108162388205528}, {"id": 1250, "seek": 623230, "start": 6239.08, "end": 6244.52, "text": " to successfully do any kind of modeling with data that looks like this.", "tokens": [50703, 281, 10727, 360, 604, 733, 295, 15983, 365, 1412, 300, 1542, 411, 341, 13, 50975], "temperature": 0.0, "avg_logprob": -0.2959575933568618, "compression_ratio": 1.515625, "no_speech_prob": 0.05108162388205528}, {"id": 1251, "seek": 623230, "start": 6244.52, "end": 6255.68, "text": " And so with that kind of memory, therefore, it can handle long-term dependencies.", "tokens": [50975, 400, 370, 365, 300, 733, 295, 4675, 11, 4412, 11, 309, 393, 4813, 938, 12, 7039, 36606, 13, 51533], "temperature": 0.0, "avg_logprob": -0.2959575933568618, "compression_ratio": 1.515625, "no_speech_prob": 0.05108162388205528}, {"id": 1252, "seek": 623230, "start": 6255.68, "end": 6259.38, "text": " Also think about these two different sentences.", "tokens": [51533, 2743, 519, 466, 613, 732, 819, 16579, 13, 51718], "temperature": 0.0, "avg_logprob": -0.2959575933568618, "compression_ratio": 1.515625, "no_speech_prob": 0.05108162388205528}, {"id": 1253, "seek": 625938, "start": 6259.38, "end": 6264.400000000001, "text": " They both mean effectively the same thing, but in order to realize that, you're going", "tokens": [50364, 814, 1293, 914, 8659, 264, 912, 551, 11, 457, 294, 1668, 281, 4325, 300, 11, 291, 434, 516, 50615], "temperature": 0.0, "avg_logprob": -0.2664935302734375, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.047418054193258286}, {"id": 1254, "seek": 625938, "start": 6264.400000000001, "end": 6269.76, "text": " to have to keep some kind of state that knows that after this has been read in, you're now", "tokens": [50615, 281, 362, 281, 1066, 512, 733, 295, 1785, 300, 3255, 300, 934, 341, 575, 668, 1401, 294, 11, 291, 434, 586, 50883], "temperature": 0.0, "avg_logprob": -0.2664935302734375, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.047418054193258286}, {"id": 1255, "seek": 625938, "start": 6269.76, "end": 6273.2, "text": " talking about something that happened in 2009.", "tokens": [50883, 1417, 466, 746, 300, 2011, 294, 11453, 13, 51055], "temperature": 0.0, "avg_logprob": -0.2664935302734375, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.047418054193258286}, {"id": 1256, "seek": 625938, "start": 6273.2, "end": 6278.8, "text": " And you then have to remember it all the way to here to know when it was that this thing", "tokens": [51055, 400, 291, 550, 362, 281, 1604, 309, 439, 264, 636, 281, 510, 281, 458, 562, 309, 390, 300, 341, 551, 51335], "temperature": 0.0, "avg_logprob": -0.2664935302734375, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.047418054193258286}, {"id": 1257, "seek": 625938, "start": 6278.8, "end": 6281.26, "text": " happened that you did in Nepal.", "tokens": [51335, 2011, 300, 291, 630, 294, 36283, 13, 51458], "temperature": 0.0, "avg_logprob": -0.2664935302734375, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.047418054193258286}, {"id": 1258, "seek": 625938, "start": 6281.26, "end": 6285.08, "text": " So we want to create some kind of stateful representation.", "tokens": [51458, 407, 321, 528, 281, 1884, 512, 733, 295, 1785, 906, 10290, 13, 51649], "temperature": 0.0, "avg_logprob": -0.2664935302734375, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.047418054193258286}, {"id": 1259, "seek": 628508, "start": 6286.08, "end": 6290.82, "text": " Furthermore, it would be nice if we're going to deal with big long pieces of language like", "tokens": [50414, 23999, 11, 309, 576, 312, 1481, 498, 321, 434, 516, 281, 2028, 365, 955, 938, 3755, 295, 2856, 411, 50651], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1260, "seek": 628508, "start": 6290.82, "end": 6295.38, "text": " this with a lot of structure to be able to handle variable-length sequences so that we", "tokens": [50651, 341, 365, 257, 688, 295, 3877, 281, 312, 1075, 281, 4813, 7006, 12, 45390, 22978, 370, 300, 321, 50879], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1261, "seek": 628508, "start": 6295.38, "end": 6298.26, "text": " can handle some things that might be really long and some things that might be really", "tokens": [50879, 393, 4813, 512, 721, 300, 1062, 312, 534, 938, 293, 512, 721, 300, 1062, 312, 534, 51023], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1262, "seek": 628508, "start": 6298.26, "end": 6299.48, "text": " short.", "tokens": [51023, 2099, 13, 51084], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1263, "seek": 628508, "start": 6299.48, "end": 6305.14, "text": " So these are all things which convolutional neural networks don't necessarily do that", "tokens": [51084, 407, 613, 366, 439, 721, 597, 45216, 304, 18161, 9590, 500, 380, 4725, 360, 300, 51367], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1264, "seek": 628508, "start": 6305.14, "end": 6306.48, "text": " well.", "tokens": [51367, 731, 13, 51434], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1265, "seek": 628508, "start": 6306.48, "end": 6310.3, "text": " So we're going to look at something else, which is a recurrent neural network, which", "tokens": [51434, 407, 321, 434, 516, 281, 574, 412, 746, 1646, 11, 597, 307, 257, 18680, 1753, 18161, 3209, 11, 597, 51625], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1266, "seek": 628508, "start": 6310.3, "end": 6312.22, "text": " handles that kind of thing well.", "tokens": [51625, 18722, 300, 733, 295, 551, 731, 13, 51721], "temperature": 0.0, "avg_logprob": -0.28083482281915073, "compression_ratio": 1.889763779527559, "no_speech_prob": 0.003945363685488701}, {"id": 1267, "seek": 631222, "start": 6312.360000000001, "end": 6317.72, "text": " Here is a great example of a good use of a recurrent neural network.", "tokens": [50371, 1692, 307, 257, 869, 1365, 295, 257, 665, 764, 295, 257, 18680, 1753, 18161, 3209, 13, 50639], "temperature": 0.0, "avg_logprob": -0.22801629234762752, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.002550893696025014}, {"id": 1268, "seek": 631222, "start": 6317.72, "end": 6323.5, "text": " At the top here, you can see that there is a convolutional neural network that is looking", "tokens": [50639, 1711, 264, 1192, 510, 11, 291, 393, 536, 300, 456, 307, 257, 45216, 304, 18161, 3209, 300, 307, 1237, 50928], "temperature": 0.0, "avg_logprob": -0.22801629234762752, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.002550893696025014}, {"id": 1269, "seek": 631222, "start": 6323.5, "end": 6330.62, "text": " at images of house numbers.", "tokens": [50928, 412, 5267, 295, 1782, 3547, 13, 51284], "temperature": 0.0, "avg_logprob": -0.22801629234762752, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.002550893696025014}, {"id": 1270, "seek": 631222, "start": 6330.62, "end": 6336.4800000000005, "text": " These images are coming from really big Google Street View pictures, so it has to figure", "tokens": [51284, 1981, 5267, 366, 1348, 490, 534, 955, 3329, 7638, 13909, 5242, 11, 370, 309, 575, 281, 2573, 51577], "temperature": 0.0, "avg_logprob": -0.22801629234762752, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.002550893696025014}, {"id": 1271, "seek": 633648, "start": 6336.48, "end": 6342.5199999999995, "text": " out what part of the image should I look at next in order to figure out the house number.", "tokens": [50364, 484, 437, 644, 295, 264, 3256, 820, 286, 574, 412, 958, 294, 1668, 281, 2573, 484, 264, 1782, 1230, 13, 50666], "temperature": 0.0, "avg_logprob": -0.2901356007793162, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.15609394013881683}, {"id": 1272, "seek": 633648, "start": 6342.5199999999995, "end": 6347.679999999999, "text": " You can see that there's a little square box that is scanning through and figuring out", "tokens": [50666, 509, 393, 536, 300, 456, 311, 257, 707, 3732, 2424, 300, 307, 27019, 807, 293, 15213, 484, 50924], "temperature": 0.0, "avg_logprob": -0.2901356007793162, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.15609394013881683}, {"id": 1273, "seek": 633648, "start": 6347.679999999999, "end": 6349.2, "text": " I want to look at this piece next.", "tokens": [50924, 286, 528, 281, 574, 412, 341, 2522, 958, 13, 51000], "temperature": 0.0, "avg_logprob": -0.2901356007793162, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.15609394013881683}, {"id": 1274, "seek": 633648, "start": 6349.2, "end": 6355.48, "text": " And at the bottom, you can see it's then showing you what it's actually seeing after each time", "tokens": [51000, 400, 412, 264, 2767, 11, 291, 393, 536, 309, 311, 550, 4099, 291, 437, 309, 311, 767, 2577, 934, 1184, 565, 51314], "temperature": 0.0, "avg_logprob": -0.2901356007793162, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.15609394013881683}, {"id": 1275, "seek": 633648, "start": 6355.48, "end": 6357.12, "text": " step.", "tokens": [51314, 1823, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2901356007793162, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.15609394013881683}, {"id": 1276, "seek": 633648, "start": 6357.12, "end": 6362.12, "text": " So the thing that is figuring out where to look next is a recurrent neural network.", "tokens": [51396, 407, 264, 551, 300, 307, 15213, 484, 689, 281, 574, 958, 307, 257, 18680, 1753, 18161, 3209, 13, 51646], "temperature": 0.0, "avg_logprob": -0.2901356007793162, "compression_ratio": 1.6995708154506437, "no_speech_prob": 0.15609394013881683}, {"id": 1277, "seek": 636212, "start": 6362.12, "end": 6366.96, "text": " It's something which is taking its previous state and figuring out what should its next", "tokens": [50364, 467, 311, 746, 597, 307, 1940, 1080, 3894, 1785, 293, 15213, 484, 437, 820, 1080, 958, 50606], "temperature": 0.0, "avg_logprob": -0.27570414255900555, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.038464996963739395}, {"id": 1278, "seek": 636212, "start": 6366.96, "end": 6369.12, "text": " state be.", "tokens": [50606, 1785, 312, 13, 50714], "temperature": 0.0, "avg_logprob": -0.27570414255900555, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.038464996963739395}, {"id": 1279, "seek": 636212, "start": 6369.12, "end": 6374.24, "text": " This kind of model is called an attentional model.", "tokens": [50714, 639, 733, 295, 2316, 307, 1219, 364, 3202, 304, 2316, 13, 50970], "temperature": 0.0, "avg_logprob": -0.27570414255900555, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.038464996963739395}, {"id": 1280, "seek": 636212, "start": 6374.24, "end": 6379.32, "text": " It's a really interesting avenue of research when it comes to dealing with things like", "tokens": [50970, 467, 311, 257, 534, 1880, 39230, 295, 2132, 562, 309, 1487, 281, 6260, 365, 721, 411, 51224], "temperature": 0.0, "avg_logprob": -0.27570414255900555, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.038464996963739395}, {"id": 1281, "seek": 636212, "start": 6379.32, "end": 6385.599999999999, "text": " very large images, images which might be too big for a single convolutional neural network", "tokens": [51224, 588, 2416, 5267, 11, 5267, 597, 1062, 312, 886, 955, 337, 257, 2167, 45216, 304, 18161, 3209, 51538], "temperature": 0.0, "avg_logprob": -0.27570414255900555, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.038464996963739395}, {"id": 1282, "seek": 636212, "start": 6385.599999999999, "end": 6389.44, "text": " with our current hardware constraints.", "tokens": [51538, 365, 527, 2190, 8837, 18491, 13, 51730], "temperature": 0.0, "avg_logprob": -0.27570414255900555, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.038464996963739395}, {"id": 1283, "seek": 638944, "start": 6389.44, "end": 6394.4, "text": " On the left is another great example of a useful recurrent neural network, which is", "tokens": [50364, 1282, 264, 1411, 307, 1071, 869, 1365, 295, 257, 4420, 18680, 1753, 18161, 3209, 11, 597, 307, 50612], "temperature": 0.0, "avg_logprob": -0.2547414720672922, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00806164089590311}, {"id": 1284, "seek": 638944, "start": 6394.4, "end": 6400.679999999999, "text": " the very popular Android and iOS text entry system called SwiftKey.", "tokens": [50612, 264, 588, 3743, 8853, 293, 17430, 2487, 8729, 1185, 1219, 25539, 42, 2030, 13, 50926], "temperature": 0.0, "avg_logprob": -0.2547414720672922, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00806164089590311}, {"id": 1285, "seek": 638944, "start": 6400.679999999999, "end": 6407.5199999999995, "text": " SwiftKey had a post up a few months ago in which they announced that they had just replaced", "tokens": [50926, 25539, 42, 2030, 632, 257, 2183, 493, 257, 1326, 2493, 2057, 294, 597, 436, 7548, 300, 436, 632, 445, 10772, 51268], "temperature": 0.0, "avg_logprob": -0.2547414720672922, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00806164089590311}, {"id": 1286, "seek": 638944, "start": 6407.5199999999995, "end": 6413.12, "text": " their language model with a neural network of this kind, which basically looked at your", "tokens": [51268, 641, 2856, 2316, 365, 257, 18161, 3209, 295, 341, 733, 11, 597, 1936, 2956, 412, 428, 51548], "temperature": 0.0, "avg_logprob": -0.2547414720672922, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00806164089590311}, {"id": 1287, "seek": 638944, "start": 6413.12, "end": 6418.639999999999, "text": " previous words and figured out what word are you likely to be typing in next and then it", "tokens": [51548, 3894, 2283, 293, 8932, 484, 437, 1349, 366, 291, 3700, 281, 312, 18444, 294, 958, 293, 550, 309, 51824], "temperature": 0.0, "avg_logprob": -0.2547414720672922, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.00806164089590311}, {"id": 1288, "seek": 641864, "start": 6418.84, "end": 6422.240000000001, "text": " could predict that word.", "tokens": [50374, 727, 6069, 300, 1349, 13, 50544], "temperature": 0.0, "avg_logprob": -0.3033950169881185, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.02517765574157238}, {"id": 1289, "seek": 641864, "start": 6422.240000000001, "end": 6430.08, "text": " A final example was Andrej Karpathy showed a really cool thing where he was able to generate", "tokens": [50544, 316, 2572, 1365, 390, 20667, 73, 591, 6529, 9527, 4712, 257, 534, 1627, 551, 689, 415, 390, 1075, 281, 8460, 50936], "temperature": 0.0, "avg_logprob": -0.3033950169881185, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.02517765574157238}, {"id": 1290, "seek": 641864, "start": 6430.08, "end": 6435.04, "text": " random mathematical papers by generating random LaTeX.", "tokens": [50936, 4974, 18894, 10577, 538, 17746, 4974, 2369, 14233, 55, 13, 51184], "temperature": 0.0, "avg_logprob": -0.3033950169881185, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.02517765574157238}, {"id": 1291, "seek": 641864, "start": 6435.04, "end": 6441.280000000001, "text": " And to generate random LaTeX, you actually have to learn things like slash begin-proof", "tokens": [51184, 400, 281, 8460, 4974, 2369, 14233, 55, 11, 291, 767, 362, 281, 1466, 721, 411, 17330, 1841, 12, 15690, 51496], "temperature": 0.0, "avg_logprob": -0.3033950169881185, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.02517765574157238}, {"id": 1292, "seek": 641864, "start": 6441.280000000001, "end": 6446.240000000001, "text": " and slash end-proof and these kind of long-term dependencies.", "tokens": [51496, 293, 17330, 917, 12, 15690, 293, 613, 733, 295, 938, 12, 7039, 36606, 13, 51744], "temperature": 0.0, "avg_logprob": -0.3033950169881185, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.02517765574157238}, {"id": 1293, "seek": 641864, "start": 6446.240000000001, "end": 6447.76, "text": " He was able to do that successfully.", "tokens": [51744, 634, 390, 1075, 281, 360, 300, 10727, 13, 51820], "temperature": 0.0, "avg_logprob": -0.3033950169881185, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.02517765574157238}, {"id": 1294, "seek": 644776, "start": 6447.88, "end": 6452.96, "text": " This is actually a randomly generated piece of LaTeX which is being created with a recurrent", "tokens": [50370, 639, 307, 767, 257, 16979, 10833, 2522, 295, 2369, 14233, 55, 597, 307, 885, 2942, 365, 257, 18680, 1753, 50624], "temperature": 0.0, "avg_logprob": -0.2770582675933838, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.003324400167912245}, {"id": 1295, "seek": 644776, "start": 6452.96, "end": 6456.400000000001, "text": " neural network.", "tokens": [50624, 18161, 3209, 13, 50796], "temperature": 0.0, "avg_logprob": -0.2770582675933838, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.003324400167912245}, {"id": 1296, "seek": 644776, "start": 6456.400000000001, "end": 6461.92, "text": " So today I am not going to show you exactly how it works, I'm going to try to give you", "tokens": [50796, 407, 965, 286, 669, 406, 516, 281, 855, 291, 2293, 577, 309, 1985, 11, 286, 478, 516, 281, 853, 281, 976, 291, 51072], "temperature": 0.0, "avg_logprob": -0.2770582675933838, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.003324400167912245}, {"id": 1297, "seek": 644776, "start": 6461.92, "end": 6462.92, "text": " an intuition.", "tokens": [51072, 364, 24002, 13, 51122], "temperature": 0.0, "avg_logprob": -0.2770582675933838, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.003324400167912245}, {"id": 1298, "seek": 644776, "start": 6462.92, "end": 6472.64, "text": " I'm going to start off by showing you how to think about neural networks as computational", "tokens": [51122, 286, 478, 516, 281, 722, 766, 538, 4099, 291, 577, 281, 519, 466, 18161, 9590, 382, 28270, 51608], "temperature": 0.0, "avg_logprob": -0.2770582675933838, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.003324400167912245}, {"id": 1299, "seek": 644776, "start": 6472.64, "end": 6473.64, "text": " graphs.", "tokens": [51608, 24877, 13, 51658], "temperature": 0.0, "avg_logprob": -0.2770582675933838, "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.003324400167912245}, {"id": 1300, "seek": 647364, "start": 6473.64, "end": 6477.92, "text": " This is coming back to that word Ben used earlier, this idea of a graph.", "tokens": [50364, 639, 307, 1348, 646, 281, 300, 1349, 3964, 1143, 3071, 11, 341, 1558, 295, 257, 4295, 13, 50578], "temperature": 0.0, "avg_logprob": -0.29266394401083184, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.12765011191368103}, {"id": 1301, "seek": 647364, "start": 6477.92, "end": 6482.4800000000005, "text": " So I started out by trying to draw, this is like my notation, you won't see this anywhere", "tokens": [50578, 407, 286, 1409, 484, 538, 1382, 281, 2642, 11, 341, 307, 411, 452, 24657, 11, 291, 1582, 380, 536, 341, 4992, 50806], "temperature": 0.0, "avg_logprob": -0.29266394401083184, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.12765011191368103}, {"id": 1302, "seek": 647364, "start": 6482.4800000000005, "end": 6484.12, "text": " else but it will do for now.", "tokens": [50806, 1646, 457, 309, 486, 360, 337, 586, 13, 50888], "temperature": 0.0, "avg_logprob": -0.29266394401083184, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.12765011191368103}, {"id": 1303, "seek": 647364, "start": 6484.12, "end": 6491.02, "text": " Here is a picture of a single hidden layer basic neural network.", "tokens": [50888, 1692, 307, 257, 3036, 295, 257, 2167, 7633, 4583, 3875, 18161, 3209, 13, 51233], "temperature": 0.0, "avg_logprob": -0.29266394401083184, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.12765011191368103}, {"id": 1304, "seek": 647364, "start": 6491.02, "end": 6499.4400000000005, "text": " We can think of it as having an input which is going to be of batch size and contain width", "tokens": [51233, 492, 393, 519, 295, 309, 382, 1419, 364, 4846, 597, 307, 516, 281, 312, 295, 15245, 2744, 293, 5304, 11402, 51654], "temperature": 0.0, "avg_logprob": -0.29266394401083184, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.12765011191368103}, {"id": 1305, "seek": 647364, "start": 6499.4400000000005, "end": 6503.200000000001, "text": " of number of inputs.", "tokens": [51654, 295, 1230, 295, 15743, 13, 51842], "temperature": 0.0, "avg_logprob": -0.29266394401083184, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.12765011191368103}, {"id": 1306, "seek": 650320, "start": 6503.76, "end": 6509.48, "text": " This orange arrow represents something that we're doing to that matrix.", "tokens": [50392, 639, 7671, 11610, 8855, 746, 300, 321, 434, 884, 281, 300, 8141, 13, 50678], "temperature": 0.0, "avg_logprob": -0.2596738815307617, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0015011728974059224}, {"id": 1307, "seek": 650320, "start": 6509.48, "end": 6516.28, "text": " So each of the boxes represents a matrix and each of the arrows represents one or more", "tokens": [50678, 407, 1184, 295, 264, 9002, 8855, 257, 8141, 293, 1184, 295, 264, 19669, 8855, 472, 420, 544, 51018], "temperature": 0.0, "avg_logprob": -0.2596738815307617, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0015011728974059224}, {"id": 1308, "seek": 650320, "start": 6516.28, "end": 6517.88, "text": " things we do to that.", "tokens": [51018, 721, 321, 360, 281, 300, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2596738815307617, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0015011728974059224}, {"id": 1309, "seek": 650320, "start": 6517.88, "end": 6523.599999999999, "text": " In this case, we do a matrix product and then we throw it through a rectified linear unit.", "tokens": [51098, 682, 341, 1389, 11, 321, 360, 257, 8141, 1674, 293, 550, 321, 3507, 309, 807, 257, 11048, 2587, 8213, 4985, 13, 51384], "temperature": 0.0, "avg_logprob": -0.2596738815307617, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0015011728974059224}, {"id": 1310, "seek": 650320, "start": 6523.599999999999, "end": 6530.84, "text": " Then we get a circle which represents a matrix, but it's now a hidden layer which is of batch", "tokens": [51384, 1396, 321, 483, 257, 6329, 597, 8855, 257, 8141, 11, 457, 309, 311, 586, 257, 7633, 4583, 597, 307, 295, 15245, 51746], "temperature": 0.0, "avg_logprob": -0.2596738815307617, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.0015011728974059224}, {"id": 1311, "seek": 653084, "start": 6530.84, "end": 6534.88, "text": " size by number of activations.", "tokens": [50364, 2744, 538, 1230, 295, 2430, 763, 13, 50566], "temperature": 0.0, "avg_logprob": -0.2788252258300781, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.345083624124527}, {"id": 1312, "seek": 653084, "start": 6534.88, "end": 6539.68, "text": " Number of activations is just when we created that dense layer, we would have said dense", "tokens": [50566, 5118, 295, 2430, 763, 307, 445, 562, 321, 2942, 300, 18011, 4583, 11, 321, 576, 362, 848, 18011, 50806], "temperature": 0.0, "avg_logprob": -0.2788252258300781, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.345083624124527}, {"id": 1313, "seek": 653084, "start": 6539.68, "end": 6542.28, "text": " and then we would have had some number.", "tokens": [50806, 293, 550, 321, 576, 362, 632, 512, 1230, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2788252258300781, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.345083624124527}, {"id": 1314, "seek": 653084, "start": 6542.28, "end": 6546.56, "text": " That number is how many activations we create.", "tokens": [50936, 663, 1230, 307, 577, 867, 2430, 763, 321, 1884, 13, 51150], "temperature": 0.0, "avg_logprob": -0.2788252258300781, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.345083624124527}, {"id": 1315, "seek": 653084, "start": 6546.56, "end": 6552.360000000001, "text": " And then we put that through another operation, which in this case is a matrix product followed", "tokens": [51150, 400, 550, 321, 829, 300, 807, 1071, 6916, 11, 597, 294, 341, 1389, 307, 257, 8141, 1674, 6263, 51440], "temperature": 0.0, "avg_logprob": -0.2788252258300781, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.345083624124527}, {"id": 1316, "seek": 653084, "start": 6552.360000000001, "end": 6554.360000000001, "text": " by a softmax.", "tokens": [51440, 538, 257, 2787, 41167, 13, 51540], "temperature": 0.0, "avg_logprob": -0.2788252258300781, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.345083624124527}, {"id": 1317, "seek": 653084, "start": 6554.360000000001, "end": 6560.04, "text": " And so triangle here represents an output matrix and that's going to be of batch size", "tokens": [51540, 400, 370, 13369, 510, 8855, 364, 5598, 8141, 293, 300, 311, 516, 281, 312, 295, 15245, 2744, 51824], "temperature": 0.0, "avg_logprob": -0.2788252258300781, "compression_ratio": 1.7327586206896552, "no_speech_prob": 0.345083624124527}, {"id": 1318, "seek": 656004, "start": 6560.24, "end": 6564.44, "text": " by 1000.", "tokens": [50374, 538, 9714, 13, 50584], "temperature": 0.0, "avg_logprob": -0.23478197247794505, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.09946709126234055}, {"id": 1319, "seek": 656004, "start": 6564.44, "end": 6572.48, "text": " So this is my little way of representing the computation graph of a basic neural network", "tokens": [50584, 407, 341, 307, 452, 707, 636, 295, 13460, 264, 24903, 4295, 295, 257, 3875, 18161, 3209, 50986], "temperature": 0.0, "avg_logprob": -0.23478197247794505, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.09946709126234055}, {"id": 1320, "seek": 656004, "start": 6572.48, "end": 6575.04, "text": " with a single hidden layer.", "tokens": [50986, 365, 257, 2167, 7633, 4583, 13, 51114], "temperature": 0.0, "avg_logprob": -0.23478197247794505, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.09946709126234055}, {"id": 1321, "seek": 656004, "start": 6575.04, "end": 6581.08, "text": " I'm now going to create some slightly more complex models, but I'm going to slightly", "tokens": [51114, 286, 478, 586, 516, 281, 1884, 512, 4748, 544, 3997, 5245, 11, 457, 286, 478, 516, 281, 4748, 51416], "temperature": 0.0, "avg_logprob": -0.23478197247794505, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.09946709126234055}, {"id": 1322, "seek": 656004, "start": 6581.08, "end": 6583.58, "text": " reduce the amount of stuff on the screen.", "tokens": [51416, 5407, 264, 2372, 295, 1507, 322, 264, 2568, 13, 51541], "temperature": 0.0, "avg_logprob": -0.23478197247794505, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.09946709126234055}, {"id": 1323, "seek": 656004, "start": 6583.58, "end": 6590.0, "text": " One thing to note is that batch size appears all the time, so I'm going to get rid of it.", "tokens": [51541, 1485, 551, 281, 3637, 307, 300, 15245, 2744, 7038, 439, 264, 565, 11, 370, 286, 478, 516, 281, 483, 3973, 295, 309, 13, 51862], "temperature": 0.0, "avg_logprob": -0.23478197247794505, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.09946709126234055}, {"id": 1324, "seek": 659000, "start": 6590.96, "end": 6593.2, "text": " So here's the same thing where I've removed batch size.", "tokens": [50412, 407, 510, 311, 264, 912, 551, 689, 286, 600, 7261, 15245, 2744, 13, 50524], "temperature": 0.0, "avg_logprob": -0.29408131705390084, "compression_ratio": 1.6245059288537549, "no_speech_prob": 0.0023596312385052443}, {"id": 1325, "seek": 659000, "start": 6593.2, "end": 6597.08, "text": " Also the specific activation function, who gives a shit?", "tokens": [50524, 2743, 264, 2685, 24433, 2445, 11, 567, 2709, 257, 4611, 30, 50718], "temperature": 0.0, "avg_logprob": -0.29408131705390084, "compression_ratio": 1.6245059288537549, "no_speech_prob": 0.0023596312385052443}, {"id": 1326, "seek": 659000, "start": 6597.08, "end": 6600.64, "text": " It's probably ReLU everywhere except the last layer where it's softmax, so I've removed", "tokens": [50718, 467, 311, 1391, 1300, 43, 52, 5315, 3993, 264, 1036, 4583, 689, 309, 311, 2787, 41167, 11, 370, 286, 600, 7261, 50896], "temperature": 0.0, "avg_logprob": -0.29408131705390084, "compression_ratio": 1.6245059288537549, "no_speech_prob": 0.0023596312385052443}, {"id": 1327, "seek": 659000, "start": 6600.64, "end": 6602.28, "text": " that as well.", "tokens": [50896, 300, 382, 731, 13, 50978], "temperature": 0.0, "avg_logprob": -0.29408131705390084, "compression_ratio": 1.6245059288537549, "no_speech_prob": 0.0023596312385052443}, {"id": 1328, "seek": 659000, "start": 6602.28, "end": 6609.12, "text": " So let's now look at what a convolutional neural network with a single dense hidden", "tokens": [50978, 407, 718, 311, 586, 574, 412, 437, 257, 45216, 304, 18161, 3209, 365, 257, 2167, 18011, 7633, 51320], "temperature": 0.0, "avg_logprob": -0.29408131705390084, "compression_ratio": 1.6245059288537549, "no_speech_prob": 0.0023596312385052443}, {"id": 1329, "seek": 659000, "start": 6609.12, "end": 6610.44, "text": " layer would look like.", "tokens": [51320, 4583, 576, 574, 411, 13, 51386], "temperature": 0.0, "avg_logprob": -0.29408131705390084, "compression_ratio": 1.6245059288537549, "no_speech_prob": 0.0023596312385052443}, {"id": 1330, "seek": 659000, "start": 6610.44, "end": 6616.0, "text": " So we'd have our input, which this time will be, remember I've removed batch size, number", "tokens": [51386, 407, 321, 1116, 362, 527, 4846, 11, 597, 341, 565, 486, 312, 11, 1604, 286, 600, 7261, 15245, 2744, 11, 1230, 51664], "temperature": 0.0, "avg_logprob": -0.29408131705390084, "compression_ratio": 1.6245059288537549, "no_speech_prob": 0.0023596312385052443}, {"id": 1331, "seek": 661600, "start": 6616.0, "end": 6618.8, "text": " of channels by height by width.", "tokens": [50364, 295, 9235, 538, 6681, 538, 11402, 13, 50504], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1332, "seek": 661600, "start": 6618.8, "end": 6623.2, "text": " The operation, and we're ignoring the activation function, is going to be a convolution followed", "tokens": [50504, 440, 6916, 11, 293, 321, 434, 26258, 264, 24433, 2445, 11, 307, 516, 281, 312, 257, 45216, 6263, 50724], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1333, "seek": 661600, "start": 6623.2, "end": 6624.2, "text": " by a max-pull.", "tokens": [50724, 538, 257, 11469, 12, 79, 858, 13, 50774], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1334, "seek": 661600, "start": 6624.2, "end": 6627.6, "text": " Remember, any shape is representing a matrix.", "tokens": [50774, 5459, 11, 604, 3909, 307, 13460, 257, 8141, 13, 50944], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1335, "seek": 661600, "start": 6627.6, "end": 6633.4, "text": " So that gives us a matrix, which will be size numfilters by height over 2 by width over", "tokens": [50944, 407, 300, 2709, 505, 257, 8141, 11, 597, 486, 312, 2744, 1031, 19776, 1559, 538, 6681, 670, 568, 538, 11402, 670, 51234], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1336, "seek": 661600, "start": 6633.4, "end": 6636.6, "text": " 2, since we did a max-pulling.", "tokens": [51234, 568, 11, 1670, 321, 630, 257, 11469, 12, 79, 858, 278, 13, 51394], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1337, "seek": 661600, "start": 6636.6, "end": 6639.08, "text": " And then we take that and we flatten it.", "tokens": [51394, 400, 550, 321, 747, 300, 293, 321, 24183, 309, 13, 51518], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1338, "seek": 661600, "start": 6639.08, "end": 6645.12, "text": " I've put flatten in parentheses because flattening mathematically does nothing at all.", "tokens": [51518, 286, 600, 829, 24183, 294, 34153, 570, 24183, 278, 44003, 775, 1825, 412, 439, 13, 51820], "temperature": 0.0, "avg_logprob": -0.2867513151250334, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.48431769013404846}, {"id": 1339, "seek": 664512, "start": 6645.24, "end": 6649.84, "text": " Flattening is just telling Keras to think of it as a vector.", "tokens": [50370, 3235, 32733, 278, 307, 445, 3585, 591, 6985, 281, 519, 295, 309, 382, 257, 8062, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1340, "seek": 664512, "start": 6649.84, "end": 6654.5599999999995, "text": " It doesn't actually calculate anything, it doesn't move anything, it doesn't really do", "tokens": [50600, 467, 1177, 380, 767, 8873, 1340, 11, 309, 1177, 380, 1286, 1340, 11, 309, 1177, 380, 534, 360, 50836], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1341, "seek": 664512, "start": 6654.5599999999995, "end": 6655.5599999999995, "text": " anything.", "tokens": [50836, 1340, 13, 50886], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1342, "seek": 664512, "start": 6655.5599999999995, "end": 6657.76, "text": " It just says think of it as being a different shape.", "tokens": [50886, 467, 445, 1619, 519, 295, 309, 382, 885, 257, 819, 3909, 13, 50996], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1343, "seek": 664512, "start": 6657.76, "end": 6659.64, "text": " That's why I put it in parentheses.", "tokens": [50996, 663, 311, 983, 286, 829, 309, 294, 34153, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1344, "seek": 664512, "start": 6659.64, "end": 6663.72, "text": " So let's then take a matrix product, and remember I'm not putting in the activation functions", "tokens": [51090, 407, 718, 311, 550, 747, 257, 8141, 1674, 11, 293, 1604, 286, 478, 406, 3372, 294, 264, 24433, 6828, 51294], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1345, "seek": 664512, "start": 6663.72, "end": 6665.3, "text": " anymore.", "tokens": [51294, 3602, 13, 51373], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1346, "seek": 664512, "start": 6665.3, "end": 6670.96, "text": " So that would be our dense layer, gives us our first fully-connected layer, which will", "tokens": [51373, 407, 300, 576, 312, 527, 18011, 4583, 11, 2709, 505, 527, 700, 4498, 12, 9826, 292, 4583, 11, 597, 486, 51656], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1347, "seek": 664512, "start": 6670.96, "end": 6673.76, "text": " be of size number of activations.", "tokens": [51656, 312, 295, 2744, 1230, 295, 2430, 763, 13, 51796], "temperature": 0.0, "avg_logprob": -0.2605465480259487, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.012431167997419834}, {"id": 1348, "seek": 667376, "start": 6673.8, "end": 6677.92, "text": " And then we put that through a final matrix product to get an output of size number of", "tokens": [50366, 400, 550, 321, 829, 300, 807, 257, 2572, 8141, 1674, 281, 483, 364, 5598, 295, 2744, 1230, 295, 50572], "temperature": 0.0, "avg_logprob": -0.39226994233972884, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.013427994213998318}, {"id": 1349, "seek": 667376, "start": 6677.92, "end": 6679.08, "text": " classes.", "tokens": [50572, 5359, 13, 50630], "temperature": 0.0, "avg_logprob": -0.39226994233972884, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.013427994213998318}, {"id": 1350, "seek": 667376, "start": 6679.08, "end": 6684.04, "text": " So here is how we can represent a convolutional neural network with a single dense hidden", "tokens": [50630, 407, 510, 307, 577, 321, 393, 2906, 257, 45216, 304, 18161, 3209, 365, 257, 2167, 18011, 7633, 50878], "temperature": 0.0, "avg_logprob": -0.39226994233972884, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.013427994213998318}, {"id": 1351, "seek": 667376, "start": 6684.04, "end": 6685.04, "text": " layer.", "tokens": [50878, 4583, 13, 50928], "temperature": 0.0, "avg_logprob": -0.39226994233972884, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.013427994213998318}, {"id": 1352, "seek": 667376, "start": 6685.04, "end": 6686.04, "text": " Question Time 5", "tokens": [50928, 14464, 220, 22233, 1025, 50978], "temperature": 0.0, "avg_logprob": -0.39226994233972884, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.013427994213998318}, {"id": 1353, "seek": 667376, "start": 6686.04, "end": 6693.280000000001, "text": " The number of activations is the same as we had last time, it's whatever the n was that", "tokens": [50978, 440, 1230, 295, 2430, 763, 307, 264, 912, 382, 321, 632, 1036, 565, 11, 309, 311, 2035, 264, 297, 390, 300, 51340], "temperature": 0.0, "avg_logprob": -0.39226994233972884, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.013427994213998318}, {"id": 1354, "seek": 667376, "start": 6693.280000000001, "end": 6697.2, "text": " we wrote dense n.", "tokens": [51340, 321, 4114, 18011, 297, 13, 51536], "temperature": 0.0, "avg_logprob": -0.39226994233972884, "compression_ratio": 1.5317073170731708, "no_speech_prob": 0.013427994213998318}, {"id": 1355, "seek": 669720, "start": 6697.2, "end": 6705.12, "text": " Just like the number of filters is when we write convolution2D, we say number of filters", "tokens": [50364, 1449, 411, 264, 1230, 295, 15995, 307, 562, 321, 2464, 45216, 17, 35, 11, 321, 584, 1230, 295, 15995, 50760], "temperature": 0.0, "avg_logprob": -0.2461768388748169, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.034617505967617035}, {"id": 1356, "seek": 669720, "start": 6705.12, "end": 6711.32, "text": " followed by its size.", "tokens": [50760, 6263, 538, 1080, 2744, 13, 51070], "temperature": 0.0, "avg_logprob": -0.2461768388748169, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.034617505967617035}, {"id": 1357, "seek": 669720, "start": 6711.32, "end": 6717.5599999999995, "text": " So I'm going to now create a slightly more complex computation graph, but again I'm going", "tokens": [51070, 407, 286, 478, 516, 281, 586, 1884, 257, 4748, 544, 3997, 24903, 4295, 11, 457, 797, 286, 478, 516, 51382], "temperature": 0.0, "avg_logprob": -0.2461768388748169, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.034617505967617035}, {"id": 1358, "seek": 669720, "start": 6717.5599999999995, "end": 6721.92, "text": " to slightly simplify what I put on the screen, which is this time I am going to remove all", "tokens": [51382, 281, 4748, 20460, 437, 286, 829, 322, 264, 2568, 11, 597, 307, 341, 565, 286, 669, 516, 281, 4159, 439, 51600], "temperature": 0.0, "avg_logprob": -0.2461768388748169, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.034617505967617035}, {"id": 1359, "seek": 669720, "start": 6721.92, "end": 6723.44, "text": " of the layer operations.", "tokens": [51600, 295, 264, 4583, 7705, 13, 51676], "temperature": 0.0, "avg_logprob": -0.2461768388748169, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.034617505967617035}, {"id": 1360, "seek": 672344, "start": 6724.24, "end": 6731.32, "text": " Now that we have removed the activation function, you can see that in every case we have some", "tokens": [50404, 823, 300, 321, 362, 7261, 264, 24433, 2445, 11, 291, 393, 536, 300, 294, 633, 1389, 321, 362, 512, 50758], "temperature": 0.0, "avg_logprob": -0.2923862208490786, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.16451096534729004}, {"id": 1361, "seek": 672344, "start": 6731.32, "end": 6737.48, "text": " kind of linear thing, either a matrix product or a convolution, and optionally there might", "tokens": [50758, 733, 295, 8213, 551, 11, 2139, 257, 8141, 1674, 420, 257, 45216, 11, 293, 3614, 379, 456, 1062, 51066], "temperature": 0.0, "avg_logprob": -0.2923862208490786, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.16451096534729004}, {"id": 1362, "seek": 672344, "start": 6737.48, "end": 6739.36, "text": " also be a max-poll.", "tokens": [51066, 611, 312, 257, 11469, 12, 2259, 285, 13, 51160], "temperature": 0.0, "avg_logprob": -0.2923862208490786, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.16451096534729004}, {"id": 1363, "seek": 672344, "start": 6739.36, "end": 6744.08, "text": " So really this is not adding much additional information, so I'm going to get rid of it", "tokens": [51160, 407, 534, 341, 307, 406, 5127, 709, 4497, 1589, 11, 370, 286, 478, 516, 281, 483, 3973, 295, 309, 51396], "temperature": 0.0, "avg_logprob": -0.2923862208490786, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.16451096534729004}, {"id": 1364, "seek": 672344, "start": 6744.08, "end": 6745.08, "text": " from now on.", "tokens": [51396, 490, 586, 322, 13, 51446], "temperature": 0.0, "avg_logprob": -0.2923862208490786, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.16451096534729004}, {"id": 1365, "seek": 672344, "start": 6745.08, "end": 6746.839999999999, "text": " So we're now not showing the layer operations.", "tokens": [51446, 407, 321, 434, 586, 406, 4099, 264, 4583, 7705, 13, 51534], "temperature": 0.0, "avg_logprob": -0.2923862208490786, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.16451096534729004}, {"id": 1366, "seek": 674684, "start": 6746.84, "end": 6753.56, "text": " So remember now, every arrow is representing one or more layer operations, which will generally", "tokens": [50364, 407, 1604, 586, 11, 633, 11610, 307, 13460, 472, 420, 544, 4583, 7705, 11, 597, 486, 5101, 50700], "temperature": 0.0, "avg_logprob": -0.2994591816362128, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.03904595598578453}, {"id": 1367, "seek": 674684, "start": 6753.56, "end": 6758.32, "text": " be a convolution or a matrix product, followed by an activation function, and maybe there", "tokens": [50700, 312, 257, 45216, 420, 257, 8141, 1674, 11, 6263, 538, 364, 24433, 2445, 11, 293, 1310, 456, 50938], "temperature": 0.0, "avg_logprob": -0.2994591816362128, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.03904595598578453}, {"id": 1368, "seek": 674684, "start": 6758.32, "end": 6762.04, "text": " will be a max-polling in there as well.", "tokens": [50938, 486, 312, 257, 11469, 12, 79, 1833, 278, 294, 456, 382, 731, 13, 51124], "temperature": 0.0, "avg_logprob": -0.2994591816362128, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.03904595598578453}, {"id": 1369, "seek": 674684, "start": 6762.04, "end": 6770.84, "text": " So let's say we wanted to predict the third word of a 3-word string based on the previous", "tokens": [51124, 407, 718, 311, 584, 321, 1415, 281, 6069, 264, 2636, 1349, 295, 257, 805, 12, 7462, 6798, 2361, 322, 264, 3894, 51564], "temperature": 0.0, "avg_logprob": -0.2994591816362128, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.03904595598578453}, {"id": 1370, "seek": 674684, "start": 6770.84, "end": 6773.28, "text": " two words.", "tokens": [51564, 732, 2283, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2994591816362128, "compression_ratio": 1.5450236966824644, "no_speech_prob": 0.03904595598578453}, {"id": 1371, "seek": 677328, "start": 6773.28, "end": 6778.44, "text": " Now there's all kinds of ways we could do this, but here is one interesting way which", "tokens": [50364, 823, 456, 311, 439, 3685, 295, 2098, 321, 727, 360, 341, 11, 457, 510, 307, 472, 1880, 636, 597, 50622], "temperature": 0.0, "avg_logprob": -0.3120737793625042, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.05032873898744583}, {"id": 1372, "seek": 677328, "start": 6778.44, "end": 6783.04, "text": " you will now recognize you could do with Keras' functional API.", "tokens": [50622, 291, 486, 586, 5521, 291, 727, 360, 365, 591, 6985, 6, 11745, 9362, 13, 50852], "temperature": 0.0, "avg_logprob": -0.3120737793625042, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.05032873898744583}, {"id": 1373, "seek": 677328, "start": 6783.04, "end": 6793.28, "text": " We could take word1 input, and that could be either a one-hot encoded thing, in which", "tokens": [50852, 492, 727, 747, 1349, 16, 4846, 11, 293, 300, 727, 312, 2139, 257, 472, 12, 12194, 2058, 12340, 551, 11, 294, 597, 51364], "temperature": 0.0, "avg_logprob": -0.3120737793625042, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.05032873898744583}, {"id": 1374, "seek": 677328, "start": 6793.28, "end": 6799.36, "text": " case its size would be vocab size, or it could be an embedding of it.", "tokens": [51364, 1389, 1080, 2744, 576, 312, 2329, 455, 2744, 11, 420, 309, 727, 312, 364, 12240, 3584, 295, 309, 13, 51668], "temperature": 0.0, "avg_logprob": -0.3120737793625042, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.05032873898744583}, {"id": 1375, "seek": 677328, "start": 6799.36, "end": 6801.639999999999, "text": " Doesn't really matter either way.", "tokens": [51668, 12955, 380, 534, 1871, 2139, 636, 13, 51782], "temperature": 0.0, "avg_logprob": -0.3120737793625042, "compression_ratio": 1.555045871559633, "no_speech_prob": 0.05032873898744583}, {"id": 1376, "seek": 680164, "start": 6802.0, "end": 6810.12, "text": " We then stick that through a layer operation to get a matrix output, which is our first", "tokens": [50382, 492, 550, 2897, 300, 807, 257, 4583, 6916, 281, 483, 257, 8141, 5598, 11, 597, 307, 527, 700, 50788], "temperature": 0.0, "avg_logprob": -0.26617389985884743, "compression_ratio": 1.700507614213198, "no_speech_prob": 0.025178609415888786}, {"id": 1377, "seek": 680164, "start": 6810.12, "end": 6812.56, "text": " fully-connected layer.", "tokens": [50788, 4498, 12, 9826, 292, 4583, 13, 50910], "temperature": 0.0, "avg_logprob": -0.26617389985884743, "compression_ratio": 1.700507614213198, "no_speech_prob": 0.025178609415888786}, {"id": 1378, "seek": 680164, "start": 6812.56, "end": 6818.9800000000005, "text": " And this thing here, we could then take and put through another layer operation, but this", "tokens": [50910, 400, 341, 551, 510, 11, 321, 727, 550, 747, 293, 829, 807, 1071, 4583, 6916, 11, 457, 341, 51231], "temperature": 0.0, "avg_logprob": -0.26617389985884743, "compression_ratio": 1.700507614213198, "no_speech_prob": 0.025178609415888786}, {"id": 1379, "seek": 680164, "start": 6818.9800000000005, "end": 6827.4800000000005, "text": " time we could also add in the word2 input, either of vocab size or the embedding of it.", "tokens": [51231, 565, 321, 727, 611, 909, 294, 264, 1349, 17, 4846, 11, 2139, 295, 2329, 455, 2744, 420, 264, 12240, 3584, 295, 309, 13, 51656], "temperature": 0.0, "avg_logprob": -0.26617389985884743, "compression_ratio": 1.700507614213198, "no_speech_prob": 0.025178609415888786}, {"id": 1380, "seek": 680164, "start": 6827.4800000000005, "end": 6830.160000000001, "text": " Put that through a layer operation of its own.", "tokens": [51656, 4935, 300, 807, 257, 4583, 6916, 295, 1080, 1065, 13, 51790], "temperature": 0.0, "avg_logprob": -0.26617389985884743, "compression_ratio": 1.700507614213198, "no_speech_prob": 0.025178609415888786}, {"id": 1381, "seek": 683016, "start": 6830.68, "end": 6835.72, "text": " When we have two arrows coming in together, that represents a merge.", "tokens": [50390, 1133, 321, 362, 732, 19669, 1348, 294, 1214, 11, 300, 8855, 257, 22183, 13, 50642], "temperature": 0.0, "avg_logprob": -0.24895644973922562, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.010652381926774979}, {"id": 1382, "seek": 683016, "start": 6835.72, "end": 6842.04, "text": " And a merge could either be done as a sum or as a concat.", "tokens": [50642, 400, 257, 22183, 727, 2139, 312, 1096, 382, 257, 2408, 420, 382, 257, 1588, 267, 13, 50958], "temperature": 0.0, "avg_logprob": -0.24895644973922562, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.010652381926774979}, {"id": 1383, "seek": 683016, "start": 6842.04, "end": 6845.88, "text": " I'm not going to say one's better than the other, but there are two ways we can take", "tokens": [50958, 286, 478, 406, 516, 281, 584, 472, 311, 1101, 813, 264, 661, 11, 457, 456, 366, 732, 2098, 321, 393, 747, 51150], "temperature": 0.0, "avg_logprob": -0.24895644973922562, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.010652381926774979}, {"id": 1384, "seek": 683016, "start": 6845.88, "end": 6849.639999999999, "text": " two input vectors and combine them together.", "tokens": [51150, 732, 4846, 18875, 293, 10432, 552, 1214, 13, 51338], "temperature": 0.0, "avg_logprob": -0.24895644973922562, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.010652381926774979}, {"id": 1385, "seek": 683016, "start": 6849.639999999999, "end": 6858.3, "text": " So now at this point, we have the input from word2 after sticking that through a layer.", "tokens": [51338, 407, 586, 412, 341, 935, 11, 321, 362, 264, 4846, 490, 1349, 17, 934, 13465, 300, 807, 257, 4583, 13, 51771], "temperature": 0.0, "avg_logprob": -0.24895644973922562, "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.010652381926774979}, {"id": 1386, "seek": 685830, "start": 6858.3, "end": 6863.18, "text": " We have the input from word1 after sticking that through two layers.", "tokens": [50364, 492, 362, 264, 4846, 490, 1349, 16, 934, 13465, 300, 807, 732, 7914, 13, 50608], "temperature": 0.0, "avg_logprob": -0.26123979356553817, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.026758432388305664}, {"id": 1387, "seek": 685830, "start": 6863.18, "end": 6867.34, "text": " Merge them together and stick that through another layer to get our output, which we", "tokens": [50608, 6124, 432, 552, 1214, 293, 2897, 300, 807, 1071, 4583, 281, 483, 527, 5598, 11, 597, 321, 50816], "temperature": 0.0, "avg_logprob": -0.26123979356553817, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.026758432388305664}, {"id": 1388, "seek": 685830, "start": 6867.34, "end": 6876.360000000001, "text": " could then compare to word3 and try to train that to recognize word3 from words1 and 2.", "tokens": [50816, 727, 550, 6794, 281, 1349, 18, 293, 853, 281, 3847, 300, 281, 5521, 1349, 18, 490, 2283, 16, 293, 568, 13, 51267], "temperature": 0.0, "avg_logprob": -0.26123979356553817, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.026758432388305664}, {"id": 1389, "seek": 685830, "start": 6876.360000000001, "end": 6878.52, "text": " So you could try this.", "tokens": [51267, 407, 291, 727, 853, 341, 13, 51375], "temperature": 0.0, "avg_logprob": -0.26123979356553817, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.026758432388305664}, {"id": 1390, "seek": 685830, "start": 6878.52, "end": 6885.46, "text": " You could try and build this network using some corpus you find online, see how it goes.", "tokens": [51375, 509, 727, 853, 293, 1322, 341, 3209, 1228, 512, 1181, 31624, 291, 915, 2950, 11, 536, 577, 309, 1709, 13, 51722], "temperature": 0.0, "avg_logprob": -0.26123979356553817, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.026758432388305664}, {"id": 1391, "seek": 688546, "start": 6885.46, "end": 6891.82, "text": " Pretty obviously then you could bring it up another level to say let's try and predict", "tokens": [50364, 10693, 2745, 550, 291, 727, 1565, 309, 493, 1071, 1496, 281, 584, 718, 311, 853, 293, 6069, 50682], "temperature": 0.0, "avg_logprob": -0.2871581654489776, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.2974581718444824}, {"id": 1392, "seek": 688546, "start": 6891.82, "end": 6899.42, "text": " the fourth word of a 3-word string using words1 and 2 and 3.", "tokens": [50682, 264, 6409, 1349, 295, 257, 805, 12, 7462, 6798, 1228, 2283, 16, 293, 568, 293, 805, 13, 51062], "temperature": 0.0, "avg_logprob": -0.2871581654489776, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.2974581718444824}, {"id": 1393, "seek": 688546, "start": 6899.42, "end": 6905.18, "text": " Now the reason I'm doing it in this way is that what's happening is each time I'm going", "tokens": [51062, 823, 264, 1778, 286, 478, 884, 309, 294, 341, 636, 307, 300, 437, 311, 2737, 307, 1184, 565, 286, 478, 516, 51350], "temperature": 0.0, "avg_logprob": -0.2871581654489776, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.2974581718444824}, {"id": 1394, "seek": 688546, "start": 6905.18, "end": 6911.22, "text": " through another layer operation and then bringing in word2 and going through a layer operation", "tokens": [51350, 807, 1071, 4583, 6916, 293, 550, 5062, 294, 1349, 17, 293, 516, 807, 257, 4583, 6916, 51652], "temperature": 0.0, "avg_logprob": -0.2871581654489776, "compression_ratio": 1.7010309278350515, "no_speech_prob": 0.2974581718444824}, {"id": 1395, "seek": 691122, "start": 6911.22, "end": 6917.860000000001, "text": " and then bringing in word3 and going through a layer operation, is I am collecting state.", "tokens": [50364, 293, 550, 5062, 294, 1349, 18, 293, 516, 807, 257, 4583, 6916, 11, 307, 286, 669, 12510, 1785, 13, 50696], "temperature": 0.0, "avg_logprob": -0.25630716933417563, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.17780058085918427}, {"id": 1396, "seek": 691122, "start": 6917.860000000001, "end": 6923.3, "text": " Each of these things has the ability to capture state about all of the words that have come", "tokens": [50696, 6947, 295, 613, 721, 575, 264, 3485, 281, 7983, 1785, 466, 439, 295, 264, 2283, 300, 362, 808, 50968], "temperature": 0.0, "avg_logprob": -0.25630716933417563, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.17780058085918427}, {"id": 1397, "seek": 691122, "start": 6923.3, "end": 6927.52, "text": " so far and the order in which they've arrived.", "tokens": [50968, 370, 1400, 293, 264, 1668, 294, 597, 436, 600, 6678, 13, 51179], "temperature": 0.0, "avg_logprob": -0.25630716933417563, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.17780058085918427}, {"id": 1398, "seek": 691122, "start": 6927.52, "end": 6935.34, "text": " So by the time I get to predicting word4, this matrix has had the opportunity to learn", "tokens": [51179, 407, 538, 264, 565, 286, 483, 281, 32884, 1349, 19, 11, 341, 8141, 575, 632, 264, 2650, 281, 1466, 51570], "temperature": 0.0, "avg_logprob": -0.25630716933417563, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.17780058085918427}, {"id": 1399, "seek": 691122, "start": 6935.34, "end": 6940.14, "text": " what does it need to know about the previous word's orderings and how they're connected", "tokens": [51570, 437, 775, 309, 643, 281, 458, 466, 264, 3894, 1349, 311, 1668, 1109, 293, 577, 436, 434, 4582, 51810], "temperature": 0.0, "avg_logprob": -0.25630716933417563, "compression_ratio": 1.665289256198347, "no_speech_prob": 0.17780058085918427}, {"id": 1400, "seek": 694014, "start": 6940.22, "end": 6944.58, "text": " to each other and so forth in order to predict this fourth word.", "tokens": [50368, 281, 1184, 661, 293, 370, 5220, 294, 1668, 281, 6069, 341, 6409, 1349, 13, 50586], "temperature": 0.0, "avg_logprob": -0.2593639709137298, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.06656483560800552}, {"id": 1401, "seek": 694014, "start": 6944.58, "end": 6947.700000000001, "text": " So we're actually capturing state here.", "tokens": [50586, 407, 321, 434, 767, 23384, 1785, 510, 13, 50742], "temperature": 0.0, "avg_logprob": -0.2593639709137298, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.06656483560800552}, {"id": 1402, "seek": 694014, "start": 6947.700000000001, "end": 6954.9800000000005, "text": " It's important to note we have not yet previously built a model in Keras which has input coming", "tokens": [50742, 467, 311, 1021, 281, 3637, 321, 362, 406, 1939, 8046, 3094, 257, 2316, 294, 591, 6985, 597, 575, 4846, 1348, 51106], "temperature": 0.0, "avg_logprob": -0.2593639709137298, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.06656483560800552}, {"id": 1403, "seek": 694014, "start": 6954.9800000000005, "end": 6961.1, "text": " in at anywhere other than the first layer, but there's no reason we can't.", "tokens": [51106, 294, 412, 4992, 661, 813, 264, 700, 4583, 11, 457, 456, 311, 572, 1778, 321, 393, 380, 13, 51412], "temperature": 0.0, "avg_logprob": -0.2593639709137298, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.06656483560800552}, {"id": 1404, "seek": 694014, "start": 6961.1, "end": 6965.860000000001, "text": " One of you asked a great question earlier, which was could we use this to bring in metadata", "tokens": [51412, 1485, 295, 291, 2351, 257, 869, 1168, 3071, 11, 597, 390, 727, 321, 764, 341, 281, 1565, 294, 26603, 51650], "temperature": 0.0, "avg_logprob": -0.2593639709137298, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.06656483560800552}, {"id": 1405, "seek": 696586, "start": 6966.0199999999995, "end": 6971.54, "text": " like the speed a car was going to add it with a convolutional neural network's image data.", "tokens": [50372, 411, 264, 3073, 257, 1032, 390, 516, 281, 909, 309, 365, 257, 45216, 304, 18161, 3209, 311, 3256, 1412, 13, 50648], "temperature": 0.0, "avg_logprob": -0.28424888390761155, "compression_ratio": 1.6587301587301588, "no_speech_prob": 0.03161699324846268}, {"id": 1406, "seek": 696586, "start": 6971.54, "end": 6972.94, "text": " I said yes we can.", "tokens": [50648, 286, 848, 2086, 321, 393, 13, 50718], "temperature": 0.0, "avg_logprob": -0.28424888390761155, "compression_ratio": 1.6587301587301588, "no_speech_prob": 0.03161699324846268}, {"id": 1407, "seek": 696586, "start": 6972.94, "end": 6978.139999999999, "text": " So in this case we're doing the same thing, which is we're bringing in an additional word's", "tokens": [50718, 407, 294, 341, 1389, 321, 434, 884, 264, 912, 551, 11, 597, 307, 321, 434, 5062, 294, 364, 4497, 1349, 311, 50978], "temperature": 0.0, "avg_logprob": -0.28424888390761155, "compression_ratio": 1.6587301587301588, "no_speech_prob": 0.03161699324846268}, {"id": 1408, "seek": 696586, "start": 6978.139999999999, "end": 6982.9, "text": " worth of data and remember each time you see two different arrows coming in, that represents", "tokens": [50978, 3163, 295, 1412, 293, 1604, 1184, 565, 291, 536, 732, 819, 19669, 1348, 294, 11, 300, 8855, 51216], "temperature": 0.0, "avg_logprob": -0.28424888390761155, "compression_ratio": 1.6587301587301588, "no_speech_prob": 0.03161699324846268}, {"id": 1409, "seek": 696586, "start": 6982.9, "end": 6985.219999999999, "text": " a merge operation.", "tokens": [51216, 257, 22183, 6916, 13, 51332], "temperature": 0.0, "avg_logprob": -0.28424888390761155, "compression_ratio": 1.6587301587301588, "no_speech_prob": 0.03161699324846268}, {"id": 1410, "seek": 696586, "start": 6985.219999999999, "end": 6991.54, "text": " So here's a perfectly reasonable way of trying to predict the fourth word from the previous", "tokens": [51332, 407, 510, 311, 257, 6239, 10585, 636, 295, 1382, 281, 6069, 264, 6409, 1349, 490, 264, 3894, 51648], "temperature": 0.0, "avg_logprob": -0.28424888390761155, "compression_ratio": 1.6587301587301588, "no_speech_prob": 0.03161699324846268}, {"id": 1411, "seek": 696586, "start": 6991.54, "end": 6992.94, "text": " three words.", "tokens": [51648, 1045, 2283, 13, 51718], "temperature": 0.0, "avg_logprob": -0.28424888390761155, "compression_ratio": 1.6587301587301588, "no_speech_prob": 0.03161699324846268}, {"id": 1412, "seek": 699294, "start": 6993.139999999999, "end": 7001.179999999999, "text": " So this leads to a really interesting question, which was what if instead we said let's bring", "tokens": [50374, 407, 341, 6689, 281, 257, 534, 1880, 1168, 11, 597, 390, 437, 498, 2602, 321, 848, 718, 311, 1565, 50776], "temperature": 0.0, "avg_logprob": -0.28499315466199604, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.008711245842278004}, {"id": 1413, "seek": 699294, "start": 7001.179999999999, "end": 7009.98, "text": " in our word1 and then we had a layer operation in order to create our hidden state.", "tokens": [50776, 294, 527, 1349, 16, 293, 550, 321, 632, 257, 4583, 6916, 294, 1668, 281, 1884, 527, 7633, 1785, 13, 51216], "temperature": 0.0, "avg_logprob": -0.28499315466199604, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.008711245842278004}, {"id": 1414, "seek": 699294, "start": 7009.98, "end": 7016.419999999999, "text": " And that would be enough to predict word2.", "tokens": [51216, 400, 300, 576, 312, 1547, 281, 6069, 1349, 17, 13, 51538], "temperature": 0.0, "avg_logprob": -0.28499315466199604, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.008711245842278004}, {"id": 1415, "seek": 701642, "start": 7016.42, "end": 7026.6, "text": " And then to predict word3, could we just do a layer operation and generate itself.", "tokens": [50364, 400, 550, 281, 6069, 1349, 18, 11, 727, 321, 445, 360, 257, 4583, 6916, 293, 8460, 2564, 13, 50873], "temperature": 0.0, "avg_logprob": -0.25835302506370106, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1347704380750656}, {"id": 1416, "seek": 701642, "start": 7026.6, "end": 7028.9, "text": " And then that could be used to predict word3.", "tokens": [50873, 400, 550, 300, 727, 312, 1143, 281, 6069, 1349, 18, 13, 50988], "temperature": 0.0, "avg_logprob": -0.25835302506370106, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1347704380750656}, {"id": 1417, "seek": 701642, "start": 7028.9, "end": 7034.64, "text": " And then run it again to predict word4 and run it again to predict word5.", "tokens": [50988, 400, 550, 1190, 309, 797, 281, 6069, 1349, 19, 293, 1190, 309, 797, 281, 6069, 1349, 20, 13, 51275], "temperature": 0.0, "avg_logprob": -0.25835302506370106, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1347704380750656}, {"id": 1418, "seek": 701642, "start": 7034.64, "end": 7038.9400000000005, "text": " This is called an RNN.", "tokens": [51275, 639, 307, 1219, 364, 45702, 45, 13, 51490], "temperature": 0.0, "avg_logprob": -0.25835302506370106, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1347704380750656}, {"id": 1419, "seek": 701642, "start": 7038.9400000000005, "end": 7044.82, "text": " And everything that you see here is exactly the same structurally as everything I've shown", "tokens": [51490, 400, 1203, 300, 291, 536, 510, 307, 2293, 264, 912, 6594, 6512, 382, 1203, 286, 600, 4898, 51784], "temperature": 0.0, "avg_logprob": -0.25835302506370106, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1347704380750656}, {"id": 1420, "seek": 701642, "start": 7044.82, "end": 7046.02, "text": " before.", "tokens": [51784, 949, 13, 51844], "temperature": 0.0, "avg_logprob": -0.25835302506370106, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1347704380750656}, {"id": 1421, "seek": 704602, "start": 7046.620000000001, "end": 7053.38, "text": " The colored-in areas represent matrices and the arrows represent layer operations.", "tokens": [50394, 440, 14332, 12, 259, 3179, 2906, 32284, 293, 264, 19669, 2906, 4583, 7705, 13, 50732], "temperature": 0.0, "avg_logprob": -0.2633773185111381, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006878444692119956}, {"id": 1422, "seek": 704602, "start": 7053.38, "end": 7058.820000000001, "text": " One of the really interesting things about an RNN is each of these arrows, that you see", "tokens": [50732, 1485, 295, 264, 534, 1880, 721, 466, 364, 45702, 45, 307, 1184, 295, 613, 19669, 11, 300, 291, 536, 51004], "temperature": 0.0, "avg_logprob": -0.2633773185111381, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006878444692119956}, {"id": 1423, "seek": 704602, "start": 7058.820000000001, "end": 7063.22, "text": " three arrows, there's only one weight matrix attached to those.", "tokens": [51004, 1045, 19669, 11, 456, 311, 787, 472, 3364, 8141, 8570, 281, 729, 13, 51224], "temperature": 0.0, "avg_logprob": -0.2633773185111381, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006878444692119956}, {"id": 1424, "seek": 704602, "start": 7063.22, "end": 7069.26, "text": " In other words, it's the equivalent thing of saying every time you see an arrow from", "tokens": [51224, 682, 661, 2283, 11, 309, 311, 264, 10344, 551, 295, 1566, 633, 565, 291, 536, 364, 11610, 490, 51526], "temperature": 0.0, "avg_logprob": -0.2633773185111381, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.0006878444692119956}, {"id": 1425, "seek": 706926, "start": 7069.26, "end": 7079.02, "text": " a circle to a circle, those two weight matrices have to be exactly the same.", "tokens": [50364, 257, 6329, 281, 257, 6329, 11, 729, 732, 3364, 32284, 362, 281, 312, 2293, 264, 912, 13, 50852], "temperature": 0.0, "avg_logprob": -0.215804049843236, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.4035182297229767}, {"id": 1426, "seek": 706926, "start": 7079.02, "end": 7086.1, "text": " Every time you see an arrow from a rectangle to a circle, those three matrices have to", "tokens": [50852, 2048, 565, 291, 536, 364, 11610, 490, 257, 21930, 281, 257, 6329, 11, 729, 1045, 32284, 362, 281, 51206], "temperature": 0.0, "avg_logprob": -0.215804049843236, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.4035182297229767}, {"id": 1427, "seek": 706926, "start": 7086.1, "end": 7087.66, "text": " be exactly the same.", "tokens": [51206, 312, 2293, 264, 912, 13, 51284], "temperature": 0.0, "avg_logprob": -0.215804049843236, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.4035182297229767}, {"id": 1428, "seek": 706926, "start": 7087.66, "end": 7092.06, "text": " And then finally you've got an arrow from a circle to a triangle and that weight matrix", "tokens": [51284, 400, 550, 2721, 291, 600, 658, 364, 11610, 490, 257, 6329, 281, 257, 13369, 293, 300, 3364, 8141, 51504], "temperature": 0.0, "avg_logprob": -0.215804049843236, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.4035182297229767}, {"id": 1429, "seek": 706926, "start": 7092.06, "end": 7093.06, "text": " is separate.", "tokens": [51504, 307, 4994, 13, 51554], "temperature": 0.0, "avg_logprob": -0.215804049843236, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.4035182297229767}, {"id": 1430, "seek": 706926, "start": 7093.06, "end": 7099.22, "text": " The idea being that if you have a word coming in and being added to some state, why would", "tokens": [51554, 440, 1558, 885, 300, 498, 291, 362, 257, 1349, 1348, 294, 293, 885, 3869, 281, 512, 1785, 11, 983, 576, 51862], "temperature": 0.0, "avg_logprob": -0.215804049843236, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.4035182297229767}, {"id": 1431, "seek": 709922, "start": 7100.18, "end": 7103.820000000001, "text": " you want to treat it differently depending on whether it's the first word in a string", "tokens": [50412, 291, 528, 281, 2387, 309, 7614, 5413, 322, 1968, 309, 311, 264, 700, 1349, 294, 257, 6798, 50594], "temperature": 0.0, "avg_logprob": -0.3002779798687629, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0053017535246908665}, {"id": 1432, "seek": 709922, "start": 7103.820000000001, "end": 7108.38, "text": " or the third word in a string, given that generally speaking we kind of split up strings", "tokens": [50594, 420, 264, 2636, 1349, 294, 257, 6798, 11, 2212, 300, 5101, 4124, 321, 733, 295, 7472, 493, 13985, 50822], "temperature": 0.0, "avg_logprob": -0.3002779798687629, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0053017535246908665}, {"id": 1433, "seek": 709922, "start": 7108.38, "end": 7109.7, "text": " pretty much at random anyway.", "tokens": [50822, 1238, 709, 412, 4974, 4033, 13, 50888], "temperature": 0.0, "avg_logprob": -0.3002779798687629, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0053017535246908665}, {"id": 1434, "seek": 709922, "start": 7109.7, "end": 7113.820000000001, "text": " We're going to be having a whole bunch of 11-word strings.", "tokens": [50888, 492, 434, 516, 281, 312, 1419, 257, 1379, 3840, 295, 2975, 12, 7462, 13985, 13, 51094], "temperature": 0.0, "avg_logprob": -0.3002779798687629, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0053017535246908665}, {"id": 1435, "seek": 709922, "start": 7113.820000000001, "end": 7120.08, "text": " So one of the nice things about this way of thinking about it where you have it going", "tokens": [51094, 407, 472, 295, 264, 1481, 721, 466, 341, 636, 295, 1953, 466, 309, 689, 291, 362, 309, 516, 51407], "temperature": 0.0, "avg_logprob": -0.3002779798687629, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0053017535246908665}, {"id": 1436, "seek": 709922, "start": 7120.08, "end": 7125.9400000000005, "text": " back to itself is that you can very clearly see there is one layer operation, one weight", "tokens": [51407, 646, 281, 2564, 307, 300, 291, 393, 588, 4448, 536, 456, 307, 472, 4583, 6916, 11, 472, 3364, 51700], "temperature": 0.0, "avg_logprob": -0.3002779798687629, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0053017535246908665}, {"id": 1437, "seek": 712594, "start": 7125.94, "end": 7134.259999999999, "text": " matrix for input to hidden, one for hidden to hidden, circle to circle, and one for hidden", "tokens": [50364, 8141, 337, 4846, 281, 7633, 11, 472, 337, 7633, 281, 7633, 11, 6329, 281, 6329, 11, 293, 472, 337, 7633, 50780], "temperature": 0.0, "avg_logprob": -0.2883006529374556, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.6075955033302307}, {"id": 1438, "seek": 712594, "start": 7134.259999999999, "end": 7138.379999999999, "text": " to output, i.e. circle to triangle.", "tokens": [50780, 281, 5598, 11, 741, 13, 68, 13, 6329, 281, 13369, 13, 50986], "temperature": 0.0, "avg_logprob": -0.2883006529374556, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.6075955033302307}, {"id": 1439, "seek": 712594, "start": 7138.379999999999, "end": 7144.54, "text": " So we're going to talk about that in a lot more detail next week.", "tokens": [50986, 407, 321, 434, 516, 281, 751, 466, 300, 294, 257, 688, 544, 2607, 958, 1243, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2883006529374556, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.6075955033302307}, {"id": 1440, "seek": 714454, "start": 7144.54, "end": 7154.78, "text": " For now, I'm just going to quickly show you something in the last one minute, which is", "tokens": [50364, 1171, 586, 11, 286, 478, 445, 516, 281, 2661, 855, 291, 746, 294, 264, 1036, 472, 3456, 11, 597, 307, 50876], "temperature": 0.0, "avg_logprob": -0.32356546842134914, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.17104817926883698}, {"id": 1441, "seek": 714454, "start": 7154.78, "end": 7164.66, "text": " that we can train something which takes, for example, all of the text of Nietzsche.", "tokens": [50876, 300, 321, 393, 3847, 746, 597, 2516, 11, 337, 1365, 11, 439, 295, 264, 2487, 295, 36583, 89, 12287, 13, 51370], "temperature": 0.0, "avg_logprob": -0.32356546842134914, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.17104817926883698}, {"id": 1442, "seek": 714454, "start": 7164.66, "end": 7167.94, "text": " So here's a bit of his text, I've just read it in here.", "tokens": [51370, 407, 510, 311, 257, 857, 295, 702, 2487, 11, 286, 600, 445, 1401, 309, 294, 510, 13, 51534], "temperature": 0.0, "avg_logprob": -0.32356546842134914, "compression_ratio": 1.421383647798742, "no_speech_prob": 0.17104817926883698}, {"id": 1443, "seek": 716794, "start": 7168.179999999999, "end": 7176.219999999999, "text": " We could split it up into every sequence of length 40.", "tokens": [50376, 492, 727, 7472, 309, 493, 666, 633, 8310, 295, 4641, 3356, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2713849782943726, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.33108440041542053}, {"id": 1444, "seek": 716794, "start": 7176.219999999999, "end": 7181.46, "text": " So I've gone through the whole text and grabbed every sequence of length 40.", "tokens": [50778, 407, 286, 600, 2780, 807, 264, 1379, 2487, 293, 18607, 633, 8310, 295, 4641, 3356, 13, 51040], "temperature": 0.0, "avg_logprob": -0.2713849782943726, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.33108440041542053}, {"id": 1445, "seek": 716794, "start": 7181.46, "end": 7186.139999999999, "text": " And then I've created an RNN and its goal is to take the sentence which represents the", "tokens": [51040, 400, 550, 286, 600, 2942, 364, 45702, 45, 293, 1080, 3387, 307, 281, 747, 264, 8174, 597, 8855, 264, 51274], "temperature": 0.0, "avg_logprob": -0.2713849782943726, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.33108440041542053}, {"id": 1446, "seek": 716794, "start": 7186.139999999999, "end": 7195.599999999999, "text": " indexes from i to i plus 40 and predict the sentence from i plus 1 to i plus 40 plus 1.", "tokens": [51274, 8186, 279, 490, 741, 281, 741, 1804, 3356, 293, 6069, 264, 8174, 490, 741, 1804, 502, 281, 741, 1804, 3356, 1804, 502, 13, 51747], "temperature": 0.0, "avg_logprob": -0.2713849782943726, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.33108440041542053}, {"id": 1447, "seek": 719560, "start": 7195.6, "end": 7204.08, "text": " So every string of length maxLen, I'm trying to predict the string one word after that.", "tokens": [50364, 407, 633, 6798, 295, 4641, 11469, 43, 268, 11, 286, 478, 1382, 281, 6069, 264, 6798, 472, 1349, 934, 300, 13, 50788], "temperature": 0.0, "avg_logprob": -0.32069568392596665, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.05582238361239433}, {"id": 1448, "seek": 719560, "start": 7204.08, "end": 7210.92, "text": " And so I can take that now and create a model, an LSTM is a kind of recurrent neural network", "tokens": [50788, 400, 370, 286, 393, 747, 300, 586, 293, 1884, 257, 2316, 11, 364, 441, 6840, 44, 307, 257, 733, 295, 18680, 1753, 18161, 3209, 51130], "temperature": 0.0, "avg_logprob": -0.32069568392596665, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.05582238361239433}, {"id": 1449, "seek": 719560, "start": 7210.92, "end": 7215.08, "text": " which we'll talk about next week, which has a recurrent neural network, starts of course", "tokens": [51130, 597, 321, 603, 751, 466, 958, 1243, 11, 597, 575, 257, 18680, 1753, 18161, 3209, 11, 3719, 295, 1164, 51338], "temperature": 0.0, "avg_logprob": -0.32069568392596665, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.05582238361239433}, {"id": 1450, "seek": 719560, "start": 7215.08, "end": 7217.76, "text": " with an embedding.", "tokens": [51338, 365, 364, 12240, 3584, 13, 51472], "temperature": 0.0, "avg_logprob": -0.32069568392596665, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.05582238361239433}, {"id": 1451, "seek": 721776, "start": 7217.76, "end": 7231.56, "text": " And then I can train that by passing in my sentences and my sentence one character later.", "tokens": [50364, 400, 550, 286, 393, 3847, 300, 538, 8437, 294, 452, 16579, 293, 452, 8174, 472, 2517, 1780, 13, 51054], "temperature": 0.0, "avg_logprob": -0.3003820441235071, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.044678330421447754}, {"id": 1452, "seek": 721776, "start": 7231.56, "end": 7237.92, "text": " I can then say, let's try and generate 300 characters by building a prediction of what", "tokens": [51054, 286, 393, 550, 584, 11, 718, 311, 853, 293, 8460, 6641, 4342, 538, 2390, 257, 17630, 295, 437, 51372], "temperature": 0.0, "avg_logprob": -0.3003820441235071, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.044678330421447754}, {"id": 1453, "seek": 721776, "start": 7237.92, "end": 7240.08, "text": " do you think the next character would be.", "tokens": [51372, 360, 291, 519, 264, 958, 2517, 576, 312, 13, 51480], "temperature": 0.0, "avg_logprob": -0.3003820441235071, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.044678330421447754}, {"id": 1454, "seek": 721776, "start": 7240.08, "end": 7242.2, "text": " And so I have to seed it with something.", "tokens": [51480, 400, 370, 286, 362, 281, 8871, 309, 365, 746, 13, 51586], "temperature": 0.0, "avg_logprob": -0.3003820441235071, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.044678330421447754}, {"id": 1455, "seek": 721776, "start": 7242.2, "end": 7247.18, "text": " So I seeded it with something that felt very Nietzschean, ethics is a basic foundation", "tokens": [51586, 407, 286, 8871, 292, 309, 365, 746, 300, 2762, 588, 36583, 89, 12287, 282, 11, 19769, 307, 257, 3875, 7030, 51835], "temperature": 0.0, "avg_logprob": -0.3003820441235071, "compression_ratio": 1.6320754716981132, "no_speech_prob": 0.044678330421447754}, {"id": 1456, "seek": 724718, "start": 7247.22, "end": 7249.9400000000005, "text": " of all that, and see what happens.", "tokens": [50366, 295, 439, 300, 11, 293, 536, 437, 2314, 13, 50502], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1457, "seek": 724718, "start": 7249.9400000000005, "end": 7254.780000000001, "text": " And after training it for only a few seconds, I get ethics is a basic foundation of all", "tokens": [50502, 400, 934, 3097, 309, 337, 787, 257, 1326, 3949, 11, 286, 483, 19769, 307, 257, 3875, 7030, 295, 439, 50744], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1458, "seek": 724718, "start": 7254.780000000001, "end": 7258.22, "text": " that blah blah blah.", "tokens": [50744, 300, 12288, 12288, 12288, 13, 50916], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1459, "seek": 724718, "start": 7258.22, "end": 7261.38, "text": " You can get the sense it's starting to learn a bit about the idea that, oh by the way,", "tokens": [50916, 509, 393, 483, 264, 2020, 309, 311, 2891, 281, 1466, 257, 857, 466, 264, 1558, 300, 11, 1954, 538, 264, 636, 11, 51074], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1460, "seek": 724718, "start": 7261.38, "end": 7265.22, "text": " one thing to mention is this Nietzsche corpus is slightly annoying.", "tokens": [51074, 472, 551, 281, 2152, 307, 341, 36583, 89, 12287, 1181, 31624, 307, 4748, 11304, 13, 51266], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1461, "seek": 724718, "start": 7265.22, "end": 7269.34, "text": " It has carriage returns after every line.", "tokens": [51266, 467, 575, 31811, 11247, 934, 633, 1622, 13, 51472], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1462, "seek": 724718, "start": 7269.34, "end": 7272.22, "text": " So you'll see it's going to throw carriage returns in all over the place.", "tokens": [51472, 407, 291, 603, 536, 309, 311, 516, 281, 3507, 31811, 11247, 294, 439, 670, 264, 1081, 13, 51616], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1463, "seek": 724718, "start": 7272.22, "end": 7275.9800000000005, "text": " And it's got some pretty hideous formatting.", "tokens": [51616, 400, 309, 311, 658, 512, 1238, 6479, 563, 39366, 13, 51804], "temperature": 0.0, "avg_logprob": -0.33644373823956747, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.02262807823717594}, {"id": 1464, "seek": 727598, "start": 7275.98, "end": 7279.339999999999, "text": " So that was after about training for 30 seconds.", "tokens": [50364, 407, 300, 390, 934, 466, 3097, 337, 2217, 3949, 13, 50532], "temperature": 0.0, "avg_logprob": -0.3057896440679377, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.018831320106983185}, {"id": 1465, "seek": 727598, "start": 7279.339999999999, "end": 7284.419999999999, "text": " I train it for another 30 seconds and I get to a point where it's kind of understanding", "tokens": [50532, 286, 3847, 309, 337, 1071, 2217, 3949, 293, 286, 483, 281, 257, 935, 689, 309, 311, 733, 295, 3701, 50786], "temperature": 0.0, "avg_logprob": -0.3057896440679377, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.018831320106983185}, {"id": 1466, "seek": 727598, "start": 7284.419999999999, "end": 7287.82, "text": " the concept of punctuation and spacing.", "tokens": [50786, 264, 3410, 295, 27006, 16073, 293, 27739, 13, 50956], "temperature": 0.0, "avg_logprob": -0.3057896440679377, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.018831320106983185}, {"id": 1467, "seek": 727598, "start": 7287.82, "end": 7295.12, "text": " And then I've trained it for 640 seconds and it's starting to actually create real words.", "tokens": [50956, 400, 550, 286, 600, 8895, 309, 337, 1386, 5254, 3949, 293, 309, 311, 2891, 281, 767, 1884, 957, 2283, 13, 51321], "temperature": 0.0, "avg_logprob": -0.3057896440679377, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.018831320106983185}, {"id": 1468, "seek": 727598, "start": 7295.12, "end": 7302.219999999999, "text": " And then I've trained it for another 640 seconds and interestingly each section of Nietzsche", "tokens": [51321, 400, 550, 286, 600, 8895, 309, 337, 1071, 1386, 5254, 3949, 293, 25873, 1184, 3541, 295, 36583, 89, 12287, 51676], "temperature": 0.0, "avg_logprob": -0.3057896440679377, "compression_ratio": 1.7684729064039408, "no_speech_prob": 0.018831320106983185}, {"id": 1469, "seek": 730222, "start": 7302.22, "end": 7306.38, "text": " starts with a numbered section that looks exactly like this.", "tokens": [50364, 3719, 365, 257, 40936, 3541, 300, 1542, 2293, 411, 341, 13, 50572], "temperature": 0.0, "avg_logprob": -0.31124137795489765, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.16665677726268768}, {"id": 1470, "seek": 730222, "start": 7306.38, "end": 7309.820000000001, "text": " It's even starting to learn to close its quotation marks.", "tokens": [50572, 467, 311, 754, 2891, 281, 1466, 281, 1998, 1080, 47312, 10640, 13, 50744], "temperature": 0.0, "avg_logprob": -0.31124137795489765, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.16665677726268768}, {"id": 1471, "seek": 730222, "start": 7309.820000000001, "end": 7314.06, "text": " It also notes that at the start of a chapter it always has 3 lines, so it's learned to", "tokens": [50744, 467, 611, 5570, 300, 412, 264, 722, 295, 257, 7187, 309, 1009, 575, 805, 3876, 11, 370, 309, 311, 3264, 281, 50956], "temperature": 0.0, "avg_logprob": -0.31124137795489765, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.16665677726268768}, {"id": 1472, "seek": 730222, "start": 7314.06, "end": 7317.66, "text": " start chapters.", "tokens": [50956, 722, 20013, 13, 51136], "temperature": 0.0, "avg_logprob": -0.31124137795489765, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.16665677726268768}, {"id": 1473, "seek": 730222, "start": 7317.66, "end": 7323.62, "text": " After another 640 seconds, and another 640 seconds.", "tokens": [51136, 2381, 1071, 1386, 5254, 3949, 11, 293, 1071, 1386, 5254, 3949, 13, 51434], "temperature": 0.0, "avg_logprob": -0.31124137795489765, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.16665677726268768}, {"id": 1474, "seek": 730222, "start": 7323.62, "end": 7327.16, "text": " And so by this time it's actually got to a point where it's saying some things which", "tokens": [51434, 400, 370, 538, 341, 565, 309, 311, 767, 658, 281, 257, 935, 689, 309, 311, 1566, 512, 721, 597, 51611], "temperature": 0.0, "avg_logprob": -0.31124137795489765, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.16665677726268768}, {"id": 1475, "seek": 732716, "start": 7327.16, "end": 7335.32, "text": " are so obscure and difficult to understand it could really be Nietzsche.", "tokens": [50364, 366, 370, 34443, 293, 2252, 281, 1223, 309, 727, 534, 312, 36583, 89, 12287, 13, 50772], "temperature": 0.0, "avg_logprob": -0.32619455698374156, "compression_ratio": 1.546875, "no_speech_prob": 0.14222639799118042}, {"id": 1476, "seek": 732716, "start": 7335.32, "end": 7342.28, "text": " These car RNN models are fun and all, but the reason that this is interesting is that", "tokens": [50772, 1981, 1032, 45702, 45, 5245, 366, 1019, 293, 439, 11, 457, 264, 1778, 300, 341, 307, 1880, 307, 300, 51120], "temperature": 0.0, "avg_logprob": -0.32619455698374156, "compression_ratio": 1.546875, "no_speech_prob": 0.14222639799118042}, {"id": 1477, "seek": 732716, "start": 7342.28, "end": 7349.32, "text": " we're showing that we only provided that amount of text and it was able to generate text out", "tokens": [51120, 321, 434, 4099, 300, 321, 787, 5649, 300, 2372, 295, 2487, 293, 309, 390, 1075, 281, 8460, 2487, 484, 51472], "temperature": 0.0, "avg_logprob": -0.32619455698374156, "compression_ratio": 1.546875, "no_speech_prob": 0.14222639799118042}, {"id": 1478, "seek": 732716, "start": 7349.32, "end": 7353.4, "text": " here because it has state, it has recurrence.", "tokens": [51472, 510, 570, 309, 575, 1785, 11, 309, 575, 18680, 10760, 13, 51676], "temperature": 0.0, "avg_logprob": -0.32619455698374156, "compression_ratio": 1.546875, "no_speech_prob": 0.14222639799118042}, {"id": 1479, "seek": 735340, "start": 7353.48, "end": 7357.799999999999, "text": " What that means is that we could use this kind of model to generate something like SwiftKey,", "tokens": [50368, 708, 300, 1355, 307, 300, 321, 727, 764, 341, 733, 295, 2316, 281, 8460, 746, 411, 25539, 42, 2030, 11, 50584], "temperature": 0.0, "avg_logprob": -0.3502732834013382, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.11593540012836456}, {"id": 1480, "seek": 735340, "start": 7357.799999999999, "end": 7363.839999999999, "text": " whereas you're typing, it's saying this is the next thing you're going to type.", "tokens": [50584, 9735, 291, 434, 18444, 11, 309, 311, 1566, 341, 307, 264, 958, 551, 291, 434, 516, 281, 2010, 13, 50886], "temperature": 0.0, "avg_logprob": -0.3502732834013382, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.11593540012836456}, {"id": 1481, "seek": 735340, "start": 7363.839999999999, "end": 7370.679999999999, "text": " I would love you to think about during the week whether this is likely to help our IMDB", "tokens": [50886, 286, 576, 959, 291, 281, 519, 466, 1830, 264, 1243, 1968, 341, 307, 3700, 281, 854, 527, 21463, 27735, 51228], "temperature": 0.0, "avg_logprob": -0.3502732834013382, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.11593540012836456}, {"id": 1482, "seek": 735340, "start": 7370.679999999999, "end": 7372.32, "text": " sentiment model or not.", "tokens": [51228, 16149, 2316, 420, 406, 13, 51310], "temperature": 0.0, "avg_logprob": -0.3502732834013382, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.11593540012836456}, {"id": 1483, "seek": 735340, "start": 7372.32, "end": 7375.32, "text": " That would be an interesting thing to talk about.", "tokens": [51310, 663, 576, 312, 364, 1880, 551, 281, 751, 466, 13, 51460], "temperature": 0.0, "avg_logprob": -0.3502732834013382, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.11593540012836456}, {"id": 1484, "seek": 735340, "start": 7375.32, "end": 7379.24, "text": " Next week we will look into the details of how RNNs work.", "tokens": [51460, 3087, 1243, 321, 486, 574, 666, 264, 4365, 295, 577, 45702, 45, 82, 589, 13, 51656], "temperature": 0.0, "avg_logprob": -0.3502732834013382, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.11593540012836456}], "language": "en"}