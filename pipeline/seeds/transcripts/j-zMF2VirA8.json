{"text": " follow you up to the to the lesson nine and I can get to number 10 is never so close to you before so amazing. Thank you. Okay. Yes, I saw you on the leaderboard, Serada. You were 10th in the PADI competition. That's very cool. So to catch people up, the most recent news on the PADI competition is I did two more entries. And I don't remember, I think I might have shown you. I can't remember if I showed you one or both, but yeah, so I on some board the. Models that we had and that improved the submission from 9876 to 988. And then the other thing I did was I. You know, since the VIT models are actually definitely better than the rest, I kind of doubled their weights and that got it from 9881 to 988. And let's see, Serada, Serada, you're down to 11th. You're going to have to put in another effort to. Yeah, my friend. Yeah. Anybody else here on this leaderboard somewhere? I'm down at, I don't know, was it 37 last time I checked? 37, that's not bad. What's your username? I think this is not you. Matt. Matt. Matt Rosinski, 45. Oh, yeah, stop taking further. You just you can't stop for a moment with these things or somebody will jump in ahead. I tried to do the 60s. Yeah, 60s is pretty good. I've had problems with paper space, so I couldn't train again. Oh, no. I've been successful. Like still just not being able to log in. Just error. And I subscribe to the paid version still. I'm not sure, maybe the restructuring. An error. Well, feel free to share it on the forum if it's an error that we might be able to help you with. I think it's just a generic when you try to set up a machine and just says error. Oh, paper space area. That's annoying. They're quite receptive if you use their support email. I know I had an issue and they got right back to me. Another thing is, if the error is your fault, I if you put something in pre run dot sh that breaks things, then just fire up a pie torch instance rather than a fast instance, because that doesn't run pre run dot sh. And so then you can fix it. I'll give it a try. And I have to say thank you for writing to set up the competition to help us to get started. What a great day. He also shared in the forum to set up the local for us. Oh, yeah. Yeah. So I think thank you for him to get me back on the Kiko. Yes. Awesome. So now Radix next job will be to become a. Kaggle notebooks, Grandmaster. That's what I'm going to be watching out for. I think he's got what it takes personally. I got a gold medal. You've had a gold on Kaggle for that books. No, it's not. I'm not sure what I have for notebooks. I haven't done that many notebooks ever. I think I have. What's your username on Kaggle? Let's find you. Radik one. Radik one with a number not written. I'm not worried. Yeah, that's me. Two silvers. OK. This one actually is on the way to being a goal. You just it's got so close, you need 50 votes from. Irregulars, I guess, I don't know what counts as irregular. Well, that's how it works, so it's not in the relative terms. No, it's just 50 votes will stop. So, and, you know, I definitely noticed like. It makes a big difference to. Yes, so therefore it makes a big difference to put notebooks in popular competitions, because that's where people are looking. So like this one got 400 votes, right? And I'm not sure it's necessarily my best notebook, but it was part of the patent competition, which. Had a lot of people. Working on it. So that's one trick. Yeah, so things which are not actually attached to any competition, it's much harder to get. So it's for. Yeah, I'm getting pretty close to notebooks, Grandmaster, actually, so it's better that what's your something to do with loving science, I'm guessing what's your it's actually well, yeah, my the the link is slightly different, actually, it's T.A. and. T.A. and L.I.K.E.S. M.A.T.H. Oh, math, not science. OK. Let's take a look. Oh, look at you, 74. Very nice. And you need two more golds. Now, these nine silvers. Well, that's my go about this stuff right now. Let's see. Huh? I'm going to go up, but oh, there we go. That's some. Do it. Channel our enthusiasm to getting to niche into notebooks, Grandmaster. That would be cool. Yeah, so just have to. Get those silver ones over the line, huh? All right, so. I've. Somebody asked about where the. The gist uploading thing is, so let me take that up. Oh, and actually, when I do what I might do here is I'm going to. I'm going to connect to my. Server. Someone asked about the gist uploading, is there a question asked in the forum somewhere? Yeah, yeah, yeah, on the forum, exactly. And we'll see when I connect to this computer, it's busy. Doing stuff. And specifically, this is what it looks like. When you're busy training a model. Using weights and biases, so you can see, I've got three windows here. I think you're rid of the dots. I always. Oh, that just means that I've got another team X session running on a different computer, which has a smaller screen than this one. And it is where some way to get rid of it by disconnecting other sessions. Connect other clients. Prefix D gives you connected clients, whichever you select is disconnected. Select is disconnected. Let's try that. No, that's not right. Oh, they probably been shifty. There we go. Right, this is what I just created, so if I hit this, there we go. So shift D and then select the one to disconnect. Oh, nice. OK, learn something new. Oh, we've got another new face today. Hello, Sophie, I don't think you've joined us before, is that right? I've been here just quietly in the background sometimes. OK, thank you for joining. Whereabouts are you visiting us from? In Brisbane. Oh, good on you. And what do you work with? I stuff where you're just getting started. Not at all. Background in psychology, doing a postdoc and psych and sort of, yeah, kind of move over into data science. OK, cool. Have you done a lot of the statistical side of psychology? Yeah, yeah, quite a bit and quite a bit of coding in R, but I'm pretty new to Python. So OK, great. Big learning curve. Well, you know what? You're you're our target market, right? So if you have any questions along the way, please jump in. Even things that you feel like everybody else must know. I guarantee not everybody else does. So yeah, definitely. These have been really helpful. I'm really grateful they're running. Awesome. Thanks for joining. OK, so training three models in parallel right now. Yeah, so I've got three GPUs in this machine. And so. Yeah, one nice thing with with weights and biases is you. Basically, let me show you. OK, so here's weights and biases. And so you don't use my Mac very much because nothing's locked in. All right. And so you can see it's running this thing called a sweep, right? So there's going to be four hundred and seventy seven runs. I don't know why it says. Create 31 seconds ago because that's so slow. So I'm going to go ahead and run this thing. Create 31 seconds ago, because that's certainly not true. That's currently running. And so it's coming from this git repo. I feel like there's a there's a sweep view because this is a particular run. This is this is a this is a particular run. That's right. I'm terrible with it, to be honest. OK, so let's go to the project. Yes. And then there are in the room and then a project has sweeps. And then. OK, this one here, I can kill because. OK. So basically, you kind of say on the on the Linux side, WNB, you know, sweet create or something like that. And then. Interesting is all grouped under this thing. Oh, OK. All right. So then, yeah, so then basically it runs. Lots of copies of your program feeding at different configurations. And yeah, you can run the client as many times as you like. So I've run it three times at each time. I've set it to a different. Cuda device, you turn your models into Python scripts and to able to do this or exactly. So. So this is fine tuned up. So it's just calling so causes Pazarks. So that's going to just go through and check what batch size, et cetera, et cetera, et cetera, you asked for, right? Sticks them all into. This Pazark thing, and then it calls train passing in those arguments. And so then train. Is going to initialize weights and biases for this particular project. For this particular entity, which is fast, I using the configuration that you requested. And so then you can say, for example, OK, this got some particular data set, some particular batch size and image size, et cetera. And then it creates a learner for some particular model name, some particular pooling type. Fine tunes it. And then at the end, it logs how much GPU memory used, what model it was, how long it took. And you don't have to look much because the fast AI weights and biases integration automatically tracks everything in the learner. So you can see here, there's all this like. Learner architecture, learner loss function, et cetera, et cetera. Out of curiosity, was this process of refactoring into a script painful? So actually, you can probably actually tell I didn't do this. Thomas Capelle did this. If I had done it, I would have used fast core script. Instead of. This stuff, I guess. But no, it wouldn't have been painful. I would have just chucked an MB dev export on the cell that I had in my notebook, and that would have become, yeah, my script. So wouldn't be painful. Hi. I have a question. Wouldn't it be interesting to track power consumption, for example? I mean, for some people, it might be not for me. As to how you would track power consumption, I have no idea. You'd have to have some kind of. Sensor connected to your power supply, I guess. They track a lot of system metrics in the runs. So like if you look on a run, they will track like GPU memory, CPU memory. Yeah, stuff like, yeah, if you click on the. The thing on the left, it looks like a CPU chip, that thing. Yeah, there's a lot of. So maybe there's power in here. I don't see how it can be right. Because like it well, unless the Nvidia power usage. That does. Here you go. GPU power. So Nvidia tells you the GPU power usage, apparently. Although that won't tell you about your CPU, et cetera, power. The thing that's useful about this, I think, is the memory. The graph. Yeah, well, the key thing is the maximum memory use. So we actually track that here in the script. Yeah, we put it into GPU. Oh, good GPU, man. OK. That's a. Yeah, I think. GPU members. Oh, OK. Blah, blah, blah. So Thomas did that as well. I don't know why it's. The power of negative three. What's that about? Stay curious. I have to ask him what that's. That's doing. Thomas works at weight devices, right? Is that right? Correct. Correct. Correct. Yeah. So he I had never used it before. So. Yeah. So probably most people have never heard of this, but fast day I actually has a thing called FastGPU, which is what I've previously used for doing this kind of thing. So in general, when you've got more than one GPU or just even if you got only one GPU and you've got a bunch of things you want to run, it's helpful to have some way to say like, OK, here's the things to run and then set a script off to go and run them all and check the results. So FastGPU was the thing I built to do that. And the way FastGPU works is that you have a whole list of the whole directory of scripts in a folder and it runs each script run at a time and puts them in. And it runs and it puts them into a separate directory, you know, to say this is completed and it tracks the results and you can do it on like as many or few GPUs as you like and it'll just go ahead and run it. And this is fine, but it's very basic. And I kind of been planning to make it a bit more sophisticated. And yeah, weights and biases. It takes it a lot further, you know, by and I kind of want to redo or add something on top of FastGPU so it is fairly compatible with weights and biases, but you could do everything locally. So the key thing. So, the thing it's actually using to for that config file is it goes through the basically the Cartesian product of all the values in this YAML. So it's going to do each of these two data sets planets and beds for this one learning rate 0.08 for every one of these models. For every one of these poolings for okay this is just the one resize method, and for every one of these experiment numbers. So, yeah. So that's a lot of projects I have to do at some point. The, the sweep allows you to run arbitrary programs doesn't have to be a script. So, potentially you could just stay in the notebook and use tiny kernel or sorry, I can be buying thing or whatever it's called. Yeah, it can be. Yeah. Yeah, yeah, it'd be fun to work on this to make the whole thing you know, run with notebooks and stick stuff in a local SQL like database and because like all this stuff, all this web GUI stuff, honestly I don't like it at all. The thing is it actually doesn't matter because I don't have to use it because they provide an API. So before I realized they have a nice API. I kept on like sending Thomas these messages saying how do I do this, how do I do that, why isn't this working when you'd have to like send me these like pages of screenshots like click here click there, turn this off, then you have to redo this three times it's like, oh, I hate this. And then I found that within this like we do have an API, and I was like I looked at the API it is so well documented it's got examples. Yeah, it's, it's really nice. So, I've put all the stuff I'm working on into this git repo. And so here's a tip by the way the the information about if you're in a git repo. Or at a git directory to clone directory the information about your git repo all lives in a file called.git slash config. So you can see here. This is the git repo. So if we now go to GitHub. One cool thing about this runs is it tracks your git commit, like the run you can get back to what code version. Yeah, that is very cool isn't it. Yeah. Yeah, I mean, I do think we could pretty easily create a local only version of this without all the fancy GUI, you know, which would also have benefits and people who want the fancy GUI and run stuff from multiple sites stuff like that would use weights and biases but, you know, you could also do stuff without weights and biases. Anyway, here's our. Yeah, so here's our repo. And this analysis.ipinb is the thing that I showed yesterday. If you want to check it out. I'll put that in the chat. There you go. Oh, by the way, you know, I think something else which would be good is we should start keeping a really good list for every walkthrough of like all the like key resources key like, you know, links key commands examples we wrote and stuff like that. So I think to do that what we should do is we should turn all of the walkthrough top topics into wikis. I don't know if you folks have used wiki topics before, but basically a wiki topic simply means that everybody will end up with an edit button. So if I just click. Okay, this one already is a wiki. Right so everybody should find on walkthrough one that you can click edit. Right. And so one thing we put in an edit for example would be probably like often Daniel has these really nice full walkthrough listings we should have like a link to his reply, which you can get by the way by. Okay. I think you click on this little date here. Yes, and that gives you a link directly to the post, which is handy. What about this one. Okay, make that a wiki. So this is going to be a little bit boring for you guys to watch but as we'll do it while I'm here. If anyone else has any questions or comments while I do that. Yeah, Jeremy, you get the fast GPU is possible to expand to high performance computing to use it on the note. Sorry to do what apply in high performance computing so in the distributed environment. Is it possible to track it as well. No, I mean, it. Yeah, I mean, anything that's running on in in Python on a Linux computer should be fine. I think some HPC things are like, use their own weird job scheduling systems and stuff. But yeah, as long as it's running a normal in video. It doesn't even have to be in video honestly. But yeah, as long as it's running a normal Linux environment, it should be fine. It's pretty generic, pretty general. Okay, so they are now all wikis and so something I did the other day for example was in walkthrough for. I added something saying like oh this is the one where we actually had a bug and you need to add CD at the end, you know, and I tried to create a little list of what was covered. So for example maybe Matt's fantastic timestamps we could copy and pastors list items into here for instance. Some of Radix examples, maybe, or even just a link to it. Yeah, so for this walkthrough we should certainly include this link to the analysis.ipy and B. Anyway, so you can see, yeah, with the API, it was just so easy just to go api.sweep.runs comes in as a dictionary, which we can then chuck a list of dictionaries into a data frame. Okay. I'm rerunning the whole lot by the way because it turns out I made a mistake at some point I thought that Thomas had told me that squish was always better than crop for resizing and he told me I was exactly wrong and it's actually the crops always better than squish resizing so I'm rerunning the whole lot. It is annoying but shouldn't take too long. Do you find that analyzing the sweep results like this was useful in relative to like what you can see in the, the UI, you know you can so much better Hamill yes so much. I was like, I mean they've done a good job with that. With that UI like it's very sophisticated and clever and stuff but I just never got to be friends with it and as soon as I turn it into a data frame I was just like, okay now I can get exactly what I want straight away, it was absolute breath of fresh air, frankly. I really like their parallel coordinates chart. I find it very difficult to reproduce that in like any visualization library, like in a way I don't like the parallel coordinates chart but yeah I mean, there must be parallel coordinates chart for Python out there. Oh there is there's like a plotly one but it's not that nice. Okay, because I don't bother with it. Like hover over it and stuff and see, you know, what is today, did they write their own. I think so. Yeah, that's impressive. And they kind of wrote their own data frame kind of language, their own visualization library, like in a sense, it's like those weights and biases reports and they have their own syntax. Okay. There isn't one in plotly or something. Yeah, there's one in plotly for sure. Plotly things are normally interactive so have you tried that. Do you know if it's. Yeah, it works. It's just, it's not as nice but yeah it works like when you hover over, like, there's a, there's at least a version doesn't. Yeah, that one. Like, it's very fiddly, you might have to draw a box around it. To, to, to highlight it. Oh, yeah, you know. Okay, so you just drag over it. That's not terrible. It's okay it's not the best UI. But, you know, Okay, this is thanks for telling me about this it's cool. You don't think you don't you don't like this that much it's not that useful for you. I haven't managed to. I mean I know other people like it so I don't doubt that it's useful for something it's just apparently not useful for the things I've tried to use it for yet, somehow. Yeah, I mean how do you do you kind of like drag over the end bit to see where they come from or something. Yeah, I mean it might be useful if you want to look at the weights and biases one. So I think it renders one by default for you for the runs. Yeah, yeah, it does. It's easier to like, let's check it out. Operate that yeah. W and B and B. All right. I think it could be in the sweeps thing. Most likely. Okay. Then, yeah, pick a sweep. Maybe that one. Okay, and then, yeah. Okay, so here we go. And then when you just hover over a section. See, I don't see how this is helping me. Well, I guess like saying so. No, no, I mean, so there's not that much variance in the, well I guess like what is the metric. We're trying to optimize doesn't really seem like it's even on this chart. You know what I mean. Oh, you know what you probably have to tell it what your metric is, and we probably didn't. So the far right hand thing is resize method rather than. Yeah. So that's. Is there some way to tell it that we care about. Yeah, there's an edit, there's like a little pencil, let's see. Okay, add the column for add like loss or something. Wait, this is no let's do accuracy and multi. Okay. Okay, now we're talking. We probably want to get rid of pool and resize method since they don't have any variance. All right. There we go now you can like cover over I actually want to do the thing I we go can I do this track area. Yeah, that doesn't mean this is definitely not going to tell me more than the number of experiments is not either. No, that's true because there. This is some other true thing anyway. There's a thing. Yeah, sometimes I learned something sometimes I don't from that visualization you know, not always. Okay. So, I'm going to do the control PD to detach. So, in general, I don't do hyper parameter Bayesian hyper parameter stuff ever. Which actually tells you this is not quite true I've used it once, and I used it specifically for finding a good set of dropouts for a WD LSTM, because there's like five of them. And I told Lucas about how I had like created a random forest that actually tries to, you know, predict how accurate something's going to be and then use that random forest to actually target better sets of hyper parameters. And then, yeah, that's what they ended up using for weights and biases which is really cool. But I kind of like to really use a much more human driven approach from like well what's the hypothesis I'm trying to test how can I test that as fast as possible, like, most hyper parameters are independent of most other hyper parameters. So, you know, like you don't have to do a huge grid search whatever and you can figure out so for example in this case it's like okay well learning rate of point oh oh eight was basically always the best. So let's not try every learning rate for every model for every resize type, etc. That that's just use that learning rate. Same thing for resize method, you know, crop was always better for the few things we tried it on so don't have to try every combination. And also like I feel like I learned a lot more about deep learning. When I, you know, ask like well what do I want to know about this thing or is that thing independent of that other thing or is it, or are they connected or not. Does it, you know, and so in the end I kind of come away feeling like okay well I now know that, you know, every model we tried the optimal learning rates basically the same every model we've tried the optimal resize methods basically the same and like so I'm come away, knowing that I don't have to try all these different things every time. And so now, next time I do another project, I can leverage my knowledge of what I've learned, rather than do yet another huge hyper parameter sweep. I see you are the Bayesian optimization. My brain is the is the thing that's learning. Exactly. And I find like people what big companies that spend all their time during these big, you know, hyper parameter optimizations like I always feel and talking to them that they don't seem to know much about the practice of deep learning, like they don't seem to know like what generally works and what generally doesn't work because they never bother trying to figure out the answers to those questions. But instead they just chuck in a huge hyper parameter optimization thing into, you know, 1000 TPUs. Yeah, it's kind of something I've observed that's really interesting I mean, like, do you, does it, do you feel like these like hyper parameters generalized across different architectures different models. Yeah, totally. Yeah, totally. In fact, yeah, that was a piece of analysis we did gosh I don't know four or five years ago along with a fellowship today folks in the platform today folks were just trying lots of different sets of hyper parameters across this different sets of data sets as possible. And the same sets of hyper parameters were the best or close enough to the best for everything we tried. That's Yeah. Yeah, it is. With different architectures like I can somewhat imagine that no data set maybe is not that super important but you know between transformers and the CNS. I mean I'm not questioning this because I don't have any experience to say that this is not correct I think this is wonderful and it is it is, it's amazing. So yeah, the fact that across 90 different models that we're testing that couldn't be more different. They all had basically the same best learning rate or close enough. You know, the very interesting aspect here is doing the learning rate is something that you dump a lot of time into. Usually when you start working on a project or in a competition, you would be naturally inclined to hey you know I'm using a different architecture. Let me try to find the, you know, experiment with learning rates, but it's nice that you can discuss. Focus on what really matters. Well I should mention, Radek, this is true of computer vision. But not necessarily for tabular. I suspect, like all computer vision problems do look pretty similar. You know, the data for them looks pretty similar. And I suspect it's also true like specifically of object recognition so like, yeah, for. I don't know. I mean these are things like nobody seems to bother testing like which I find a bit crazy but we should do similar tests for segmentation and, you know, bounding boxes and so forth. I feel we're fine. The same thing. You have the learning rate binder. So we suggest maybe some different learning rates are good in different places. Well, the learning rate finder I built before I had done any of this research right. Oh, okay. Like you might have noticed that I hardly ever use it nowadays in the course. I don't even know if we've mentioned it yet in this course, maybe we have the last lesson. I can't remember. Does anybody remember did we done the learning rate finder yet in course 22? Yeah, I think we did. You think we did? Yeah. Can I just add that one of the really, you can sit there and play with parameters all you like and skid your wheels and get nowhere. And it's one of the things I'm really taking away from the course is the fact that you're talking about strategy, and which goes back to Renato Copiates 2002 paper, he had a term called strategy of analysis, and that's something that really stuck with me. So that sort of transcends that idea of just mucking around with parameters. Yep. Exactly. I suppose these magic parameters, these are the defaults in fast AI. Yeah, pretty much, although with learning rate. Oh, that's weird. With learning rate. The defaults a bit lower than the optimal. Just because I didn't want to like push it, you know, I'd rather it always worked pretty well, rather than be pretty much the best, you know. Yeah, yeah, makes sense. Okay, I'm just gonna go and disconnect my other computer because it's connected to port 8888, which is going to mess things up. I'll be back in one tick. Okay. Okay. Actually, now I think about it, I don't quite know why this is connecting on port 8889. But part of this is to learn how to debug problems right so normally the Jupiter server uses port 8888. And I've only got my SSH connected to forward port 8888 so it's currently not working. So the fact that using a different port suggests this already, it's already running somewhere. So to find out where it's running, you can use PS which lists all the processes running on your computer. And generally speaking I find I get used to some standard set of options that I nearly always want and then I forget what they mean. So I have no idea what WAU or X means I just know that there are a set of options that I always use. So that basically lists all your processes, which obviously is a bit too many. So we want to now filter out the ones that contain Jupiter or notebook. So, pipe is how you do that in Linux so that's going to send the output of this into the input of another program. And a program that just prints out a list of matching lines is called grep. So we can grep for Jupiter. Okay, there it is. So I'm kind of wondering where that, how that's running. I wonder if we've got like multiple sessions of TMUX running. No, we don't. So TMUX-LS lists all your TMUX sessions. Oh, I've got a stopped version in the background. Okay, that's why. So I just have to foreground it. There we go. That was a bit weird. Okay, so now that should work. FG foreground? FG. FG to put it in the background. FG to put it in the foreground. And when you control Z somebody, it actually stops it. Right? You can put it in the background and have it keep running by, actually I'll show you. So if I press control Z and type jobs, that's stopped. Right? So if I now try to refresh this window, I'm going to sit there waiting forever and never going to finish. Okay. Because it's background, it's in the, it's stopped in the background. If you type BG, optionally followed by a job number, which would be number one, and it defaults to the last thing that you put in the background, it will start running it in the background. Even after you stop there. Yeah. So it's now running in the background. So if I now type jobs, it's now running. Okay. And it's still attached to this console. So if I open up this, you'll see it's still printing out things, right? But I can also do other things. And I don't do this very much because normally if I want something running at the same time, I would just chuck it in another T-Mux pane. I don't know. It's kind of nice to know this exists. Something else to point out is once I said BG, it added this ampersand after the job. That's because if you run something with an ampersand at the end, it always runs it in the background. So if you want to like fire off six processes to run in parallel, just put an ampersand at the end of each one, and it'll run in the background. So for example, there's a script that runs LS six times. And so if I run it, you can see they're all interspersed with each other, because it ran all six times at the same time. I see. And let's say like you create a process like this in the background without T-Mux, and you want to kill it. You use the PS thing. You could type FG to foreground it, and then press control C. Yeah, something like that would be fine. Or you can kill a single job. So in general, like you probably would want to search for bash job control to learn how to do these things. And as I said, one of the key things to know is that a job number has a percent of the start. So this is actually percent one. Knowing what to Google is definitely. Yes. Knowing what to Google is the key thing. Although often you can just put in a few examples. So you could, I'm guessing, like if I take control C, BG, FG jobs, which are the things we just learned about. There we go. It kind of gets us pretty close. Now we know they're called drop control commands. All right. Now. So when I kind of iterate through notebooks. What I tend to do is like, once I've got something vaguely working, I generally duplicate it, and then I try to get something else vaguely working and once that starts vaguely working, I then rename it to the thing that I want. So then from time to time, then I just clean up the duplicated versions that I didn't end up using and I can tell which they are, because I haven't renamed them yet. And so this is kind of how you can duplicate it. Like, you make it looks like you're making copies of it and yeah so you can just click File, make a copy. Yep. Or in here you can click it and click duplicate. And so, you like, what do you do after you duplicate it you try to open up that I'll open up that duplicate and I'll try something else some different type of parameter and different method or whatever. So in this case, I started out here in Patty, and kind of just experimented. Right. And show batch and LR find and try to get something running. And then, you know, after that I was like, okay, I've got something working, how do I make it better. And so I created Patty small, but literally made a copy and it would have been called Patty copy.ipnb. And I was like, I wonder about different architectures. So I created this like us like okay well basically I want to try different item transforms different batch transforms and different architectures. So create a train which takes those three things. And so it creates a set of image loaders with those item transforms and those batch transforms. Use a fixed seed to get the same validation so each time. Train it with that architecture. And then return the TTA error rate. And so then, this is kind of like your weights and biases, like, this is how you keep your different experiments ideas. Yeah, so. So now you can see I've kind of gone through and tried a few different sets of item and batch transforms for this architecture. And this is like some small architectures so they'll run reasonably quickly so these ran at about six minutes or so. This is very handy right if you go sell all output toggle, you can quickly get an overview of what you're doing. And so from that I kind of got a sense of which things seem to work pretty well for this one and then I replicated that for a different architecture and found those things which you know these are very very different ones transformers based ones confident based, you know find the things which work pretty well consistently across very different architectures. And for those then try those on other ones SwinB2 and Swin. And yeah then find you know, so then let's toggle the results back on. So I'm kind of looking at two things. The first is what's the error rate at the end of training the other is what's the TTA error rate. So my squish worked pretty well for both crop worked pretty well for both. This is all for conv next. So 640 by 480 288 by 224 didn't work so well. I mean it's not terrible but it's definitely worse. And 320 by 240 instead. You know, you talk a little bit about what you're looking for in the TTA versus the I just want to say like I mean the thing I care about is TTA because that's what I'm going to end up using. Yeah, that's the main one but like, let's see. In this case, this one's not really any better or worse than our best conv next, but the TTA is way better. So that that's very encouraging, which is interesting. So this is now for VIT right. Now VIT we can't do the rectangular ones because VIT has a fixed input size it has to say their final transformation has to be 224 by 224. So if you pass an int instead of a tuple, it's going to create square final images. And, you know, on the other hand, this one looks crappy. Right, so definitely want to use squish for VIT. And then this one looks pretty good. You know, so this was using padding. So like for VIT I probably wouldn't use crop. Last time I looked at TTA was not really a thing and other modeling frameworks that is given to you. Is that still the case? As far as I know that's true. Yeah. You know, so there are a lot of people. Well, one group in particular has been copying without credit everything they can from Fast.ai, they might have done it. I won't mention their name. But yeah, so Swin V2, apparently, Tanisha told me is what all the cool kids on Kaggle use nowadays. That's a fixed resolution. And I found that for the larger sizes there was no 224. You had the choice of 192 or 256. 256 it got so slow I couldn't bear it. But interestingly, even going down to 192, Swin's TTA is actually nearly as good as the best VIT. So that's I thought that was pretty encouraging. This one interestingly, like VIT, didn't do nearly as well for the crop. And again, like VIT, it did pretty well on the pad. And then this is Swin V1, which does have a 224. And so here this TTA is okay, but the final results not great. And so to me I'm like, no, it's not fantastic. This one's again, you know, it's interesting, the crop, none of them are going well, except for ConvNext. This one's not great either, right? So Swin V1, little unimpressive. So basically that's what I did next. And then I was like, okay, let's pick the ones that look good. And I made a duplicate of Paddy Small. And I just did a search and replace of small with large. So we've now got ConvNext Large. And the other things I did differently was I got rid of the fixed random seed. So there's no seed equals 42 here. And so that means we're going to have a different trainings at each time. And so these are now not comparable, which is fine. You'll see if one of them's like totally crap, right? But they're not totally comparable. But the point is now once I train each of these, they're training on a different architecture, a different resizing method. And I append to a list. So I start off with a LEPTList and I append the TTA predictions. And so I deleted the cells from the duplicate that weren't very good in Paddy Small. So you'll see there's no crop anymore. Just Squish and Pad for VIT. And for SwinV2. Probably shouldn't have kept both of the SwinV1s actually. They weren't so good. And then what I did in the very last Kaggle entry was I took the two VIT ones because they were the clear best. And I appended them to the list. So they were there twice. So it's just a slightly clunky way of doing a weighted average, if you like. Yes, stack them all together. Take the mean of their predictions. Find the argmax across the mean of their predictions to get the predictions and then submit in the same way as before. So that was basically my process. It's very not particularly thoughtful. It's pretty mechanical, which is what I like about it. In fact, you could probably automate this whole thing. How critical is this model stacking in Kaggle? Just curious how you think about that. I mean, it's like I, I mean, you can kind of, I mean, we should try, right? We should probably submit. In fact, let's, well, we're kind of out of time. How about next time? Let's submit just the VIT, the best VIT, and we'll see how it goes. And that will give us, yeah, that will give us a sense of how much the ensembling matters. We kind of know ahead of time, it's not going to matter hugely. I mean, you specifically said on Kaggle. On Kaggle, it definitely matters because in Kaggle you want to win. But in real life, my small convexed got 97, well rounded up, that's 98%. And my ensemble got 98.8%. Now that's, in terms of error rate, that's nearly halving the error. So I guess that's actually pretty good. Really important question. How do you keep track of what submissions are tied to which notebook? Oh, I just put a description to remind me, but you know, a better approach would actually be to write the notebook name there, which is what I normally do. But in this case, I wasn't taking it particularly seriously, I guess. I was only planning to do these ones and that was it. So it's basically like, okay, do one with a single small model, then do one with an ensemble of small models and then do one with an ensemble of big models. And then it's after I submitted that that I thought, oh, I should probably wait the VITs a bit higher. So I ended up with the fourth one. So it's pretty easy for me, though, I did four significant submissions. So easy to track. Yeah, I think now that I know actually that I'm doing a little bit more, because I actually did want to try one more thing. I think what I'll probably do is I'll go back and I'm going to, you can edit these, I'm going to go and I'll put in the notebook name in each one. And then, and then I wouldn't go back and change those notebooks later, unless there was like, I probably never, I would, I would just duplicate them and make changes in the duplicate and rename them to something sensible. And then of course this all ends up back in GitHub. So I will always see. Yeah, see what's going on. So this is like MLOps Hamel without it's like you have a, you'd like every like quote run is a notebook, like in a sense like the way to buy and kind of keep track. Yeah. Yeah. Exactly. But I mean the only reason I can kind of do this is because I had already done, like, lots of runs of models to find out which ones I can focus on right so I didn't have to try 100 architectures. I mean in a way, it forces you to really look at it closely. Yeah, if you just like have this dashboard. Right. My view is that this approach, you will actually become a better deep learning practitioner. And I also believe almost nobody does this approach and I almost feel like there are very few people I come across who are actually good deep learning practitioners, but not many people seem to know what works and what doesn't. So, yeah. All right. Well, that's it, I think. Thanks for joining again, and yeah. See you all next time. Bye. Thank you. Take care everybody.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.28, "text": " follow you up to the to the lesson nine and I can get to number 10 is never so close to you before so amazing.", "tokens": [1524, 291, 493, 281, 264, 281, 264, 6898, 4949, 293, 286, 393, 483, 281, 1230, 1266, 307, 1128, 370, 1998, 281, 291, 949, 370, 2243, 13], "temperature": 0.0, "avg_logprob": -0.37650572364010026, "compression_ratio": 1.3674698795180722, "no_speech_prob": 0.03723820298910141}, {"id": 1, "seek": 0, "start": 8.28, "end": 9.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.37650572364010026, "compression_ratio": 1.3674698795180722, "no_speech_prob": 0.03723820298910141}, {"id": 2, "seek": 0, "start": 9.0, "end": 18.48, "text": " Okay. Yes, I saw you on the leaderboard, Serada. You were 10th in the PADI competition. That's very cool.", "tokens": [1033, 13, 1079, 11, 286, 1866, 291, 322, 264, 5263, 3787, 11, 4210, 1538, 13, 509, 645, 1266, 392, 294, 264, 17718, 3085, 6211, 13, 663, 311, 588, 1627, 13], "temperature": 0.0, "avg_logprob": -0.37650572364010026, "compression_ratio": 1.3674698795180722, "no_speech_prob": 0.03723820298910141}, {"id": 3, "seek": 1848, "start": 18.48, "end": 36.16, "text": " So to catch people up, the most recent news on the PADI competition is I did two more entries.", "tokens": [407, 281, 3745, 561, 493, 11, 264, 881, 5162, 2583, 322, 264, 17718, 3085, 6211, 307, 286, 630, 732, 544, 23041, 13], "temperature": 0.0, "avg_logprob": -0.13875844365074522, "compression_ratio": 1.2131147540983607, "no_speech_prob": 8.078396058408543e-05}, {"id": 4, "seek": 1848, "start": 36.16, "end": 40.72, "text": " And I don't remember, I think I might have shown you.", "tokens": [400, 286, 500, 380, 1604, 11, 286, 519, 286, 1062, 362, 4898, 291, 13], "temperature": 0.0, "avg_logprob": -0.13875844365074522, "compression_ratio": 1.2131147540983607, "no_speech_prob": 8.078396058408543e-05}, {"id": 5, "seek": 4072, "start": 40.72, "end": 49.4, "text": " I can't remember if I showed you one or both, but yeah, so I on some board the.", "tokens": [286, 393, 380, 1604, 498, 286, 4712, 291, 472, 420, 1293, 11, 457, 1338, 11, 370, 286, 322, 512, 3150, 264, 13], "temperature": 0.0, "avg_logprob": -0.23410962975543478, "compression_ratio": 1.5207373271889402, "no_speech_prob": 8.20477434899658e-05}, {"id": 6, "seek": 4072, "start": 49.4, "end": 56.6, "text": " Models that we had and that improved the submission from 9876 to 988.", "tokens": [6583, 1625, 300, 321, 632, 293, 300, 9689, 264, 23689, 490, 20860, 25026, 281, 20860, 23, 13], "temperature": 0.0, "avg_logprob": -0.23410962975543478, "compression_ratio": 1.5207373271889402, "no_speech_prob": 8.20477434899658e-05}, {"id": 7, "seek": 4072, "start": 56.6, "end": 60.0, "text": " And then the other thing I did was I.", "tokens": [400, 550, 264, 661, 551, 286, 630, 390, 286, 13], "temperature": 0.0, "avg_logprob": -0.23410962975543478, "compression_ratio": 1.5207373271889402, "no_speech_prob": 8.20477434899658e-05}, {"id": 8, "seek": 4072, "start": 60.0, "end": 70.64, "text": " You know, since the VIT models are actually definitely better than the rest, I kind of doubled their weights and that got it from 9881 to 988.", "tokens": [509, 458, 11, 1670, 264, 691, 3927, 5245, 366, 767, 2138, 1101, 813, 264, 1472, 11, 286, 733, 295, 24405, 641, 17443, 293, 300, 658, 309, 490, 20860, 32875, 281, 1722, 16919, 13], "temperature": 0.0, "avg_logprob": -0.23410962975543478, "compression_ratio": 1.5207373271889402, "no_speech_prob": 8.20477434899658e-05}, {"id": 9, "seek": 7064, "start": 70.64, "end": 81.24, "text": " And let's see, Serada, Serada, you're down to 11th. You're going to have to put in another effort to.", "tokens": [400, 718, 311, 536, 11, 4210, 1538, 11, 4210, 1538, 11, 291, 434, 760, 281, 2975, 392, 13, 509, 434, 516, 281, 362, 281, 829, 294, 1071, 4630, 281, 13], "temperature": 0.0, "avg_logprob": -0.26597693159773544, "compression_ratio": 1.3832335329341316, "no_speech_prob": 8.203140168916434e-05}, {"id": 10, "seek": 7064, "start": 81.24, "end": 86.16, "text": " Yeah, my friend. Yeah.", "tokens": [865, 11, 452, 1277, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.26597693159773544, "compression_ratio": 1.3832335329341316, "no_speech_prob": 8.203140168916434e-05}, {"id": 11, "seek": 7064, "start": 86.16, "end": 91.4, "text": " Anybody else here on this leaderboard somewhere?", "tokens": [19082, 1646, 510, 322, 341, 5263, 3787, 4079, 30], "temperature": 0.0, "avg_logprob": -0.26597693159773544, "compression_ratio": 1.3832335329341316, "no_speech_prob": 8.203140168916434e-05}, {"id": 12, "seek": 7064, "start": 91.4, "end": 95.56, "text": " I'm down at, I don't know, was it 37 last time I checked?", "tokens": [286, 478, 760, 412, 11, 286, 500, 380, 458, 11, 390, 309, 13435, 1036, 565, 286, 10033, 30], "temperature": 0.0, "avg_logprob": -0.26597693159773544, "compression_ratio": 1.3832335329341316, "no_speech_prob": 8.203140168916434e-05}, {"id": 13, "seek": 9556, "start": 95.56, "end": 100.92, "text": " 37, that's not bad. What's your username? I think this is not you.", "tokens": [13435, 11, 300, 311, 406, 1578, 13, 708, 311, 428, 30351, 30, 286, 519, 341, 307, 406, 291, 13], "temperature": 0.0, "avg_logprob": -0.4352604184831892, "compression_ratio": 1.360759493670886, "no_speech_prob": 7.963442476466298e-05}, {"id": 14, "seek": 9556, "start": 100.92, "end": 104.84, "text": " Matt.", "tokens": [7397, 13], "temperature": 0.0, "avg_logprob": -0.4352604184831892, "compression_ratio": 1.360759493670886, "no_speech_prob": 7.963442476466298e-05}, {"id": 15, "seek": 9556, "start": 104.84, "end": 111.32000000000001, "text": " Matt.", "tokens": [7397, 13], "temperature": 0.0, "avg_logprob": -0.4352604184831892, "compression_ratio": 1.360759493670886, "no_speech_prob": 7.963442476466298e-05}, {"id": 16, "seek": 9556, "start": 111.32000000000001, "end": 113.80000000000001, "text": " Matt Rosinski, 45.", "tokens": [7397, 11144, 38984, 11, 6905, 13], "temperature": 0.0, "avg_logprob": -0.4352604184831892, "compression_ratio": 1.360759493670886, "no_speech_prob": 7.963442476466298e-05}, {"id": 17, "seek": 9556, "start": 113.80000000000001, "end": 116.2, "text": " Oh, yeah, stop taking further.", "tokens": [876, 11, 1338, 11, 1590, 1940, 3052, 13], "temperature": 0.0, "avg_logprob": -0.4352604184831892, "compression_ratio": 1.360759493670886, "no_speech_prob": 7.963442476466298e-05}, {"id": 18, "seek": 9556, "start": 116.2, "end": 123.08, "text": " You just you can't stop for a moment with these things or somebody will jump in ahead.", "tokens": [509, 445, 291, 393, 380, 1590, 337, 257, 1623, 365, 613, 721, 420, 2618, 486, 3012, 294, 2286, 13], "temperature": 0.0, "avg_logprob": -0.4352604184831892, "compression_ratio": 1.360759493670886, "no_speech_prob": 7.963442476466298e-05}, {"id": 19, "seek": 12308, "start": 123.08, "end": 125.92, "text": " I tried to do the 60s.", "tokens": [286, 3031, 281, 360, 264, 4060, 82, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 20, "seek": 12308, "start": 125.92, "end": 129.32, "text": " Yeah, 60s is pretty good.", "tokens": [865, 11, 4060, 82, 307, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 21, "seek": 12308, "start": 129.32, "end": 133.68, "text": " I've had problems with paper space, so I couldn't train again.", "tokens": [286, 600, 632, 2740, 365, 3035, 1901, 11, 370, 286, 2809, 380, 3847, 797, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 22, "seek": 12308, "start": 133.68, "end": 136.16, "text": " Oh, no.", "tokens": [876, 11, 572, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 23, "seek": 12308, "start": 136.16, "end": 137.84, "text": " I've been successful.", "tokens": [286, 600, 668, 4406, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 24, "seek": 12308, "start": 137.84, "end": 140.8, "text": " Like still just not being able to log in.", "tokens": [1743, 920, 445, 406, 885, 1075, 281, 3565, 294, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 25, "seek": 12308, "start": 140.8, "end": 146.48, "text": " Just error. And I subscribe to the paid version still.", "tokens": [1449, 6713, 13, 400, 286, 3022, 281, 264, 4835, 3037, 920, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 26, "seek": 12308, "start": 146.48, "end": 149.68, "text": " I'm not sure, maybe the restructuring.", "tokens": [286, 478, 406, 988, 11, 1310, 264, 1472, 1757, 1345, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 27, "seek": 12308, "start": 149.68, "end": 152.72, "text": " An error.", "tokens": [1107, 6713, 13], "temperature": 0.0, "avg_logprob": -0.3405022621154785, "compression_ratio": 1.4568527918781726, "no_speech_prob": 2.5459932658122852e-05}, {"id": 28, "seek": 15272, "start": 152.72, "end": 157.56, "text": " Well, feel free to share it on the forum if it's an error that we might be able to help you with.", "tokens": [1042, 11, 841, 1737, 281, 2073, 309, 322, 264, 17542, 498, 309, 311, 364, 6713, 300, 321, 1062, 312, 1075, 281, 854, 291, 365, 13], "temperature": 0.0, "avg_logprob": -0.21303974313938873, "compression_ratio": 1.5424528301886793, "no_speech_prob": 8.611330849817023e-05}, {"id": 29, "seek": 15272, "start": 157.56, "end": 162.44, "text": " I think it's just a generic when you try to set up a machine and just says error.", "tokens": [286, 519, 309, 311, 445, 257, 19577, 562, 291, 853, 281, 992, 493, 257, 3479, 293, 445, 1619, 6713, 13], "temperature": 0.0, "avg_logprob": -0.21303974313938873, "compression_ratio": 1.5424528301886793, "no_speech_prob": 8.611330849817023e-05}, {"id": 30, "seek": 15272, "start": 162.44, "end": 165.4, "text": " Oh, paper space area.", "tokens": [876, 11, 3035, 1901, 1859, 13], "temperature": 0.0, "avg_logprob": -0.21303974313938873, "compression_ratio": 1.5424528301886793, "no_speech_prob": 8.611330849817023e-05}, {"id": 31, "seek": 15272, "start": 165.4, "end": 166.68, "text": " That's annoying.", "tokens": [663, 311, 11304, 13], "temperature": 0.0, "avg_logprob": -0.21303974313938873, "compression_ratio": 1.5424528301886793, "no_speech_prob": 8.611330849817023e-05}, {"id": 32, "seek": 15272, "start": 166.68, "end": 172.4, "text": " They're quite receptive if you use their support email.", "tokens": [814, 434, 1596, 45838, 498, 291, 764, 641, 1406, 3796, 13], "temperature": 0.0, "avg_logprob": -0.21303974313938873, "compression_ratio": 1.5424528301886793, "no_speech_prob": 8.611330849817023e-05}, {"id": 33, "seek": 15272, "start": 172.4, "end": 177.68, "text": " I know I had an issue and they got right back to me.", "tokens": [286, 458, 286, 632, 364, 2734, 293, 436, 658, 558, 646, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.21303974313938873, "compression_ratio": 1.5424528301886793, "no_speech_prob": 8.611330849817023e-05}, {"id": 34, "seek": 17768, "start": 177.68, "end": 187.12, "text": " Another thing is, if the error is your fault, I if you put something in pre run dot sh that breaks things,", "tokens": [3996, 551, 307, 11, 498, 264, 6713, 307, 428, 7441, 11, 286, 498, 291, 829, 746, 294, 659, 1190, 5893, 402, 300, 9857, 721, 11], "temperature": 0.0, "avg_logprob": -0.2788831092215873, "compression_ratio": 1.5748502994011977, "no_speech_prob": 2.177376882173121e-05}, {"id": 35, "seek": 17768, "start": 187.12, "end": 196.12, "text": " then just fire up a pie torch instance rather than a fast instance, because that doesn't run pre run dot sh.", "tokens": [550, 445, 2610, 493, 257, 1730, 27822, 5197, 2831, 813, 257, 2370, 5197, 11, 570, 300, 1177, 380, 1190, 659, 1190, 5893, 402, 13], "temperature": 0.0, "avg_logprob": -0.2788831092215873, "compression_ratio": 1.5748502994011977, "no_speech_prob": 2.177376882173121e-05}, {"id": 36, "seek": 17768, "start": 196.12, "end": 197.92000000000002, "text": " And so then you can fix it.", "tokens": [400, 370, 550, 291, 393, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.2788831092215873, "compression_ratio": 1.5748502994011977, "no_speech_prob": 2.177376882173121e-05}, {"id": 37, "seek": 17768, "start": 197.92000000000002, "end": 205.68, "text": " I'll give it a try.", "tokens": [286, 603, 976, 309, 257, 853, 13], "temperature": 0.0, "avg_logprob": -0.2788831092215873, "compression_ratio": 1.5748502994011977, "no_speech_prob": 2.177376882173121e-05}, {"id": 38, "seek": 20568, "start": 205.68, "end": 216.56, "text": " And I have to say thank you for writing to set up the competition to help us to get started.", "tokens": [400, 286, 362, 281, 584, 1309, 291, 337, 3579, 281, 992, 493, 264, 6211, 281, 854, 505, 281, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.28049489614125844, "compression_ratio": 1.49375, "no_speech_prob": 0.00011575806274777278}, {"id": 39, "seek": 20568, "start": 216.56, "end": 219.24, "text": " What a great day.", "tokens": [708, 257, 869, 786, 13], "temperature": 0.0, "avg_logprob": -0.28049489614125844, "compression_ratio": 1.49375, "no_speech_prob": 0.00011575806274777278}, {"id": 40, "seek": 20568, "start": 219.24, "end": 226.4, "text": " He also shared in the forum to set up the local for us.", "tokens": [634, 611, 5507, 294, 264, 17542, 281, 992, 493, 264, 2654, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.28049489614125844, "compression_ratio": 1.49375, "no_speech_prob": 0.00011575806274777278}, {"id": 41, "seek": 20568, "start": 226.4, "end": 228.0, "text": " Oh, yeah. Yeah.", "tokens": [876, 11, 1338, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.28049489614125844, "compression_ratio": 1.49375, "no_speech_prob": 0.00011575806274777278}, {"id": 42, "seek": 20568, "start": 228.0, "end": 233.88, "text": " So I think thank you for him to get me back on the Kiko.", "tokens": [407, 286, 519, 1309, 291, 337, 796, 281, 483, 385, 646, 322, 264, 591, 10770, 13], "temperature": 0.0, "avg_logprob": -0.28049489614125844, "compression_ratio": 1.49375, "no_speech_prob": 0.00011575806274777278}, {"id": 43, "seek": 23388, "start": 233.88, "end": 236.56, "text": " Yes. Awesome.", "tokens": [1079, 13, 10391, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 44, "seek": 23388, "start": 236.56, "end": 241.04, "text": " So now Radix next job will be to become a.", "tokens": [407, 586, 9654, 970, 958, 1691, 486, 312, 281, 1813, 257, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 45, "seek": 23388, "start": 241.04, "end": 243.28, "text": " Kaggle notebooks, Grandmaster.", "tokens": [48751, 22631, 43782, 11, 6757, 21640, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 46, "seek": 23388, "start": 243.28, "end": 244.84, "text": " That's what I'm going to be watching out for.", "tokens": [663, 311, 437, 286, 478, 516, 281, 312, 1976, 484, 337, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 47, "seek": 23388, "start": 244.84, "end": 251.76, "text": " I think he's got what it takes personally.", "tokens": [286, 519, 415, 311, 658, 437, 309, 2516, 5665, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 48, "seek": 23388, "start": 251.76, "end": 255.88, "text": " I got a gold medal.", "tokens": [286, 658, 257, 3821, 21364, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 49, "seek": 23388, "start": 255.88, "end": 259.04, "text": " You've had a gold on Kaggle for that books.", "tokens": [509, 600, 632, 257, 3821, 322, 48751, 22631, 337, 300, 3642, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 50, "seek": 23388, "start": 259.04, "end": 260.68, "text": " No, it's not.", "tokens": [883, 11, 309, 311, 406, 13], "temperature": 0.0, "avg_logprob": -0.2970452582699129, "compression_ratio": 1.4189944134078212, "no_speech_prob": 3.8205409509828314e-05}, {"id": 51, "seek": 26068, "start": 260.68, "end": 266.52, "text": " I'm not sure what I have for notebooks. I haven't done that many notebooks ever.", "tokens": [286, 478, 406, 988, 437, 286, 362, 337, 43782, 13, 286, 2378, 380, 1096, 300, 867, 43782, 1562, 13], "temperature": 0.0, "avg_logprob": -0.29844506916246916, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.468634870136157e-05}, {"id": 52, "seek": 26068, "start": 266.52, "end": 268.92, "text": " I think I have. What's your username on Kaggle?", "tokens": [286, 519, 286, 362, 13, 708, 311, 428, 30351, 322, 48751, 22631, 30], "temperature": 0.0, "avg_logprob": -0.29844506916246916, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.468634870136157e-05}, {"id": 53, "seek": 26068, "start": 268.92, "end": 271.28000000000003, "text": " Let's find you. Radik one.", "tokens": [961, 311, 915, 291, 13, 9654, 1035, 472, 13], "temperature": 0.0, "avg_logprob": -0.29844506916246916, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.468634870136157e-05}, {"id": 54, "seek": 26068, "start": 271.28000000000003, "end": 276.84000000000003, "text": " Radik one with a number not written.", "tokens": [9654, 1035, 472, 365, 257, 1230, 406, 3720, 13], "temperature": 0.0, "avg_logprob": -0.29844506916246916, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.468634870136157e-05}, {"id": 55, "seek": 26068, "start": 276.84000000000003, "end": 280.16, "text": " I'm not worried. Yeah, that's me.", "tokens": [286, 478, 406, 5804, 13, 865, 11, 300, 311, 385, 13], "temperature": 0.0, "avg_logprob": -0.29844506916246916, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.468634870136157e-05}, {"id": 56, "seek": 26068, "start": 280.16, "end": 284.04, "text": " Two silvers. OK.", "tokens": [4453, 3425, 840, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.29844506916246916, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.468634870136157e-05}, {"id": 57, "seek": 26068, "start": 284.04, "end": 286.84000000000003, "text": " This one actually is on the way to being a goal.", "tokens": [639, 472, 767, 307, 322, 264, 636, 281, 885, 257, 3387, 13], "temperature": 0.0, "avg_logprob": -0.29844506916246916, "compression_ratio": 1.5051546391752577, "no_speech_prob": 4.468634870136157e-05}, {"id": 58, "seek": 28684, "start": 286.84, "end": 292.56, "text": " You just it's got so close, you need 50 votes from.", "tokens": [509, 445, 309, 311, 658, 370, 1998, 11, 291, 643, 2625, 12068, 490, 13], "temperature": 0.0, "avg_logprob": -0.24691619402096596, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.00013306495384313166}, {"id": 59, "seek": 28684, "start": 292.56, "end": 300.0, "text": " Irregulars, I guess, I don't know what counts as irregular.", "tokens": [9151, 3375, 42891, 11, 286, 2041, 11, 286, 500, 380, 458, 437, 14893, 382, 29349, 13], "temperature": 0.0, "avg_logprob": -0.24691619402096596, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.00013306495384313166}, {"id": 60, "seek": 28684, "start": 300.0, "end": 304.35999999999996, "text": " Well, that's how it works, so it's not in the relative terms.", "tokens": [1042, 11, 300, 311, 577, 309, 1985, 11, 370, 309, 311, 406, 294, 264, 4972, 2115, 13], "temperature": 0.0, "avg_logprob": -0.24691619402096596, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.00013306495384313166}, {"id": 61, "seek": 28684, "start": 304.35999999999996, "end": 306.35999999999996, "text": " No, it's just 50 votes will stop.", "tokens": [883, 11, 309, 311, 445, 2625, 12068, 486, 1590, 13], "temperature": 0.0, "avg_logprob": -0.24691619402096596, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.00013306495384313166}, {"id": 62, "seek": 28684, "start": 306.35999999999996, "end": 314.52, "text": " So, and, you know, I definitely noticed like.", "tokens": [407, 11, 293, 11, 291, 458, 11, 286, 2138, 5694, 411, 13], "temperature": 0.0, "avg_logprob": -0.24691619402096596, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.00013306495384313166}, {"id": 63, "seek": 31452, "start": 314.52, "end": 319.2, "text": " It makes a big difference to.", "tokens": [467, 1669, 257, 955, 2649, 281, 13], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 64, "seek": 31452, "start": 319.2, "end": 323.71999999999997, "text": " Yes, so therefore it makes a big difference to put notebooks in popular competitions,", "tokens": [1079, 11, 370, 4412, 309, 1669, 257, 955, 2649, 281, 829, 43782, 294, 3743, 26185, 11], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 65, "seek": 31452, "start": 323.71999999999997, "end": 325.03999999999996, "text": " because that's where people are looking.", "tokens": [570, 300, 311, 689, 561, 366, 1237, 13], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 66, "seek": 31452, "start": 325.03999999999996, "end": 329.2, "text": " So like this one got 400 votes, right?", "tokens": [407, 411, 341, 472, 658, 8423, 12068, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 67, "seek": 31452, "start": 329.2, "end": 332.08, "text": " And I'm not sure it's necessarily my best notebook,", "tokens": [400, 286, 478, 406, 988, 309, 311, 4725, 452, 1151, 21060, 11], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 68, "seek": 31452, "start": 332.08, "end": 338.59999999999997, "text": " but it was part of the patent competition, which.", "tokens": [457, 309, 390, 644, 295, 264, 20495, 6211, 11, 597, 13], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 69, "seek": 31452, "start": 338.59999999999997, "end": 341.76, "text": " Had a lot of people. Working on it.", "tokens": [12298, 257, 688, 295, 561, 13, 18337, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 70, "seek": 31452, "start": 341.76, "end": 343.79999999999995, "text": " So that's one trick.", "tokens": [407, 300, 311, 472, 4282, 13], "temperature": 0.0, "avg_logprob": -0.13913731672325913, "compression_ratio": 1.6090909090909091, "no_speech_prob": 1.2213877198519185e-05}, {"id": 71, "seek": 34380, "start": 343.8, "end": 349.44, "text": " Yeah, so things which are not actually attached to any competition, it's much harder to get.", "tokens": [865, 11, 370, 721, 597, 366, 406, 767, 8570, 281, 604, 6211, 11, 309, 311, 709, 6081, 281, 483, 13], "temperature": 0.0, "avg_logprob": -0.28103791450967597, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.7528039936441928e-05}, {"id": 72, "seek": 34380, "start": 349.44, "end": 354.68, "text": " So it's for. Yeah, I'm getting pretty close to notebooks,", "tokens": [407, 309, 311, 337, 13, 865, 11, 286, 478, 1242, 1238, 1998, 281, 43782, 11], "temperature": 0.0, "avg_logprob": -0.28103791450967597, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.7528039936441928e-05}, {"id": 73, "seek": 34380, "start": 354.68, "end": 360.04, "text": " Grandmaster, actually, so it's better that what's your something to do with loving science,", "tokens": [6757, 21640, 11, 767, 11, 370, 309, 311, 1101, 300, 437, 311, 428, 746, 281, 360, 365, 9344, 3497, 11], "temperature": 0.0, "avg_logprob": -0.28103791450967597, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.7528039936441928e-05}, {"id": 74, "seek": 34380, "start": 360.04, "end": 365.12, "text": " I'm guessing what's your it's actually well, yeah, my the the link is slightly different,", "tokens": [286, 478, 17939, 437, 311, 428, 309, 311, 767, 731, 11, 1338, 11, 452, 264, 264, 2113, 307, 4748, 819, 11], "temperature": 0.0, "avg_logprob": -0.28103791450967597, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.7528039936441928e-05}, {"id": 75, "seek": 34380, "start": 365.12, "end": 368.8, "text": " actually, it's T.A. and.", "tokens": [767, 11, 309, 311, 314, 13, 32, 13, 293, 13], "temperature": 0.0, "avg_logprob": -0.28103791450967597, "compression_ratio": 1.63013698630137, "no_speech_prob": 2.7528039936441928e-05}, {"id": 76, "seek": 36880, "start": 368.8, "end": 375.96000000000004, "text": " T.A. and L.I.K.E.S.", "tokens": [314, 13, 32, 13, 293, 441, 13, 40, 13, 42, 13, 36, 13, 50, 13], "temperature": 0.0, "avg_logprob": -0.32375970178720903, "compression_ratio": 1.304093567251462, "no_speech_prob": 5.0619175453903154e-05}, {"id": 77, "seek": 36880, "start": 375.96000000000004, "end": 382.88, "text": " M.A.T.H. Oh, math, not science. OK.", "tokens": [376, 13, 32, 13, 51, 13, 39, 13, 876, 11, 5221, 11, 406, 3497, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.32375970178720903, "compression_ratio": 1.304093567251462, "no_speech_prob": 5.0619175453903154e-05}, {"id": 78, "seek": 36880, "start": 382.88, "end": 386.0, "text": " Let's take a look. Oh, look at you, 74.", "tokens": [961, 311, 747, 257, 574, 13, 876, 11, 574, 412, 291, 11, 28868, 13], "temperature": 0.0, "avg_logprob": -0.32375970178720903, "compression_ratio": 1.304093567251462, "no_speech_prob": 5.0619175453903154e-05}, {"id": 79, "seek": 36880, "start": 386.0, "end": 388.08000000000004, "text": " Very nice. And you need two more golds.", "tokens": [4372, 1481, 13, 400, 291, 643, 732, 544, 3821, 82, 13], "temperature": 0.0, "avg_logprob": -0.32375970178720903, "compression_ratio": 1.304093567251462, "no_speech_prob": 5.0619175453903154e-05}, {"id": 80, "seek": 36880, "start": 388.08000000000004, "end": 390.04, "text": " Now, these nine silvers.", "tokens": [823, 11, 613, 4949, 3425, 840, 13], "temperature": 0.0, "avg_logprob": -0.32375970178720903, "compression_ratio": 1.304093567251462, "no_speech_prob": 5.0619175453903154e-05}, {"id": 81, "seek": 36880, "start": 390.04, "end": 392.8, "text": " Well, that's my go about this stuff right now.", "tokens": [1042, 11, 300, 311, 452, 352, 466, 341, 1507, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.32375970178720903, "compression_ratio": 1.304093567251462, "no_speech_prob": 5.0619175453903154e-05}, {"id": 82, "seek": 36880, "start": 392.8, "end": 396.24, "text": " Let's see. Huh?", "tokens": [961, 311, 536, 13, 8063, 30], "temperature": 0.0, "avg_logprob": -0.32375970178720903, "compression_ratio": 1.304093567251462, "no_speech_prob": 5.0619175453903154e-05}, {"id": 83, "seek": 39624, "start": 396.24, "end": 399.08, "text": " I'm going to go up, but oh, there we go.", "tokens": [286, 478, 516, 281, 352, 493, 11, 457, 1954, 11, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.40855568724793273, "compression_ratio": 1.380952380952381, "no_speech_prob": 1.280161268368829e-05}, {"id": 84, "seek": 39624, "start": 399.08, "end": 401.28000000000003, "text": " That's some.", "tokens": [663, 311, 512, 13], "temperature": 0.0, "avg_logprob": -0.40855568724793273, "compression_ratio": 1.380952380952381, "no_speech_prob": 1.280161268368829e-05}, {"id": 85, "seek": 39624, "start": 401.28000000000003, "end": 408.2, "text": " Do it. Channel our enthusiasm to getting to niche into notebooks, Grandmaster.", "tokens": [1144, 309, 13, 13553, 527, 23417, 281, 1242, 281, 19956, 666, 43782, 11, 6757, 21640, 13], "temperature": 0.0, "avg_logprob": -0.40855568724793273, "compression_ratio": 1.380952380952381, "no_speech_prob": 1.280161268368829e-05}, {"id": 86, "seek": 39624, "start": 408.2, "end": 410.2, "text": " That would be cool.", "tokens": [663, 576, 312, 1627, 13], "temperature": 0.0, "avg_logprob": -0.40855568724793273, "compression_ratio": 1.380952380952381, "no_speech_prob": 1.280161268368829e-05}, {"id": 87, "seek": 39624, "start": 413.64, "end": 417.16, "text": " Yeah, so just have to.", "tokens": [865, 11, 370, 445, 362, 281, 13], "temperature": 0.0, "avg_logprob": -0.40855568724793273, "compression_ratio": 1.380952380952381, "no_speech_prob": 1.280161268368829e-05}, {"id": 88, "seek": 39624, "start": 417.16, "end": 420.6, "text": " Get those silver ones over the line, huh?", "tokens": [3240, 729, 8753, 2306, 670, 264, 1622, 11, 7020, 30], "temperature": 0.0, "avg_logprob": -0.40855568724793273, "compression_ratio": 1.380952380952381, "no_speech_prob": 1.280161268368829e-05}, {"id": 89, "seek": 39624, "start": 420.6, "end": 422.72, "text": " All right, so.", "tokens": [1057, 558, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.40855568724793273, "compression_ratio": 1.380952380952381, "no_speech_prob": 1.280161268368829e-05}, {"id": 90, "seek": 42272, "start": 422.72, "end": 425.72, "text": " I've.", "tokens": [286, 600, 13], "temperature": 0.0, "avg_logprob": -0.5342069139667586, "compression_ratio": 1.2682926829268293, "no_speech_prob": 2.077914177789353e-05}, {"id": 91, "seek": 42272, "start": 431.36, "end": 436.24, "text": " Somebody asked about where the.", "tokens": [13463, 2351, 466, 689, 264, 13], "temperature": 0.0, "avg_logprob": -0.5342069139667586, "compression_ratio": 1.2682926829268293, "no_speech_prob": 2.077914177789353e-05}, {"id": 92, "seek": 42272, "start": 438.24, "end": 444.32000000000005, "text": " The gist uploading thing is, so let me take that up.", "tokens": [440, 290, 468, 27301, 551, 307, 11, 370, 718, 385, 747, 300, 493, 13], "temperature": 0.0, "avg_logprob": -0.5342069139667586, "compression_ratio": 1.2682926829268293, "no_speech_prob": 2.077914177789353e-05}, {"id": 93, "seek": 44432, "start": 444.32, "end": 451.84, "text": " Oh, and actually, when I do what I might do here is I'm going to.", "tokens": [876, 11, 293, 767, 11, 562, 286, 360, 437, 286, 1062, 360, 510, 307, 286, 478, 516, 281, 13], "temperature": 0.0, "avg_logprob": -0.43230025409019157, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.6694357327651232e-05}, {"id": 94, "seek": 44432, "start": 454.04, "end": 456.24, "text": " I'm going to connect to my.", "tokens": [286, 478, 516, 281, 1745, 281, 452, 13], "temperature": 0.0, "avg_logprob": -0.43230025409019157, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.6694357327651232e-05}, {"id": 95, "seek": 44432, "start": 456.6, "end": 458.15999999999997, "text": " Server.", "tokens": [25684, 13], "temperature": 0.0, "avg_logprob": -0.43230025409019157, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.6694357327651232e-05}, {"id": 96, "seek": 44432, "start": 461.12, "end": 463.64, "text": " Someone asked about the gist uploading,", "tokens": [8734, 2351, 466, 264, 290, 468, 27301, 11], "temperature": 0.0, "avg_logprob": -0.43230025409019157, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.6694357327651232e-05}, {"id": 97, "seek": 44432, "start": 463.64, "end": 466.08, "text": " is there a question asked in the forum somewhere?", "tokens": [307, 456, 257, 1168, 2351, 294, 264, 17542, 4079, 30], "temperature": 0.0, "avg_logprob": -0.43230025409019157, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.6694357327651232e-05}, {"id": 98, "seek": 44432, "start": 466.08, "end": 470.08, "text": " Yeah, yeah, yeah, on the forum, exactly.", "tokens": [865, 11, 1338, 11, 1338, 11, 322, 264, 17542, 11, 2293, 13], "temperature": 0.0, "avg_logprob": -0.43230025409019157, "compression_ratio": 1.4967741935483871, "no_speech_prob": 1.6694357327651232e-05}, {"id": 99, "seek": 47008, "start": 470.08, "end": 474.76, "text": " And we'll see when I connect to this computer, it's busy.", "tokens": [400, 321, 603, 536, 562, 286, 1745, 281, 341, 3820, 11, 309, 311, 5856, 13], "temperature": 0.0, "avg_logprob": -0.501815361312673, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.3416150977718644e-05}, {"id": 100, "seek": 47008, "start": 475.96, "end": 477.96, "text": " Doing stuff.", "tokens": [18496, 1507, 13], "temperature": 0.0, "avg_logprob": -0.501815361312673, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.3416150977718644e-05}, {"id": 101, "seek": 47008, "start": 479.96, "end": 483.59999999999997, "text": " And specifically, this is what it looks like.", "tokens": [400, 4682, 11, 341, 307, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.501815361312673, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.3416150977718644e-05}, {"id": 102, "seek": 47008, "start": 484.59999999999997, "end": 487.36, "text": " When you're busy training a model.", "tokens": [1133, 291, 434, 5856, 3097, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.501815361312673, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.3416150977718644e-05}, {"id": 103, "seek": 47008, "start": 488.36, "end": 492.56, "text": " Using weights and biases, so you can see, I've got three windows here.", "tokens": [11142, 17443, 293, 32152, 11, 370, 291, 393, 536, 11, 286, 600, 658, 1045, 9309, 510, 13], "temperature": 0.0, "avg_logprob": -0.501815361312673, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.3416150977718644e-05}, {"id": 104, "seek": 47008, "start": 494.28, "end": 496.36, "text": " I think you're rid of the dots. I always.", "tokens": [286, 519, 291, 434, 3973, 295, 264, 15026, 13, 286, 1009, 13], "temperature": 0.0, "avg_logprob": -0.501815361312673, "compression_ratio": 1.411764705882353, "no_speech_prob": 1.3416150977718644e-05}, {"id": 105, "seek": 49636, "start": 496.36, "end": 501.64, "text": " Oh, that just means that I've got another team X session running on a different", "tokens": [876, 11, 300, 445, 1355, 300, 286, 600, 658, 1071, 1469, 1783, 5481, 2614, 322, 257, 819], "temperature": 0.0, "avg_logprob": -0.4165547772457725, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.669406447035726e-05}, {"id": 106, "seek": 49636, "start": 501.64, "end": 504.64, "text": " computer, which has a smaller screen than this one.", "tokens": [3820, 11, 597, 575, 257, 4356, 2568, 813, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.4165547772457725, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.669406447035726e-05}, {"id": 107, "seek": 49636, "start": 505.64, "end": 511.28000000000003, "text": " And it is where some way to get rid of it by disconnecting other sessions.", "tokens": [400, 309, 307, 689, 512, 636, 281, 483, 3973, 295, 309, 538, 14299, 278, 661, 11081, 13], "temperature": 0.0, "avg_logprob": -0.4165547772457725, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.669406447035726e-05}, {"id": 108, "seek": 49636, "start": 511.28000000000003, "end": 514.28, "text": " Connect other clients.", "tokens": [11653, 661, 6982, 13], "temperature": 0.0, "avg_logprob": -0.4165547772457725, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.669406447035726e-05}, {"id": 109, "seek": 49636, "start": 519.5600000000001, "end": 524.08, "text": " Prefix D gives you connected clients, whichever you select is disconnected.", "tokens": [6001, 69, 970, 413, 2709, 291, 4582, 6982, 11, 24123, 291, 3048, 307, 29426, 13], "temperature": 0.0, "avg_logprob": -0.4165547772457725, "compression_ratio": 1.5561224489795917, "no_speech_prob": 1.669406447035726e-05}, {"id": 110, "seek": 52408, "start": 524.08, "end": 526.08, "text": " Select is disconnected.", "tokens": [13638, 307, 29426, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 111, "seek": 52408, "start": 527.76, "end": 529.36, "text": " Let's try that.", "tokens": [961, 311, 853, 300, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 112, "seek": 52408, "start": 531.36, "end": 533.96, "text": " No, that's not right. Oh, they probably been shifty.", "tokens": [883, 11, 300, 311, 406, 558, 13, 876, 11, 436, 1391, 668, 402, 37177, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 113, "seek": 52408, "start": 536.1600000000001, "end": 537.5600000000001, "text": " There we go.", "tokens": [821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 114, "seek": 52408, "start": 540.64, "end": 544.36, "text": " Right, this is what I just created, so if I hit this, there we go.", "tokens": [1779, 11, 341, 307, 437, 286, 445, 2942, 11, 370, 498, 286, 2045, 341, 11, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 115, "seek": 52408, "start": 544.36, "end": 546.8000000000001, "text": " So shift D and then select the one to disconnect.", "tokens": [407, 5513, 413, 293, 550, 3048, 264, 472, 281, 14299, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 116, "seek": 52408, "start": 548.0, "end": 550.96, "text": " Oh, nice. OK, learn something new.", "tokens": [876, 11, 1481, 13, 2264, 11, 1466, 746, 777, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 117, "seek": 52408, "start": 551.6400000000001, "end": 553.2, "text": " Oh, we've got another new face today.", "tokens": [876, 11, 321, 600, 658, 1071, 777, 1851, 965, 13], "temperature": 0.0, "avg_logprob": -0.3260905146598816, "compression_ratio": 1.544502617801047, "no_speech_prob": 1.4062862646824215e-05}, {"id": 118, "seek": 55320, "start": 553.2, "end": 555.72, "text": " Hello, Sophie, I don't think you've joined us before, is that right?", "tokens": [2425, 11, 29645, 11, 286, 500, 380, 519, 291, 600, 6869, 505, 949, 11, 307, 300, 558, 30], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 119, "seek": 55320, "start": 557.4000000000001, "end": 559.6800000000001, "text": " I've been here just quietly in the background sometimes.", "tokens": [286, 600, 668, 510, 445, 19141, 294, 264, 3678, 2171, 13], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 120, "seek": 55320, "start": 560.0, "end": 562.6, "text": " OK, thank you for joining. Whereabouts are you visiting us from?", "tokens": [2264, 11, 1309, 291, 337, 5549, 13, 2305, 41620, 366, 291, 11700, 505, 490, 30], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 121, "seek": 55320, "start": 563.2800000000001, "end": 566.0, "text": " In Brisbane. Oh, good on you.", "tokens": [682, 32222, 13, 876, 11, 665, 322, 291, 13], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 122, "seek": 55320, "start": 566.0, "end": 568.8000000000001, "text": " And what do you work with?", "tokens": [400, 437, 360, 291, 589, 365, 30], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 123, "seek": 55320, "start": 568.96, "end": 571.6, "text": " I stuff where you're just getting started.", "tokens": [286, 1507, 689, 291, 434, 445, 1242, 1409, 13], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 124, "seek": 55320, "start": 571.6, "end": 572.6400000000001, "text": " Not at all.", "tokens": [1726, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 125, "seek": 55320, "start": 572.6400000000001, "end": 575.6400000000001, "text": " Background in psychology, doing a postdoc and psych and sort of,", "tokens": [36904, 294, 15105, 11, 884, 257, 2183, 39966, 293, 4681, 293, 1333, 295, 11], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 126, "seek": 55320, "start": 575.6400000000001, "end": 577.72, "text": " yeah, kind of move over into data science.", "tokens": [1338, 11, 733, 295, 1286, 670, 666, 1412, 3497, 13], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 127, "seek": 55320, "start": 577.84, "end": 582.36, "text": " OK, cool. Have you done a lot of the statistical side of psychology?", "tokens": [2264, 11, 1627, 13, 3560, 291, 1096, 257, 688, 295, 264, 22820, 1252, 295, 15105, 30], "temperature": 0.0, "avg_logprob": -0.22680104573567708, "compression_ratio": 1.6020066889632107, "no_speech_prob": 4.262939910404384e-05}, {"id": 128, "seek": 58236, "start": 582.36, "end": 585.28, "text": " Yeah, yeah, quite a bit and quite a bit of coding in R,", "tokens": [865, 11, 1338, 11, 1596, 257, 857, 293, 1596, 257, 857, 295, 17720, 294, 497, 11], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 129, "seek": 58236, "start": 585.28, "end": 586.8000000000001, "text": " but I'm pretty new to Python.", "tokens": [457, 286, 478, 1238, 777, 281, 15329, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 130, "seek": 58236, "start": 586.8000000000001, "end": 589.08, "text": " So OK, great. Big learning curve.", "tokens": [407, 2264, 11, 869, 13, 5429, 2539, 7605, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 131, "seek": 58236, "start": 589.08, "end": 591.64, "text": " Well, you know what? You're you're our target market, right?", "tokens": [1042, 11, 291, 458, 437, 30, 509, 434, 291, 434, 527, 3779, 2142, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 132, "seek": 58236, "start": 591.64, "end": 595.48, "text": " So if you have any questions along the way, please jump in.", "tokens": [407, 498, 291, 362, 604, 1651, 2051, 264, 636, 11, 1767, 3012, 294, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 133, "seek": 58236, "start": 595.48, "end": 597.72, "text": " Even things that you feel like everybody else must know.", "tokens": [2754, 721, 300, 291, 841, 411, 2201, 1646, 1633, 458, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 134, "seek": 58236, "start": 597.72, "end": 599.96, "text": " I guarantee not everybody else does.", "tokens": [286, 10815, 406, 2201, 1646, 775, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 135, "seek": 58236, "start": 599.96, "end": 602.48, "text": " So yeah, definitely. These have been really helpful.", "tokens": [407, 1338, 11, 2138, 13, 1981, 362, 668, 534, 4961, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 136, "seek": 58236, "start": 602.48, "end": 604.44, "text": " I'm really grateful they're running.", "tokens": [286, 478, 534, 7941, 436, 434, 2614, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 137, "seek": 58236, "start": 604.44, "end": 606.72, "text": " Awesome. Thanks for joining.", "tokens": [10391, 13, 2561, 337, 5549, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 138, "seek": 58236, "start": 607.48, "end": 611.28, "text": " OK, so training three models in parallel right now.", "tokens": [2264, 11, 370, 3097, 1045, 5245, 294, 8952, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.2875186839002244, "compression_ratio": 1.6031746031746033, "no_speech_prob": 4.1316863644169644e-05}, {"id": 139, "seek": 61128, "start": 611.28, "end": 614.16, "text": " Yeah, so I've got three GPUs in this machine.", "tokens": [865, 11, 370, 286, 600, 658, 1045, 18407, 82, 294, 341, 3479, 13], "temperature": 0.0, "avg_logprob": -0.29557061876569474, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.5200557754724286e-05}, {"id": 140, "seek": 61128, "start": 617.0799999999999, "end": 623.04, "text": " And so. Yeah, one nice thing with with weights and biases is you.", "tokens": [400, 370, 13, 865, 11, 472, 1481, 551, 365, 365, 17443, 293, 32152, 307, 291, 13], "temperature": 0.0, "avg_logprob": -0.29557061876569474, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.5200557754724286e-05}, {"id": 141, "seek": 61128, "start": 624.0, "end": 626.04, "text": " Basically, let me show you.", "tokens": [8537, 11, 718, 385, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.29557061876569474, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.5200557754724286e-05}, {"id": 142, "seek": 61128, "start": 631.8399999999999, "end": 633.88, "text": " OK, so here's weights and biases.", "tokens": [2264, 11, 370, 510, 311, 17443, 293, 32152, 13], "temperature": 0.0, "avg_logprob": -0.29557061876569474, "compression_ratio": 1.4465408805031446, "no_speech_prob": 1.5200557754724286e-05}, {"id": 143, "seek": 63388, "start": 633.88, "end": 637.4, "text": " And so you don't use my Mac very much because nothing's locked in.", "tokens": [400, 370, 291, 500, 380, 764, 452, 5707, 588, 709, 570, 1825, 311, 9376, 294, 13], "temperature": 0.0, "avg_logprob": -0.6694010858950408, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.2604945368366316e-05}, {"id": 144, "seek": 63388, "start": 644.6, "end": 648.4, "text": " All right. And so you can see it's running this thing called a sweep, right?", "tokens": [1057, 558, 13, 400, 370, 291, 393, 536, 309, 311, 2614, 341, 551, 1219, 257, 22169, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.6694010858950408, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.2604945368366316e-05}, {"id": 145, "seek": 63388, "start": 649.12, "end": 652.24, "text": " So there's going to be four hundred and seventy seven runs.", "tokens": [407, 456, 311, 516, 281, 312, 1451, 3262, 293, 25662, 3407, 6676, 13], "temperature": 0.0, "avg_logprob": -0.6694010858950408, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.2604945368366316e-05}, {"id": 146, "seek": 63388, "start": 655.0, "end": 656.28, "text": " I don't know why it says.", "tokens": [286, 500, 380, 458, 983, 309, 1619, 13], "temperature": 0.0, "avg_logprob": -0.6694010858950408, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.2604945368366316e-05}, {"id": 147, "seek": 63388, "start": 657.0, "end": 659.76, "text": " Create 31 seconds ago because that's so slow.", "tokens": [20248, 10353, 3949, 2057, 570, 300, 311, 370, 2964, 13], "temperature": 0.0, "avg_logprob": -0.6694010858950408, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.2604945368366316e-05}, {"id": 148, "seek": 63388, "start": 659.76, "end": 661.88, "text": " So I'm going to go ahead and run this thing.", "tokens": [407, 286, 478, 516, 281, 352, 2286, 293, 1190, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.6694010858950408, "compression_ratio": 1.5686274509803921, "no_speech_prob": 1.2604945368366316e-05}, {"id": 149, "seek": 66188, "start": 661.88, "end": 664.88, "text": " Create 31 seconds ago, because that's certainly not true.", "tokens": [20248, 10353, 3949, 2057, 11, 570, 300, 311, 3297, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.2968422657734639, "compression_ratio": 1.6369426751592357, "no_speech_prob": 7.0707669692637865e-06}, {"id": 150, "seek": 66188, "start": 667.4, "end": 669.32, "text": " That's currently running.", "tokens": [663, 311, 4362, 2614, 13], "temperature": 0.0, "avg_logprob": -0.2968422657734639, "compression_ratio": 1.6369426751592357, "no_speech_prob": 7.0707669692637865e-06}, {"id": 151, "seek": 66188, "start": 671.4, "end": 675.24, "text": " And so it's coming from this git repo.", "tokens": [400, 370, 309, 311, 1348, 490, 341, 18331, 49040, 13], "temperature": 0.0, "avg_logprob": -0.2968422657734639, "compression_ratio": 1.6369426751592357, "no_speech_prob": 7.0707669692637865e-06}, {"id": 152, "seek": 66188, "start": 678.28, "end": 683.92, "text": " I feel like there's a there's a sweep view because this is a particular run.", "tokens": [286, 841, 411, 456, 311, 257, 456, 311, 257, 22169, 1910, 570, 341, 307, 257, 1729, 1190, 13], "temperature": 0.0, "avg_logprob": -0.2968422657734639, "compression_ratio": 1.6369426751592357, "no_speech_prob": 7.0707669692637865e-06}, {"id": 153, "seek": 66188, "start": 684.2, "end": 687.64, "text": " This is this is a this is a particular run.", "tokens": [639, 307, 341, 307, 257, 341, 307, 257, 1729, 1190, 13], "temperature": 0.0, "avg_logprob": -0.2968422657734639, "compression_ratio": 1.6369426751592357, "no_speech_prob": 7.0707669692637865e-06}, {"id": 154, "seek": 66188, "start": 687.64, "end": 688.76, "text": " That's right.", "tokens": [663, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.2968422657734639, "compression_ratio": 1.6369426751592357, "no_speech_prob": 7.0707669692637865e-06}, {"id": 155, "seek": 68876, "start": 688.76, "end": 692.72, "text": " I'm terrible with it, to be honest.", "tokens": [286, 478, 6237, 365, 309, 11, 281, 312, 3245, 13], "temperature": 0.0, "avg_logprob": -0.29963998909456185, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.938404789660126e-06}, {"id": 156, "seek": 68876, "start": 693.08, "end": 695.72, "text": " OK, so let's go to the project.", "tokens": [2264, 11, 370, 718, 311, 352, 281, 264, 1716, 13], "temperature": 0.0, "avg_logprob": -0.29963998909456185, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.938404789660126e-06}, {"id": 157, "seek": 68876, "start": 695.8, "end": 700.6, "text": " Yes. And then there are in the room and then a project has sweeps.", "tokens": [1079, 13, 400, 550, 456, 366, 294, 264, 1808, 293, 550, 257, 1716, 575, 2484, 10653, 13], "temperature": 0.0, "avg_logprob": -0.29963998909456185, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.938404789660126e-06}, {"id": 158, "seek": 68876, "start": 701.36, "end": 706.24, "text": " And then. OK, this one here, I can kill because.", "tokens": [400, 550, 13, 2264, 11, 341, 472, 510, 11, 286, 393, 1961, 570, 13], "temperature": 0.0, "avg_logprob": -0.29963998909456185, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.938404789660126e-06}, {"id": 159, "seek": 68876, "start": 709.24, "end": 716.28, "text": " OK. So basically, you kind of say on the on the Linux side, WNB,", "tokens": [2264, 13, 407, 1936, 11, 291, 733, 295, 584, 322, 264, 322, 264, 18734, 1252, 11, 343, 45, 33, 11], "temperature": 0.0, "avg_logprob": -0.29963998909456185, "compression_ratio": 1.4761904761904763, "no_speech_prob": 8.938404789660126e-06}, {"id": 160, "seek": 71628, "start": 716.28, "end": 718.9599999999999, "text": " you know, sweet create or something like that.", "tokens": [291, 458, 11, 3844, 1884, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1964025704757027, "compression_ratio": 1.5395348837209302, "no_speech_prob": 3.0891180813341634e-06}, {"id": 161, "seek": 71628, "start": 719.68, "end": 724.04, "text": " And then. Interesting is all grouped under this thing.", "tokens": [400, 550, 13, 14711, 307, 439, 41877, 833, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.1964025704757027, "compression_ratio": 1.5395348837209302, "no_speech_prob": 3.0891180813341634e-06}, {"id": 162, "seek": 71628, "start": 724.0799999999999, "end": 725.0799999999999, "text": " Oh, OK. All right.", "tokens": [876, 11, 2264, 13, 1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1964025704757027, "compression_ratio": 1.5395348837209302, "no_speech_prob": 3.0891180813341634e-06}, {"id": 163, "seek": 71628, "start": 725.0799999999999, "end": 728.12, "text": " So then, yeah, so then basically it runs.", "tokens": [407, 550, 11, 1338, 11, 370, 550, 1936, 309, 6676, 13], "temperature": 0.0, "avg_logprob": -0.1964025704757027, "compression_ratio": 1.5395348837209302, "no_speech_prob": 3.0891180813341634e-06}, {"id": 164, "seek": 71628, "start": 731.0, "end": 734.6, "text": " Lots of copies of your program feeding at different configurations.", "tokens": [15908, 295, 14341, 295, 428, 1461, 12919, 412, 819, 31493, 13], "temperature": 0.0, "avg_logprob": -0.1964025704757027, "compression_ratio": 1.5395348837209302, "no_speech_prob": 3.0891180813341634e-06}, {"id": 165, "seek": 71628, "start": 738.8, "end": 741.8, "text": " And yeah, you can run the client as many times as you like.", "tokens": [400, 1338, 11, 291, 393, 1190, 264, 6423, 382, 867, 1413, 382, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.1964025704757027, "compression_ratio": 1.5395348837209302, "no_speech_prob": 3.0891180813341634e-06}, {"id": 166, "seek": 71628, "start": 741.8, "end": 744.56, "text": " So I've run it three times at each time.", "tokens": [407, 286, 600, 1190, 309, 1045, 1413, 412, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.1964025704757027, "compression_ratio": 1.5395348837209302, "no_speech_prob": 3.0891180813341634e-06}, {"id": 167, "seek": 74456, "start": 744.56, "end": 746.4799999999999, "text": " I've set it to a different.", "tokens": [286, 600, 992, 309, 281, 257, 819, 13], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 168, "seek": 74456, "start": 746.8, "end": 751.28, "text": " Cuda device, you turn your models into Python scripts", "tokens": [383, 11152, 4302, 11, 291, 1261, 428, 5245, 666, 15329, 23294], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 169, "seek": 74456, "start": 751.28, "end": 754.28, "text": " and to able to do this or exactly.", "tokens": [293, 281, 1075, 281, 360, 341, 420, 2293, 13], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 170, "seek": 74456, "start": 754.4, "end": 760.4, "text": " So. So this is fine tuned up.", "tokens": [407, 13, 407, 341, 307, 2489, 10870, 493, 13], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 171, "seek": 74456, "start": 760.4, "end": 763.56, "text": " So it's just calling so causes Pazarks.", "tokens": [407, 309, 311, 445, 5141, 370, 7700, 430, 921, 20851, 13], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 172, "seek": 74456, "start": 763.88, "end": 766.56, "text": " So that's going to just go through and check what batch size,", "tokens": [407, 300, 311, 516, 281, 445, 352, 807, 293, 1520, 437, 15245, 2744, 11], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 173, "seek": 74456, "start": 766.56, "end": 768.4799999999999, "text": " et cetera, et cetera, et cetera, you asked for, right?", "tokens": [1030, 11458, 11, 1030, 11458, 11, 1030, 11458, 11, 291, 2351, 337, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 174, "seek": 74456, "start": 769.92, "end": 773.28, "text": " Sticks them all into.", "tokens": [745, 7663, 552, 439, 666, 13], "temperature": 0.0, "avg_logprob": -0.38620236840578587, "compression_ratio": 1.555023923444976, "no_speech_prob": 4.092579274583841e-06}, {"id": 175, "seek": 77328, "start": 773.28, "end": 779.3199999999999, "text": " This Pazark thing, and then it calls train passing in those arguments.", "tokens": [639, 430, 921, 809, 551, 11, 293, 550, 309, 5498, 3847, 8437, 294, 729, 12869, 13], "temperature": 0.0, "avg_logprob": -0.31919281823294504, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.288901891413843e-06}, {"id": 176, "seek": 77328, "start": 780.0799999999999, "end": 782.48, "text": " And so then train.", "tokens": [400, 370, 550, 3847, 13], "temperature": 0.0, "avg_logprob": -0.31919281823294504, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.288901891413843e-06}, {"id": 177, "seek": 77328, "start": 783.48, "end": 788.48, "text": " Is going to initialize weights and biases for this particular project.", "tokens": [1119, 516, 281, 5883, 1125, 17443, 293, 32152, 337, 341, 1729, 1716, 13], "temperature": 0.0, "avg_logprob": -0.31919281823294504, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.288901891413843e-06}, {"id": 178, "seek": 77328, "start": 789.88, "end": 793.4399999999999, "text": " For this particular entity, which is fast, I using the configuration", "tokens": [1171, 341, 1729, 13977, 11, 597, 307, 2370, 11, 286, 1228, 264, 11694], "temperature": 0.0, "avg_logprob": -0.31919281823294504, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.288901891413843e-06}, {"id": 179, "seek": 77328, "start": 793.4399999999999, "end": 795.28, "text": " that you requested.", "tokens": [300, 291, 16436, 13], "temperature": 0.0, "avg_logprob": -0.31919281823294504, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.288901891413843e-06}, {"id": 180, "seek": 77328, "start": 797.04, "end": 800.24, "text": " And so then you can say, for example, OK, this got some particular data set,", "tokens": [400, 370, 550, 291, 393, 584, 11, 337, 1365, 11, 2264, 11, 341, 658, 512, 1729, 1412, 992, 11], "temperature": 0.0, "avg_logprob": -0.31919281823294504, "compression_ratio": 1.5902439024390245, "no_speech_prob": 4.288901891413843e-06}, {"id": 181, "seek": 80024, "start": 800.24, "end": 804.12, "text": " some particular batch size and image size, et cetera.", "tokens": [512, 1729, 15245, 2744, 293, 3256, 2744, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 182, "seek": 80024, "start": 804.12, "end": 807.36, "text": " And then it creates a learner for some particular model name,", "tokens": [400, 550, 309, 7829, 257, 33347, 337, 512, 1729, 2316, 1315, 11], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 183, "seek": 80024, "start": 807.36, "end": 811.04, "text": " some particular pooling type.", "tokens": [512, 1729, 7005, 278, 2010, 13], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 184, "seek": 80024, "start": 811.2, "end": 813.08, "text": " Fine tunes it.", "tokens": [12024, 38498, 309, 13], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 185, "seek": 80024, "start": 813.08, "end": 817.0, "text": " And then at the end, it logs how much GPU memory used,", "tokens": [400, 550, 412, 264, 917, 11, 309, 20820, 577, 709, 18407, 4675, 1143, 11], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 186, "seek": 80024, "start": 817.4, "end": 820.04, "text": " what model it was, how long it took.", "tokens": [437, 2316, 309, 390, 11, 577, 938, 309, 1890, 13], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 187, "seek": 80024, "start": 820.04, "end": 823.4, "text": " And you don't have to look much because the fast AI", "tokens": [400, 291, 500, 380, 362, 281, 574, 709, 570, 264, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 188, "seek": 80024, "start": 825.28, "end": 829.12, "text": " weights and biases integration automatically tracks everything in the learner.", "tokens": [17443, 293, 32152, 10980, 6772, 10218, 1203, 294, 264, 33347, 13], "temperature": 0.0, "avg_logprob": -0.23914311856639628, "compression_ratio": 1.6367521367521367, "no_speech_prob": 7.29585872250027e-06}, {"id": 189, "seek": 82912, "start": 829.12, "end": 831.52, "text": " So you can see here, there's all this like.", "tokens": [407, 291, 393, 536, 510, 11, 456, 311, 439, 341, 411, 13], "temperature": 0.0, "avg_logprob": -0.4184979756673177, "compression_ratio": 1.5502392344497609, "no_speech_prob": 7.071008894854458e-06}, {"id": 190, "seek": 82912, "start": 834.2, "end": 838.72, "text": " Learner architecture, learner loss function, et cetera, et cetera.", "tokens": [17216, 260, 9482, 11, 33347, 4470, 2445, 11, 1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.4184979756673177, "compression_ratio": 1.5502392344497609, "no_speech_prob": 7.071008894854458e-06}, {"id": 191, "seek": 82912, "start": 840.52, "end": 846.52, "text": " Out of curiosity, was this process of refactoring into a script painful?", "tokens": [5925, 295, 18769, 11, 390, 341, 1399, 295, 1895, 578, 3662, 666, 257, 5755, 11697, 30], "temperature": 0.0, "avg_logprob": -0.4184979756673177, "compression_ratio": 1.5502392344497609, "no_speech_prob": 7.071008894854458e-06}, {"id": 192, "seek": 82912, "start": 846.52, "end": 851.52, "text": " So actually, you can probably actually tell I didn't do this.", "tokens": [407, 767, 11, 291, 393, 1391, 767, 980, 286, 994, 380, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.4184979756673177, "compression_ratio": 1.5502392344497609, "no_speech_prob": 7.071008894854458e-06}, {"id": 193, "seek": 82912, "start": 851.52, "end": 853.24, "text": " Thomas Capelle did this.", "tokens": [8500, 27517, 2447, 630, 341, 13], "temperature": 0.0, "avg_logprob": -0.4184979756673177, "compression_ratio": 1.5502392344497609, "no_speech_prob": 7.071008894854458e-06}, {"id": 194, "seek": 82912, "start": 853.24, "end": 858.64, "text": " If I had done it, I would have used fast core script.", "tokens": [759, 286, 632, 1096, 309, 11, 286, 576, 362, 1143, 2370, 4965, 5755, 13], "temperature": 0.0, "avg_logprob": -0.4184979756673177, "compression_ratio": 1.5502392344497609, "no_speech_prob": 7.071008894854458e-06}, {"id": 195, "seek": 85864, "start": 858.64, "end": 861.72, "text": " Instead of. This stuff, I guess.", "tokens": [7156, 295, 13, 639, 1507, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.28255236667135486, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.533948620315641e-05}, {"id": 196, "seek": 85864, "start": 863.16, "end": 865.08, "text": " But no, it wouldn't have been painful.", "tokens": [583, 572, 11, 309, 2759, 380, 362, 668, 11697, 13], "temperature": 0.0, "avg_logprob": -0.28255236667135486, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.533948620315641e-05}, {"id": 197, "seek": 85864, "start": 865.08, "end": 868.84, "text": " I would have just chucked an MB dev export on the cell that I had in my notebook,", "tokens": [286, 576, 362, 445, 20870, 292, 364, 28866, 1905, 10725, 322, 264, 2815, 300, 286, 632, 294, 452, 21060, 11], "temperature": 0.0, "avg_logprob": -0.28255236667135486, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.533948620315641e-05}, {"id": 198, "seek": 85864, "start": 868.84, "end": 872.1999999999999, "text": " and that would have become, yeah, my script.", "tokens": [293, 300, 576, 362, 1813, 11, 1338, 11, 452, 5755, 13], "temperature": 0.0, "avg_logprob": -0.28255236667135486, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.533948620315641e-05}, {"id": 199, "seek": 85864, "start": 872.88, "end": 877.12, "text": " So wouldn't be painful.", "tokens": [407, 2759, 380, 312, 11697, 13], "temperature": 0.0, "avg_logprob": -0.28255236667135486, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.533948620315641e-05}, {"id": 200, "seek": 85864, "start": 877.12, "end": 880.84, "text": " Hi. I have a question.", "tokens": [2421, 13, 286, 362, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.28255236667135486, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.533948620315641e-05}, {"id": 201, "seek": 85864, "start": 880.84, "end": 886.56, "text": " Wouldn't it be interesting to track power consumption, for example?", "tokens": [26291, 380, 309, 312, 1880, 281, 2837, 1347, 12126, 11, 337, 1365, 30], "temperature": 0.0, "avg_logprob": -0.28255236667135486, "compression_ratio": 1.5048076923076923, "no_speech_prob": 3.533948620315641e-05}, {"id": 202, "seek": 88656, "start": 886.56, "end": 889.56, "text": " I mean, for some people, it might be not for me.", "tokens": [286, 914, 11, 337, 512, 561, 11, 309, 1062, 312, 406, 337, 385, 13], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 203, "seek": 88656, "start": 890.56, "end": 893.56, "text": " As to how you would track power consumption, I have no idea.", "tokens": [1018, 281, 577, 291, 576, 2837, 1347, 12126, 11, 286, 362, 572, 1558, 13], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 204, "seek": 88656, "start": 893.56, "end": 895.56, "text": " You'd have to have some kind of.", "tokens": [509, 1116, 362, 281, 362, 512, 733, 295, 13], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 205, "seek": 88656, "start": 895.56, "end": 898.0799999999999, "text": " Sensor connected to your power supply, I guess.", "tokens": [318, 23153, 4582, 281, 428, 1347, 5847, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 206, "seek": 88656, "start": 899.0799999999999, "end": 901.56, "text": " They track a lot of system metrics in the runs.", "tokens": [814, 2837, 257, 688, 295, 1185, 16367, 294, 264, 6676, 13], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 207, "seek": 88656, "start": 901.56, "end": 907.56, "text": " So like if you look on a run, they will track like GPU memory, CPU memory.", "tokens": [407, 411, 498, 291, 574, 322, 257, 1190, 11, 436, 486, 2837, 411, 18407, 4675, 11, 13199, 4675, 13], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 208, "seek": 88656, "start": 907.56, "end": 911.56, "text": " Yeah, stuff like, yeah, if you click on the.", "tokens": [865, 11, 1507, 411, 11, 1338, 11, 498, 291, 2052, 322, 264, 13], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 209, "seek": 88656, "start": 912.56, "end": 914.56, "text": " The thing on the left,", "tokens": [440, 551, 322, 264, 1411, 11], "temperature": 0.0, "avg_logprob": -0.3324676513671875, "compression_ratio": 1.6144067796610169, "no_speech_prob": 9.969289749278687e-06}, {"id": 210, "seek": 91456, "start": 914.56, "end": 918.56, "text": " it looks like a CPU chip, that thing. Yeah, there's a lot of.", "tokens": [309, 1542, 411, 257, 13199, 11409, 11, 300, 551, 13, 865, 11, 456, 311, 257, 688, 295, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 211, "seek": 91456, "start": 918.56, "end": 920.56, "text": " So maybe there's power in here.", "tokens": [407, 1310, 456, 311, 1347, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 212, "seek": 91456, "start": 920.56, "end": 922.56, "text": " I don't see how it can be right.", "tokens": [286, 500, 380, 536, 577, 309, 393, 312, 558, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 213, "seek": 91456, "start": 922.56, "end": 926.56, "text": " Because like it well, unless the Nvidia power usage.", "tokens": [1436, 411, 309, 731, 11, 5969, 264, 46284, 1347, 14924, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 214, "seek": 91456, "start": 926.56, "end": 928.56, "text": " That does. Here you go.", "tokens": [663, 775, 13, 1692, 291, 352, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 215, "seek": 91456, "start": 928.56, "end": 932.56, "text": " GPU power. So Nvidia tells you the GPU power usage, apparently.", "tokens": [18407, 1347, 13, 407, 46284, 5112, 291, 264, 18407, 1347, 14924, 11, 7970, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 216, "seek": 91456, "start": 935.56, "end": 938.56, "text": " Although that won't tell you about your CPU, et cetera, power.", "tokens": [5780, 300, 1582, 380, 980, 291, 466, 428, 13199, 11, 1030, 11458, 11, 1347, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 217, "seek": 91456, "start": 939.56, "end": 942.56, "text": " The thing that's useful about this, I think, is the memory.", "tokens": [440, 551, 300, 311, 4420, 466, 341, 11, 286, 519, 11, 307, 264, 4675, 13], "temperature": 0.0, "avg_logprob": -0.2523755533941861, "compression_ratio": 1.625, "no_speech_prob": 1.5933250324451365e-05}, {"id": 218, "seek": 94256, "start": 942.56, "end": 947.56, "text": " The graph. Yeah, well, the key thing is the maximum memory use.", "tokens": [440, 4295, 13, 865, 11, 731, 11, 264, 2141, 551, 307, 264, 6674, 4675, 764, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 219, "seek": 94256, "start": 947.56, "end": 952.56, "text": " So we actually track that here in the script.", "tokens": [407, 321, 767, 2837, 300, 510, 294, 264, 5755, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 220, "seek": 94256, "start": 952.56, "end": 954.56, "text": " Yeah, we put it into GPU.", "tokens": [865, 11, 321, 829, 309, 666, 18407, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 221, "seek": 94256, "start": 956.56, "end": 958.56, "text": " Oh, good GPU, man. OK.", "tokens": [876, 11, 665, 18407, 11, 587, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 222, "seek": 94256, "start": 959.56, "end": 961.56, "text": " That's a. Yeah, I think.", "tokens": [663, 311, 257, 13, 865, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 223, "seek": 94256, "start": 961.56, "end": 963.56, "text": " GPU members. Oh, OK.", "tokens": [18407, 2679, 13, 876, 11, 2264, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 224, "seek": 94256, "start": 963.56, "end": 965.56, "text": " Blah, blah, blah.", "tokens": [2177, 545, 11, 12288, 11, 12288, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 225, "seek": 94256, "start": 968.56, "end": 970.56, "text": " So Thomas did that as well.", "tokens": [407, 8500, 630, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.3489440456851498, "compression_ratio": 1.4619883040935673, "no_speech_prob": 1.7501712136436254e-05}, {"id": 226, "seek": 97056, "start": 970.56, "end": 972.56, "text": " I don't know why it's.", "tokens": [286, 500, 380, 458, 983, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 227, "seek": 97056, "start": 973.56, "end": 975.56, "text": " The power of negative three.", "tokens": [440, 1347, 295, 3671, 1045, 13], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 228, "seek": 97056, "start": 976.56, "end": 978.56, "text": " What's that about?", "tokens": [708, 311, 300, 466, 30], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 229, "seek": 97056, "start": 979.56, "end": 981.56, "text": " Stay curious.", "tokens": [8691, 6369, 13], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 230, "seek": 97056, "start": 982.56, "end": 984.56, "text": " I have to ask him what that's.", "tokens": [286, 362, 281, 1029, 796, 437, 300, 311, 13], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 231, "seek": 97056, "start": 987.56, "end": 989.56, "text": " That's doing.", "tokens": [663, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 232, "seek": 97056, "start": 990.56, "end": 994.56, "text": " Thomas works at weight devices, right? Is that right? Correct. Correct. Correct. Yeah.", "tokens": [8500, 1985, 412, 3364, 5759, 11, 558, 30, 1119, 300, 558, 30, 12753, 13, 12753, 13, 12753, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 233, "seek": 97056, "start": 994.56, "end": 997.56, "text": " So he I had never used it before.", "tokens": [407, 415, 286, 632, 1128, 1143, 309, 949, 13], "temperature": 0.0, "avg_logprob": -0.31392837152248476, "compression_ratio": 1.4880952380952381, "no_speech_prob": 2.1110594389028847e-05}, {"id": 234, "seek": 99756, "start": 997.56, "end": 1002.56, "text": " So. Yeah.", "tokens": [407, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.14095997209308528, "compression_ratio": 1.6581818181818182, "no_speech_prob": 7.765524969727267e-06}, {"id": 235, "seek": 99756, "start": 1002.56, "end": 1012.56, "text": " So probably most people have never heard of this, but fast day I actually has a thing called FastGPU, which is what I've previously used for doing this kind of thing.", "tokens": [407, 1391, 881, 561, 362, 1128, 2198, 295, 341, 11, 457, 2370, 786, 286, 767, 575, 257, 551, 1219, 15968, 38, 8115, 11, 597, 307, 437, 286, 600, 8046, 1143, 337, 884, 341, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.14095997209308528, "compression_ratio": 1.6581818181818182, "no_speech_prob": 7.765524969727267e-06}, {"id": 236, "seek": 99756, "start": 1012.56, "end": 1025.56, "text": " So in general, when you've got more than one GPU or just even if you got only one GPU and you've got a bunch of things you want to run, it's helpful to have some way to say like, OK, here's the things to run and then set a script off to go and run them all and check the results.", "tokens": [407, 294, 2674, 11, 562, 291, 600, 658, 544, 813, 472, 18407, 420, 445, 754, 498, 291, 658, 787, 472, 18407, 293, 291, 600, 658, 257, 3840, 295, 721, 291, 528, 281, 1190, 11, 309, 311, 4961, 281, 362, 512, 636, 281, 584, 411, 11, 2264, 11, 510, 311, 264, 721, 281, 1190, 293, 550, 992, 257, 5755, 766, 281, 352, 293, 1190, 552, 439, 293, 1520, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.14095997209308528, "compression_ratio": 1.6581818181818182, "no_speech_prob": 7.765524969727267e-06}, {"id": 237, "seek": 102556, "start": 1025.56, "end": 1029.56, "text": " So FastGPU was the thing I built to do that.", "tokens": [407, 15968, 38, 8115, 390, 264, 551, 286, 3094, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.14477244445255824, "compression_ratio": 1.4326241134751774, "no_speech_prob": 1.3004702850594185e-05}, {"id": 238, "seek": 102556, "start": 1029.56, "end": 1039.56, "text": " And the way FastGPU works is that you have a whole list of the whole directory of scripts in a folder and it runs each script run at a time and puts them in.", "tokens": [400, 264, 636, 15968, 38, 8115, 1985, 307, 300, 291, 362, 257, 1379, 1329, 295, 264, 1379, 21120, 295, 23294, 294, 257, 10820, 293, 309, 6676, 1184, 5755, 1190, 412, 257, 565, 293, 8137, 552, 294, 13], "temperature": 0.0, "avg_logprob": -0.14477244445255824, "compression_ratio": 1.4326241134751774, "no_speech_prob": 1.3004702850594185e-05}, {"id": 239, "seek": 103956, "start": 1039.56, "end": 1055.56, "text": " And it runs and it puts them into a separate directory, you know, to say this is completed and it tracks the results and you can do it on like as many or few GPUs as you like and it'll just go ahead and run it.", "tokens": [400, 309, 6676, 293, 309, 8137, 552, 666, 257, 4994, 21120, 11, 291, 458, 11, 281, 584, 341, 307, 7365, 293, 309, 10218, 264, 3542, 293, 291, 393, 360, 309, 322, 411, 382, 867, 420, 1326, 18407, 82, 382, 291, 411, 293, 309, 603, 445, 352, 2286, 293, 1190, 309, 13], "temperature": 0.0, "avg_logprob": -0.14530268868247231, "compression_ratio": 1.5707762557077625, "no_speech_prob": 2.68383359980362e-06}, {"id": 240, "seek": 103956, "start": 1055.56, "end": 1060.56, "text": " And this is fine, but it's very basic.", "tokens": [400, 341, 307, 2489, 11, 457, 309, 311, 588, 3875, 13], "temperature": 0.0, "avg_logprob": -0.14530268868247231, "compression_ratio": 1.5707762557077625, "no_speech_prob": 2.68383359980362e-06}, {"id": 241, "seek": 103956, "start": 1060.56, "end": 1067.56, "text": " And I kind of been planning to make it a bit more sophisticated. And yeah, weights and biases.", "tokens": [400, 286, 733, 295, 668, 5038, 281, 652, 309, 257, 857, 544, 16950, 13, 400, 1338, 11, 17443, 293, 32152, 13], "temperature": 0.0, "avg_logprob": -0.14530268868247231, "compression_ratio": 1.5707762557077625, "no_speech_prob": 2.68383359980362e-06}, {"id": 242, "seek": 106756, "start": 1067.56, "end": 1081.56, "text": " It takes it a lot further, you know, by and I kind of want to redo or add something on top of FastGPU so it is fairly compatible with weights and biases, but you could do everything locally.", "tokens": [467, 2516, 309, 257, 688, 3052, 11, 291, 458, 11, 538, 293, 286, 733, 295, 528, 281, 29956, 420, 909, 746, 322, 1192, 295, 15968, 38, 8115, 370, 309, 307, 6457, 18218, 365, 17443, 293, 32152, 11, 457, 291, 727, 360, 1203, 16143, 13], "temperature": 0.0, "avg_logprob": -0.1640235207297585, "compression_ratio": 1.3506493506493507, "no_speech_prob": 6.8535464379237965e-06}, {"id": 243, "seek": 106756, "start": 1081.56, "end": 1084.56, "text": " So the key thing.", "tokens": [407, 264, 2141, 551, 13], "temperature": 0.0, "avg_logprob": -0.1640235207297585, "compression_ratio": 1.3506493506493507, "no_speech_prob": 6.8535464379237965e-06}, {"id": 244, "seek": 108456, "start": 1084.56, "end": 1103.56, "text": " So, the thing it's actually using to for that config file is it goes through the basically the Cartesian product of all the values in this YAML. So it's going to do each of these two data sets planets and beds for this one learning rate 0.08 for every one of these models.", "tokens": [407, 11, 264, 551, 309, 311, 767, 1228, 281, 337, 300, 6662, 3991, 307, 309, 1709, 807, 264, 1936, 264, 22478, 42434, 1674, 295, 439, 264, 4190, 294, 341, 398, 2865, 43, 13, 407, 309, 311, 516, 281, 360, 1184, 295, 613, 732, 1412, 6352, 15126, 293, 18068, 337, 341, 472, 2539, 3314, 1958, 13, 16133, 337, 633, 472, 295, 613, 5245, 13], "temperature": 0.0, "avg_logprob": -0.19178379209418045, "compression_ratio": 1.5751295336787565, "no_speech_prob": 7.765902410028502e-06}, {"id": 245, "seek": 108456, "start": 1103.56, "end": 1107.56, "text": " For every one of these poolings", "tokens": [1171, 633, 472, 295, 613, 7005, 1109], "temperature": 0.0, "avg_logprob": -0.19178379209418045, "compression_ratio": 1.5751295336787565, "no_speech_prob": 7.765902410028502e-06}, {"id": 246, "seek": 110756, "start": 1107.56, "end": 1114.56, "text": " for okay this is just the one resize method, and for every one of these experiment numbers.", "tokens": [337, 1392, 341, 307, 445, 264, 472, 50069, 3170, 11, 293, 337, 633, 472, 295, 613, 5120, 3547, 13], "temperature": 0.0, "avg_logprob": -0.1459799101858428, "compression_ratio": 1.4082840236686391, "no_speech_prob": 5.173634235688951e-06}, {"id": 247, "seek": 110756, "start": 1114.56, "end": 1117.56, "text": " So, yeah.", "tokens": [407, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.1459799101858428, "compression_ratio": 1.4082840236686391, "no_speech_prob": 5.173634235688951e-06}, {"id": 248, "seek": 110756, "start": 1117.56, "end": 1120.56, "text": " So that's a lot of projects I have to do at some point.", "tokens": [407, 300, 311, 257, 688, 295, 4455, 286, 362, 281, 360, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.1459799101858428, "compression_ratio": 1.4082840236686391, "no_speech_prob": 5.173634235688951e-06}, {"id": 249, "seek": 110756, "start": 1120.56, "end": 1125.56, "text": " The, the sweep allows you to run arbitrary programs doesn't have to be a script.", "tokens": [440, 11, 264, 22169, 4045, 291, 281, 1190, 23211, 4268, 1177, 380, 362, 281, 312, 257, 5755, 13], "temperature": 0.0, "avg_logprob": -0.1459799101858428, "compression_ratio": 1.4082840236686391, "no_speech_prob": 5.173634235688951e-06}, {"id": 250, "seek": 112556, "start": 1125.56, "end": 1138.56, "text": " So, potentially you could just stay in the notebook and use tiny kernel or sorry, I can be buying thing or whatever it's called. Yeah, it can be. Yeah.", "tokens": [407, 11, 7263, 291, 727, 445, 1754, 294, 264, 21060, 293, 764, 5870, 28256, 420, 2597, 11, 286, 393, 312, 6382, 551, 420, 2035, 309, 311, 1219, 13, 865, 11, 309, 393, 312, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.17400621630481838, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.9032755744774477e-06}, {"id": 251, "seek": 112556, "start": 1138.56, "end": 1154.56, "text": " Yeah, yeah, it'd be fun to work on this to make the whole thing you know, run with notebooks and stick stuff in a local SQL like database and because like all this stuff, all this web GUI stuff, honestly I don't like it at all.", "tokens": [865, 11, 1338, 11, 309, 1116, 312, 1019, 281, 589, 322, 341, 281, 652, 264, 1379, 551, 291, 458, 11, 1190, 365, 43782, 293, 2897, 1507, 294, 257, 2654, 19200, 411, 8149, 293, 570, 411, 439, 341, 1507, 11, 439, 341, 3670, 17917, 40, 1507, 11, 6095, 286, 500, 380, 411, 309, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.17400621630481838, "compression_ratio": 1.6336206896551724, "no_speech_prob": 1.9032755744774477e-06}, {"id": 252, "seek": 115456, "start": 1154.56, "end": 1163.56, "text": " The thing is it actually doesn't matter because I don't have to use it because they provide an API. So before I realized they have a nice API.", "tokens": [440, 551, 307, 309, 767, 1177, 380, 1871, 570, 286, 500, 380, 362, 281, 764, 309, 570, 436, 2893, 364, 9362, 13, 407, 949, 286, 5334, 436, 362, 257, 1481, 9362, 13], "temperature": 0.0, "avg_logprob": -0.11472151212603132, "compression_ratio": 1.7469387755102042, "no_speech_prob": 2.2123209419078194e-05}, {"id": 253, "seek": 115456, "start": 1163.56, "end": 1176.56, "text": " I kept on like sending Thomas these messages saying how do I do this, how do I do that, why isn't this working when you'd have to like send me these like pages of screenshots like click here click there, turn this off, then you have to redo this three times", "tokens": [286, 4305, 322, 411, 7750, 8500, 613, 7897, 1566, 577, 360, 286, 360, 341, 11, 577, 360, 286, 360, 300, 11, 983, 1943, 380, 341, 1364, 562, 291, 1116, 362, 281, 411, 2845, 385, 613, 411, 7183, 295, 40661, 411, 2052, 510, 2052, 456, 11, 1261, 341, 766, 11, 550, 291, 362, 281, 29956, 341, 1045, 1413], "temperature": 0.0, "avg_logprob": -0.11472151212603132, "compression_ratio": 1.7469387755102042, "no_speech_prob": 2.2123209419078194e-05}, {"id": 254, "seek": 115456, "start": 1176.56, "end": 1180.56, "text": " it's like, oh, I hate this.", "tokens": [309, 311, 411, 11, 1954, 11, 286, 4700, 341, 13], "temperature": 0.0, "avg_logprob": -0.11472151212603132, "compression_ratio": 1.7469387755102042, "no_speech_prob": 2.2123209419078194e-05}, {"id": 255, "seek": 118056, "start": 1180.56, "end": 1188.56, "text": " And then I found that within this like we do have an API, and I was like I looked at the API it is so well documented it's got examples.", "tokens": [400, 550, 286, 1352, 300, 1951, 341, 411, 321, 360, 362, 364, 9362, 11, 293, 286, 390, 411, 286, 2956, 412, 264, 9362, 309, 307, 370, 731, 23007, 309, 311, 658, 5110, 13], "temperature": 0.0, "avg_logprob": -0.1214052586073286, "compression_ratio": 1.5270935960591132, "no_speech_prob": 5.862396847078344e-06}, {"id": 256, "seek": 118056, "start": 1188.56, "end": 1194.56, "text": " Yeah, it's, it's really nice.", "tokens": [865, 11, 309, 311, 11, 309, 311, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.1214052586073286, "compression_ratio": 1.5270935960591132, "no_speech_prob": 5.862396847078344e-06}, {"id": 257, "seek": 118056, "start": 1194.56, "end": 1205.56, "text": " So, I've put all the stuff I'm working on into this git repo. And so here's a tip by the way the the information about if you're in a git repo.", "tokens": [407, 11, 286, 600, 829, 439, 264, 1507, 286, 478, 1364, 322, 666, 341, 18331, 49040, 13, 400, 370, 510, 311, 257, 4125, 538, 264, 636, 264, 264, 1589, 466, 498, 291, 434, 294, 257, 18331, 49040, 13], "temperature": 0.0, "avg_logprob": -0.1214052586073286, "compression_ratio": 1.5270935960591132, "no_speech_prob": 5.862396847078344e-06}, {"id": 258, "seek": 120556, "start": 1205.56, "end": 1215.56, "text": " Or at a git directory to clone directory the information about your git repo all lives in a file called.git slash config.", "tokens": [1610, 412, 257, 18331, 21120, 281, 26506, 21120, 264, 1589, 466, 428, 18331, 49040, 439, 2909, 294, 257, 3991, 1219, 2411, 70, 270, 17330, 6662, 13], "temperature": 0.0, "avg_logprob": -0.15575087070465088, "compression_ratio": 1.4148148148148147, "no_speech_prob": 1.0615522114676423e-05}, {"id": 259, "seek": 120556, "start": 1215.56, "end": 1218.56, "text": " So you can see here.", "tokens": [407, 291, 393, 536, 510, 13], "temperature": 0.0, "avg_logprob": -0.15575087070465088, "compression_ratio": 1.4148148148148147, "no_speech_prob": 1.0615522114676423e-05}, {"id": 260, "seek": 120556, "start": 1218.56, "end": 1224.56, "text": " This is the git repo.", "tokens": [639, 307, 264, 18331, 49040, 13], "temperature": 0.0, "avg_logprob": -0.15575087070465088, "compression_ratio": 1.4148148148148147, "no_speech_prob": 1.0615522114676423e-05}, {"id": 261, "seek": 120556, "start": 1224.56, "end": 1228.56, "text": " So if we now go to GitHub.", "tokens": [407, 498, 321, 586, 352, 281, 23331, 13], "temperature": 0.0, "avg_logprob": -0.15575087070465088, "compression_ratio": 1.4148148148148147, "no_speech_prob": 1.0615522114676423e-05}, {"id": 262, "seek": 122856, "start": 1228.56, "end": 1239.56, "text": " One cool thing about this runs is it tracks your git commit, like the run you can get back to what code version. Yeah, that is very cool isn't it. Yeah.", "tokens": [1485, 1627, 551, 466, 341, 6676, 307, 309, 10218, 428, 18331, 5599, 11, 411, 264, 1190, 291, 393, 483, 646, 281, 437, 3089, 3037, 13, 865, 11, 300, 307, 588, 1627, 1943, 380, 309, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.14212306340535483, "compression_ratio": 1.6652719665271967, "no_speech_prob": 5.421844889497152e-06}, {"id": 263, "seek": 122856, "start": 1239.56, "end": 1253.56, "text": " Yeah, I mean, I do think we could pretty easily create a local only version of this without all the fancy GUI, you know, which would also have benefits and people who want the fancy GUI and run stuff from multiple sites stuff like that would use", "tokens": [865, 11, 286, 914, 11, 286, 360, 519, 321, 727, 1238, 3612, 1884, 257, 2654, 787, 3037, 295, 341, 1553, 439, 264, 10247, 17917, 40, 11, 291, 458, 11, 597, 576, 611, 362, 5311, 293, 561, 567, 528, 264, 10247, 17917, 40, 293, 1190, 1507, 490, 3866, 7533, 1507, 411, 300, 576, 764], "temperature": 0.0, "avg_logprob": -0.14212306340535483, "compression_ratio": 1.6652719665271967, "no_speech_prob": 5.421844889497152e-06}, {"id": 264, "seek": 125356, "start": 1253.56, "end": 1268.56, "text": " weights and biases but, you know, you could also do stuff without weights and biases. Anyway, here's our. Yeah, so here's our repo. And this analysis.ipinb is the thing that I showed yesterday.", "tokens": [17443, 293, 32152, 457, 11, 291, 458, 11, 291, 727, 611, 360, 1507, 1553, 17443, 293, 32152, 13, 5684, 11, 510, 311, 527, 13, 865, 11, 370, 510, 311, 527, 49040, 13, 400, 341, 5215, 13, 647, 259, 65, 307, 264, 551, 300, 286, 4712, 5186, 13], "temperature": 0.0, "avg_logprob": -0.17126846313476562, "compression_ratio": 1.5, "no_speech_prob": 1.9636943306977628e-06}, {"id": 265, "seek": 125356, "start": 1268.56, "end": 1273.56, "text": " If you want to check it out.", "tokens": [759, 291, 528, 281, 1520, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.17126846313476562, "compression_ratio": 1.5, "no_speech_prob": 1.9636943306977628e-06}, {"id": 266, "seek": 125356, "start": 1273.56, "end": 1279.56, "text": " I'll put that in the chat.", "tokens": [286, 603, 829, 300, 294, 264, 5081, 13], "temperature": 0.0, "avg_logprob": -0.17126846313476562, "compression_ratio": 1.5, "no_speech_prob": 1.9636943306977628e-06}, {"id": 267, "seek": 127956, "start": 1279.56, "end": 1299.56, "text": " There you go. Oh, by the way, you know, I think something else which would be good is we should start keeping a really good list for every walkthrough of like all the like key resources key like, you know, links key commands examples we wrote and stuff like that.", "tokens": [821, 291, 352, 13, 876, 11, 538, 264, 636, 11, 291, 458, 11, 286, 519, 746, 1646, 597, 576, 312, 665, 307, 321, 820, 722, 5145, 257, 534, 665, 1329, 337, 633, 1792, 11529, 295, 411, 439, 264, 411, 2141, 3593, 2141, 411, 11, 291, 458, 11, 6123, 2141, 16901, 5110, 321, 4114, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.12509619036028463, "compression_ratio": 1.5290697674418605, "no_speech_prob": 2.0143883375567384e-05}, {"id": 268, "seek": 129956, "start": 1299.56, "end": 1319.56, "text": " So I think to do that what we should do is we should turn all of the walkthrough top topics into wikis. I don't know if you folks have used wiki topics before, but basically a wiki topic simply means that everybody will end up with an edit button.", "tokens": [407, 286, 519, 281, 360, 300, 437, 321, 820, 360, 307, 321, 820, 1261, 439, 295, 264, 1792, 11529, 1192, 8378, 666, 261, 1035, 271, 13, 286, 500, 380, 458, 498, 291, 4024, 362, 1143, 261, 9850, 8378, 949, 11, 457, 1936, 257, 261, 9850, 4829, 2935, 1355, 300, 2201, 486, 917, 493, 365, 364, 8129, 2960, 13], "temperature": 0.0, "avg_logprob": -0.07289933576816465, "compression_ratio": 1.5357142857142858, "no_speech_prob": 1.0129499969480094e-05}, {"id": 269, "seek": 129956, "start": 1319.56, "end": 1323.56, "text": " So if I just click.", "tokens": [407, 498, 286, 445, 2052, 13], "temperature": 0.0, "avg_logprob": -0.07289933576816465, "compression_ratio": 1.5357142857142858, "no_speech_prob": 1.0129499969480094e-05}, {"id": 270, "seek": 129956, "start": 1323.56, "end": 1326.56, "text": " Okay, this one already is a wiki.", "tokens": [1033, 11, 341, 472, 1217, 307, 257, 261, 9850, 13], "temperature": 0.0, "avg_logprob": -0.07289933576816465, "compression_ratio": 1.5357142857142858, "no_speech_prob": 1.0129499969480094e-05}, {"id": 271, "seek": 132656, "start": 1326.56, "end": 1335.56, "text": " Right so everybody should find on walkthrough one that you can click edit. Right. And so", "tokens": [1779, 370, 2201, 820, 915, 322, 1792, 11529, 472, 300, 291, 393, 2052, 8129, 13, 1779, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.13068132258173246, "compression_ratio": 1.5212765957446808, "no_speech_prob": 3.393013685126789e-06}, {"id": 272, "seek": 132656, "start": 1335.56, "end": 1349.56, "text": " one thing we put in an edit for example would be probably like often Daniel has these really nice full walkthrough listings we should have like a link to his reply, which you can get by the way by.", "tokens": [472, 551, 321, 829, 294, 364, 8129, 337, 1365, 576, 312, 1391, 411, 2049, 8033, 575, 613, 534, 1481, 1577, 1792, 11529, 45615, 321, 820, 362, 411, 257, 2113, 281, 702, 16972, 11, 597, 291, 393, 483, 538, 264, 636, 538, 13], "temperature": 0.0, "avg_logprob": -0.13068132258173246, "compression_ratio": 1.5212765957446808, "no_speech_prob": 3.393013685126789e-06}, {"id": 273, "seek": 134956, "start": 1349.56, "end": 1360.56, "text": " Okay. I think you click on this little date here. Yes, and that gives you a link directly to the post, which is handy.", "tokens": [1033, 13, 286, 519, 291, 2052, 322, 341, 707, 4002, 510, 13, 1079, 11, 293, 300, 2709, 291, 257, 2113, 3838, 281, 264, 2183, 11, 597, 307, 13239, 13], "temperature": 0.0, "avg_logprob": -0.1289216188284067, "compression_ratio": 1.4775280898876404, "no_speech_prob": 4.682737562688999e-05}, {"id": 274, "seek": 134956, "start": 1360.56, "end": 1363.56, "text": " What about this one.", "tokens": [708, 466, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1289216188284067, "compression_ratio": 1.4775280898876404, "no_speech_prob": 4.682737562688999e-05}, {"id": 275, "seek": 134956, "start": 1363.56, "end": 1365.56, "text": " Okay, make that a wiki.", "tokens": [1033, 11, 652, 300, 257, 261, 9850, 13], "temperature": 0.0, "avg_logprob": -0.1289216188284067, "compression_ratio": 1.4775280898876404, "no_speech_prob": 4.682737562688999e-05}, {"id": 276, "seek": 134956, "start": 1365.56, "end": 1373.56, "text": " So this is going to be a little bit boring for you guys to watch but as we'll do it while I'm here.", "tokens": [407, 341, 307, 516, 281, 312, 257, 707, 857, 9989, 337, 291, 1074, 281, 1159, 457, 382, 321, 603, 360, 309, 1339, 286, 478, 510, 13], "temperature": 0.0, "avg_logprob": -0.1289216188284067, "compression_ratio": 1.4775280898876404, "no_speech_prob": 4.682737562688999e-05}, {"id": 277, "seek": 137356, "start": 1373.56, "end": 1387.56, "text": " If anyone else has any questions or comments while I do that. Yeah, Jeremy, you get the fast GPU is possible to expand to high performance computing to use it on the note.", "tokens": [759, 2878, 1646, 575, 604, 1651, 420, 3053, 1339, 286, 360, 300, 13, 865, 11, 17809, 11, 291, 483, 264, 2370, 18407, 307, 1944, 281, 5268, 281, 1090, 3389, 15866, 281, 764, 309, 322, 264, 3637, 13], "temperature": 0.0, "avg_logprob": -0.20058469629999418, "compression_ratio": 1.5526315789473684, "no_speech_prob": 7.601173274451867e-05}, {"id": 278, "seek": 137356, "start": 1387.56, "end": 1399.56, "text": " Sorry to do what apply in high performance computing so in the distributed environment. Is it possible to track it as well.", "tokens": [4919, 281, 360, 437, 3079, 294, 1090, 3389, 15866, 370, 294, 264, 12631, 2823, 13, 1119, 309, 1944, 281, 2837, 309, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.20058469629999418, "compression_ratio": 1.5526315789473684, "no_speech_prob": 7.601173274451867e-05}, {"id": 279, "seek": 139956, "start": 1399.56, "end": 1415.56, "text": " No, I mean, it. Yeah, I mean, anything that's running on in in Python on a Linux computer should be fine.", "tokens": [883, 11, 286, 914, 11, 309, 13, 865, 11, 286, 914, 11, 1340, 300, 311, 2614, 322, 294, 294, 15329, 322, 257, 18734, 3820, 820, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.21257412433624268, "compression_ratio": 1.1666666666666667, "no_speech_prob": 4.637000529328361e-06}, {"id": 280, "seek": 141556, "start": 1415.56, "end": 1430.56, "text": " I think some HPC things are like, use their own weird job scheduling systems and stuff. But yeah, as long as it's running a normal in video.", "tokens": [286, 519, 512, 12557, 34, 721, 366, 411, 11, 764, 641, 1065, 3657, 1691, 29055, 3652, 293, 1507, 13, 583, 1338, 11, 382, 938, 382, 309, 311, 2614, 257, 2710, 294, 960, 13], "temperature": 0.0, "avg_logprob": -0.1372799389604209, "compression_ratio": 1.598802395209581, "no_speech_prob": 2.6013563001470175e-06}, {"id": 281, "seek": 141556, "start": 1430.56, "end": 1439.56, "text": " It doesn't even have to be in video honestly. But yeah, as long as it's running a normal Linux environment, it should be fine.", "tokens": [467, 1177, 380, 754, 362, 281, 312, 294, 960, 6095, 13, 583, 1338, 11, 382, 938, 382, 309, 311, 2614, 257, 2710, 18734, 2823, 11, 309, 820, 312, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1372799389604209, "compression_ratio": 1.598802395209581, "no_speech_prob": 2.6013563001470175e-06}, {"id": 282, "seek": 143956, "start": 1439.56, "end": 1450.56, "text": " It's pretty generic, pretty general. Okay, so they are now all wikis and so something I did the other day for example was in walkthrough for.", "tokens": [467, 311, 1238, 19577, 11, 1238, 2674, 13, 1033, 11, 370, 436, 366, 586, 439, 261, 1035, 271, 293, 370, 746, 286, 630, 264, 661, 786, 337, 1365, 390, 294, 1792, 11529, 337, 13], "temperature": 0.0, "avg_logprob": -0.12611349617562642, "compression_ratio": 1.5686274509803921, "no_speech_prob": 9.515320016362239e-06}, {"id": 283, "seek": 143956, "start": 1450.56, "end": 1458.56, "text": " I added something saying like oh this is the one where we actually had a bug and you need to add CD at the end, you know, and I tried to create a little list of what was covered.", "tokens": [286, 3869, 746, 1566, 411, 1954, 341, 307, 264, 472, 689, 321, 767, 632, 257, 7426, 293, 291, 643, 281, 909, 6743, 412, 264, 917, 11, 291, 458, 11, 293, 286, 3031, 281, 1884, 257, 707, 1329, 295, 437, 390, 5343, 13], "temperature": 0.0, "avg_logprob": -0.12611349617562642, "compression_ratio": 1.5686274509803921, "no_speech_prob": 9.515320016362239e-06}, {"id": 284, "seek": 145856, "start": 1458.56, "end": 1473.56, "text": " So for example maybe Matt's fantastic timestamps we could copy and pastors list items into here for instance.", "tokens": [407, 337, 1365, 1310, 7397, 311, 5456, 49108, 23150, 321, 727, 5055, 293, 1791, 830, 1329, 4754, 666, 510, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.17127772894772617, "compression_ratio": 1.31496062992126, "no_speech_prob": 6.437592674046755e-06}, {"id": 285, "seek": 145856, "start": 1473.56, "end": 1482.56, "text": " Some of Radix examples, maybe, or even just a link to it.", "tokens": [2188, 295, 9654, 970, 5110, 11, 1310, 11, 420, 754, 445, 257, 2113, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.17127772894772617, "compression_ratio": 1.31496062992126, "no_speech_prob": 6.437592674046755e-06}, {"id": 286, "seek": 148256, "start": 1482.56, "end": 1506.56, "text": " Yeah, so for this walkthrough we should certainly include this link to the analysis.ipy and B. Anyway, so you can see, yeah, with the API, it was just so easy just to go api.sweep.runs comes in as a dictionary, which we can then chuck a list of dictionaries into a data frame.", "tokens": [865, 11, 370, 337, 341, 1792, 11529, 321, 820, 3297, 4090, 341, 2113, 281, 264, 5215, 13, 647, 88, 293, 363, 13, 5684, 11, 370, 291, 393, 536, 11, 1338, 11, 365, 264, 9362, 11, 309, 390, 445, 370, 1858, 445, 281, 352, 1882, 72, 13, 82, 826, 595, 13, 12997, 82, 1487, 294, 382, 257, 25890, 11, 597, 321, 393, 550, 20870, 257, 1329, 295, 22352, 4889, 666, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.21730710314465806, "compression_ratio": 1.491891891891892, "no_speech_prob": 1.2409987903083675e-05}, {"id": 287, "seek": 150656, "start": 1506.56, "end": 1523.56, "text": " Okay. I'm rerunning the whole lot by the way because it turns out I made a mistake at some point I thought that Thomas had told me that squish was always better than crop for resizing and he told me I was exactly wrong and it's actually the crops always", "tokens": [1033, 13, 286, 478, 43819, 25589, 264, 1379, 688, 538, 264, 636, 570, 309, 4523, 484, 286, 1027, 257, 6146, 412, 512, 935, 286, 1194, 300, 8500, 632, 1907, 385, 300, 31379, 390, 1009, 1101, 813, 9086, 337, 725, 3319, 293, 415, 1907, 385, 286, 390, 2293, 2085, 293, 309, 311, 767, 264, 16829, 1009], "temperature": 0.0, "avg_logprob": -0.14902398778104234, "compression_ratio": 1.75, "no_speech_prob": 2.7965548724750988e-05}, {"id": 288, "seek": 150656, "start": 1523.56, "end": 1528.56, "text": " better than squish resizing so I'm rerunning the whole lot.", "tokens": [1101, 813, 31379, 725, 3319, 370, 286, 478, 43819, 25589, 264, 1379, 688, 13], "temperature": 0.0, "avg_logprob": -0.14902398778104234, "compression_ratio": 1.75, "no_speech_prob": 2.7965548724750988e-05}, {"id": 289, "seek": 150656, "start": 1528.56, "end": 1534.56, "text": " It is annoying but shouldn't take too long.", "tokens": [467, 307, 11304, 457, 4659, 380, 747, 886, 938, 13], "temperature": 0.0, "avg_logprob": -0.14902398778104234, "compression_ratio": 1.75, "no_speech_prob": 2.7965548724750988e-05}, {"id": 290, "seek": 153456, "start": 1534.56, "end": 1550.56, "text": " Do you find that analyzing the sweep results like this was useful in relative to like what you can see in the, the UI, you know you can so much better Hamill yes so much.", "tokens": [1144, 291, 915, 300, 23663, 264, 22169, 3542, 411, 341, 390, 4420, 294, 4972, 281, 411, 437, 291, 393, 536, 294, 264, 11, 264, 15682, 11, 291, 458, 291, 393, 370, 709, 1101, 8234, 373, 2086, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.19046978950500487, "compression_ratio": 1.4451612903225806, "no_speech_prob": 1.5686046026530676e-05}, {"id": 291, "seek": 153456, "start": 1550.56, "end": 1556.56, "text": " I was like, I mean they've done a good job with that.", "tokens": [286, 390, 411, 11, 286, 914, 436, 600, 1096, 257, 665, 1691, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.19046978950500487, "compression_ratio": 1.4451612903225806, "no_speech_prob": 1.5686046026530676e-05}, {"id": 292, "seek": 155656, "start": 1556.56, "end": 1572.56, "text": " With that UI like it's very sophisticated and clever and stuff but I just never got to be friends with it and as soon as I turn it into a data frame I was just like, okay now I can get exactly what I want straight away, it was absolute breath of fresh air, frankly.", "tokens": [2022, 300, 15682, 411, 309, 311, 588, 16950, 293, 13494, 293, 1507, 457, 286, 445, 1128, 658, 281, 312, 1855, 365, 309, 293, 382, 2321, 382, 286, 1261, 309, 666, 257, 1412, 3920, 286, 390, 445, 411, 11, 1392, 586, 286, 393, 483, 2293, 437, 286, 528, 2997, 1314, 11, 309, 390, 8236, 6045, 295, 4451, 1988, 11, 11939, 13], "temperature": 0.0, "avg_logprob": -0.13513639810922984, "compression_ratio": 1.526829268292683, "no_speech_prob": 3.237455530324951e-06}, {"id": 293, "seek": 155656, "start": 1572.56, "end": 1575.56, "text": " I really like their parallel coordinates chart.", "tokens": [286, 534, 411, 641, 8952, 21056, 6927, 13], "temperature": 0.0, "avg_logprob": -0.13513639810922984, "compression_ratio": 1.526829268292683, "no_speech_prob": 3.237455530324951e-06}, {"id": 294, "seek": 157556, "start": 1575.56, "end": 1591.56, "text": " I find it very difficult to reproduce that in like any visualization library, like in a way I don't like the parallel coordinates chart but yeah I mean, there must be parallel coordinates chart for Python out there.", "tokens": [286, 915, 309, 588, 2252, 281, 29501, 300, 294, 411, 604, 25801, 6405, 11, 411, 294, 257, 636, 286, 500, 380, 411, 264, 8952, 21056, 6927, 457, 1338, 286, 914, 11, 456, 1633, 312, 8952, 21056, 6927, 337, 15329, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.21094383856262824, "compression_ratio": 1.7046413502109705, "no_speech_prob": 5.771188625658397e-06}, {"id": 295, "seek": 157556, "start": 1591.56, "end": 1598.56, "text": " Oh there is there's like a plotly one but it's not that nice. Okay, because I don't bother with it.", "tokens": [876, 456, 307, 456, 311, 411, 257, 7542, 356, 472, 457, 309, 311, 406, 300, 1481, 13, 1033, 11, 570, 286, 500, 380, 8677, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.21094383856262824, "compression_ratio": 1.7046413502109705, "no_speech_prob": 5.771188625658397e-06}, {"id": 296, "seek": 157556, "start": 1598.56, "end": 1603.56, "text": " Like hover over it and stuff and see, you know, what is today, did they write their own.", "tokens": [1743, 20076, 670, 309, 293, 1507, 293, 536, 11, 291, 458, 11, 437, 307, 965, 11, 630, 436, 2464, 641, 1065, 13], "temperature": 0.0, "avg_logprob": -0.21094383856262824, "compression_ratio": 1.7046413502109705, "no_speech_prob": 5.771188625658397e-06}, {"id": 297, "seek": 160356, "start": 1603.56, "end": 1608.56, "text": " I think so. Yeah, that's impressive.", "tokens": [286, 519, 370, 13, 865, 11, 300, 311, 8992, 13], "temperature": 0.0, "avg_logprob": -0.1778988661589446, "compression_ratio": 1.4966442953020134, "no_speech_prob": 6.7463879531715065e-06}, {"id": 298, "seek": 160356, "start": 1608.56, "end": 1620.56, "text": " And they kind of wrote their own data frame kind of language, their own visualization library, like in a sense, it's like those weights and biases reports and they have their own syntax.", "tokens": [400, 436, 733, 295, 4114, 641, 1065, 1412, 3920, 733, 295, 2856, 11, 641, 1065, 25801, 6405, 11, 411, 294, 257, 2020, 11, 309, 311, 411, 729, 17443, 293, 32152, 7122, 293, 436, 362, 641, 1065, 28431, 13], "temperature": 0.0, "avg_logprob": -0.1778988661589446, "compression_ratio": 1.4966442953020134, "no_speech_prob": 6.7463879531715065e-06}, {"id": 299, "seek": 162056, "start": 1620.56, "end": 1635.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19340576315825841, "compression_ratio": 1.346774193548387, "no_speech_prob": 1.0951536751235835e-05}, {"id": 300, "seek": 162056, "start": 1635.56, "end": 1638.56, "text": " There isn't one in plotly or something.", "tokens": [821, 1943, 380, 472, 294, 7542, 356, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.19340576315825841, "compression_ratio": 1.346774193548387, "no_speech_prob": 1.0951536751235835e-05}, {"id": 301, "seek": 162056, "start": 1638.56, "end": 1644.56, "text": " Yeah, there's one in plotly for sure. Plotly things are normally interactive so have you tried that.", "tokens": [865, 11, 456, 311, 472, 294, 7542, 356, 337, 988, 13, 2149, 310, 356, 721, 366, 5646, 15141, 370, 362, 291, 3031, 300, 13], "temperature": 0.0, "avg_logprob": -0.19340576315825841, "compression_ratio": 1.346774193548387, "no_speech_prob": 1.0951536751235835e-05}, {"id": 302, "seek": 162056, "start": 1644.56, "end": 1646.56, "text": " Do you know if it's.", "tokens": [1144, 291, 458, 498, 309, 311, 13], "temperature": 0.0, "avg_logprob": -0.19340576315825841, "compression_ratio": 1.346774193548387, "no_speech_prob": 1.0951536751235835e-05}, {"id": 303, "seek": 164656, "start": 1646.56, "end": 1650.56, "text": " Yeah, it works.", "tokens": [865, 11, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1697086921105018, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.3210846191213932e-05}, {"id": 304, "seek": 164656, "start": 1650.56, "end": 1660.56, "text": " It's just, it's not as nice but yeah it works like when you hover over, like, there's a, there's at least a version doesn't. Yeah, that one.", "tokens": [467, 311, 445, 11, 309, 311, 406, 382, 1481, 457, 1338, 309, 1985, 411, 562, 291, 20076, 670, 11, 411, 11, 456, 311, 257, 11, 456, 311, 412, 1935, 257, 3037, 1177, 380, 13, 865, 11, 300, 472, 13], "temperature": 0.0, "avg_logprob": -0.1697086921105018, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.3210846191213932e-05}, {"id": 305, "seek": 164656, "start": 1660.56, "end": 1665.56, "text": " Like, it's very fiddly, you might have to draw a box around it.", "tokens": [1743, 11, 309, 311, 588, 283, 14273, 356, 11, 291, 1062, 362, 281, 2642, 257, 2424, 926, 309, 13], "temperature": 0.0, "avg_logprob": -0.1697086921105018, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.3210846191213932e-05}, {"id": 306, "seek": 164656, "start": 1665.56, "end": 1670.56, "text": " To, to, to highlight it.", "tokens": [1407, 11, 281, 11, 281, 5078, 309, 13], "temperature": 0.0, "avg_logprob": -0.1697086921105018, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.3210846191213932e-05}, {"id": 307, "seek": 164656, "start": 1670.56, "end": 1675.56, "text": " Oh, yeah, you know. Okay, so you just drag over it. That's not terrible.", "tokens": [876, 11, 1338, 11, 291, 458, 13, 1033, 11, 370, 291, 445, 5286, 670, 309, 13, 663, 311, 406, 6237, 13], "temperature": 0.0, "avg_logprob": -0.1697086921105018, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.3210846191213932e-05}, {"id": 308, "seek": 167556, "start": 1675.56, "end": 1679.56, "text": " It's okay it's not the best UI.", "tokens": [467, 311, 1392, 309, 311, 406, 264, 1151, 15682, 13], "temperature": 0.0, "avg_logprob": -0.15768522586462633, "compression_ratio": 1.731818181818182, "no_speech_prob": 2.7530681109055877e-05}, {"id": 309, "seek": 167556, "start": 1679.56, "end": 1683.56, "text": " But, you know,", "tokens": [583, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.15768522586462633, "compression_ratio": 1.731818181818182, "no_speech_prob": 2.7530681109055877e-05}, {"id": 310, "seek": 167556, "start": 1683.56, "end": 1687.56, "text": " Okay, this is thanks for telling me about this it's cool.", "tokens": [1033, 11, 341, 307, 3231, 337, 3585, 385, 466, 341, 309, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.15768522586462633, "compression_ratio": 1.731818181818182, "no_speech_prob": 2.7530681109055877e-05}, {"id": 311, "seek": 167556, "start": 1687.56, "end": 1702.56, "text": " You don't think you don't you don't like this that much it's not that useful for you. I haven't managed to. I mean I know other people like it so I don't doubt that it's useful for something it's just apparently not useful for the things I've tried to use it for yet, somehow.", "tokens": [509, 500, 380, 519, 291, 500, 380, 291, 500, 380, 411, 341, 300, 709, 309, 311, 406, 300, 4420, 337, 291, 13, 286, 2378, 380, 6453, 281, 13, 286, 914, 286, 458, 661, 561, 411, 309, 370, 286, 500, 380, 6385, 300, 309, 311, 4420, 337, 746, 309, 311, 445, 7970, 406, 4420, 337, 264, 721, 286, 600, 3031, 281, 764, 309, 337, 1939, 11, 6063, 13], "temperature": 0.0, "avg_logprob": -0.15768522586462633, "compression_ratio": 1.731818181818182, "no_speech_prob": 2.7530681109055877e-05}, {"id": 312, "seek": 170256, "start": 1702.56, "end": 1712.56, "text": " Yeah, I mean how do you do you kind of like drag over the end bit to see where they come from or something. Yeah, I mean it might be useful if you want to look at the weights and biases one.", "tokens": [865, 11, 286, 914, 577, 360, 291, 360, 291, 733, 295, 411, 5286, 670, 264, 917, 857, 281, 536, 689, 436, 808, 490, 420, 746, 13, 865, 11, 286, 914, 309, 1062, 312, 4420, 498, 291, 528, 281, 574, 412, 264, 17443, 293, 32152, 472, 13], "temperature": 0.0, "avg_logprob": -0.14595929781595865, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.4969808034948073e-05}, {"id": 313, "seek": 170256, "start": 1712.56, "end": 1723.56, "text": " So I think it renders one by default for you for the runs. Yeah, yeah, it does. It's easier to like, let's check it out. Operate that yeah.", "tokens": [407, 286, 519, 309, 6125, 433, 472, 538, 7576, 337, 291, 337, 264, 6676, 13, 865, 11, 1338, 11, 309, 775, 13, 467, 311, 3571, 281, 411, 11, 718, 311, 1520, 309, 484, 13, 12480, 473, 300, 1338, 13], "temperature": 0.0, "avg_logprob": -0.14595929781595865, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.4969808034948073e-05}, {"id": 314, "seek": 170256, "start": 1723.56, "end": 1729.56, "text": " W and B", "tokens": [343, 293, 363], "temperature": 0.0, "avg_logprob": -0.14595929781595865, "compression_ratio": 1.6018957345971565, "no_speech_prob": 1.4969808034948073e-05}, {"id": 315, "seek": 172956, "start": 1729.56, "end": 1741.56, "text": " and B.", "tokens": [293, 363, 13], "temperature": 0.0, "avg_logprob": -0.34213429405575707, "compression_ratio": 1.0957446808510638, "no_speech_prob": 1.3004040738451295e-05}, {"id": 316, "seek": 172956, "start": 1741.56, "end": 1745.56, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.34213429405575707, "compression_ratio": 1.0957446808510638, "no_speech_prob": 1.3004040738451295e-05}, {"id": 317, "seek": 172956, "start": 1745.56, "end": 1749.56, "text": " I think it could be in the sweeps thing.", "tokens": [286, 519, 309, 727, 312, 294, 264, 2484, 10653, 551, 13], "temperature": 0.0, "avg_logprob": -0.34213429405575707, "compression_ratio": 1.0957446808510638, "no_speech_prob": 1.3004040738451295e-05}, {"id": 318, "seek": 172956, "start": 1749.56, "end": 1752.56, "text": " Most likely. Okay.", "tokens": [4534, 3700, 13, 1033, 13], "temperature": 0.0, "avg_logprob": -0.34213429405575707, "compression_ratio": 1.0957446808510638, "no_speech_prob": 1.3004040738451295e-05}, {"id": 319, "seek": 172956, "start": 1752.56, "end": 1756.56, "text": " Then, yeah, pick a sweep.", "tokens": [1396, 11, 1338, 11, 1888, 257, 22169, 13], "temperature": 0.0, "avg_logprob": -0.34213429405575707, "compression_ratio": 1.0957446808510638, "no_speech_prob": 1.3004040738451295e-05}, {"id": 320, "seek": 175656, "start": 1756.56, "end": 1763.56, "text": " Maybe that one. Okay, and then, yeah. Okay, so here we go.", "tokens": [2704, 300, 472, 13, 1033, 11, 293, 550, 11, 1338, 13, 1033, 11, 370, 510, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.19891479640331083, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.66995214385679e-05}, {"id": 321, "seek": 175656, "start": 1763.56, "end": 1767.56, "text": " And then when you just hover over a section.", "tokens": [400, 550, 562, 291, 445, 20076, 670, 257, 3541, 13], "temperature": 0.0, "avg_logprob": -0.19891479640331083, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.66995214385679e-05}, {"id": 322, "seek": 175656, "start": 1767.56, "end": 1771.56, "text": " See, I don't see how this is helping me.", "tokens": [3008, 11, 286, 500, 380, 536, 577, 341, 307, 4315, 385, 13], "temperature": 0.0, "avg_logprob": -0.19891479640331083, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.66995214385679e-05}, {"id": 323, "seek": 175656, "start": 1771.56, "end": 1773.56, "text": " Well, I guess like saying so.", "tokens": [1042, 11, 286, 2041, 411, 1566, 370, 13], "temperature": 0.0, "avg_logprob": -0.19891479640331083, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.66995214385679e-05}, {"id": 324, "seek": 175656, "start": 1773.56, "end": 1783.56, "text": " No, no, I mean, so there's not that much variance in the, well I guess like what is the metric. We're trying to optimize doesn't really seem like it's even on this chart.", "tokens": [883, 11, 572, 11, 286, 914, 11, 370, 456, 311, 406, 300, 709, 21977, 294, 264, 11, 731, 286, 2041, 411, 437, 307, 264, 20678, 13, 492, 434, 1382, 281, 19719, 1177, 380, 534, 1643, 411, 309, 311, 754, 322, 341, 6927, 13], "temperature": 0.0, "avg_logprob": -0.19891479640331083, "compression_ratio": 1.5753424657534247, "no_speech_prob": 1.66995214385679e-05}, {"id": 325, "seek": 178356, "start": 1783.56, "end": 1792.56, "text": " You know what I mean. Oh, you know what you probably have to tell it what your metric is, and we probably didn't. So the far right hand thing is resize method rather than.", "tokens": [509, 458, 437, 286, 914, 13, 876, 11, 291, 458, 437, 291, 1391, 362, 281, 980, 309, 437, 428, 20678, 307, 11, 293, 321, 1391, 994, 380, 13, 407, 264, 1400, 558, 1011, 551, 307, 50069, 3170, 2831, 813, 13], "temperature": 0.0, "avg_logprob": -0.1921825408935547, "compression_ratio": 1.625615763546798, "no_speech_prob": 8.2675032899715e-06}, {"id": 326, "seek": 178356, "start": 1792.56, "end": 1795.56, "text": " Yeah. So that's.", "tokens": [865, 13, 407, 300, 311, 13], "temperature": 0.0, "avg_logprob": -0.1921825408935547, "compression_ratio": 1.625615763546798, "no_speech_prob": 8.2675032899715e-06}, {"id": 327, "seek": 178356, "start": 1795.56, "end": 1802.56, "text": " Is there some way to tell it that we care about. Yeah, there's an edit, there's like a little pencil, let's see.", "tokens": [1119, 456, 512, 636, 281, 980, 309, 300, 321, 1127, 466, 13, 865, 11, 456, 311, 364, 8129, 11, 456, 311, 411, 257, 707, 10985, 11, 718, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.1921825408935547, "compression_ratio": 1.625615763546798, "no_speech_prob": 8.2675032899715e-06}, {"id": 328, "seek": 178356, "start": 1802.56, "end": 1809.56, "text": " Okay, add the column for add", "tokens": [1033, 11, 909, 264, 7738, 337, 909], "temperature": 0.0, "avg_logprob": -0.1921825408935547, "compression_ratio": 1.625615763546798, "no_speech_prob": 8.2675032899715e-06}, {"id": 329, "seek": 180956, "start": 1809.56, "end": 1814.56, "text": " like loss or something.", "tokens": [411, 4470, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.3201286262936062, "compression_ratio": 1.1111111111111112, "no_speech_prob": 5.474889985634945e-05}, {"id": 330, "seek": 180956, "start": 1814.56, "end": 1818.56, "text": " Wait, this is no let's do accuracy and multi.", "tokens": [3802, 11, 341, 307, 572, 718, 311, 360, 14170, 293, 4825, 13], "temperature": 0.0, "avg_logprob": -0.3201286262936062, "compression_ratio": 1.1111111111111112, "no_speech_prob": 5.474889985634945e-05}, {"id": 331, "seek": 180956, "start": 1818.56, "end": 1821.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3201286262936062, "compression_ratio": 1.1111111111111112, "no_speech_prob": 5.474889985634945e-05}, {"id": 332, "seek": 180956, "start": 1821.56, "end": 1824.56, "text": " Okay, now we're talking.", "tokens": [1033, 11, 586, 321, 434, 1417, 13], "temperature": 0.0, "avg_logprob": -0.3201286262936062, "compression_ratio": 1.1111111111111112, "no_speech_prob": 5.474889985634945e-05}, {"id": 333, "seek": 182456, "start": 1824.56, "end": 1841.56, "text": " We probably want to get rid of pool and resize method since they don't have any variance.", "tokens": [492, 1391, 528, 281, 483, 3973, 295, 7005, 293, 50069, 3170, 1670, 436, 500, 380, 362, 604, 21977, 13], "temperature": 0.0, "avg_logprob": -0.24519952705928258, "compression_ratio": 1.401360544217687, "no_speech_prob": 8.664184861117974e-06}, {"id": 334, "seek": 182456, "start": 1841.56, "end": 1842.56, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.24519952705928258, "compression_ratio": 1.401360544217687, "no_speech_prob": 8.664184861117974e-06}, {"id": 335, "seek": 182456, "start": 1842.56, "end": 1851.56, "text": " There we go now you can like cover over I actually want to do the thing I we go can I do this track area.", "tokens": [821, 321, 352, 586, 291, 393, 411, 2060, 670, 286, 767, 528, 281, 360, 264, 551, 286, 321, 352, 393, 286, 360, 341, 2837, 1859, 13], "temperature": 0.0, "avg_logprob": -0.24519952705928258, "compression_ratio": 1.401360544217687, "no_speech_prob": 8.664184861117974e-06}, {"id": 336, "seek": 185156, "start": 1851.56, "end": 1861.56, "text": " Yeah, that doesn't mean this is definitely not going to tell me more than the number of experiments is not either.", "tokens": [865, 11, 300, 1177, 380, 914, 341, 307, 2138, 406, 516, 281, 980, 385, 544, 813, 264, 1230, 295, 12050, 307, 406, 2139, 13], "temperature": 0.0, "avg_logprob": -0.20086279782381924, "compression_ratio": 1.4492753623188406, "no_speech_prob": 8.800298928690609e-06}, {"id": 337, "seek": 185156, "start": 1861.56, "end": 1864.56, "text": " No, that's true because there.", "tokens": [883, 11, 300, 311, 2074, 570, 456, 13], "temperature": 0.0, "avg_logprob": -0.20086279782381924, "compression_ratio": 1.4492753623188406, "no_speech_prob": 8.800298928690609e-06}, {"id": 338, "seek": 185156, "start": 1864.56, "end": 1866.56, "text": " This is some other true thing anyway.", "tokens": [639, 307, 512, 661, 2074, 551, 4033, 13], "temperature": 0.0, "avg_logprob": -0.20086279782381924, "compression_ratio": 1.4492753623188406, "no_speech_prob": 8.800298928690609e-06}, {"id": 339, "seek": 185156, "start": 1866.56, "end": 1872.56, "text": " There's a thing.", "tokens": [821, 311, 257, 551, 13], "temperature": 0.0, "avg_logprob": -0.20086279782381924, "compression_ratio": 1.4492753623188406, "no_speech_prob": 8.800298928690609e-06}, {"id": 340, "seek": 187256, "start": 1872.56, "end": 1882.56, "text": " Yeah, sometimes I learned something sometimes I don't from that visualization you know, not always.", "tokens": [865, 11, 2171, 286, 3264, 746, 2171, 286, 500, 380, 490, 300, 25801, 291, 458, 11, 406, 1009, 13], "temperature": 0.0, "avg_logprob": -0.27963878262427544, "compression_ratio": 1.1720430107526882, "no_speech_prob": 2.3317554678214947e-06}, {"id": 341, "seek": 187256, "start": 1882.56, "end": 1883.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.27963878262427544, "compression_ratio": 1.1720430107526882, "no_speech_prob": 2.3317554678214947e-06}, {"id": 342, "seek": 187256, "start": 1883.56, "end": 1888.56, "text": " So,", "tokens": [407, 11], "temperature": 0.0, "avg_logprob": -0.27963878262427544, "compression_ratio": 1.1720430107526882, "no_speech_prob": 2.3317554678214947e-06}, {"id": 343, "seek": 188856, "start": 1888.56, "end": 1906.56, "text": " I'm going to do the control PD to detach.", "tokens": [286, 478, 516, 281, 360, 264, 1969, 10464, 281, 43245, 13], "temperature": 0.0, "avg_logprob": -0.9280389149983724, "compression_ratio": 0.8913043478260869, "no_speech_prob": 1.3287491356095416e-06}, {"id": 344, "seek": 190656, "start": 1906.56, "end": 1921.56, "text": " So, in general, I don't do hyper parameter Bayesian hyper parameter stuff ever.", "tokens": [407, 11, 294, 2674, 11, 286, 500, 380, 360, 9848, 13075, 7840, 42434, 9848, 13075, 1507, 1562, 13], "temperature": 0.0, "avg_logprob": -0.33496965061534534, "compression_ratio": 1.144927536231884, "no_speech_prob": 5.421848072728608e-06}, {"id": 345, "seek": 192156, "start": 1921.56, "end": 1941.56, "text": " Which actually tells you this is not quite true I've used it once, and I used it specifically for finding a good set of dropouts for a WD LSTM, because there's like five of them. And I told Lucas about how I had like created a random forest that actually tries to, you know, predict", "tokens": [3013, 767, 5112, 291, 341, 307, 406, 1596, 2074, 286, 600, 1143, 309, 1564, 11, 293, 286, 1143, 309, 4682, 337, 5006, 257, 665, 992, 295, 3270, 7711, 337, 257, 343, 35, 441, 6840, 44, 11, 570, 456, 311, 411, 1732, 295, 552, 13, 400, 286, 1907, 19178, 466, 577, 286, 632, 411, 2942, 257, 4974, 6719, 300, 767, 9898, 281, 11, 291, 458, 11, 6069], "temperature": 0.0, "avg_logprob": -0.15240780966622489, "compression_ratio": 1.4536082474226804, "no_speech_prob": 1.5934121620375663e-05}, {"id": 346, "seek": 194156, "start": 1941.56, "end": 1953.56, "text": " how accurate something's going to be and then use that random forest to actually target better sets of hyper parameters. And then, yeah, that's what they ended up using for weights and biases which is really cool.", "tokens": [577, 8559, 746, 311, 516, 281, 312, 293, 550, 764, 300, 4974, 6719, 281, 767, 3779, 1101, 6352, 295, 9848, 9834, 13, 400, 550, 11, 1338, 11, 300, 311, 437, 436, 4590, 493, 1228, 337, 17443, 293, 32152, 597, 307, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.0845876121520996, "compression_ratio": 1.749034749034749, "no_speech_prob": 5.093294021207839e-06}, {"id": 347, "seek": 194156, "start": 1953.56, "end": 1957.56, "text": " But I kind of like to really", "tokens": [583, 286, 733, 295, 411, 281, 534], "temperature": 0.0, "avg_logprob": -0.0845876121520996, "compression_ratio": 1.749034749034749, "no_speech_prob": 5.093294021207839e-06}, {"id": 348, "seek": 194156, "start": 1957.56, "end": 1970.56, "text": " use a much more human driven approach from like well what's the hypothesis I'm trying to test how can I test that as fast as possible, like, most hyper parameters are independent of most other hyper parameters.", "tokens": [764, 257, 709, 544, 1952, 9555, 3109, 490, 411, 731, 437, 311, 264, 17291, 286, 478, 1382, 281, 1500, 577, 393, 286, 1500, 300, 382, 2370, 382, 1944, 11, 411, 11, 881, 9848, 9834, 366, 6695, 295, 881, 661, 9848, 9834, 13], "temperature": 0.0, "avg_logprob": -0.0845876121520996, "compression_ratio": 1.749034749034749, "no_speech_prob": 5.093294021207839e-06}, {"id": 349, "seek": 197056, "start": 1970.56, "end": 1980.56, "text": " So, you know, like you don't have to do a huge grid search whatever and you can figure out so for example in this case it's like okay well learning rate of point oh oh eight was basically always the best.", "tokens": [407, 11, 291, 458, 11, 411, 291, 500, 380, 362, 281, 360, 257, 2603, 10748, 3164, 2035, 293, 291, 393, 2573, 484, 370, 337, 1365, 294, 341, 1389, 309, 311, 411, 1392, 731, 2539, 3314, 295, 935, 1954, 1954, 3180, 390, 1936, 1009, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.11060347857775989, "compression_ratio": 1.7876447876447876, "no_speech_prob": 3.2374696274928283e-06}, {"id": 350, "seek": 197056, "start": 1980.56, "end": 1988.56, "text": " So let's not try every learning rate for every model for every resize type, etc. That that's just use that learning rate.", "tokens": [407, 718, 311, 406, 853, 633, 2539, 3314, 337, 633, 2316, 337, 633, 50069, 2010, 11, 5183, 13, 663, 300, 311, 445, 764, 300, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.11060347857775989, "compression_ratio": 1.7876447876447876, "no_speech_prob": 3.2374696274928283e-06}, {"id": 351, "seek": 197056, "start": 1988.56, "end": 1996.56, "text": " Same thing for resize method, you know, crop was always better for the few things we tried it on so don't have to try every combination.", "tokens": [10635, 551, 337, 50069, 3170, 11, 291, 458, 11, 9086, 390, 1009, 1101, 337, 264, 1326, 721, 321, 3031, 309, 322, 370, 500, 380, 362, 281, 853, 633, 6562, 13], "temperature": 0.0, "avg_logprob": -0.11060347857775989, "compression_ratio": 1.7876447876447876, "no_speech_prob": 3.2374696274928283e-06}, {"id": 352, "seek": 199656, "start": 1996.56, "end": 2000.56, "text": " And also like I feel like I learned a lot more about deep learning.", "tokens": [400, 611, 411, 286, 841, 411, 286, 3264, 257, 688, 544, 466, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.10302724157060895, "compression_ratio": 1.903225806451613, "no_speech_prob": 3.8442944969574455e-06}, {"id": 353, "seek": 199656, "start": 2000.56, "end": 2010.56, "text": " When I, you know, ask like well what do I want to know about this thing or is that thing independent of that other thing or is it, or are they connected or not.", "tokens": [1133, 286, 11, 291, 458, 11, 1029, 411, 731, 437, 360, 286, 528, 281, 458, 466, 341, 551, 420, 307, 300, 551, 6695, 295, 300, 661, 551, 420, 307, 309, 11, 420, 366, 436, 4582, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.10302724157060895, "compression_ratio": 1.903225806451613, "no_speech_prob": 3.8442944969574455e-06}, {"id": 354, "seek": 199656, "start": 2010.56, "end": 2024.56, "text": " Does it, you know, and so in the end I kind of come away feeling like okay well I now know that, you know, every model we tried the optimal learning rates basically the same every model we've tried the optimal resize methods basically the same", "tokens": [4402, 309, 11, 291, 458, 11, 293, 370, 294, 264, 917, 286, 733, 295, 808, 1314, 2633, 411, 1392, 731, 286, 586, 458, 300, 11, 291, 458, 11, 633, 2316, 321, 3031, 264, 16252, 2539, 6846, 1936, 264, 912, 633, 2316, 321, 600, 3031, 264, 16252, 50069, 7150, 1936, 264, 912], "temperature": 0.0, "avg_logprob": -0.10302724157060895, "compression_ratio": 1.903225806451613, "no_speech_prob": 3.8442944969574455e-06}, {"id": 355, "seek": 202456, "start": 2024.56, "end": 2031.56, "text": " and like so I'm come away, knowing that I don't have to try all these different things every time.", "tokens": [293, 411, 370, 286, 478, 808, 1314, 11, 5276, 300, 286, 500, 380, 362, 281, 853, 439, 613, 819, 721, 633, 565, 13], "temperature": 0.0, "avg_logprob": -0.14141146341959634, "compression_ratio": 1.5, "no_speech_prob": 8.396938028454315e-06}, {"id": 356, "seek": 202456, "start": 2031.56, "end": 2044.56, "text": " And so now, next time I do another project, I can leverage my knowledge of what I've learned, rather than do yet another huge hyper parameter sweep.", "tokens": [400, 370, 586, 11, 958, 565, 286, 360, 1071, 1716, 11, 286, 393, 13982, 452, 3601, 295, 437, 286, 600, 3264, 11, 2831, 813, 360, 1939, 1071, 2603, 9848, 13075, 22169, 13], "temperature": 0.0, "avg_logprob": -0.14141146341959634, "compression_ratio": 1.5, "no_speech_prob": 8.396938028454315e-06}, {"id": 357, "seek": 202456, "start": 2044.56, "end": 2047.56, "text": " I see you are the Bayesian optimization.", "tokens": [286, 536, 291, 366, 264, 7840, 42434, 19618, 13], "temperature": 0.0, "avg_logprob": -0.14141146341959634, "compression_ratio": 1.5, "no_speech_prob": 8.396938028454315e-06}, {"id": 358, "seek": 204756, "start": 2047.56, "end": 2062.56, "text": " My brain is the is the thing that's learning. Exactly. And I find like people what big companies that spend all their time during these big, you know, hyper parameter optimizations like I always feel and talking to them that they don't seem to know much about the", "tokens": [1222, 3567, 307, 264, 307, 264, 551, 300, 311, 2539, 13, 7587, 13, 400, 286, 915, 411, 561, 437, 955, 3431, 300, 3496, 439, 641, 565, 1830, 613, 955, 11, 291, 458, 11, 9848, 13075, 5028, 14455, 411, 286, 1009, 841, 293, 1417, 281, 552, 300, 436, 500, 380, 1643, 281, 458, 709, 466, 264], "temperature": 0.0, "avg_logprob": -0.15127012901699421, "compression_ratio": 1.7606177606177607, "no_speech_prob": 6.04777915214072e-06}, {"id": 359, "seek": 204756, "start": 2062.56, "end": 2072.56, "text": " practice of deep learning, like they don't seem to know like what generally works and what generally doesn't work because they never bother trying to figure out the answers to those questions.", "tokens": [3124, 295, 2452, 2539, 11, 411, 436, 500, 380, 1643, 281, 458, 411, 437, 5101, 1985, 293, 437, 5101, 1177, 380, 589, 570, 436, 1128, 8677, 1382, 281, 2573, 484, 264, 6338, 281, 729, 1651, 13], "temperature": 0.0, "avg_logprob": -0.15127012901699421, "compression_ratio": 1.7606177606177607, "no_speech_prob": 6.04777915214072e-06}, {"id": 360, "seek": 207256, "start": 2072.56, "end": 2086.56, "text": " But instead they just chuck in a huge hyper parameter optimization thing into, you know, 1000 TPUs.", "tokens": [583, 2602, 436, 445, 20870, 294, 257, 2603, 9848, 13075, 19618, 551, 666, 11, 291, 458, 11, 9714, 314, 8115, 82, 13], "temperature": 0.0, "avg_logprob": -0.17798799708269644, "compression_ratio": 1.50990099009901, "no_speech_prob": 2.0578836483764462e-06}, {"id": 361, "seek": 207256, "start": 2086.56, "end": 2097.56, "text": " Yeah, it's kind of something I've observed that's really interesting I mean, like, do you, does it, do you feel like these like hyper parameters generalized across different architectures different models.", "tokens": [865, 11, 309, 311, 733, 295, 746, 286, 600, 13095, 300, 311, 534, 1880, 286, 914, 11, 411, 11, 360, 291, 11, 775, 309, 11, 360, 291, 841, 411, 613, 411, 9848, 9834, 44498, 2108, 819, 6331, 1303, 819, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17798799708269644, "compression_ratio": 1.50990099009901, "no_speech_prob": 2.0578836483764462e-06}, {"id": 362, "seek": 209756, "start": 2097.56, "end": 2103.56, "text": " Yeah, totally. Yeah, totally.", "tokens": [865, 11, 3879, 13, 865, 11, 3879, 13], "temperature": 0.0, "avg_logprob": -0.12159887250963149, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.6957630577962846e-05}, {"id": 363, "seek": 209756, "start": 2103.56, "end": 2116.56, "text": " In fact, yeah, that was a piece of analysis we did gosh I don't know four or five years ago along with a fellowship today folks in the platform today folks were just trying lots of different sets of hyper parameters across this different sets of data sets as possible.", "tokens": [682, 1186, 11, 1338, 11, 300, 390, 257, 2522, 295, 5215, 321, 630, 6502, 286, 500, 380, 458, 1451, 420, 1732, 924, 2057, 2051, 365, 257, 24989, 965, 4024, 294, 264, 3663, 965, 4024, 645, 445, 1382, 3195, 295, 819, 6352, 295, 9848, 9834, 2108, 341, 819, 6352, 295, 1412, 6352, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.12159887250963149, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.6957630577962846e-05}, {"id": 364, "seek": 209756, "start": 2116.56, "end": 2125.56, "text": " And the same sets of hyper parameters were the best or close enough to the best for everything we tried.", "tokens": [400, 264, 912, 6352, 295, 9848, 9834, 645, 264, 1151, 420, 1998, 1547, 281, 264, 1151, 337, 1203, 321, 3031, 13], "temperature": 0.0, "avg_logprob": -0.12159887250963149, "compression_ratio": 1.775330396475771, "no_speech_prob": 1.6957630577962846e-05}, {"id": 365, "seek": 212556, "start": 2125.56, "end": 2129.56, "text": " That's", "tokens": [663, 311], "temperature": 0.0, "avg_logprob": -0.255923589070638, "compression_ratio": 1.4641148325358853, "no_speech_prob": 0.00010216012015007436}, {"id": 366, "seek": 212556, "start": 2129.56, "end": 2130.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.255923589070638, "compression_ratio": 1.4641148325358853, "no_speech_prob": 0.00010216012015007436}, {"id": 367, "seek": 212556, "start": 2130.56, "end": 2132.56, "text": " Yeah, it is.", "tokens": [865, 11, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.255923589070638, "compression_ratio": 1.4641148325358853, "no_speech_prob": 0.00010216012015007436}, {"id": 368, "seek": 212556, "start": 2132.56, "end": 2149.56, "text": " With different architectures like I can somewhat imagine that no data set maybe is not that super important but you know between transformers and the CNS. I mean I'm not questioning this because I don't have any experience to say that this is not correct I think this is wonderful", "tokens": [2022, 819, 6331, 1303, 411, 286, 393, 8344, 3811, 300, 572, 1412, 992, 1310, 307, 406, 300, 1687, 1021, 457, 291, 458, 1296, 4088, 433, 293, 264, 14589, 50, 13, 286, 914, 286, 478, 406, 21257, 341, 570, 286, 500, 380, 362, 604, 1752, 281, 584, 300, 341, 307, 406, 3006, 286, 519, 341, 307, 3715], "temperature": 0.0, "avg_logprob": -0.255923589070638, "compression_ratio": 1.4641148325358853, "no_speech_prob": 0.00010216012015007436}, {"id": 369, "seek": 214956, "start": 2149.56, "end": 2162.56, "text": " and it is it is, it's amazing. So yeah, the fact that across 90 different models that we're testing that couldn't be more different.", "tokens": [293, 309, 307, 309, 307, 11, 309, 311, 2243, 13, 407, 1338, 11, 264, 1186, 300, 2108, 4289, 819, 5245, 300, 321, 434, 4997, 300, 2809, 380, 312, 544, 819, 13], "temperature": 0.0, "avg_logprob": -0.1753247000954368, "compression_ratio": 1.4093959731543624, "no_speech_prob": 1.568769584991969e-05}, {"id": 370, "seek": 214956, "start": 2162.56, "end": 2166.56, "text": " They all had basically the same best learning rate or close enough.", "tokens": [814, 439, 632, 1936, 264, 912, 1151, 2539, 3314, 420, 1998, 1547, 13], "temperature": 0.0, "avg_logprob": -0.1753247000954368, "compression_ratio": 1.4093959731543624, "no_speech_prob": 1.568769584991969e-05}, {"id": 371, "seek": 214956, "start": 2166.56, "end": 2168.56, "text": " You know,", "tokens": [509, 458, 11], "temperature": 0.0, "avg_logprob": -0.1753247000954368, "compression_ratio": 1.4093959731543624, "no_speech_prob": 1.568769584991969e-05}, {"id": 372, "seek": 216856, "start": 2168.56, "end": 2185.56, "text": " the very interesting aspect here is doing the learning rate is something that you dump a lot of time into. Usually when you start working on a project or in a competition, you would be naturally inclined to hey you know I'm using a different architecture.", "tokens": [264, 588, 1880, 4171, 510, 307, 884, 264, 2539, 3314, 307, 746, 300, 291, 11430, 257, 688, 295, 565, 666, 13, 11419, 562, 291, 722, 1364, 322, 257, 1716, 420, 294, 257, 6211, 11, 291, 576, 312, 8195, 28173, 281, 4177, 291, 458, 286, 478, 1228, 257, 819, 9482, 13], "temperature": 0.0, "avg_logprob": -0.2135547876358032, "compression_ratio": 1.6081081081081081, "no_speech_prob": 4.7519755753455684e-05}, {"id": 373, "seek": 216856, "start": 2185.56, "end": 2193.56, "text": " Let me try to find the, you know, experiment with learning rates, but it's nice that you can discuss.", "tokens": [961, 385, 853, 281, 915, 264, 11, 291, 458, 11, 5120, 365, 2539, 6846, 11, 457, 309, 311, 1481, 300, 291, 393, 2248, 13], "temperature": 0.0, "avg_logprob": -0.2135547876358032, "compression_ratio": 1.6081081081081081, "no_speech_prob": 4.7519755753455684e-05}, {"id": 374, "seek": 219356, "start": 2193.56, "end": 2202.56, "text": " Focus on what really matters. Well I should mention, Radek, this is true of computer vision.", "tokens": [21862, 322, 437, 534, 7001, 13, 1042, 286, 820, 2152, 11, 497, 762, 74, 11, 341, 307, 2074, 295, 3820, 5201, 13], "temperature": 0.0, "avg_logprob": -0.20510992456655033, "compression_ratio": 1.5061728395061729, "no_speech_prob": 1.9217653971281834e-05}, {"id": 375, "seek": 219356, "start": 2202.56, "end": 2208.56, "text": " But not necessarily for tabular.", "tokens": [583, 406, 4725, 337, 4421, 1040, 13], "temperature": 0.0, "avg_logprob": -0.20510992456655033, "compression_ratio": 1.5061728395061729, "no_speech_prob": 1.9217653971281834e-05}, {"id": 376, "seek": 219356, "start": 2208.56, "end": 2217.56, "text": " I suspect, like all computer vision problems do look pretty similar. You know, the data for them looks pretty similar.", "tokens": [286, 9091, 11, 411, 439, 3820, 5201, 2740, 360, 574, 1238, 2531, 13, 509, 458, 11, 264, 1412, 337, 552, 1542, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.20510992456655033, "compression_ratio": 1.5061728395061729, "no_speech_prob": 1.9217653971281834e-05}, {"id": 377, "seek": 221756, "start": 2217.56, "end": 2227.56, "text": " And I suspect it's also true like specifically of object recognition so like, yeah, for.", "tokens": [400, 286, 9091, 309, 311, 611, 2074, 411, 4682, 295, 2657, 11150, 370, 411, 11, 1338, 11, 337, 13], "temperature": 0.0, "avg_logprob": -0.12188668181930763, "compression_ratio": 1.4947368421052631, "no_speech_prob": 8.528708349331282e-06}, {"id": 378, "seek": 221756, "start": 2227.56, "end": 2244.56, "text": " I don't know. I mean these are things like nobody seems to bother testing like which I find a bit crazy but we should do similar tests for segmentation and, you know, bounding boxes and so forth.", "tokens": [286, 500, 380, 458, 13, 286, 914, 613, 366, 721, 411, 5079, 2544, 281, 8677, 4997, 411, 597, 286, 915, 257, 857, 3219, 457, 321, 820, 360, 2531, 6921, 337, 9469, 399, 293, 11, 291, 458, 11, 5472, 278, 9002, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.12188668181930763, "compression_ratio": 1.4947368421052631, "no_speech_prob": 8.528708349331282e-06}, {"id": 379, "seek": 224456, "start": 2244.56, "end": 2260.56, "text": " I feel we're fine. The same thing. You have the learning rate binder. So we suggest maybe some different learning rates are good in different places. Well, the learning rate finder I built before I had done any of this research right.", "tokens": [286, 841, 321, 434, 2489, 13, 440, 912, 551, 13, 509, 362, 264, 2539, 3314, 45630, 13, 407, 321, 3402, 1310, 512, 819, 2539, 6846, 366, 665, 294, 819, 3190, 13, 1042, 11, 264, 2539, 3314, 915, 260, 286, 3094, 949, 286, 632, 1096, 604, 295, 341, 2132, 558, 13], "temperature": 0.0, "avg_logprob": -0.21427254799084786, "compression_ratio": 1.5784313725490196, "no_speech_prob": 5.506675279320916e-06}, {"id": 380, "seek": 224456, "start": 2260.56, "end": 2262.56, "text": " Oh, okay.", "tokens": [876, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.21427254799084786, "compression_ratio": 1.5784313725490196, "no_speech_prob": 5.506675279320916e-06}, {"id": 381, "seek": 224456, "start": 2262.56, "end": 2267.56, "text": " Like you might have noticed that I hardly ever use it nowadays in the course.", "tokens": [1743, 291, 1062, 362, 5694, 300, 286, 13572, 1562, 764, 309, 13434, 294, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.21427254799084786, "compression_ratio": 1.5784313725490196, "no_speech_prob": 5.506675279320916e-06}, {"id": 382, "seek": 226756, "start": 2267.56, "end": 2277.56, "text": " I don't even know if we've mentioned it yet in this course, maybe we have the last lesson. I can't remember. Does anybody remember did we done the learning rate finder yet in course 22?", "tokens": [286, 500, 380, 754, 458, 498, 321, 600, 2835, 309, 1939, 294, 341, 1164, 11, 1310, 321, 362, 264, 1036, 6898, 13, 286, 393, 380, 1604, 13, 4402, 4472, 1604, 630, 321, 1096, 264, 2539, 3314, 915, 260, 1939, 294, 1164, 5853, 30], "temperature": 0.0, "avg_logprob": -0.21408190046037948, "compression_ratio": 1.4807692307692308, "no_speech_prob": 0.00016576172492932528}, {"id": 383, "seek": 226756, "start": 2277.56, "end": 2282.56, "text": " Yeah, I think we did. You think we did? Yeah.", "tokens": [865, 11, 286, 519, 321, 630, 13, 509, 519, 321, 630, 30, 865, 13], "temperature": 0.0, "avg_logprob": -0.21408190046037948, "compression_ratio": 1.4807692307692308, "no_speech_prob": 0.00016576172492932528}, {"id": 384, "seek": 228256, "start": 2282.56, "end": 2300.56, "text": " Can I just add that one of the really, you can sit there and play with parameters all you like and skid your wheels and get nowhere. And it's one of the things I'm really taking away from the course is the fact that you're talking about strategy,", "tokens": [1664, 286, 445, 909, 300, 472, 295, 264, 534, 11, 291, 393, 1394, 456, 293, 862, 365, 9834, 439, 291, 411, 293, 1110, 327, 428, 10046, 293, 483, 11159, 13, 400, 309, 311, 472, 295, 264, 721, 286, 478, 534, 1940, 1314, 490, 264, 1164, 307, 264, 1186, 300, 291, 434, 1417, 466, 5206, 11], "temperature": 0.0, "avg_logprob": -0.18600425073655985, "compression_ratio": 1.5185185185185186, "no_speech_prob": 9.161335765384138e-05}, {"id": 385, "seek": 230056, "start": 2300.56, "end": 2312.56, "text": " and which goes back to Renato Copiates 2002 paper, he had a term called strategy of analysis, and that's something that really stuck with me.", "tokens": [293, 597, 1709, 646, 281, 12883, 2513, 11579, 72, 1024, 17822, 3035, 11, 415, 632, 257, 1433, 1219, 5206, 295, 5215, 11, 293, 300, 311, 746, 300, 534, 5541, 365, 385, 13], "temperature": 0.0, "avg_logprob": -0.20514488220214844, "compression_ratio": 1.380952380952381, "no_speech_prob": 5.732780118705705e-05}, {"id": 386, "seek": 230056, "start": 2312.56, "end": 2320.56, "text": " So that sort of transcends that idea of just mucking around with parameters.", "tokens": [407, 300, 1333, 295, 43800, 2581, 300, 1558, 295, 445, 275, 33260, 926, 365, 9834, 13], "temperature": 0.0, "avg_logprob": -0.20514488220214844, "compression_ratio": 1.380952380952381, "no_speech_prob": 5.732780118705705e-05}, {"id": 387, "seek": 230056, "start": 2320.56, "end": 2322.56, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.20514488220214844, "compression_ratio": 1.380952380952381, "no_speech_prob": 5.732780118705705e-05}, {"id": 388, "seek": 230056, "start": 2322.56, "end": 2327.56, "text": " Exactly.", "tokens": [7587, 13], "temperature": 0.0, "avg_logprob": -0.20514488220214844, "compression_ratio": 1.380952380952381, "no_speech_prob": 5.732780118705705e-05}, {"id": 389, "seek": 232756, "start": 2327.56, "end": 2334.56, "text": " I suppose these magic parameters, these are the defaults in fast AI.", "tokens": [286, 7297, 613, 5585, 9834, 11, 613, 366, 264, 7576, 82, 294, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.23523470412853154, "compression_ratio": 1.305084745762712, "no_speech_prob": 9.970438441087026e-06}, {"id": 390, "seek": 232756, "start": 2334.56, "end": 2344.56, "text": " Yeah, pretty much, although with learning rate.", "tokens": [865, 11, 1238, 709, 11, 4878, 365, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.23523470412853154, "compression_ratio": 1.305084745762712, "no_speech_prob": 9.970438441087026e-06}, {"id": 391, "seek": 232756, "start": 2344.56, "end": 2351.56, "text": " Oh, that's weird. With learning rate.", "tokens": [876, 11, 300, 311, 3657, 13, 2022, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.23523470412853154, "compression_ratio": 1.305084745762712, "no_speech_prob": 9.970438441087026e-06}, {"id": 392, "seek": 235156, "start": 2351.56, "end": 2357.56, "text": " The defaults a bit lower than the optimal.", "tokens": [440, 7576, 82, 257, 857, 3126, 813, 264, 16252, 13], "temperature": 0.0, "avg_logprob": -0.1530383158538301, "compression_ratio": 1.4315068493150684, "no_speech_prob": 1.9217284716432914e-05}, {"id": 393, "seek": 235156, "start": 2357.56, "end": 2367.56, "text": " Just because I didn't want to like push it, you know, I'd rather it always worked pretty well, rather than be pretty much the best, you know.", "tokens": [1449, 570, 286, 994, 380, 528, 281, 411, 2944, 309, 11, 291, 458, 11, 286, 1116, 2831, 309, 1009, 2732, 1238, 731, 11, 2831, 813, 312, 1238, 709, 264, 1151, 11, 291, 458, 13], "temperature": 0.0, "avg_logprob": -0.1530383158538301, "compression_ratio": 1.4315068493150684, "no_speech_prob": 1.9217284716432914e-05}, {"id": 394, "seek": 235156, "start": 2367.56, "end": 2371.56, "text": " Yeah, yeah, makes sense.", "tokens": [865, 11, 1338, 11, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1530383158538301, "compression_ratio": 1.4315068493150684, "no_speech_prob": 1.9217284716432914e-05}, {"id": 395, "seek": 237156, "start": 2371.56, "end": 2386.56, "text": " Okay, I'm just gonna go and disconnect my other computer because it's connected to port 8888, which is going to mess things up. I'll be back in one tick.", "tokens": [50364, 1033, 11, 286, 478, 445, 799, 352, 293, 14299, 452, 661, 3820, 570, 309, 311, 4582, 281, 2436, 1649, 16919, 23, 11, 597, 307, 516, 281, 2082, 721, 493, 13, 286, 603, 312, 646, 294, 472, 5204, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12304421169001882, "compression_ratio": 1.2644628099173554, "no_speech_prob": 3.646560071501881e-05}, {"id": 396, "seek": 240156, "start": 2401.56, "end": 2423.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6088503996531168, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.9708545804023743}, {"id": 397, "seek": 242356, "start": 2423.56, "end": 2451.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.32310030857721966, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0005023296689614654}, {"id": 398, "seek": 245156, "start": 2451.56, "end": 2469.56, "text": " Actually, now I think about it, I don't quite know why this is connecting on port 8889. But part of this is to learn how to debug problems right so normally the Jupiter server uses port 8888.", "tokens": [5135, 11, 586, 286, 519, 466, 309, 11, 286, 500, 380, 1596, 458, 983, 341, 307, 11015, 322, 2436, 1649, 16919, 24, 13, 583, 644, 295, 341, 307, 281, 1466, 577, 281, 24083, 2740, 558, 370, 5646, 264, 24567, 7154, 4960, 2436, 1649, 16919, 23, 13], "temperature": 0.0, "avg_logprob": -0.11536858532879804, "compression_ratio": 1.455497382198953, "no_speech_prob": 1.921950388350524e-05}, {"id": 399, "seek": 245156, "start": 2469.56, "end": 2475.56, "text": " And I've only got my SSH connected to forward port 8888 so it's currently not working.", "tokens": [400, 286, 600, 787, 658, 452, 12238, 39, 4582, 281, 2128, 2436, 1649, 16919, 23, 370, 309, 311, 4362, 406, 1364, 13], "temperature": 0.0, "avg_logprob": -0.11536858532879804, "compression_ratio": 1.455497382198953, "no_speech_prob": 1.921950388350524e-05}, {"id": 400, "seek": 247556, "start": 2475.56, "end": 2487.56, "text": " So the fact that using a different port suggests this already, it's already running somewhere. So to find out where it's running, you can use PS which lists all the processes running on your computer.", "tokens": [407, 264, 1186, 300, 1228, 257, 819, 2436, 13409, 341, 1217, 11, 309, 311, 1217, 2614, 4079, 13, 407, 281, 915, 484, 689, 309, 311, 2614, 11, 291, 393, 764, 8168, 597, 14511, 439, 264, 7555, 2614, 322, 428, 3820, 13], "temperature": 0.0, "avg_logprob": -0.13150136947631835, "compression_ratio": 1.715415019762846, "no_speech_prob": 1.3210092220106162e-05}, {"id": 401, "seek": 247556, "start": 2487.56, "end": 2501.56, "text": " And generally speaking I find I get used to some standard set of options that I nearly always want and then I forget what they mean. So I have no idea what WAU or X means I just know that there are a set of options that I always use.", "tokens": [400, 5101, 4124, 286, 915, 286, 483, 1143, 281, 512, 3832, 992, 295, 3956, 300, 286, 6217, 1009, 528, 293, 550, 286, 2870, 437, 436, 914, 13, 407, 286, 362, 572, 1558, 437, 343, 2340, 420, 1783, 1355, 286, 445, 458, 300, 456, 366, 257, 992, 295, 3956, 300, 286, 1009, 764, 13], "temperature": 0.0, "avg_logprob": -0.13150136947631835, "compression_ratio": 1.715415019762846, "no_speech_prob": 1.3210092220106162e-05}, {"id": 402, "seek": 250156, "start": 2501.56, "end": 2511.56, "text": " So that basically lists all your processes, which obviously is a bit too many. So we want to now filter out the ones that contain Jupiter or notebook.", "tokens": [407, 300, 1936, 14511, 439, 428, 7555, 11, 597, 2745, 307, 257, 857, 886, 867, 13, 407, 321, 528, 281, 586, 6608, 484, 264, 2306, 300, 5304, 24567, 420, 21060, 13], "temperature": 0.0, "avg_logprob": -0.09131224076826494, "compression_ratio": 1.6473214285714286, "no_speech_prob": 9.079727533389814e-06}, {"id": 403, "seek": 250156, "start": 2511.56, "end": 2524.56, "text": " So, pipe is how you do that in Linux so that's going to send the output of this into the input of another program. And a program that just prints out a list of matching lines is called grep.", "tokens": [407, 11, 11240, 307, 577, 291, 360, 300, 294, 18734, 370, 300, 311, 516, 281, 2845, 264, 5598, 295, 341, 666, 264, 4846, 295, 1071, 1461, 13, 400, 257, 1461, 300, 445, 22305, 484, 257, 1329, 295, 14324, 3876, 307, 1219, 6066, 79, 13], "temperature": 0.0, "avg_logprob": -0.09131224076826494, "compression_ratio": 1.6473214285714286, "no_speech_prob": 9.079727533389814e-06}, {"id": 404, "seek": 250156, "start": 2524.56, "end": 2528.56, "text": " So we can grep for Jupiter.", "tokens": [407, 321, 393, 6066, 79, 337, 24567, 13], "temperature": 0.0, "avg_logprob": -0.09131224076826494, "compression_ratio": 1.6473214285714286, "no_speech_prob": 9.079727533389814e-06}, {"id": 405, "seek": 252856, "start": 2528.56, "end": 2535.56, "text": " Okay, there it is. So I'm kind of wondering", "tokens": [1033, 11, 456, 309, 307, 13, 407, 286, 478, 733, 295, 6359], "temperature": 0.0, "avg_logprob": -0.1582381081959558, "compression_ratio": 1.3714285714285714, "no_speech_prob": 1.7777825632947497e-05}, {"id": 406, "seek": 252856, "start": 2535.56, "end": 2543.56, "text": " where that, how that's running. I wonder if we've got like multiple sessions of TMUX running.", "tokens": [689, 300, 11, 577, 300, 311, 2614, 13, 286, 2441, 498, 321, 600, 658, 411, 3866, 11081, 295, 33550, 52, 55, 2614, 13], "temperature": 0.0, "avg_logprob": -0.1582381081959558, "compression_ratio": 1.3714285714285714, "no_speech_prob": 1.7777825632947497e-05}, {"id": 407, "seek": 252856, "start": 2543.56, "end": 2551.56, "text": " No, we don't. So TMUX-LS lists all your TMUX sessions.", "tokens": [883, 11, 321, 500, 380, 13, 407, 33550, 52, 55, 12, 19198, 14511, 439, 428, 33550, 52, 55, 11081, 13], "temperature": 0.0, "avg_logprob": -0.1582381081959558, "compression_ratio": 1.3714285714285714, "no_speech_prob": 1.7777825632947497e-05}, {"id": 408, "seek": 255156, "start": 2551.56, "end": 2558.56, "text": " Oh, I've got a stopped version in the background. Okay, that's why. So I just have to foreground it. There we go.", "tokens": [876, 11, 286, 600, 658, 257, 5936, 3037, 294, 264, 3678, 13, 1033, 11, 300, 311, 983, 13, 407, 286, 445, 362, 281, 32058, 309, 13, 821, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.16161763863485368, "compression_ratio": 1.330935251798561, "no_speech_prob": 2.8854157790192403e-05}, {"id": 409, "seek": 255156, "start": 2558.56, "end": 2561.56, "text": " That was a bit weird.", "tokens": [663, 390, 257, 857, 3657, 13], "temperature": 0.0, "avg_logprob": -0.16161763863485368, "compression_ratio": 1.330935251798561, "no_speech_prob": 2.8854157790192403e-05}, {"id": 410, "seek": 255156, "start": 2561.56, "end": 2566.56, "text": " Okay, so now that should work.", "tokens": [1033, 11, 370, 586, 300, 820, 589, 13], "temperature": 0.0, "avg_logprob": -0.16161763863485368, "compression_ratio": 1.330935251798561, "no_speech_prob": 2.8854157790192403e-05}, {"id": 411, "seek": 255156, "start": 2566.56, "end": 2571.56, "text": " FG foreground? FG.", "tokens": [479, 38, 32058, 30, 479, 38, 13], "temperature": 0.0, "avg_logprob": -0.16161763863485368, "compression_ratio": 1.330935251798561, "no_speech_prob": 2.8854157790192403e-05}, {"id": 412, "seek": 257156, "start": 2571.56, "end": 2585.56, "text": " FG to put it in the background. FG to put it in the foreground. And when you control Z somebody, it actually stops it. Right? You can put it in the background and have it keep running by, actually I'll show you.", "tokens": [479, 38, 281, 829, 309, 294, 264, 3678, 13, 479, 38, 281, 829, 309, 294, 264, 32058, 13, 400, 562, 291, 1969, 1176, 2618, 11, 309, 767, 10094, 309, 13, 1779, 30, 509, 393, 829, 309, 294, 264, 3678, 293, 362, 309, 1066, 2614, 538, 11, 767, 286, 603, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.1948037828717913, "compression_ratio": 1.638743455497382, "no_speech_prob": 4.710770554083865e-06}, {"id": 413, "seek": 257156, "start": 2585.56, "end": 2593.56, "text": " So if I press control Z and type jobs, that's stopped. Right? So if I now try to refresh this window,", "tokens": [407, 498, 286, 1886, 1969, 1176, 293, 2010, 4782, 11, 300, 311, 5936, 13, 1779, 30, 407, 498, 286, 586, 853, 281, 15134, 341, 4910, 11], "temperature": 0.0, "avg_logprob": -0.1948037828717913, "compression_ratio": 1.638743455497382, "no_speech_prob": 4.710770554083865e-06}, {"id": 414, "seek": 259356, "start": 2593.56, "end": 2602.56, "text": " I'm going to sit there waiting forever and never going to finish. Okay. Because it's background, it's in the, it's stopped in the background.", "tokens": [286, 478, 516, 281, 1394, 456, 3806, 5680, 293, 1128, 516, 281, 2413, 13, 1033, 13, 1436, 309, 311, 3678, 11, 309, 311, 294, 264, 11, 309, 311, 5936, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.16013834849897637, "compression_ratio": 1.6717171717171717, "no_speech_prob": 6.540068170579616e-06}, {"id": 415, "seek": 259356, "start": 2602.56, "end": 2614.56, "text": " If you type BG, optionally followed by a job number, which would be number one, and it defaults to the last thing that you put in the background, it will start running it in the background.", "tokens": [759, 291, 2010, 363, 38, 11, 3614, 379, 6263, 538, 257, 1691, 1230, 11, 597, 576, 312, 1230, 472, 11, 293, 309, 7576, 82, 281, 264, 1036, 551, 300, 291, 829, 294, 264, 3678, 11, 309, 486, 722, 2614, 309, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.16013834849897637, "compression_ratio": 1.6717171717171717, "no_speech_prob": 6.540068170579616e-06}, {"id": 416, "seek": 261456, "start": 2614.56, "end": 2623.56, "text": " Even after you stop there. Yeah. So it's now running in the background. So if I now type jobs, it's now running.", "tokens": [2754, 934, 291, 1590, 456, 13, 865, 13, 407, 309, 311, 586, 2614, 294, 264, 3678, 13, 407, 498, 286, 586, 2010, 4782, 11, 309, 311, 586, 2614, 13], "temperature": 0.0, "avg_logprob": -0.12806781350749813, "compression_ratio": 1.5290697674418605, "no_speech_prob": 3.844711955025559e-06}, {"id": 417, "seek": 261456, "start": 2623.56, "end": 2634.56, "text": " Okay. And it's still attached to this console. So if I open up this, you'll see it's still printing out things, right? But I can also do other things.", "tokens": [1033, 13, 400, 309, 311, 920, 8570, 281, 341, 11076, 13, 407, 498, 286, 1269, 493, 341, 11, 291, 603, 536, 309, 311, 920, 14699, 484, 721, 11, 558, 30, 583, 286, 393, 611, 360, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.12806781350749813, "compression_ratio": 1.5290697674418605, "no_speech_prob": 3.844711955025559e-06}, {"id": 418, "seek": 263456, "start": 2634.56, "end": 2646.56, "text": " And I don't do this very much because normally if I want something running at the same time, I would just chuck it in another T-Mux pane. I don't know. It's kind of nice to know this exists.", "tokens": [400, 286, 500, 380, 360, 341, 588, 709, 570, 5646, 498, 286, 528, 746, 2614, 412, 264, 912, 565, 11, 286, 576, 445, 20870, 309, 294, 1071, 314, 12, 44, 2449, 32605, 13, 286, 500, 380, 458, 13, 467, 311, 733, 295, 1481, 281, 458, 341, 8198, 13], "temperature": 0.0, "avg_logprob": -0.11496694564819336, "compression_ratio": 1.6127659574468085, "no_speech_prob": 5.3379499149741605e-06}, {"id": 419, "seek": 263456, "start": 2646.56, "end": 2659.56, "text": " Something else to point out is once I said BG, it added this ampersand after the job. That's because if you run something with an ampersand at the end, it always runs it in the background.", "tokens": [6595, 1646, 281, 935, 484, 307, 1564, 286, 848, 363, 38, 11, 309, 3869, 341, 18648, 433, 474, 934, 264, 1691, 13, 663, 311, 570, 498, 291, 1190, 746, 365, 364, 18648, 433, 474, 412, 264, 917, 11, 309, 1009, 6676, 309, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.11496694564819336, "compression_ratio": 1.6127659574468085, "no_speech_prob": 5.3379499149741605e-06}, {"id": 420, "seek": 265956, "start": 2659.56, "end": 2674.56, "text": " So if you want to like fire off six processes to run in parallel, just put an ampersand at the end of each one, and it'll run in the background.", "tokens": [407, 498, 291, 528, 281, 411, 2610, 766, 2309, 7555, 281, 1190, 294, 8952, 11, 445, 829, 364, 18648, 433, 474, 412, 264, 917, 295, 1184, 472, 11, 293, 309, 603, 1190, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.10828634966974673, "compression_ratio": 1.3114754098360655, "no_speech_prob": 1.0450456102262251e-05}, {"id": 421, "seek": 265956, "start": 2674.56, "end": 2686.56, "text": " So for example,", "tokens": [407, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.10828634966974673, "compression_ratio": 1.3114754098360655, "no_speech_prob": 1.0450456102262251e-05}, {"id": 422, "seek": 268656, "start": 2686.56, "end": 2692.56, "text": " there's a script that runs LS six times.", "tokens": [456, 311, 257, 5755, 300, 6676, 36657, 2309, 1413, 13], "temperature": 0.0, "avg_logprob": -0.12182732061906294, "compression_ratio": 1.505050505050505, "no_speech_prob": 8.664386768941768e-06}, {"id": 423, "seek": 268656, "start": 2692.56, "end": 2696.56, "text": " And so if I run it,", "tokens": [400, 370, 498, 286, 1190, 309, 11], "temperature": 0.0, "avg_logprob": -0.12182732061906294, "compression_ratio": 1.505050505050505, "no_speech_prob": 8.664386768941768e-06}, {"id": 424, "seek": 268656, "start": 2696.56, "end": 2702.56, "text": " you can see they're all interspersed with each other, because it ran all six times at the same time.", "tokens": [291, 393, 536, 436, 434, 439, 728, 4952, 433, 292, 365, 1184, 661, 11, 570, 309, 5872, 439, 2309, 1413, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.12182732061906294, "compression_ratio": 1.505050505050505, "no_speech_prob": 8.664386768941768e-06}, {"id": 425, "seek": 268656, "start": 2702.56, "end": 2712.56, "text": " I see. And let's say like you create a process like this in the background without T-Mux, and you want to kill it. You use the PS thing.", "tokens": [286, 536, 13, 400, 718, 311, 584, 411, 291, 1884, 257, 1399, 411, 341, 294, 264, 3678, 1553, 314, 12, 44, 2449, 11, 293, 291, 528, 281, 1961, 309, 13, 509, 764, 264, 8168, 551, 13], "temperature": 0.0, "avg_logprob": -0.12182732061906294, "compression_ratio": 1.505050505050505, "no_speech_prob": 8.664386768941768e-06}, {"id": 426, "seek": 271256, "start": 2712.56, "end": 2722.56, "text": " You could type FG to foreground it, and then press control C.", "tokens": [509, 727, 2010, 479, 38, 281, 32058, 309, 11, 293, 550, 1886, 1969, 383, 13], "temperature": 0.0, "avg_logprob": -0.13683155604771205, "compression_ratio": 1.437125748502994, "no_speech_prob": 8.267376870207954e-06}, {"id": 427, "seek": 271256, "start": 2722.56, "end": 2729.56, "text": " Yeah, something like that would be fine. Or you can kill a single job.", "tokens": [865, 11, 746, 411, 300, 576, 312, 2489, 13, 1610, 291, 393, 1961, 257, 2167, 1691, 13], "temperature": 0.0, "avg_logprob": -0.13683155604771205, "compression_ratio": 1.437125748502994, "no_speech_prob": 8.267376870207954e-06}, {"id": 428, "seek": 271256, "start": 2729.56, "end": 2740.56, "text": " So in general, like you probably would want to search for bash job control to learn how to do these things.", "tokens": [407, 294, 2674, 11, 411, 291, 1391, 576, 528, 281, 3164, 337, 46183, 1691, 1969, 281, 1466, 577, 281, 360, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.13683155604771205, "compression_ratio": 1.437125748502994, "no_speech_prob": 8.267376870207954e-06}, {"id": 429, "seek": 274056, "start": 2740.56, "end": 2756.56, "text": " And as I said, one of the key things to know is that a job number has a percent of the start. So this is actually percent one.", "tokens": [400, 382, 286, 848, 11, 472, 295, 264, 2141, 721, 281, 458, 307, 300, 257, 1691, 1230, 575, 257, 3043, 295, 264, 722, 13, 407, 341, 307, 767, 3043, 472, 13], "temperature": 0.0, "avg_logprob": -0.26469954200412915, "compression_ratio": 1.310077519379845, "no_speech_prob": 1.1299998732283711e-05}, {"id": 430, "seek": 274056, "start": 2756.56, "end": 2759.56, "text": " Knowing what to Google is definitely. Yes.", "tokens": [25499, 437, 281, 3329, 307, 2138, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.26469954200412915, "compression_ratio": 1.310077519379845, "no_speech_prob": 1.1299998732283711e-05}, {"id": 431, "seek": 275956, "start": 2759.56, "end": 2773.56, "text": " Knowing what to Google is the key thing. Although often you can just put in a few examples. So you could, I'm guessing, like if I take control C, BG, FG jobs, which are the things we just learned about.", "tokens": [25499, 437, 281, 3329, 307, 264, 2141, 551, 13, 5780, 2049, 291, 393, 445, 829, 294, 257, 1326, 5110, 13, 407, 291, 727, 11, 286, 478, 17939, 11, 411, 498, 286, 747, 1969, 383, 11, 363, 38, 11, 479, 38, 4782, 11, 597, 366, 264, 721, 321, 445, 3264, 466, 13], "temperature": 0.0, "avg_logprob": -0.1823258098167709, "compression_ratio": 1.4056603773584906, "no_speech_prob": 2.812794718920486e-06}, {"id": 432, "seek": 275956, "start": 2773.56, "end": 2787.56, "text": " There we go. It kind of gets us pretty close. Now we know they're called drop control commands.", "tokens": [821, 321, 352, 13, 467, 733, 295, 2170, 505, 1238, 1998, 13, 823, 321, 458, 436, 434, 1219, 3270, 1969, 16901, 13], "temperature": 0.0, "avg_logprob": -0.1823258098167709, "compression_ratio": 1.4056603773584906, "no_speech_prob": 2.812794718920486e-06}, {"id": 433, "seek": 278756, "start": 2787.56, "end": 2793.56, "text": " All right. Now.", "tokens": [1057, 558, 13, 823, 13], "temperature": 0.0, "avg_logprob": -0.1285421675530033, "compression_ratio": 1.6374269005847952, "no_speech_prob": 7.766700036881957e-06}, {"id": 434, "seek": 278756, "start": 2793.56, "end": 2809.56, "text": " So when I kind of iterate through notebooks. What I tend to do is like, once I've got something vaguely working, I generally duplicate it, and then I try to get something else vaguely working and once that starts vaguely working, I then rename it to the thing that", "tokens": [407, 562, 286, 733, 295, 44497, 807, 43782, 13, 708, 286, 3928, 281, 360, 307, 411, 11, 1564, 286, 600, 658, 746, 13501, 48863, 1364, 11, 286, 5101, 23976, 309, 11, 293, 550, 286, 853, 281, 483, 746, 1646, 13501, 48863, 1364, 293, 1564, 300, 3719, 13501, 48863, 1364, 11, 286, 550, 36741, 309, 281, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.1285421675530033, "compression_ratio": 1.6374269005847952, "no_speech_prob": 7.766700036881957e-06}, {"id": 435, "seek": 280956, "start": 2809.56, "end": 2821.56, "text": " I want. So then from time to time, then I just clean up the duplicated versions that I didn't end up using and I can tell which they are, because I haven't renamed them yet.", "tokens": [286, 528, 13, 407, 550, 490, 565, 281, 565, 11, 550, 286, 445, 2541, 493, 264, 1581, 564, 3587, 9606, 300, 286, 994, 380, 917, 493, 1228, 293, 286, 393, 980, 597, 436, 366, 11, 570, 286, 2378, 380, 40949, 552, 1939, 13], "temperature": 0.0, "avg_logprob": -0.20824665289658767, "compression_ratio": 1.6536796536796536, "no_speech_prob": 9.079595656658057e-06}, {"id": 436, "seek": 280956, "start": 2821.56, "end": 2824.56, "text": " And so this is kind of how you can duplicate it.", "tokens": [400, 370, 341, 307, 733, 295, 577, 291, 393, 23976, 309, 13], "temperature": 0.0, "avg_logprob": -0.20824665289658767, "compression_ratio": 1.6536796536796536, "no_speech_prob": 9.079595656658057e-06}, {"id": 437, "seek": 280956, "start": 2824.56, "end": 2832.56, "text": " Like, you make it looks like you're making copies of it and yeah so you can just click File, make a copy. Yep.", "tokens": [1743, 11, 291, 652, 309, 1542, 411, 291, 434, 1455, 14341, 295, 309, 293, 1338, 370, 291, 393, 445, 2052, 26196, 11, 652, 257, 5055, 13, 7010, 13], "temperature": 0.0, "avg_logprob": -0.20824665289658767, "compression_ratio": 1.6536796536796536, "no_speech_prob": 9.079595656658057e-06}, {"id": 438, "seek": 280956, "start": 2832.56, "end": 2836.56, "text": " Or in here you can click it and click duplicate.", "tokens": [1610, 294, 510, 291, 393, 2052, 309, 293, 2052, 23976, 13], "temperature": 0.0, "avg_logprob": -0.20824665289658767, "compression_ratio": 1.6536796536796536, "no_speech_prob": 9.079595656658057e-06}, {"id": 439, "seek": 283656, "start": 2836.56, "end": 2848.56, "text": " And so, you like, what do you do after you duplicate it you try to open up that I'll open up that duplicate and I'll try something else some different type of parameter and different method or whatever.", "tokens": [400, 370, 11, 291, 411, 11, 437, 360, 291, 360, 934, 291, 23976, 309, 291, 853, 281, 1269, 493, 300, 286, 603, 1269, 493, 300, 23976, 293, 286, 603, 853, 746, 1646, 512, 819, 2010, 295, 13075, 293, 819, 3170, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.21394895104800954, "compression_ratio": 1.5942857142857143, "no_speech_prob": 2.190680334024364e-06}, {"id": 440, "seek": 283656, "start": 2848.56, "end": 2858.56, "text": " So in this case, I started out here in Patty, and kind of just experimented.", "tokens": [407, 294, 341, 1389, 11, 286, 1409, 484, 510, 294, 44116, 11, 293, 733, 295, 445, 5120, 292, 13], "temperature": 0.0, "avg_logprob": -0.21394895104800954, "compression_ratio": 1.5942857142857143, "no_speech_prob": 2.190680334024364e-06}, {"id": 441, "seek": 285856, "start": 2858.56, "end": 2871.56, "text": " Right. And show batch and LR find and try to get something running. And then, you know, after that I was like, okay, I've got something working, how do I make it better.", "tokens": [1779, 13, 400, 855, 15245, 293, 441, 49, 915, 293, 853, 281, 483, 746, 2614, 13, 400, 550, 11, 291, 458, 11, 934, 300, 286, 390, 411, 11, 1392, 11, 286, 600, 658, 746, 1364, 11, 577, 360, 286, 652, 309, 1101, 13], "temperature": 0.0, "avg_logprob": -0.24067948081276633, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.0128324902325403e-05}, {"id": 442, "seek": 285856, "start": 2871.56, "end": 2881.56, "text": " And so I created Patty small, but literally made a copy and it would have been called Patty copy.ipnb.", "tokens": [400, 370, 286, 2942, 44116, 1359, 11, 457, 3736, 1027, 257, 5055, 293, 309, 576, 362, 668, 1219, 44116, 5055, 13, 647, 77, 65, 13], "temperature": 0.0, "avg_logprob": -0.24067948081276633, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.0128324902325403e-05}, {"id": 443, "seek": 285856, "start": 2881.56, "end": 2886.56, "text": " And I was like, I wonder about different architectures.", "tokens": [400, 286, 390, 411, 11, 286, 2441, 466, 819, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.24067948081276633, "compression_ratio": 1.5769230769230769, "no_speech_prob": 1.0128324902325403e-05}, {"id": 444, "seek": 288656, "start": 2886.56, "end": 2897.56, "text": " So I created this like us like okay well basically I want to try different item transforms different batch transforms and different architectures. So create a train which takes those three things.", "tokens": [407, 286, 2942, 341, 411, 505, 411, 1392, 731, 1936, 286, 528, 281, 853, 819, 3174, 35592, 819, 15245, 35592, 293, 819, 6331, 1303, 13, 407, 1884, 257, 3847, 597, 2516, 729, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.13888806882111923, "compression_ratio": 1.8539823008849559, "no_speech_prob": 1.028897622745717e-05}, {"id": 445, "seek": 288656, "start": 2897.56, "end": 2907.56, "text": " And so it creates a set of image loaders with those item transforms and those batch transforms. Use a fixed seed to get the same validation so each time.", "tokens": [400, 370, 309, 7829, 257, 992, 295, 3256, 3677, 433, 365, 729, 3174, 35592, 293, 729, 15245, 35592, 13, 8278, 257, 6806, 8871, 281, 483, 264, 912, 24071, 370, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.13888806882111923, "compression_ratio": 1.8539823008849559, "no_speech_prob": 1.028897622745717e-05}, {"id": 446, "seek": 288656, "start": 2907.56, "end": 2911.56, "text": " Train it with that architecture.", "tokens": [28029, 309, 365, 300, 9482, 13], "temperature": 0.0, "avg_logprob": -0.13888806882111923, "compression_ratio": 1.8539823008849559, "no_speech_prob": 1.028897622745717e-05}, {"id": 447, "seek": 288656, "start": 2911.56, "end": 2914.56, "text": " And then return the TTA error rate.", "tokens": [400, 550, 2736, 264, 314, 8241, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.13888806882111923, "compression_ratio": 1.8539823008849559, "no_speech_prob": 1.028897622745717e-05}, {"id": 448, "seek": 291456, "start": 2914.56, "end": 2923.56, "text": " And so then, this is kind of like your weights and biases, like, this is how you keep your different experiments ideas.", "tokens": [400, 370, 550, 11, 341, 307, 733, 295, 411, 428, 17443, 293, 32152, 11, 411, 11, 341, 307, 577, 291, 1066, 428, 819, 12050, 3487, 13], "temperature": 0.0, "avg_logprob": -0.21156205071343315, "compression_ratio": 1.2647058823529411, "no_speech_prob": 1.2804710422642529e-05}, {"id": 449, "seek": 291456, "start": 2923.56, "end": 2928.56, "text": " Yeah, so.", "tokens": [865, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.21156205071343315, "compression_ratio": 1.2647058823529411, "no_speech_prob": 1.2804710422642529e-05}, {"id": 450, "seek": 292856, "start": 2928.56, "end": 2946.56, "text": " So now you can see I've kind of gone through and tried a few different sets of item and batch transforms for this architecture. And this is like some small architectures so they'll run reasonably quickly so these ran at about six minutes or so.", "tokens": [407, 586, 291, 393, 536, 286, 600, 733, 295, 2780, 807, 293, 3031, 257, 1326, 819, 6352, 295, 3174, 293, 15245, 35592, 337, 341, 9482, 13, 400, 341, 307, 411, 512, 1359, 6331, 1303, 370, 436, 603, 1190, 23551, 2661, 370, 613, 5872, 412, 466, 2309, 2077, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.08940260498612015, "compression_ratio": 1.5726872246696035, "no_speech_prob": 1.5293660453608027e-06}, {"id": 451, "seek": 292856, "start": 2946.56, "end": 2955.56, "text": " This is very handy right if you go sell all output toggle, you can quickly get an overview of what you're doing.", "tokens": [639, 307, 588, 13239, 558, 498, 291, 352, 3607, 439, 5598, 31225, 11, 291, 393, 2661, 483, 364, 12492, 295, 437, 291, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.08940260498612015, "compression_ratio": 1.5726872246696035, "no_speech_prob": 1.5293660453608027e-06}, {"id": 452, "seek": 295556, "start": 2955.56, "end": 2969.56, "text": " And so from that I kind of got a sense of which things seem to work pretty well for this one and then I replicated that for a different architecture and found those things which you know these are very very different ones transformers based ones confident", "tokens": [400, 370, 490, 300, 286, 733, 295, 658, 257, 2020, 295, 597, 721, 1643, 281, 589, 1238, 731, 337, 341, 472, 293, 550, 286, 46365, 300, 337, 257, 819, 9482, 293, 1352, 729, 721, 597, 291, 458, 613, 366, 588, 588, 819, 2306, 4088, 433, 2361, 2306, 6679], "temperature": 0.0, "avg_logprob": -0.1316788276929534, "compression_ratio": 1.8666666666666667, "no_speech_prob": 1.2026579497614875e-05}, {"id": 453, "seek": 295556, "start": 2969.56, "end": 2984.56, "text": " based, you know find the things which work pretty well consistently across very different architectures. And for those then try those on other ones SwinB2 and Swin.", "tokens": [2361, 11, 291, 458, 915, 264, 721, 597, 589, 1238, 731, 14961, 2108, 588, 819, 6331, 1303, 13, 400, 337, 729, 550, 853, 729, 322, 661, 2306, 3926, 259, 33, 17, 293, 3926, 259, 13], "temperature": 0.0, "avg_logprob": -0.1316788276929534, "compression_ratio": 1.8666666666666667, "no_speech_prob": 1.2026579497614875e-05}, {"id": 454, "seek": 298456, "start": 2984.56, "end": 2992.56, "text": " And yeah then find you know, so then let's toggle the results back on.", "tokens": [400, 1338, 550, 915, 291, 458, 11, 370, 550, 718, 311, 31225, 264, 3542, 646, 322, 13], "temperature": 0.0, "avg_logprob": -0.1225454424634392, "compression_ratio": 1.701657458563536, "no_speech_prob": 8.800654541119002e-06}, {"id": 455, "seek": 298456, "start": 2992.56, "end": 2999.56, "text": " So I'm kind of looking at two things. The first is what's the error rate at the end of training the other is what's the TTA error rate.", "tokens": [407, 286, 478, 733, 295, 1237, 412, 732, 721, 13, 440, 700, 307, 437, 311, 264, 6713, 3314, 412, 264, 917, 295, 3097, 264, 661, 307, 437, 311, 264, 314, 8241, 6713, 3314, 13], "temperature": 0.0, "avg_logprob": -0.1225454424634392, "compression_ratio": 1.701657458563536, "no_speech_prob": 8.800654541119002e-06}, {"id": 456, "seek": 298456, "start": 2999.56, "end": 3011.56, "text": " So my squish worked pretty well for both crop worked pretty well for both. This is all for conv next.", "tokens": [407, 452, 31379, 2732, 1238, 731, 337, 1293, 9086, 2732, 1238, 731, 337, 1293, 13, 639, 307, 439, 337, 3754, 958, 13], "temperature": 0.0, "avg_logprob": -0.1225454424634392, "compression_ratio": 1.701657458563536, "no_speech_prob": 8.800654541119002e-06}, {"id": 457, "seek": 301156, "start": 3011.56, "end": 3019.56, "text": " So 640 by 480 288 by 224 didn't work so well. I mean it's not terrible but it's definitely worse.", "tokens": [407, 1386, 5254, 538, 1017, 4702, 7562, 23, 538, 5853, 19, 994, 380, 589, 370, 731, 13, 286, 914, 309, 311, 406, 6237, 457, 309, 311, 2138, 5324, 13], "temperature": 0.0, "avg_logprob": -0.18389944977812714, "compression_ratio": 1.455813953488372, "no_speech_prob": 1.0952328011626378e-05}, {"id": 458, "seek": 301156, "start": 3019.56, "end": 3025.56, "text": " And 320 by 240 instead.", "tokens": [400, 42429, 538, 26837, 2602, 13], "temperature": 0.0, "avg_logprob": -0.18389944977812714, "compression_ratio": 1.455813953488372, "no_speech_prob": 1.0952328011626378e-05}, {"id": 459, "seek": 301156, "start": 3025.56, "end": 3036.56, "text": " You know, you talk a little bit about what you're looking for in the TTA versus the I just want to say like I mean the thing I care about is TTA because that's what I'm going to end up using.", "tokens": [509, 458, 11, 291, 751, 257, 707, 857, 466, 437, 291, 434, 1237, 337, 294, 264, 314, 8241, 5717, 264, 286, 445, 528, 281, 584, 411, 286, 914, 264, 551, 286, 1127, 466, 307, 314, 8241, 570, 300, 311, 437, 286, 478, 516, 281, 917, 493, 1228, 13], "temperature": 0.0, "avg_logprob": -0.18389944977812714, "compression_ratio": 1.455813953488372, "no_speech_prob": 1.0952328011626378e-05}, {"id": 460, "seek": 303656, "start": 3036.56, "end": 3050.56, "text": " Yeah, that's the main one but like, let's see. In this case, this one's not really any better or worse than our best conv next, but the TTA is way better.", "tokens": [865, 11, 300, 311, 264, 2135, 472, 457, 411, 11, 718, 311, 536, 13, 682, 341, 1389, 11, 341, 472, 311, 406, 534, 604, 1101, 420, 5324, 813, 527, 1151, 3754, 958, 11, 457, 264, 314, 8241, 307, 636, 1101, 13], "temperature": 0.0, "avg_logprob": -0.15789766453984957, "compression_ratio": 1.49375, "no_speech_prob": 4.495044777286239e-06}, {"id": 461, "seek": 303656, "start": 3050.56, "end": 3057.56, "text": " So that that's very encouraging, which is interesting. So this is now for VIT right.", "tokens": [407, 300, 300, 311, 588, 14580, 11, 597, 307, 1880, 13, 407, 341, 307, 586, 337, 691, 3927, 558, 13], "temperature": 0.0, "avg_logprob": -0.15789766453984957, "compression_ratio": 1.49375, "no_speech_prob": 4.495044777286239e-06}, {"id": 462, "seek": 305756, "start": 3057.56, "end": 3074.56, "text": " Now VIT we can't do the rectangular ones because VIT has a fixed input size it has to say their final transformation has to be 224 by 224. So if you pass an int instead of a tuple, it's going to create square final images.", "tokens": [823, 691, 3927, 321, 393, 380, 360, 264, 31167, 2306, 570, 691, 3927, 575, 257, 6806, 4846, 2744, 309, 575, 281, 584, 641, 2572, 9887, 575, 281, 312, 5853, 19, 538, 5853, 19, 13, 407, 498, 291, 1320, 364, 560, 2602, 295, 257, 2604, 781, 11, 309, 311, 516, 281, 1884, 3732, 2572, 5267, 13], "temperature": 0.0, "avg_logprob": -0.12302215475785105, "compression_ratio": 1.4234693877551021, "no_speech_prob": 1.788026111171348e-06}, {"id": 463, "seek": 305756, "start": 3074.56, "end": 3079.56, "text": " And, you know, on the other hand, this one looks crappy.", "tokens": [400, 11, 291, 458, 11, 322, 264, 661, 1011, 11, 341, 472, 1542, 36531, 13], "temperature": 0.0, "avg_logprob": -0.12302215475785105, "compression_ratio": 1.4234693877551021, "no_speech_prob": 1.788026111171348e-06}, {"id": 464, "seek": 307956, "start": 3079.56, "end": 3087.56, "text": " Right, so definitely want to use squish for VIT.", "tokens": [1779, 11, 370, 2138, 528, 281, 764, 31379, 337, 691, 3927, 13], "temperature": 0.0, "avg_logprob": -0.14848052754121668, "compression_ratio": 1.263157894736842, "no_speech_prob": 4.784716111316811e-06}, {"id": 465, "seek": 307956, "start": 3087.56, "end": 3089.56, "text": " And then this one looks pretty good.", "tokens": [400, 550, 341, 472, 1542, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.14848052754121668, "compression_ratio": 1.263157894736842, "no_speech_prob": 4.784716111316811e-06}, {"id": 466, "seek": 307956, "start": 3089.56, "end": 3094.56, "text": " You know, so this was using padding.", "tokens": [509, 458, 11, 370, 341, 390, 1228, 39562, 13], "temperature": 0.0, "avg_logprob": -0.14848052754121668, "compression_ratio": 1.263157894736842, "no_speech_prob": 4.784716111316811e-06}, {"id": 467, "seek": 307956, "start": 3094.56, "end": 3100.56, "text": " So like for VIT I probably wouldn't use crop.", "tokens": [407, 411, 337, 691, 3927, 286, 1391, 2759, 380, 764, 9086, 13], "temperature": 0.0, "avg_logprob": -0.14848052754121668, "compression_ratio": 1.263157894736842, "no_speech_prob": 4.784716111316811e-06}, {"id": 468, "seek": 310056, "start": 3100.56, "end": 3111.56, "text": " Last time I looked at TTA was not really a thing and other modeling frameworks that is given to you. Is that still the case? As far as I know that's true.", "tokens": [5264, 565, 286, 2956, 412, 314, 8241, 390, 406, 534, 257, 551, 293, 661, 15983, 29834, 300, 307, 2212, 281, 291, 13, 1119, 300, 920, 264, 1389, 30, 1018, 1400, 382, 286, 458, 300, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.1806182446687118, "compression_ratio": 1.4725738396624473, "no_speech_prob": 5.862232683284674e-06}, {"id": 469, "seek": 310056, "start": 3111.56, "end": 3114.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1806182446687118, "compression_ratio": 1.4725738396624473, "no_speech_prob": 5.862232683284674e-06}, {"id": 470, "seek": 310056, "start": 3114.56, "end": 3124.56, "text": " You know, so there are a lot of people. Well, one group in particular has been copying without credit everything they can from Fast.ai, they might have done it.", "tokens": [509, 458, 11, 370, 456, 366, 257, 688, 295, 561, 13, 1042, 11, 472, 1594, 294, 1729, 575, 668, 27976, 1553, 5397, 1203, 436, 393, 490, 15968, 13, 1301, 11, 436, 1062, 362, 1096, 309, 13], "temperature": 0.0, "avg_logprob": -0.1806182446687118, "compression_ratio": 1.4725738396624473, "no_speech_prob": 5.862232683284674e-06}, {"id": 471, "seek": 310056, "start": 3124.56, "end": 3126.56, "text": " I won't mention their name.", "tokens": [286, 1582, 380, 2152, 641, 1315, 13], "temperature": 0.0, "avg_logprob": -0.1806182446687118, "compression_ratio": 1.4725738396624473, "no_speech_prob": 5.862232683284674e-06}, {"id": 472, "seek": 312656, "start": 3126.56, "end": 3138.56, "text": " But yeah, so Swin V2, apparently, Tanisha told me is what all the cool kids on Kaggle use nowadays.", "tokens": [583, 1338, 11, 370, 3926, 259, 691, 17, 11, 7970, 11, 314, 7524, 64, 1907, 385, 307, 437, 439, 264, 1627, 2301, 322, 48751, 22631, 764, 13434, 13], "temperature": 0.0, "avg_logprob": -0.21310986578464508, "compression_ratio": 1.2705882352941176, "no_speech_prob": 3.11985713778995e-05}, {"id": 473, "seek": 312656, "start": 3138.56, "end": 3149.56, "text": " That's a fixed resolution. And I found that for the larger sizes there was no 224. You had the choice of 192 or 256.", "tokens": [663, 311, 257, 6806, 8669, 13, 400, 286, 1352, 300, 337, 264, 4833, 11602, 456, 390, 572, 5853, 19, 13, 509, 632, 264, 3922, 295, 1294, 17, 420, 38882, 13], "temperature": 0.0, "avg_logprob": -0.21310986578464508, "compression_ratio": 1.2705882352941176, "no_speech_prob": 3.11985713778995e-05}, {"id": 474, "seek": 314956, "start": 3149.56, "end": 3159.56, "text": " 256 it got so slow I couldn't bear it. But interestingly, even going down to 192, Swin's TTA is actually nearly as good as the best VIT.", "tokens": [38882, 309, 658, 370, 2964, 286, 2809, 380, 6155, 309, 13, 583, 25873, 11, 754, 516, 760, 281, 1294, 17, 11, 3926, 259, 311, 314, 8241, 307, 767, 6217, 382, 665, 382, 264, 1151, 691, 3927, 13], "temperature": 0.0, "avg_logprob": -0.14403438568115234, "compression_ratio": 1.5346534653465347, "no_speech_prob": 8.139346391544677e-06}, {"id": 475, "seek": 314956, "start": 3159.56, "end": 3165.56, "text": " So that's I thought that was pretty encouraging.", "tokens": [407, 300, 311, 286, 1194, 300, 390, 1238, 14580, 13], "temperature": 0.0, "avg_logprob": -0.14403438568115234, "compression_ratio": 1.5346534653465347, "no_speech_prob": 8.139346391544677e-06}, {"id": 476, "seek": 314956, "start": 3165.56, "end": 3172.56, "text": " This one interestingly, like VIT, didn't do nearly as well for the crop.", "tokens": [639, 472, 25873, 11, 411, 691, 3927, 11, 994, 380, 360, 6217, 382, 731, 337, 264, 9086, 13], "temperature": 0.0, "avg_logprob": -0.14403438568115234, "compression_ratio": 1.5346534653465347, "no_speech_prob": 8.139346391544677e-06}, {"id": 477, "seek": 314956, "start": 3172.56, "end": 3177.56, "text": " And again, like VIT, it did pretty well on the pad.", "tokens": [400, 797, 11, 411, 691, 3927, 11, 309, 630, 1238, 731, 322, 264, 6887, 13], "temperature": 0.0, "avg_logprob": -0.14403438568115234, "compression_ratio": 1.5346534653465347, "no_speech_prob": 8.139346391544677e-06}, {"id": 478, "seek": 317756, "start": 3177.56, "end": 3182.56, "text": " And then this is Swin V1, which does have a 224.", "tokens": [400, 550, 341, 307, 3926, 259, 691, 16, 11, 597, 775, 362, 257, 5853, 19, 13], "temperature": 0.0, "avg_logprob": -0.16136399234633847, "compression_ratio": 1.3968253968253967, "no_speech_prob": 1.520513524155831e-05}, {"id": 479, "seek": 317756, "start": 3182.56, "end": 3192.56, "text": " And so here this TTA is okay, but the final results not great. And so to me I'm like, no, it's not fantastic.", "tokens": [400, 370, 510, 341, 314, 8241, 307, 1392, 11, 457, 264, 2572, 3542, 406, 869, 13, 400, 370, 281, 385, 286, 478, 411, 11, 572, 11, 309, 311, 406, 5456, 13], "temperature": 0.0, "avg_logprob": -0.16136399234633847, "compression_ratio": 1.3968253968253967, "no_speech_prob": 1.520513524155831e-05}, {"id": 480, "seek": 317756, "start": 3192.56, "end": 3202.56, "text": " This one's again, you know, it's interesting, the crop, none of them are going well, except for ConvNext.", "tokens": [639, 472, 311, 797, 11, 291, 458, 11, 309, 311, 1880, 11, 264, 9086, 11, 6022, 295, 552, 366, 516, 731, 11, 3993, 337, 2656, 85, 31002, 13], "temperature": 0.0, "avg_logprob": -0.16136399234633847, "compression_ratio": 1.3968253968253967, "no_speech_prob": 1.520513524155831e-05}, {"id": 481, "seek": 320256, "start": 3202.56, "end": 3211.56, "text": " This one's not great either, right? So Swin V1, little unimpressive.", "tokens": [639, 472, 311, 406, 869, 2139, 11, 558, 30, 407, 3926, 259, 691, 16, 11, 707, 517, 8814, 22733, 13], "temperature": 0.0, "avg_logprob": -0.12174607394786363, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.520599744253559e-05}, {"id": 482, "seek": 320256, "start": 3211.56, "end": 3215.56, "text": " So basically that's what I did next. And then I was like, okay, let's pick the ones that look good.", "tokens": [407, 1936, 300, 311, 437, 286, 630, 958, 13, 400, 550, 286, 390, 411, 11, 1392, 11, 718, 311, 1888, 264, 2306, 300, 574, 665, 13], "temperature": 0.0, "avg_logprob": -0.12174607394786363, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.520599744253559e-05}, {"id": 483, "seek": 320256, "start": 3215.56, "end": 3220.56, "text": " And I made a duplicate of Paddy Small.", "tokens": [400, 286, 1027, 257, 23976, 295, 18691, 3173, 15287, 13], "temperature": 0.0, "avg_logprob": -0.12174607394786363, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.520599744253559e-05}, {"id": 484, "seek": 320256, "start": 3220.56, "end": 3226.56, "text": " And I just did a search and replace of small with large. So we've now got ConvNext Large.", "tokens": [400, 286, 445, 630, 257, 3164, 293, 7406, 295, 1359, 365, 2416, 13, 407, 321, 600, 586, 658, 2656, 85, 31002, 33092, 13], "temperature": 0.0, "avg_logprob": -0.12174607394786363, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.520599744253559e-05}, {"id": 485, "seek": 322656, "start": 3226.56, "end": 3233.56, "text": " And the other things I did differently was I got rid of the fixed random seed. So there's no seed equals 42 here.", "tokens": [400, 264, 661, 721, 286, 630, 7614, 390, 286, 658, 3973, 295, 264, 6806, 4974, 8871, 13, 407, 456, 311, 572, 8871, 6915, 14034, 510, 13], "temperature": 0.0, "avg_logprob": -0.06765106083017534, "compression_ratio": 1.7626459143968871, "no_speech_prob": 8.267627890745644e-06}, {"id": 486, "seek": 322656, "start": 3233.56, "end": 3236.56, "text": " And so that means we're going to have a different trainings at each time.", "tokens": [400, 370, 300, 1355, 321, 434, 516, 281, 362, 257, 819, 33856, 412, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.06765106083017534, "compression_ratio": 1.7626459143968871, "no_speech_prob": 8.267627890745644e-06}, {"id": 487, "seek": 322656, "start": 3236.56, "end": 3239.56, "text": " And so these are now not comparable, which is fine.", "tokens": [400, 370, 613, 366, 586, 406, 25323, 11, 597, 307, 2489, 13], "temperature": 0.0, "avg_logprob": -0.06765106083017534, "compression_ratio": 1.7626459143968871, "no_speech_prob": 8.267627890745644e-06}, {"id": 488, "seek": 322656, "start": 3239.56, "end": 3243.56, "text": " You'll see if one of them's like totally crap, right? But they're not totally comparable.", "tokens": [509, 603, 536, 498, 472, 295, 552, 311, 411, 3879, 12426, 11, 558, 30, 583, 436, 434, 406, 3879, 25323, 13], "temperature": 0.0, "avg_logprob": -0.06765106083017534, "compression_ratio": 1.7626459143968871, "no_speech_prob": 8.267627890745644e-06}, {"id": 489, "seek": 322656, "start": 3243.56, "end": 3254.56, "text": " But the point is now once I train each of these, they're training on a different architecture, a different resizing method.", "tokens": [583, 264, 935, 307, 586, 1564, 286, 3847, 1184, 295, 613, 11, 436, 434, 3097, 322, 257, 819, 9482, 11, 257, 819, 725, 3319, 3170, 13], "temperature": 0.0, "avg_logprob": -0.06765106083017534, "compression_ratio": 1.7626459143968871, "no_speech_prob": 8.267627890745644e-06}, {"id": 490, "seek": 325456, "start": 3254.56, "end": 3266.56, "text": " And I append to a list. So I start off with a LEPTList and I append the TTA predictions.", "tokens": [400, 286, 34116, 281, 257, 1329, 13, 407, 286, 722, 766, 365, 257, 441, 8929, 51, 43, 468, 293, 286, 34116, 264, 314, 8241, 21264, 13], "temperature": 0.0, "avg_logprob": -0.16276831097073025, "compression_ratio": 1.4011627906976745, "no_speech_prob": 1.8341417671763338e-05}, {"id": 491, "seek": 325456, "start": 3266.56, "end": 3274.56, "text": " And so I deleted the cells from the duplicate that weren't very good in Paddy Small.", "tokens": [400, 370, 286, 22981, 264, 5438, 490, 264, 23976, 300, 4999, 380, 588, 665, 294, 18691, 3173, 15287, 13], "temperature": 0.0, "avg_logprob": -0.16276831097073025, "compression_ratio": 1.4011627906976745, "no_speech_prob": 1.8341417671763338e-05}, {"id": 492, "seek": 325456, "start": 3274.56, "end": 3281.56, "text": " So you'll see there's no crop anymore. Just Squish and Pad for VIT.", "tokens": [407, 291, 603, 536, 456, 311, 572, 9086, 3602, 13, 1449, 8683, 742, 293, 18691, 337, 691, 3927, 13], "temperature": 0.0, "avg_logprob": -0.16276831097073025, "compression_ratio": 1.4011627906976745, "no_speech_prob": 1.8341417671763338e-05}, {"id": 493, "seek": 328156, "start": 3281.56, "end": 3286.56, "text": " And for SwinV2.", "tokens": [400, 337, 3926, 259, 53, 17, 13], "temperature": 0.0, "avg_logprob": -0.14208235059465682, "compression_ratio": 1.3506493506493507, "no_speech_prob": 6.0484771893243305e-06}, {"id": 494, "seek": 328156, "start": 3286.56, "end": 3293.56, "text": " Probably shouldn't have kept both of the SwinV1s actually. They weren't so good.", "tokens": [9210, 4659, 380, 362, 4305, 1293, 295, 264, 3926, 259, 53, 16, 82, 767, 13, 814, 4999, 380, 370, 665, 13], "temperature": 0.0, "avg_logprob": -0.14208235059465682, "compression_ratio": 1.3506493506493507, "no_speech_prob": 6.0484771893243305e-06}, {"id": 495, "seek": 328156, "start": 3293.56, "end": 3305.56, "text": " And then what I did in the very last Kaggle entry was I took the two VIT ones because they were the clear best.", "tokens": [400, 550, 437, 286, 630, 294, 264, 588, 1036, 48751, 22631, 8729, 390, 286, 1890, 264, 732, 691, 3927, 2306, 570, 436, 645, 264, 1850, 1151, 13], "temperature": 0.0, "avg_logprob": -0.14208235059465682, "compression_ratio": 1.3506493506493507, "no_speech_prob": 6.0484771893243305e-06}, {"id": 496, "seek": 330556, "start": 3305.56, "end": 3316.56, "text": " And I appended them to the list. So they were there twice. So it's just a slightly clunky way of doing a weighted average, if you like.", "tokens": [400, 286, 724, 3502, 552, 281, 264, 1329, 13, 407, 436, 645, 456, 6091, 13, 407, 309, 311, 445, 257, 4748, 596, 25837, 636, 295, 884, 257, 32807, 4274, 11, 498, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.10101135969161987, "compression_ratio": 1.6326530612244898, "no_speech_prob": 7.766244380036369e-06}, {"id": 497, "seek": 330556, "start": 3316.56, "end": 3322.56, "text": " Yes, stack them all together. Take the mean of their predictions.", "tokens": [1079, 11, 8630, 552, 439, 1214, 13, 3664, 264, 914, 295, 641, 21264, 13], "temperature": 0.0, "avg_logprob": -0.10101135969161987, "compression_ratio": 1.6326530612244898, "no_speech_prob": 7.766244380036369e-06}, {"id": 498, "seek": 330556, "start": 3322.56, "end": 3330.56, "text": " Find the argmax across the mean of their predictions to get the predictions and then submit in the same way as before.", "tokens": [11809, 264, 3882, 41167, 2108, 264, 914, 295, 641, 21264, 281, 483, 264, 21264, 293, 550, 10315, 294, 264, 912, 636, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.10101135969161987, "compression_ratio": 1.6326530612244898, "no_speech_prob": 7.766244380036369e-06}, {"id": 499, "seek": 333056, "start": 3330.56, "end": 3341.56, "text": " So that was basically my process. It's very not particularly thoughtful. It's pretty mechanical, which is what I like about it.", "tokens": [407, 300, 390, 1936, 452, 1399, 13, 467, 311, 588, 406, 4098, 21566, 13, 467, 311, 1238, 12070, 11, 597, 307, 437, 286, 411, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1193009689450264, "compression_ratio": 1.4486486486486487, "no_speech_prob": 1.8057056877296418e-05}, {"id": 500, "seek": 333056, "start": 3341.56, "end": 3346.56, "text": " In fact, you could probably automate this whole thing.", "tokens": [682, 1186, 11, 291, 727, 1391, 31605, 341, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.1193009689450264, "compression_ratio": 1.4486486486486487, "no_speech_prob": 1.8057056877296418e-05}, {"id": 501, "seek": 333056, "start": 3346.56, "end": 3358.56, "text": " How critical is this model stacking in Kaggle? Just curious how you think about that.", "tokens": [1012, 4924, 307, 341, 2316, 41376, 294, 48751, 22631, 30, 1449, 6369, 577, 291, 519, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1193009689450264, "compression_ratio": 1.4486486486486487, "no_speech_prob": 1.8057056877296418e-05}, {"id": 502, "seek": 335856, "start": 3358.56, "end": 3366.56, "text": " I mean, it's like I, I mean, you can kind of, I mean, we should try, right? We should probably submit.", "tokens": [286, 914, 11, 309, 311, 411, 286, 11, 286, 914, 11, 291, 393, 733, 295, 11, 286, 914, 11, 321, 820, 853, 11, 558, 30, 492, 820, 1391, 10315, 13], "temperature": 0.0, "avg_logprob": -0.1301215644021636, "compression_ratio": 1.6067961165048543, "no_speech_prob": 8.529505066690035e-06}, {"id": 503, "seek": 335856, "start": 3366.56, "end": 3377.56, "text": " In fact, let's, well, we're kind of out of time. How about next time? Let's submit just the VIT, the best VIT, and we'll see how it goes.", "tokens": [682, 1186, 11, 718, 311, 11, 731, 11, 321, 434, 733, 295, 484, 295, 565, 13, 1012, 466, 958, 565, 30, 961, 311, 10315, 445, 264, 691, 3927, 11, 264, 1151, 691, 3927, 11, 293, 321, 603, 536, 577, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.1301215644021636, "compression_ratio": 1.6067961165048543, "no_speech_prob": 8.529505066690035e-06}, {"id": 504, "seek": 335856, "start": 3377.56, "end": 3383.56, "text": " And that will give us, yeah, that will give us a sense of how much the ensembling matters.", "tokens": [400, 300, 486, 976, 505, 11, 1338, 11, 300, 486, 976, 505, 257, 2020, 295, 577, 709, 264, 12567, 2504, 1688, 7001, 13], "temperature": 0.0, "avg_logprob": -0.1301215644021636, "compression_ratio": 1.6067961165048543, "no_speech_prob": 8.529505066690035e-06}, {"id": 505, "seek": 338356, "start": 3383.56, "end": 3391.56, "text": " We kind of know ahead of time, it's not going to matter hugely.", "tokens": [492, 733, 295, 458, 2286, 295, 565, 11, 309, 311, 406, 516, 281, 1871, 27417, 13], "temperature": 0.0, "avg_logprob": -0.15225510795911154, "compression_ratio": 1.3543307086614174, "no_speech_prob": 1.696318395261187e-05}, {"id": 506, "seek": 338356, "start": 3391.56, "end": 3397.56, "text": " I mean, you specifically said on Kaggle. On Kaggle, it definitely matters because in Kaggle you want to win.", "tokens": [286, 914, 11, 291, 4682, 848, 322, 48751, 22631, 13, 1282, 48751, 22631, 11, 309, 2138, 7001, 570, 294, 48751, 22631, 291, 528, 281, 1942, 13], "temperature": 0.0, "avg_logprob": -0.15225510795911154, "compression_ratio": 1.3543307086614174, "no_speech_prob": 1.696318395261187e-05}, {"id": 507, "seek": 339756, "start": 3397.56, "end": 3413.56, "text": " But in real life, my small convexed got 97, well rounded up, that's 98%.", "tokens": [583, 294, 957, 993, 11, 452, 1359, 42432, 292, 658, 23399, 11, 731, 23382, 493, 11, 300, 311, 20860, 6856], "temperature": 0.0, "avg_logprob": -0.15280407764872567, "compression_ratio": 1.3594771241830066, "no_speech_prob": 3.555501280061435e-06}, {"id": 508, "seek": 339756, "start": 3413.56, "end": 3426.56, "text": " And my ensemble got 98.8%. Now that's, in terms of error rate, that's nearly halving the error. So I guess that's actually pretty good.", "tokens": [400, 452, 19492, 658, 20860, 13, 23, 6856, 823, 300, 311, 11, 294, 2115, 295, 6713, 3314, 11, 300, 311, 6217, 7523, 798, 264, 6713, 13, 407, 286, 2041, 300, 311, 767, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.15280407764872567, "compression_ratio": 1.3594771241830066, "no_speech_prob": 3.555501280061435e-06}, {"id": 509, "seek": 342656, "start": 3426.56, "end": 3433.56, "text": " Really important question. How do you keep track of what submissions are tied to which notebook?", "tokens": [4083, 1021, 1168, 13, 1012, 360, 291, 1066, 2837, 295, 437, 40429, 366, 9601, 281, 597, 21060, 30], "temperature": 0.0, "avg_logprob": -0.10791613260904948, "compression_ratio": 1.5537190082644627, "no_speech_prob": 5.223598418524489e-05}, {"id": 510, "seek": 342656, "start": 3433.56, "end": 3441.56, "text": " Oh, I just put a description to remind me, but you know, a better approach would actually be to write the notebook name there, which is what I normally do.", "tokens": [876, 11, 286, 445, 829, 257, 3855, 281, 4160, 385, 11, 457, 291, 458, 11, 257, 1101, 3109, 576, 767, 312, 281, 2464, 264, 21060, 1315, 456, 11, 597, 307, 437, 286, 5646, 360, 13], "temperature": 0.0, "avg_logprob": -0.10791613260904948, "compression_ratio": 1.5537190082644627, "no_speech_prob": 5.223598418524489e-05}, {"id": 511, "seek": 342656, "start": 3441.56, "end": 3449.56, "text": " But in this case, I wasn't taking it particularly seriously, I guess. I was only planning to do these ones and that was it.", "tokens": [583, 294, 341, 1389, 11, 286, 2067, 380, 1940, 309, 4098, 6638, 11, 286, 2041, 13, 286, 390, 787, 5038, 281, 360, 613, 2306, 293, 300, 390, 309, 13], "temperature": 0.0, "avg_logprob": -0.10791613260904948, "compression_ratio": 1.5537190082644627, "no_speech_prob": 5.223598418524489e-05}, {"id": 512, "seek": 344956, "start": 3449.56, "end": 3457.56, "text": " So it's basically like, okay, do one with a single small model, then do one with an ensemble of small models and then do one with an ensemble of big models.", "tokens": [407, 309, 311, 1936, 411, 11, 1392, 11, 360, 472, 365, 257, 2167, 1359, 2316, 11, 550, 360, 472, 365, 364, 19492, 295, 1359, 5245, 293, 550, 360, 472, 365, 364, 19492, 295, 955, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1700946044921875, "compression_ratio": 1.761467889908257, "no_speech_prob": 1.6182671970454976e-05}, {"id": 513, "seek": 344956, "start": 3457.56, "end": 3464.56, "text": " And then it's after I submitted that that I thought, oh, I should probably wait the VITs a bit higher. So I ended up with the fourth one.", "tokens": [400, 550, 309, 311, 934, 286, 14405, 300, 300, 286, 1194, 11, 1954, 11, 286, 820, 1391, 1699, 264, 691, 3927, 82, 257, 857, 2946, 13, 407, 286, 4590, 493, 365, 264, 6409, 472, 13], "temperature": 0.0, "avg_logprob": -0.1700946044921875, "compression_ratio": 1.761467889908257, "no_speech_prob": 1.6182671970454976e-05}, {"id": 514, "seek": 344956, "start": 3464.56, "end": 3471.56, "text": " So it's pretty easy for me, though, I did four significant submissions. So easy to track.", "tokens": [407, 309, 311, 1238, 1858, 337, 385, 11, 1673, 11, 286, 630, 1451, 4776, 40429, 13, 407, 1858, 281, 2837, 13], "temperature": 0.0, "avg_logprob": -0.1700946044921875, "compression_ratio": 1.761467889908257, "no_speech_prob": 1.6182671970454976e-05}, {"id": 515, "seek": 347156, "start": 3471.56, "end": 3485.56, "text": " Yeah, I think now that I know actually that I'm doing a little bit more, because I actually did want to try one more thing. I think what I'll probably do is I'll go back and I'm going to, you can edit these, I'm going to go and I'll put in the notebook name in each one.", "tokens": [865, 11, 286, 519, 586, 300, 286, 458, 767, 300, 286, 478, 884, 257, 707, 857, 544, 11, 570, 286, 767, 630, 528, 281, 853, 472, 544, 551, 13, 286, 519, 437, 286, 603, 1391, 360, 307, 286, 603, 352, 646, 293, 286, 478, 516, 281, 11, 291, 393, 8129, 613, 11, 286, 478, 516, 281, 352, 293, 286, 603, 829, 294, 264, 21060, 1315, 294, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.12945448662623887, "compression_ratio": 1.896153846153846, "no_speech_prob": 1.921828879858367e-05}, {"id": 516, "seek": 347156, "start": 3485.56, "end": 3500.56, "text": " And then, and then I wouldn't go back and change those notebooks later, unless there was like, I probably never, I would, I would just duplicate them and make changes in the duplicate and rename them to something sensible.", "tokens": [400, 550, 11, 293, 550, 286, 2759, 380, 352, 646, 293, 1319, 729, 43782, 1780, 11, 5969, 456, 390, 411, 11, 286, 1391, 1128, 11, 286, 576, 11, 286, 576, 445, 23976, 552, 293, 652, 2962, 294, 264, 23976, 293, 36741, 552, 281, 746, 25380, 13], "temperature": 0.0, "avg_logprob": -0.12945448662623887, "compression_ratio": 1.896153846153846, "no_speech_prob": 1.921828879858367e-05}, {"id": 517, "seek": 350056, "start": 3500.56, "end": 3504.56, "text": " And then of course this all ends up back in GitHub.", "tokens": [400, 550, 295, 1164, 341, 439, 5314, 493, 646, 294, 23331, 13], "temperature": 0.0, "avg_logprob": -0.2603095920606591, "compression_ratio": 1.481283422459893, "no_speech_prob": 9.817755199037492e-06}, {"id": 518, "seek": 350056, "start": 3504.56, "end": 3510.56, "text": " So I will always see. Yeah, see what's going on.", "tokens": [407, 286, 486, 1009, 536, 13, 865, 11, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.2603095920606591, "compression_ratio": 1.481283422459893, "no_speech_prob": 9.817755199037492e-06}, {"id": 519, "seek": 350056, "start": 3510.56, "end": 3517.56, "text": " So this is like MLOps Hamel without", "tokens": [407, 341, 307, 411, 376, 20184, 1878, 8234, 338, 1553], "temperature": 0.0, "avg_logprob": -0.2603095920606591, "compression_ratio": 1.481283422459893, "no_speech_prob": 9.817755199037492e-06}, {"id": 520, "seek": 350056, "start": 3517.56, "end": 3525.56, "text": " it's like you have a, you'd like every like quote run is a notebook, like in a sense like the way to buy and kind of keep track.", "tokens": [309, 311, 411, 291, 362, 257, 11, 291, 1116, 411, 633, 411, 6513, 1190, 307, 257, 21060, 11, 411, 294, 257, 2020, 411, 264, 636, 281, 2256, 293, 733, 295, 1066, 2837, 13], "temperature": 0.0, "avg_logprob": -0.2603095920606591, "compression_ratio": 1.481283422459893, "no_speech_prob": 9.817755199037492e-06}, {"id": 521, "seek": 350056, "start": 3525.56, "end": 3526.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2603095920606591, "compression_ratio": 1.481283422459893, "no_speech_prob": 9.817755199037492e-06}, {"id": 522, "seek": 350056, "start": 3526.56, "end": 3528.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2603095920606591, "compression_ratio": 1.481283422459893, "no_speech_prob": 9.817755199037492e-06}, {"id": 523, "seek": 352856, "start": 3528.56, "end": 3541.56, "text": " Exactly. But I mean the only reason I can kind of do this is because I had already done, like, lots of runs of models to find out which ones I can focus on right so I didn't have to try 100 architectures.", "tokens": [7587, 13, 583, 286, 914, 264, 787, 1778, 286, 393, 733, 295, 360, 341, 307, 570, 286, 632, 1217, 1096, 11, 411, 11, 3195, 295, 6676, 295, 5245, 281, 915, 484, 597, 2306, 286, 393, 1879, 322, 558, 370, 286, 994, 380, 362, 281, 853, 2319, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.13207211213953354, "compression_ratio": 1.5551330798479088, "no_speech_prob": 3.905334779119585e-06}, {"id": 524, "seek": 352856, "start": 3541.56, "end": 3551.56, "text": " I mean in a way, it forces you to really look at it closely. Yeah, if you just like have this dashboard. Right.", "tokens": [286, 914, 294, 257, 636, 11, 309, 5874, 291, 281, 534, 574, 412, 309, 8185, 13, 865, 11, 498, 291, 445, 411, 362, 341, 18342, 13, 1779, 13], "temperature": 0.0, "avg_logprob": -0.13207211213953354, "compression_ratio": 1.5551330798479088, "no_speech_prob": 3.905334779119585e-06}, {"id": 525, "seek": 352856, "start": 3551.56, "end": 3557.56, "text": " My view is that this approach, you will actually become a better deep learning practitioner.", "tokens": [1222, 1910, 307, 300, 341, 3109, 11, 291, 486, 767, 1813, 257, 1101, 2452, 2539, 32125, 13], "temperature": 0.0, "avg_logprob": -0.13207211213953354, "compression_ratio": 1.5551330798479088, "no_speech_prob": 3.905334779119585e-06}, {"id": 526, "seek": 355756, "start": 3557.56, "end": 3572.56, "text": " And I also believe almost nobody does this approach and I almost feel like there are very few people I come across who are actually good deep learning practitioners, but not many people seem to know what works and what doesn't.", "tokens": [400, 286, 611, 1697, 1920, 5079, 775, 341, 3109, 293, 286, 1920, 841, 411, 456, 366, 588, 1326, 561, 286, 808, 2108, 567, 366, 767, 665, 2452, 2539, 25742, 11, 457, 406, 867, 561, 1643, 281, 458, 437, 1985, 293, 437, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.10897706747055054, "compression_ratio": 1.497584541062802, "no_speech_prob": 4.06787839892786e-05}, {"id": 527, "seek": 355756, "start": 3572.56, "end": 3575.56, "text": " So, yeah.", "tokens": [407, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.10897706747055054, "compression_ratio": 1.497584541062802, "no_speech_prob": 4.06787839892786e-05}, {"id": 528, "seek": 355756, "start": 3575.56, "end": 3576.56, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.10897706747055054, "compression_ratio": 1.497584541062802, "no_speech_prob": 4.06787839892786e-05}, {"id": 529, "seek": 355756, "start": 3576.56, "end": 3579.56, "text": " Well, that's it, I think.", "tokens": [1042, 11, 300, 311, 309, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.10897706747055054, "compression_ratio": 1.497584541062802, "no_speech_prob": 4.06787839892786e-05}, {"id": 530, "seek": 355756, "start": 3579.56, "end": 3583.56, "text": " Thanks for joining again, and yeah.", "tokens": [2561, 337, 5549, 797, 11, 293, 1338, 13], "temperature": 0.0, "avg_logprob": -0.10897706747055054, "compression_ratio": 1.497584541062802, "no_speech_prob": 4.06787839892786e-05}, {"id": 531, "seek": 358356, "start": 3583.56, "end": 3587.56, "text": " See you all next time. Bye.", "tokens": [3008, 291, 439, 958, 565, 13, 4621, 13], "temperature": 0.0, "avg_logprob": -0.29421305656433105, "compression_ratio": 0.9516129032258065, "no_speech_prob": 0.00016735294775571674}, {"id": 532, "seek": 358756, "start": 3587.56, "end": 3614.56, "text": " Thank you. Take care everybody.", "tokens": [50364, 1044, 291, 13, 3664, 1127, 2201, 13, 51714], "temperature": 0.0, "avg_logprob": -0.33033242225646975, "compression_ratio": 0.7948717948717948, "no_speech_prob": 0.0001479991478845477}], "language": "en"}