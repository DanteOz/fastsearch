{"text": " OK, hi everybody and welcome to practical deep learning for coders lesson 5. We're at a stage now where we're going to be getting deeper and deeper into the details of how these networks actually work. Last week we saw how to use a slightly lower level library than Fast.ai, Huggings First Transformers, to train a pretty nice NLP model. And today we're going to be going back to tabular data and we're going to be trying to build a tabular model actually from scratch. We're going to build a couple of different types of tabular model from scratch. So the problem that I'm going to be working through is the Titanic problem, which if you remember back a couple of weeks is the data set that we looked at on Microsoft Excel. And it has each row is one passenger on the Titanic. This is a real world data set, historic data set, tells you both of that passenger survived, what class they were on in the ship, their sex, age, how many siblings, how many other family members, how much they spent in the fair and whereabouts they embarked, one of three different cities. And you might remember that we built a linear model. We then did the same thing using matrix multiplication and we also created a very, very simple neural network. You know, Excel can do nearly everything we need, as you saw, to build a neural network, but it starts to get unwieldy. And so that's why people don't use Excel for neural networks in practice. Instead we use a programming language like Python. So what we're going to do today is we're going to do the same thing with Python. So we're going to start working through the linear model and neural net from scratch notebook, which you can find on Kaggle or on the course repository. And today what we're going to do is we're going to work through the one in the clean folder. So both for fast book, the book, and course 22, these lessons, the clean folder contains all of our notebooks, but without any pros or any outputs. So here's what it looks like when I open up the linear model and neural net from scratch in Jupyter. What I'm using here is a paper space gradient, which as I mentioned a couple of weeks ago is what I'm going to be doing most things in. It looks a little bit different to the normal paper space gradient because the default view for paper space gradient, at least as I do this course, is their rather awkward notebook editor, which at first glance has the same features as the real Jupyter notebooks and Jupyter lab environments, but in practice are actually missing lots of things. So this is the normal paper space. So remember you have to click this button. And the only reason you might keep this window running is then you might go over here to the machine to remind yourself when you close the other tab to click stop machine. If you're using the free one, it doesn't matter too much. And also when I started, I make sure I've got something to shut down automatically in case I forget. So other than that, we can stay in this tab. And because this is Jupyter lab that runs, and you can always switch over to classic Jupyter notebook if you want to. So given that they've got tabs inside tabs, I normally maximize it at this point. And it's really helpful to know the keyboard shortcuts. So control shift square bracket right and left switch between tabs. That's one of the key things to know about. OK. So I've opened up the clean version of the linear model and neural net from scratch notebook. And so remember when you go back through the video the second time or through the notebook a second time, this is generally what you want to be doing is going through the clean notebook and before you run each cell, try to think about like, oh, what Jeremy say? Why are we doing this? What output would I expect? Make sure you get the output you'd expect. And if you're not sure why something is the way it is, try changing it and see what happens. And then if you're still not sure, well, why did that thing not work the way I expect? You know, search the forums if anybody's asked that question before, and you can ask the question on the forum yourself if you're still not sure. So as I think we've mentioned briefly before, I find it really nice to be able to use the same notebook both on Kaggle and off Kaggle. So most of my notebooks start with basically the same cell, which is something that just checks whether we're on Kaggle. So Kaggle sets an environment variable. So we can just check for it. And that way we know if we're on Kaggle. And so then if we are on Kaggle, a notebook that's part of a competition will already have the data downloaded and unzipped for you. Otherwise, if I haven't downloaded the data before, then I need to download it and unzip it. OK? So Kaggle is a PIP installable module. So you type PIP install Kaggle. If you're not sure how to do that, you should check our deep dive lessons to see exactly the steps. But roughly speaking, you can use your console, PIP install, and whatever you want to install. Or as we've seen before, you can do it directly in a notebook by putting an explanation mark at the start. So that's going to run not Python but a shell command. OK, so that's enough to ensure that we have the data downloaded and a variable called path that's pointing at it. Most of the time we're going to be using at least PyTorch and NumPy. So we import those so they're available to Python. And when we're working with tabular data, as we talked about before, we're generally also going to want to use pandas, and it's really important that you're somewhat familiar with the kind of basic API of these three libraries. And I've recommended Wes McKinney's book before, particularly for these ones. One thing just by the way is that these things tend to assume you've got a very narrow screen, which is really annoying because it always wraps things. So if you want to put these three lines as well, then it just makes sure that everything is going to use up the screen properly. OK. So as we've seen before, you can read a common separated values file with pandas and you can take a look at the first two lines and the last two lines and how big it is. And so here's the same thing as our spreadsheet. OK, so there's our data from the spreadsheet and here it is as a data frame. So if we go data frame dot is and a that returns a new data frame in which every column it tells us whether or not that particular value is nan. So nan is not a number. And most the most common reason you get that is because it was missing. OK, so a missing value is obviously not a number. So we in the Excel version, we did something you should never usually do. We deleted all the rows with missing data just because in Excel it's a little bit harder to work with. In pandas, it's very easy to work with. First of all, we can just sum up what I just showed you. Now, if you call sum on a data frame, it sums up each column. Right. So you can see that there's kind of some small foundational concepts in pandas, which when you put them together, take you a long way. So one idea is this idea that you can call a method on a data frame and it calls it on every row. And then you can call a reduction on that and it reduces each column. And so now we've got the total and in Python and pandas and NumPy and PyTorch, you can treat a Boolean as a number and true will be one, false will be zero. So this is the number of missing values in each column. So we can see that cabin out of 891 rows, it's nearly always empty. Age is empty a bit of the time. Embarked is almost never empty. So if you remember from Excel, we need to multiply a coefficient by each column. That's how we create a linear model. So how would you multiply a coefficient by a missing value? You can't. There's lots of ways of it's called imputing missing values or replacing missing value with a number. The easiest, which always works is to replace missing values with the mode of a column. The mode is the most common value that works both the categorical variables. It's the most common category and continuous variables. That's the most common number. So you can get the mode by calling df.mode. One thing that's a bit awkward is that if there's a tie for the mode, so there's more than one thing that's the most common, it's going to return multiple rows. So I need to return the zeroth row. So here is the mode of every column. So we can replace the missing values for age with 24 and the missing values for cabin with b96, b98, and embarked with s. I'll just mention in passing, I am not going to describe every single method we call and every single function we use. And that is not because you're an idiot if you don't already know them. Nobody knows them all. But I don't know which particular subset of them you don't know. So let's assume just to pick a number at random that the average fast AI student knows 80% of the functions we call. Then I could tell you what every function is, in which case 80% of the time I'm wasting your time because you already know. Or I could pick 20% of them at random, in which case I'm still not helping because most of the time it's not the ones you don't know. My approach is that for the ones that are pretty common, I'm just not going to mention it at all because I'm assuming that you'll Google it. So it's really important to know. So for example, if you don't know what iloc is, that's not a problem. It doesn't mean you're stupid. It just means you haven't used it yet and you should Google it. So I'll mention in this particular case, this is one of the most important pandas, methods, because it gives you the row located at this index, i for index and loc for location. So this is the zeroth row. But yeah, I do kind of go through things a little bit quickly on the assumption that students, fast AI students, are proactive, curious people. And if you're not a proactive, curious person, then you could either decide to become one for the purpose of this course, or maybe this course isn't for you. All right. So a data frame has a very convenient method called fillNA, and that's going to replace the not a numbers with whatever I put here. And the nice thing about pandas is it kind of has this understanding that columns match to columns. So it's going to take the mode from each column and match it to the same column in the data frame and fill in those missing values. Normally, that would return a new data frame. Many things, including this one in pandas, have an in place argument that says actually modify the original one. And so if I run that, now if I call.isNA.sum, they're all zero. So that's like the world's simplest way to get rid of missing values. OK, so why did we do it the world's simplest way? Because honestly, this doesn't make much difference most of the time. And so I'm not going to spend time the first time I go through and build a baseline model doing complicated things when I don't necessarily know that I need complicated things. And so imputing missing values is an example of something that most of the time, this dumb way, which always works without even thinking about it, will be quite good enough for nearly all the time. So we keep things simple where we can. John, question. Jeremy, we've got a question on this topic. Javier is sort of commenting on the assumption involved in substituting with the mode. And he's asking, in your experience, what are the pros and cons of doing this versus, for example, discarding cabin or age as fields that we even train the model on? Yeah, so I would certainly never throw them out, right? There's just no reason to throw away data. And there's lots of reasons to not throw away data. So for example, when we use the Fast AI library, which we'll use later, one of the things it does, which is actually a really good idea, is it creates a new column for everything that's got missing values, which is Boolean, which is, did that column have a missing value for this row? And so maybe it turns out that cabin being empty is a great predictor. So yeah, I don't throw out rows and I don't throw out columns. Okay. So it's helpful to understand a bit more about our data set and a really helpful, I've already imported this, a really helpful quick method. And again, it's kind of nice to know a few quick things you can do to get a picture of what's happening in your data is describe. And so describe, you can say, okay, describe all the numeric variables. And that gives me a quick sense of what's going on here. So we can see survive clearly is just zeros and ones, because all of the quartiles are zeros and ones. Looks like p class is one, two, three. What else do we see? Fair is an interesting one, right? Lots of smallish numbers and one really big number, so probably long tailed. So yeah, good to have a look at this to see what's going on for your numeric variables. So as I said, fair looks kind of interesting. To find out what's going on there, I would generally go with a histogram. So if you can't quite remember what a histogram is, again, Google it. But in short, it shows you for each amount of fair, how often does that fair appear. It shows me here that the vast majority of fairs are less than $50, but there's a few right up here to 500. So this is what we call a long tailed distribution, a small number of really big values and lots of small ones. There are some types of model which do not like long tailed distributions. Linear models is certainly one of them. And neural nets are generally better behaved without them as well. Luckily there's an almost surefire way to turn a long tailed distribution into a more reasonably centered distribution, and that is to take the log. We use logs a lot in machine learning. For those of you that haven't touched them since year 10 math, it would be a very good time to go to Khan Academy or something and remind yourself about what logs are and what they look like because they're actually really, really important. But the basic shape of the log curve causes it to make really big numbers less really big and doesn't change really small numbers very much at all. So if we take the log, now log of 0 is nan, so a useful trick is to just do log plus 1. And in fact there is a log p1 if you want to do that, it does the same thing. So if we look at the histogram of that, you can see it's much more sensible now. It's kind of centered and it doesn't have this big long tail. So that's pretty good. So we'll be using that column in the future. As a rule of thumb, stuff like money or population, things that can grow exponentially, you very often want to take the log of. So if you have a column with a dollar sign on it, that's a good sign it might be something to take the log of. So there was another one here which is we had a numeric, which actually doesn't look numeric at all. It looks like it's actually categories. So pandas gives us a.unique and so we can see, yep, they're just 1, 2 and 3 are all the levels of p class. That's their first class, second class or third class. We can also describe all the non-numeric variables. And so we can see here that not surprisingly names are unique because the count of names is the same as count unique. There's two sexes, 681 different tickets, 147 different cabins and three levels of embarked. So we cannot multiply the letter S by a coefficient or the word male by a coefficient. So what do we do? What we do is we create something called dummy variables. Dummy variables are, we can just go get dummies, a column that says for example is sex female, is sex male, is p class 1, is p class 2, is p class 3. So for every possible level of every possible categorical variable is a Boolean column of did that row have that value of that column. So I think we've briefly talked about this before that there's a couple of different ways we can do this. One is that for an n level categorical variable we could use n minus 1 levels, in which case we also need a constant term in our model. Pandas by default shows all n levels, although you can pass an argument to change that if you want. There we are, dropped first. I kind of like having all of them sometimes because then you don't have to put in a constant term and it's a bit less annoying and it can be a bit easier to interpret, but I don't feel strongly about it either way. Okay, so here's a list of all of the columns that pandas added. I guess strictly speaking I probably should have automated that, but never mind, I just copied and pasted them. And so here are a few examples of the added columns. In Unix, pandas, lots of things like that, head means the first few rows or the first few lines. So 5 by default in pandas, so here you can see they're never both male and female, they're never neither, they're always one or the other. So with that now we've got numbers, which we can multiply by coefficients. It's not going to work for name, obviously, because we'd have 891 columns and all of them would be unique. So we'll ignore that for now. That doesn't mean it's have to always ignore it. And in fact something I did do on the forum topic, because I made a list of some nice Titanic notebooks that I found, and quite a few of them really go hard on this name column. And in fact one of them, yeah this one, in what I believe is, yes, Krestyot's first ever Kaggle notebook. He's now the number one ranked Kaggle notebook person in the world. So this is a very good start. And he got a much better score than any model that we're going to create in this course, using only that column name. And basically, yeah, he came up with this simple little decision tree by recognizing all of the information that's in a name column. So yeah, we don't have to treat a big string of letters like this as a random big string of letters. We can use our domain expertise to recognize that things like Mr. have meaning, and that people with the same surname might be in the same family, and actually figure out quite a lot from that. But that's not something I'm going to do. I'll let you look at those notebooks if you're interested in the feature engineering. And I do think that they're very interesting, so do check them out. Our focus today is on building a linear model in a neural net from scratch, not on tabular feature engineering, even though that's also a very important subject. OK, so we talked about how matrix multiplication makes linear models much easier. And the other thing we did in Excel was element-wise multiplication. Both of those things are much easier if we use PyTorch instead of plain Python. Or we could use NumPy. But I tend to just stick with PyTorch when I can, because it's easier to learn one library than two. So I just do everything in PyTorch. I almost never touch NumPy nowadays. They're both great, but they do everything each other does, except PyTorch also does differentiation and GPUs, so why not just learn PyTorch? So to turn a column into something that I can do PyTorch calculations on, I have to turn it into a tensor. So a tensor is just what NumPy calls an array. It's what mathematicians would call either a vector or a matrix. Or once we go to higher ranks, mathematicians and physicists just call them tensors. In fact, this idea originally in computer science came from a notation developed in the 50s called APL, which has turned into a programming language in the 60s by a guy called Ken Iverson. And Ken Iverson actually came up with this idea from, he said, his time doing tensor analysis in physics. So these areas are very related. So we can turn the survived column into a tensor, and we'll call that tensor our dependent variable. That's the thing we're trying to predict. So now we need some independent variables. So our independent variables are age, siblings. That one is, oh, yeah, number of other family members. The log of fare that we just created plus all of those dummy columns we added. And so we can now grab those values and turn them into a tensor. And we have to make sure they're floats. We want them all to be the same data type. And PyTorch wants things to be floats if you're going to multiply things together. So there we are. And so one of the most important attributes of a tensor, probably the most important attribute, is its shape, which is how many rows does it have and how many columns does it have. The length of the shape is called its rank. That's the rank of the tensor. It's the number of dimensions or axes that it has. So a vector is rank one, a matrix is rank two, a scalar is rank zero, and so forth. I try not to use too much jargon, but there's some pieces of jargon that are really important because otherwise you're going to have to say the length of the shape again and again. It's much easier to say rank. So we'll use that word a lot. So a table is a rank two tensor. OK. So we've now got the data in good shape. These are independent variables and we've got our dependent variable. So we can now go ahead and do exactly what we did in Excel, which is to multiply our rows of data by some coefficients. And remember, to start with, we create random coefficients. So we're going to need one coefficient for each column. Now in Excel we also had a constant, but in our case now we've got every column, every level in our dummy variables, so we don't need a constant. So the number of coefficients we need is equal to the shape of the independent variables and it's the index one element. That's the number of columns. That's how many coefficients we want. So we can now ask PyTorch to give us some random numbers and coef of them. They're between zero and one. So if we subtract a half, then they'll be centered. And there we go. Before I do that, I set the seed. What that means is in computers, computers in general cannot create truly random numbers. Instead, they can calculate a sequence of numbers that behave in a random like way. That's actually good for us because often in my teaching, I like to be able to say, you know, in the pros, oh, look, that was two. Now it's three or whatever. And if I was using really random numbers, then I couldn't do that because it'd be different each time. So this makes my results reproducible. That means if you run it, you'll get the same random numbers as I do by saying start the pseudo random sequence with this number. I mentioned in passing, a lot of people are very, very into reproducible results. They think it's really important to always do this. I strongly disagree with that. In my opinion, an important part of understanding your data is understanding how much it varies from run to run. So if I'm not teaching and wanting to be able to write things about these pseudo random numbers, I almost never use a manual seed. Instead I like to run things a few times and get an intuitive sense of like, oh, this is like very, very stable. Or oh, this is all over the place. Getting an intuitive understanding of how your data behaves and your model behaves is really important. Now here's one of the coolest lines of code you'll ever see. I know it doesn't look like much, but think about what it's doing. Yeah, that'll do. Okay, so we've multiplied a matrix by a vector. Now that's pretty interesting. Now mathematicians amongst you will know that you can certainly do a matrix vector product, but that's not what we've done here at all. We've used element-wise multiplication. So normally if we did the element-wise multiplication of two vectors, it would multiply element one with element one, element two with element two, and so forth, and create a vector of the same size output. But here we've done a matrix times a vector. How does that work? This is using the incredibly powerful technique of broadcasting. And broadcasting again comes from APL, a notation invented in the 50s and a programming language developed in the 60s. And it's got a number of benefits. Basically what it's going to do is it's going to take each coefficient and multiply them in turn by every row in our matrix. So if you look at the shape of our independent variable and the shape of our coefficients, you can see that each one of these coefficients can be multiplied by each of these 891 values in turn. And so the reason we call it broadcasting is it's as if this is 891 columns by 12 rows by 12 columns. It's as if this was broadcast 891 times. It's as if we had a loop looping 891 times and doing coefficients times row zero, coefficients times row one, coefficients times row zero, two, and so forth, which is exactly what we want. Now, reasons to use broadcasting. Obviously, the code is much more concise. It looks more like math rather than flunky programming with lots of boilerplate. So that's good. Also that broadcasting all happened in optimized C code. And if in fact it's being done on a GPU, it's being done in optimized GPU assembler, CUDA code. It's going to run very, very fast indeed. And this is a trick of why we can use a so-called slow language like Python to do very fast big models is because a single line of code like this can run very quickly on optimized hardware on lots and lots of data. The rules of broadcasting are a little bit subtle and important to know. And so I would strongly encourage you to Google NumPy broadcasting rules and see exactly how they work. But the kind of intuitive understanding of them hopefully you'll get pretty quickly, which is generally speaking, you can kind of as long as the last axes match, it'll broadcast over those axes. You can broadcast a rank three thing with a rank one thing or, you know, the most simple version would be tensor one, two, three times two. So broadcast a scalar over a vector. That's exactly what you would expect. So it's copying effectively that too into each of these spots, multiplying them together. But it doesn't use up any memory to do that. It's kind of a virtual copying if you like. So this line of code, independence by coefficients, is very, very important. And it's the key step that we wanted to take, which is now we know exactly what happens when we multiply the coefficients in. And if you remember back to Excel, we did that product and then in Excel there's a sum product we then added it all together because that's what a linear model is. It's the coefficients times the values added together. So we're now going to add those together. But before we do that, if we did add up this row, you can see that the very first value has a very large magnitude and all the other ones are small. Same with row two. Same with row three. Same with row four. What's going on here? Well, what's going on is that the very first column was age. And age is much bigger than any of the other columns. It's not the end of the world, but it's not ideal, right? Because it means that a coefficient of, say, 0.5 times age means something very different to a coefficient of, say, 0.5 times log fair. And that means that that random coefficient we start with, it's going to mean very different things for different columns. And that's going to make it really hard to optimize. So we would like all the columns to have about the same range. So what we could do, as we did in Excel, is to divide them by the maximum. So the maximum, so we did it for age and we also did it for fair. In this case, I didn't use log. So we can get the max of each row by calling.max. And you can pass in a dimension. Do you want the maximum of the rows or the maximum of the columns? We want the maximum over the rows. So we pass in dimension 0. So those different parts of the shape are called either axes or dimensions. PyTorch calls them dimensions. So that's going to give us the maximum of each row. And if you look at the docs for PyTorch's max function, it'll tell you it returns two things. The actual value of each maximum and the index of which row it was. We want the values. So now, thanks to broadcasting, we can just say take the independent variables and divide them by the vector of values. Again we've got a matrix and a vector. And so this is going to do an element-wise division of each row of this divided by this vector. Again, in a very optimized way. So if we now look at our normalized independent variables by the coefficients, you can see they're all pretty similar values. So that's good. There's lots of different ways of normalizing, but the main ones you'll come across is either dividing by the maximum or subtracting the mean and dividing by the standard deviation. It normally doesn't matter too much. Because I'm lazy, I just pick the easier one. And being lazy and picking the easier one is a very good plan in my opinion. So now that we can see that multiplying them together is working pretty well, we can now add them up. And now we want to add up over the columns. And that would give us predictions. Now obviously just like in Excel, when we started out, they're not useful predictions because they're random coefficients, but they are predictions nonetheless. And here's the first 10 of them. So then remember, we want to use gradient descent to try to make these better. So to do gradient descent, we need a loss. The loss is the measure of how good or bad are these coefficients. My favorite loss function as a kind of like, don't think about it, just chuck something out there, is the mean absolute value. And here it is, torch.absoluteValue of the error, the difference, take the mean. And often stuff like this, you'll see people will use prewritten mean absolute error functions, which is also fine, but I quite like to write it out because I can see exactly what's going on. No confusion. No chance of misunderstanding. So those are all the steps I'm going to need to create coefficients, run a linear model, and get its loss. So what I like to do in my notebooks, like not just for teaching, but all the time, is to like do everything step by step manually, and then just copy and paste the steps into a function. So here's my calc preds function is exactly what I just did. Here's my calc loss function, exactly what I just did. And that way, a lot of people go back and delete all their explorations, or they do them in a different notebook, or they're working in an IDE, they'll go and do it in some line oriented rep or whatever. But if you think about the benefits of keeping it here, when you come back to it in six months, you'll see exactly why you did what you did and how we got there, or if you're showing it to your boss or your colleague, you can see exactly what's happening, what does each step look like. I think this is really very helpful indeed. I know not many people code that way, but I feel strongly that it's a huge productivity win to individuals and teams. So remember from our gradient descent from scratch that the one bit we don't want to do from scratch is calculating derivatives, because it's just menial and boring. So to get PyTorch to do it for us, you have to say, well, what things do you want derivatives for? And of course, we want it for the coefficients. So then we have to say requires grad. And remember, very important in PyTorch, if there's an underscore at the end, that's an in place operation. So this is actually going to change coefs. It also returns them, right, but it also changes them in place. So now we've got exactly the same numbers as before, but with requires grad turned on. So now when we calculate our loss, that doesn't do any other calculations, but what it does store is a gradient function. It's the function that Python has remembered that it would have to do to undo those steps to get back to the gradient. And to say, oh, please actually call that backward gradient function, you call backward. And at that point, it sticks into a.grad attribute, the coefficients, the coefficients gradients. So this tells us that if we increased the age coefficient, the loss would go down. So therefore we should do that. Right? So since negative means increasing this would decrease the loss, that means we need to, if you remember back to the gradient descent from scratch notebook, we need to subtract the coefficients times the learning rate. So we haven't got any particular ideas yet of how to set the learning rate. So for now I just pick, just try a few and still find out what works best. In this case I found.1 worked pretty well. So I now subtract, so again this is sub underscore, so subtract in place from the coefficients the gradient times the learning rate. And so the loss has gone down. That's great, from.54 to.52. So there is one step. So we've now got everything we need to train a linear model. So let's do it. Now as we discussed last week, to see whether your model is any good, it's important that you split your data into training and validation. For the Titanic data set it's actually pretty much fine to use a random split, because back when my friend Margit and I actually created this competition for Kaggle many years ago, that's basically what we did if I remember correctly. So we can split them randomly into a training set and a validation set. So we're just going to use Fast.ai for that. It's very easy to do it manually with NumPy or PyTorch. You can use scikit-learns, train test split. I'm using Fast.ai here partly because it's easy just to remember one way to do things, and this works everywhere, and partly because in the next notebook we're going to be seeing how to do more stuff in Fast.ai, so I want to make sure we have exactly the same split. So those are a list of the indexes of the rows that will be, for example, in the validation set. That's why I call it validation split. So to create the validation independent variables you have to use those to index into the independent variables. And ditto for the dependent variables. And so now we've got our independent variable training set and our validation set, and we've also got the same for the dependent variables. So like I said before, I normally take stuff that I've already done in a notebook, seems to be working, and put them into functions. So here's the step which actually updates coefficients. So let's chuck that into a function. And then the steps that go count loss, stop backward, update coefficients, and then print the loss, we'll chuck that in one function. So just copying and pasting stuff into cells here. And then the bit on the very top of the previous section that got the random numbers, minus.5 requires grad, chuck that in the function. So here we've got something that initializes coefficients, something that does one epoch by updating coefficients. So we can put that together into something that trains the model for any epochs with some learning rate by setting the manual seed, initializing the coefficients, doing one epoch in a loop, and then return the coefficients. So let's go ahead and run that function. So it's printing at the end of each one the loss, and you can see the loss going down from.53, down, down, down, down, down, to a bit under.3. So that's good. So we've successfully built and trained a linear model on a real data set. I mean, it's a Kaggle data set, but it's important to not underestimate how real Kaggle data sets are. They're real data. And this one's a playground data set, so it's not like anybody actually cares about predicting who survived the Titanic, because we already know. But it has all the same features of different data types and missing values and normalization and so forth. So it's a good playground. So it would be nice to see what the coefficients are attached to each variable. So if we just zip together the independent variables and the coefficients, and we don't need the regret anymore, and create a dict of that, there we go. So it looks like older people had less chance of surviving. That makes sense. Older people had less chance of surviving. Also makes sense. So it's good to kind of eyeball these and check that they seem reasonable. Now the metric for this Kaggle competition is not mean absolute error. It's accuracy. Now, of course, we can't use accuracy as a loss function, because it doesn't have a sensible gradient, really. But we should measure accuracy to see how we're doing, because that's going to tell us how we're going against the thing that the Kaggle competition cares about. So we can calculate our predictions. And we'll just say, OK, well, any time the prediction is over 0.5, we'll say that's predicting survival. So that's our predictors of survival. This is the actual in a validation set. So if they're the same, then we predicted it correctly. So here's are we right or wrong for the first 16 rows? We're right more often than not. So if we take the mean of those, remember true equals 1, then that's our accuracy. So we are right about 79% of the time. So that's not bad. So we've successfully created something that's actually predicting who survived the Titanic. That's cool. From scratch. So let's create a function for that, an accuracy function that just does what I showed. And there it is. Now, I'll say another thing like, you know, a weird coding thing for me, you know, weird as it's not that common, is I use less comments than most people, because all of my code lives in notebooks. And the real version of this notebook is full of pros. Right? So when I've taken people through a whole journey about what I've built here and why I've built it and what intermediate results are and check them along the way, the function itself, for me, doesn't need extensive comments. I'd rather explain the thinking of how I got there and show examples of how to use it and so forth. Okay. Now here's the first few predictions we made. And some of the time we're predicting negatives for survival and greater than one for survival. Which doesn't really make much sense, right? People either survived one or they didn't, zero. It would be nice if we had a way to automatically squish everything between zero and one. That's going to make it much easier to optimize. The optimizer doesn't have to try hard to hit exactly one or hit exactly zero, but it can just like try to create a really big number to mean survived or a really small number to mean perished. Here's a great function. Here's a function that as I increase, let's make it even bigger, range. As my numbers get beyond four or five, it's asymptoting to one. And on the negative side, as they get beyond negative four or five, they asymptote to zero. Got to zoom in a bit. But then around about zero, it's pretty much a straight line. This is actually perfect. This is exactly what we want. So here is the equation one over one plus e to the negative minus x. And this is called the sigmoid function. By the way, if you haven't checked out simpi before, definitely do so. This is the symbolic Python package, which can do it's kind of like Mathematica or Wolfram style symbolic calculations, including the ability to plot symbolic expressions, which is pretty nice. PyTorch already has a sigmoid function. I mean, it just calculates this, but it does it in an optimized way. So what if we replaced calc preds? Remember before calc preds was just this. What if we took that and then put it through a sigmoid? So calc preds will now basically the bigger this number is, the closer it's going to get to one, and the smaller it is, the closer it's going to get to zero. This should be a much easier thing to optimize and ensures that all of our values are in a sensible range. Now here's another cool thing about using Jupyter plus Python. Python is a dynamic language. Even though I called calc preds train model calls one epoch, which calls calc loss, which calls calc preds, I can redefine calc preds now. And I don't have to do anything. That's now inserted into Python symbol table, and that's the calc preds that train model will eventually call. So if I now call train model, that's actually going to call my new version of calc preds. So that's a really neat way of doing exploratory programming in Python. I wouldn't release a library that redefines calc preds multiple times. When I'm done, I would just keep the final version, of course. But it's a great way to try things, as you'll see. And so look what's happened. I found I was able to increase the learning rate from 0.1 to 2. It was much easier to optimize, as I guessed. And the loss has improved from 0.295 to 0.197. The accuracy has improved from 0.79 to 0.82, nearly 0.83. So as a rule, this is something that we're pretty much always going to do when we have a binary dependent variable. So a dependent variable that's 1 or 0 is the very last step is chuck it through a sigmoid. Generally speaking, if you're wondering why is my model with a binary dependent variable not training very well, this is the thing you want to check. Oh, are you chucking it through a sigmoid? Or is the thing you're calling chucking it through a sigmoid or not? It can be surprisingly hard to find out if that's happening. So for example, with hugging face transformers, I actually found I had to look in their source code to find out. And I discovered that something I was doing wasn't and didn't seem to be documented anywhere. But it is important to find these things out. As we'll discuss in the next lesson, we'll talk a lot about neural net architecture details. But the details we'll focus on are what happens to the inputs at the very first stage and what happens to the outputs at the very last stage. We'll talk a bit about what happens in the middle, but a lot less. And the reason why is it's the things that you put into the inputs that's going to change for every single data set you do. And what do you want to happen to the outputs? It's just going to happen for every different target that you're trying to hit. So those are the things that you actually need to know about. So for example, this thing of like, well, you need to know about the sigmoid function and you need to know that you need to use it. Fast AI is very good at handling this for you. That's why we haven't had to talk about it much until now. If you say, oh, it's a category block dependent variable, it's going to use the right kind of thing for you. But most things are not so convenient. John, is there a question? Yes, there is. It's back in the feature engineering topic, but a couple of people have liked it, so I thought we'd put it out there. So Shivam says, one concern I have while using get dummies, so it's in that get dummies phase, is what happens while using test data? I have a new category, let's say male, female, and other, and this will have an extra column missing from the training data. How do you take care of that? That's a great question. Yeah, so normally you've got to think about this pretty carefully and check pretty carefully, unless you use fast AI. So fast AI always creates an extra category called other, and at test time, inference time, if you have some level that didn't exist before, we put it into the other category for you. Otherwise, you basically have to do that yourself, or at least check. Generally speaking, it's pretty likely that otherwise your extra level will be silently ignored because it's going to be in the data set, but it's not going to be matched to a column. So yeah, it's a good point and definitely worth checking. For categorical variables with lots of levels, I actually normally like to put the less common ones into an other category, and again, that's something that fast AI will do for you automatically. But yeah, definitely something to keep an eye out for. Good question. Okay, so before we take our break, we'll just do one last thing, which is we will submit this to Kaggle because I think it's quite cool that we have successfully built a model from scratch. So Kaggle provides us with a test.csv, which is exactly the same structure as the training csv, except that it doesn't have a survived column. Now, interestingly, when I tried to submit to Kaggle, I got an error in my code saying that one of my fairs is empty. So that was interesting because the training set doesn't have any empty fairs. So sometimes this will happen that the training set and the test set have different things to deal with. So in this case, I just said, oh, there's only one row. I don't care. So I just replaced the empty one with a zero for fair. So then I just copied and pasted the preprocessing steps from my training data frame and stuck them here for the test data frame and the normalization as well. And so now I just call cat pretz. Is it greater than 0.5? Turn it into a zero or one because that's what Kaggle expects and put that into the survived column, which previously, remember, didn't exist. So then finally, I created data frame with just the two columns, ID and survived. Take it in a CSV file and then I can call the Unix command head just to look at the first few rows. And if you look at the Kaggle competitions data page, you'll see this is what the submission file is expected to look like. So that made me feel good. So I went ahead and submitted it. I didn't mention it. Okay. So anyway, I submitted it and I remember I got like, I think I was basically right in the middle, about 50%, you know, better than half the people who have entered the competition, worse than half the people. So you know, solid middle of the pack result for a linear model from scratch, I think is a pretty good result. So that's a great place to start. So let's take a 10 minute break. We'll come back at 7.17 and continue on our journey. All right, welcome back. You might remember from Excel that after we did the sum product version, we then replaced it with a matrix model play. Wait, not there. Must be here. Here we are. We're the matrix model play. So let's do that step now. So matrix times vector dot sum over axis equals one is the same thing as matrix model play. So here is the times dot sum version. Now we can't use this character for a matrix model play because it means element wise operation. All of the times plus minus divide in pie torch numpy mean element wise. So corresponding elements. So in Python instead we use this character. As far as I know, it's pretty arbitrary. It's one of the ones that wasn't used. So that is an official Python. It's a bit unusual. It's an official Python operator. It means matrix model play. But Python doesn't come with an implementation of it. So because we've imported, because these are tensors, and in pie torch it will use pie torches. And as you can see, they're exactly the same. So we can now just simplify a little bit what we had before. Calc preds is now torch dot sigmoid of the matrix model play. Now there is one thing I'd like to move towards now is that we're going to try to create a neural net in a moment. And so that means rather than treat this as a matrix times a vector, I want to treat this as a matrix times a matrix. Because we're about to add some more columns of coefficients. So we're going to change in a coefs so that rather than creating an n coef vector, we're going to create an n coef by one matrix. So in math we would probably call that a column vector. But I think that's kind of a dumb name in some ways. Because it's a matrix, right? It's a rank two tensor. So the matrix model play will work fine either way. But the key difference is that if we do it this way, then the result of the matrix model play will also be a matrix. It will be again a n rows by one matrix. That means when we compare it to the dependent variable, we need the dependent variable to be an n rows by one matrix as well. So effectively we need to take the n rows long vector and turn it into an n rows by one matrix. So there's some useful, very useful, and at first maybe a bit weird notation in PyTorch NumPy for this, which is if I take my training dependent variables vector, I index into it and colon means every row. So in other words, that just means the whole vector. It's the same basically as that. And then I index into a second dimension. Now this doesn't have a second dimension. So there's a special thing you can do, which is if you index into a second dimension with a special value none, it creates that dimension. So this has the effect of adding an extra trailing dimension to train dependence. So it turns it from a vector to a matrix with one column. So if we look at the shape after that, as you see, it's now got, we call this a unit axis. It's got a trailing unit axis, 713 rows in one column. So now if we train our model, we'll get coefficients just like before, except that it's now a column vector also known as a rank-2 matrix with a trailing unit axis. Okay, so that hasn't changed anything. It's just repeated what we did in the previous section, but it's kind of set us up to expand. Because now that we've done this using matrix model play, we can go crazy and we can go ahead and create a neural network. So with our neural network, remember back to the Excel days, notice here it's the same thing, right? We created a column vector, but we didn't create a column vector. We actually created a matrix with kind of two sets of coefficients. So when we did our matrix multiply, every row gave us two sets of outputs, which we then chucked through value, right? Which remember we just used an if statement and we added them together. So our coefs now, to make a proper neural net, we need one set of coefs here. And so here they are, torch.rand and coef by what? Well, in Excel we just did two, because I kind of got bored of getting everything working properly. But you don't have to worry about filling right and creating columns and blah, blah, blah. You can create as many as you like. So I made something you can change, I called it n hidden, number of hidden activations. I just set it to 20. And as before, we centralize them by making them go from minus 0.5 to 0.5. Now when you do stuff by hand, everything does get more fiddly. If our coefficients aren't, if they're too big or too small, it's not going to train at all. Obviously the gradients will kind of vaguely point in the right direction, but you'll jump too far or not far enough or whatever. So I want my gradients to be about the same as they were before. So I divide by n hidden, because otherwise at the next step when I add up the next matrix multiply it's going to be much bigger than it was before. So it's all very fiddly. So then I want to take, so that's going to give me for every row, it's going to give me 20 activations, 20 values. Just like in Excel we had two values because we had two sets of coefficients. And so to create a neural net, I now need to multiply each of those 20 things by a coefficient. And this time it's going to be a column vector because I want to create one output, predictor of survival. So again, torch.rand, and this time the n hidden will be the number of coefficients by 1. And again, like trying to find something that actually trains properly required me some fiddling around to figure out how much to subtract. And I found if I subtract 0.3 I could get it to train. And then finally, I didn't need a constant term for the first layer as we discussed because our dummy variables have n columns rather than n minus 1 columns. But layer 2 absolutely needs a constant term. And we could do that as we discussed last time by having a column of ones. Although in practice I actually find it's just easier just to create a constant term. So here is a single scalar random number. So those are the coefficients we need. One set of coefficients to go from input to hidden, one goes from hidden to a single output and a constant. So they are all going to need grad. And so now we can change how we calculate predictions. So we're going to pass in all of our coefficients. So a nice thing in Python is if you've got a list or a tuple of values, on the left-hand side you can expand them out into variables. So this is going to be a list of three things. So we'll call them layer 1, layer 2, and the constant term. Because those are the list of three things we returned. So in Python if you just chuck things with commas between them like this, it creates a tuple. A tuple is a list, it's an immutable list. So now we're going to grab those three things. So step one is to do our matrix multiply. And as we discussed, we then have to replace the negatives with zeros. And then we put that through our second matrix multiply, so our second layer, and add the constant term. And remember, of course, at the end, chuck it through a sigmoid. So here is a neural network. Now update coefs previously subtracted the coefficients, the gradients, times the learning rate from the coefficients. But now we've got three sets of those. So we have to just chuck that in a for loop. So change that as well. And now we can go ahead and train our model. Ta-da! We just trained a model. And how does that compare? So the loss function is a little better than before. Accuracy exactly the same as before. And I will say it was very annoying to get to this point, trying to get these constants right and find a learning rate that worked. Like it was super fiddly. But we got there. We got there. It's a very small test set. I don't know if this is necessarily better or worse than the linear model, but it's certainly fine. And I think that's pretty cool that we were able to build a neural net from scratch. That's doing pretty well. But I hear that all the cool kids nowadays are doing deep learning, not just neural nets. So we better make this deep learning. So this one only has one hidden layer. So let's create one with n hidden layers. So for example, let's say we want two hidden layers, 10 activations in each. You can put as many as you like here. So init coefs now is going to have to create a torch.rand for every one of those hidden layers. And then another torch.rand for your constant terms. Stick requires grad and all of them. And then we can return that. So that's how we can just initialize as many layers as we want of coefficients. So the first one, the first layer, so the sizes of each one, the first layer will go from n coef to 10. The second matrix will go from 10 to 10. And the third matrix will go from 10 to 1. So it's worth working through these matrix modellpliers on a spreadsheet or a piece of paper or something to convince yourself that there's the right number of activations at each point. And so then we need to update calc preds so that rather than doing each of these steps manually we now need to loop through all the layers, do the matrix modellply, add the constant, and as long as it's not the last layer, do the ReLU. Why not the last layer? Because remember the last layer has sigmoid. So these things about like, remember what happens on the last layer? This is an important thing you need to know about. You need to kind of check if things aren't working. This thing here is called the activation function, torch.sigmoid and f.relu. They're the activation functions for these layers. One of the most common mistakes amongst people trying to kind of create their own architectures or kind of variants of architectures is to mess up their final activation function and that makes things very hard to train. So make sure we've got a torch.sigmoid at the end and no ReLU at the end. So there's our deep learning calc preds. And then just one last change is now when we update our coefficients we go through all the layers and all the constants. And again, there was so much messing around here with trying to find like exact ranges of random numbers that end up training okay. But eventually I found some and as you can see it gets to about the same loss and about the same accuracy. This code is worth spending time with and when the code's inside a function it can be a little difficult to experiment with. So you know what I would be inclined to do to understand this code is to kind of copy and paste this cell, make it so it's not in a function anymore and then use control shift dash to separate these out into separate cells. And then try to kind of set it up so you can run a single layer at a time or a single coefficient. Make sure you can see what's going on. And that's why we use notebooks is so that we can experiment. And it's only through experimenting like that that at least for me I find that I can really understand what's going on. Nobody can look at this code and immediately say I don't think anybody can. I get it. That all makes perfect sense. But once you try running through it yourself you'll be like oh I see why that's as it is. So you know one thing to point out here is that our neural nets and deep learning models didn't particularly seem to help. So does that mean that deep learning is a waste of time and you just did five lessons that you shouldn't have done? No, not necessarily. This is a playground competition. You're doing it because it's easy to get your head around. But for very small data sets like this with very, very few columns and the columns are really simple, you know deep learning is not necessarily going to give you the best result. In fact, as I mentioned, nothing we do is going to be as good as a carefully designed model that uses just the name column. So you know I think that's an interesting insight, right? Is that the kind of data types which have a very consistent structure like for example images or natural language text documents, quite often you can somewhat brainlessly chuck a deep learning neural net at it and get a great result. Generally for tabular data, I find that's not the case. I find I normally have to think pretty long and hard about the feature engineering in order to get good results. But once you've got good features, you then want a good model. And generally like the more features you have and the more levels in your categorical features and stuff like that, the more value you'll get from more sophisticated models. But yeah, I definitely would say an insight here is that you want to include simple baselines as well. And we're going to be seeing even more of that in a couple of notebooks time. So we've just seen how you can build stuff from scratch. We'll now see why you shouldn't. I mean I say you shouldn't. You should to learn that why you probably won't want to in real life. When you're doing stuff in real life, you don't want to be fiddling around with all this annoying initialization stuff and learning rate stuff and dummy variable stuff and normalization stuff and so forth because we can do it for you. And it's not like everything's so automated that you don't get to make choices, but you want to make the choice not to do things the obvious way and have everything else done the obvious way for you. So that's why we're going to look at this why you should use a framework notebook. And again I'm going to look at the clean version of it. And again in the clean version of it, step one is to download the data as appropriate for the Kaggle or non Kaggle environment and set the display options and set the random seed and read the data frame. Now there was so much fussing around with the doing it from scratch version that I did not want to do any feature engineering because every column I added was another thing I had to think about dummy variables and normalization and random coefficient initialization and blah blah blah. But with a framework everything's so easy you can do all the feature engineering you want. Because this isn't a lesson about feature engineering instead I plagiarized entirely from this fantastic advanced feature engineering tutorial on Kaggle. And what this tutorial found was that in addition to the log fair we've already done that you can do cool stuff with the deck with adding up the number of family members with the people traveling alone how many people are on each ticket and finally we're going to do stuff with the name which is we're going to grab the Mr. Miss Mrs. Master whatever. So we're going to create a function to like do some feature engineering and if you want to learn a bit of Python pandas here's some great lines of code to step through one by one and again like take this out of a function put them into individual cells run each one look up the tutorials what does juror do what does map do what does group by and transform do what does value counts do like these are all like part of the reason I put this here was the folks that haven't done much if any pandas to have some you know examples of functions that I think are useful and actually refactored this code quite a bit to try to show off some features of pandas I think are really nice. So we'll do the same random split as before so passing in the same seed and so now we're going to do the same set of steps that we did manually with fast AI. So we want to create a tabular model data set based on a pandas data frame and here is the data frame these are the train versus validation splits I want to use here's a list of all the stuff I want done please deal with dummy variables for me deal with missing values for me normalize continuous variables for me I'm going to tell you which ones of the categorical variables so here's for example P class is a number but I'm telling fast AI to treat it as categorical here's all the continuous variables here's my dependent variable and the dependent variable is a category. So create data loaders from that place and save models right here in this directory. That's it that's all the pre-processing I need to do even with all those extra engineered features create a learner okay so this remember is something that contains a model and data and I want you to put in two hidden layers with 10 units and 10 units just like we did in our final example. What learning rate should I use? Make a suggestion for me please so call LR find you can use this for any fast AI model now what this does is it starts at a learning rate that's very very small 10 to the negative 7 and it puts in one batch of data and it calculates the loss and then it puts through and then it increases the learning rate slightly and puts through another batch of data and it keeps doing that for higher and higher learning rates and it keeps track of the loss as it increases the learning rate just one batch of data at a time and what happens is for the very small learning rates nothing happens but then once you get high enough the loss starts improving and then as it gets higher it improves faster until you make the learning rate so big that it overshoots and then it kills it and so generally somewhere around here is the learning rate you want fast AI has a few different ways of recommending a learning rate you can look up the docs to see what they mean I generally find if you choose slide and valley and pick one between the two you get a pretty good learning rate so here we've got about 0.01 and about 0.08 so I picked 0.03 so just run a bunch of epochs away it goes ta-da this is a bit crazy after all that we've ended up exactly the same accuracy as the last two models that's just a coincidence right I mean there's nothing particularly about that accuracy and so at this point we can now submit that to Kaggle now remember with the linear model we had to repeat all of the pre-processing steps on the test set in exactly the same way don't have to worry about it with fast AI and fast AI I mean we still have to deal with the fill missing for fair because that's that's that we have to add our feature engineering features but all the pre-processing we just have to use this one function called test DL that says create a data loader that contains exactly the same pre-processing steps that our learner used and that's it that's all you need so just because you want to make sure that your inference time transformations pre-processing are exactly the same as a training time so this is the magic method which does that just one line of code and then to get your predictions you just say get preds and pass in that data loader I just built and so then these three lines of code are the same as the previous notebook and we can take a look at the top and you can see there it is so how did that go I don't remember no I didn't say I think it was again basically middle of the pack if I remember correctly so one of the nice things about now that it's so easy so like add features and build models is we can experiment with things much more quickly so I'm going to show you how easy it is to experiment with you know what's often considered a fairly advanced idea which is called ensembling there's lots of ways of doing ensembling but basically ensembling is about creating multiple models and combining their predictions and the easiest kind of ensemble to do is just to literally just build multiple models and so each one is going to have a different set of randomly initialized coefficients and therefore each one is going to end up with a different set of predictions so I just create a function called ensemble which creates a learner exactly the same as before fits exactly the same as before and returns the predictions and so we'll just use a list comprehension to do that five times so that's going to create a set of five predictions done so now we can take all those predictions and stack them together and take the mean over the rows so that's going to give us the was actually so the mean over the over the first dimension so the mean over the sets of predictions and so that will give us the average prediction of our five models but again we can turn that into a CSV and submit it to cattle and that one I think that went a bit better let's check yeah okay so that one actually finally gets into the top 20 percent 25 percent in the competition so I mean not amazing by any means but you can see that you know this simple step of creating five independently trained models just starting from different starting points in terms of random coefficients actually improved us from top 50 percent to top 25 percent John is there an argument because you've got a categorical result you're zero one effectively is there an argument that you might use the mode of the ensemble rather than the numerical mean I mean yes there's an argument that's been made and yeah something I would just try I generally find it's less good but not always and I don't feel like I've got a great intuition as to why and I don't feel like I've seen any studies as to why you could predict like there's a few there's there's at least three things you could do right you could take the is it greater or less than 0.5 ones and zeros and average them or you could take the mode of them or you could take the actual probable probability predictions and take the average of those and then threshold that and I've seen examples where certainly both of the different averaging versions each of them has been better I don't think I've seen one where the modes better but that was very popular back in the 90s so yeah so it's so easy to try you might as well give it a go okay we don't have time to finish the next notebook but let's make a start on it so the next notebook is random forests how random forests really work who here has heard of random forests before nearly everybody okay so very popular developed I think initially in 1999 but you know gradually improved in popularity during the 2000s I was like everybody kind of knew me as Mr. Random Forests for years I implemented them like a couple of days after the original technical report came out I was such a fan all of my early Kaggle results random forests I love them and I think hopefully you'll see why I'm such a fan of them because they're so elegant and they're almost impossible to mess up a lot of people will say like oh why are you using machine learning why don't you use something simple like logistic regression and I think like oh gosh in industry I've seen far more examples of people screwing up logistic regression than successfully using logistic regression because it's very very very very difficult to do correctly you know you've got to make sure you've got the correct transformations and the correct interactions and the correct outlier handling and blah blah blah and anything you get wrong the entire thing falls apart random forests I it's very rare to that I've seen somebody screw up a random forest in industry they're very hard to screw up because they're they're so resilient and you'll see why so in this notebook just by the way rather than importing lump I and pandas and that plot lid and blah blah blah there's a little handy shortcut which is if you just import everything from fast AI imports that imports all the things that you normally want so I mean doesn't do anything special but it's just save so messing around so again we've got our cell here to grab the data and I'm just going to do some basic pre-processing here with my fill in a for the fair only needed for the test set of course grab the modes and do the fill in a on the in the modes take the log fair and then I got a couple of new steps here which is converting embarked and sex into categorical variables what does that mean well let's just run this on both the data frame a data frame and the test data frame split things into set into categories and continuous and sex is a categorical variable so let's look at it well that's interesting it looks exactly the same as before male and female but now it's got a category and it's got a list of categories what's happened here well what's happened is pandas has made a list of all of the unique values of this field and behind the scenes if you look at the cat codes you can see behind the scenes it's actually turned them into numbers it looks up this one into this list to get male looks at this zero into this list to get female so when it printed out it prints out the friendly version but it stores it as numbers now you'll see in a moment why this is helpful but a key thing to point out is we're not going to have to create any dummy variables and even that first second or third class we're not going to consider that categorical at all and just see why in a moment a random forest is an ensemble of trees a tree is an ensemble of binary splits and so we're going to work from the bottom up we're going to first work we're going to first learn about what is a binary split and we're going to do it by looking at example let's consider what would happen if we took all the passengers on the Titanic and grouped them into males and females and let's look at two things the first is let's look at their survival rate so about 20 percent survival rate for males and about 75 percent for females and let's look at the histogram how many of them are there about twice as many males as females consider what would happen if you created the world's simplest model which was what sex are they that wouldn't be bad would it because there's a big difference between the males and the females a huge difference in survival rate so if we said oh if you're a man you probably died if you're a woman you probably survived or not just a man or a boy so male or female that would be a pretty good model because it's done a good job of splitting it into two groups that have very different survival rates this is called a binary split a binary split is something that splits the rows into two groups hence binary let's talk about another example of a binary split I'm getting ahead of myself before we do that let's look at what would happen if we used this model so if we created a model which just looked at sex how good would it be so to figure that out we first have to split into training and test sets so let's go ahead and do that and then let's convert all of our categorical variables into their codes so we've now got 0 1 2 whatever we don't have male female there anymore and let's also create something that returns the independent variables which will call the X's and the dependent variable which will call Y and so we can now get the X's and the Y for each of the training set and the validation set and so now let's create some predictions we'll predict that they survived if their sex is zero so if they're female so how good is that model remember I told you that to calculate mean absolute error we can get psychic learn or pytorch whatever do it for us instead of doing it ourselves so just showing you here's how you do it just by importing it directly this is exactly the same as the one we did manually in the last notebook so that's a 21.5% error so that's a pretty good model could we do better well here's another example what about fair so fair is different to sex because fair is continuous or log fair I'll take that we could still split it into two groups so here's for all the people that didn't survive this is their median fair here and then this is their quartiles for bigger fairs and quartiles for smaller fairs and here's the median fair for those that survived and their quartiles so you can see the median fair for those that survived is higher than the median fair for those that didn't we can't create a histogram exactly for fair because it's continuous we could bucket it into groups to create a histogram so I guess we can create a histogram that wasn't true what I should say is we could create something better which is a kernel density plot which is just like a histogram but it's like with infinitely small bins so we can see most people have a log fair of about two so what if we split on about a bit under three you know that seems to be a point at which there's a difference in survival between people that are greater than or less than that amount so here's another model log fair greater than point two point seven oh much worse point three three six versus point two one five well I don't know maybe this is something better we could create a little interactive tool so what I want is something that can give us a quick score of how good a binary split is and I want it to be able to work regardless of whether we're dealing with categorical or continuous or whatever data so I just came up with a simple little way of scoring which is I said okay if you split your data into two groups a good split would be one in which all of the values of the dependent variable on one side are all pretty much the same and all of the dependent variables on the other side are all pretty much the same for example if pretty much all the males had the same survival outcome which is didn't survive and all the females had about the same survival outcome which is they did survive that would be a good split right it doesn't just work for categorical variables it would work if your dependent variable was continuous as well you basically want each of your groups within group to be as similar as possible on the dependent variable and then the other group you want them to be as similar as possible on the dependent variable so how similar is all the things in a group that's a standard deviation so what I want to do to get is basically add the standard deviations of the two groups of the dependent variable and then if there's a really small standard deviation but it's a really small group that's not very interesting so I'll multiply it by the size right so this is something which says what's the score for one of my groups one of my sides it's the standard deviation multiplied by how many things are in that group so the total score is the score for the left hand side so all the things in one group plus the score for the right hand side which is tilde means not so not left hand side is right hand side and then we'll just take the average of that so for example if we split by sex is greater than or less than point five that'll create two groups males and females and that gives us this score and if we do log fair greater than or less than two point seven it gives us this score and lower scores better so sex is better than log fair so now that we've got that we can use our favorite interact tool to create a little gooey and so we can say you know let's try like oh what about this one can we uh oops can we find something that's a bit better 4.8.485 no not very good what about p-class 0.468.460 so we can fiddle around with these we could do the same thing for the categorical variables so we already know that sex we can get to 0.407 what about embarked hmm all right so it looks like sex might be our best well that was pretty inefficient right it would be nice if we could find some automatic way to do all that well of course we can for example if we wanted to find what's the best split point for age then we just have to create let's do this again if we want to find the best split point for age we could just create a list of all of the unique values of age and try each one in turn and see what score we get if we made a binary split on that level of age so here's a list of all of the possible binary split thresholds for age let's go through all of them for each of them calculate the score and then numpy and pytorch have an argmin function which tells you what index into that list is the smallest so just to show you here's the scores and 0 1 2 3 4 5 6 oh sorry 0 1 2 3 4 5 6 so apparently that that value has the smallest score so that tells us that for age the threshold of 6 would be best so here's something that just calculates that for a column it calculates the best split point so here's 6 right and it also tells us what the score is at that point which is 0.478 so now we can just go through and calculates the score for the best split point for each column and if we do that we find that the lowest score is 6 so that is how we calculate the best binary split so we now know that the model that we created earlier this one is the best single binary split model we can find so next week we're going to be we're going to learn how we can recursively do this to create a decision tree and then do that multiple times to create a random forest but before we do I want to point something out which is this ridiculously simple thing which is find a single binary split and stop is a type of model it has a name it's called 1r and the 1r model it turned out in a review of machine learning methods in the 90s turned out to be one of the best if not the best machine learning classifiers for a wide range of real world data sets so that is to say don't assume that you have to go complicated it's not a bad idea to always start creating a baseline of 1r a decision tree with a single binary split and in fact for the Titanic competition that's exactly what we do if we look at the Titanic competition on Kaggle you'll find that what we did is our sample submission is one that just splits into male versus female alright thanks everybody hope you found that interesting and I will see you next lesson bye", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.0, "text": " OK, hi everybody and welcome to practical deep learning for coders lesson 5.", "tokens": [2264, 11, 4879, 2201, 293, 2928, 281, 8496, 2452, 2539, 337, 17656, 433, 6898, 1025, 13], "temperature": 0.0, "avg_logprob": -0.2883776302995353, "compression_ratio": 1.4652173913043478, "no_speech_prob": 0.0683543011546135}, {"id": 1, "seek": 0, "start": 6.0, "end": 13.0, "text": " We're at a stage now where we're going to be getting deeper and deeper into the details", "tokens": [492, 434, 412, 257, 3233, 586, 689, 321, 434, 516, 281, 312, 1242, 7731, 293, 7731, 666, 264, 4365], "temperature": 0.0, "avg_logprob": -0.2883776302995353, "compression_ratio": 1.4652173913043478, "no_speech_prob": 0.0683543011546135}, {"id": 2, "seek": 0, "start": 13.0, "end": 16.6, "text": " of how these networks actually work.", "tokens": [295, 577, 613, 9590, 767, 589, 13], "temperature": 0.0, "avg_logprob": -0.2883776302995353, "compression_ratio": 1.4652173913043478, "no_speech_prob": 0.0683543011546135}, {"id": 3, "seek": 0, "start": 16.6, "end": 23.44, "text": " Last week we saw how to use a slightly lower level library than Fast.ai, Huggings First", "tokens": [5264, 1243, 321, 1866, 577, 281, 764, 257, 4748, 3126, 1496, 6405, 813, 15968, 13, 1301, 11, 389, 3562, 1109, 2386], "temperature": 0.0, "avg_logprob": -0.2883776302995353, "compression_ratio": 1.4652173913043478, "no_speech_prob": 0.0683543011546135}, {"id": 4, "seek": 0, "start": 23.44, "end": 29.5, "text": " Transformers, to train a pretty nice NLP model.", "tokens": [27938, 433, 11, 281, 3847, 257, 1238, 1481, 426, 45196, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2883776302995353, "compression_ratio": 1.4652173913043478, "no_speech_prob": 0.0683543011546135}, {"id": 5, "seek": 2950, "start": 29.5, "end": 35.54, "text": " And today we're going to be going back to tabular data and we're going to be trying", "tokens": [400, 965, 321, 434, 516, 281, 312, 516, 646, 281, 4421, 1040, 1412, 293, 321, 434, 516, 281, 312, 1382], "temperature": 0.0, "avg_logprob": -0.1261208707636053, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.00031481310725212097}, {"id": 6, "seek": 2950, "start": 35.54, "end": 38.480000000000004, "text": " to build a tabular model actually from scratch.", "tokens": [281, 1322, 257, 4421, 1040, 2316, 767, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1261208707636053, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.00031481310725212097}, {"id": 7, "seek": 2950, "start": 38.480000000000004, "end": 42.76, "text": " We're going to build a couple of different types of tabular model from scratch.", "tokens": [492, 434, 516, 281, 1322, 257, 1916, 295, 819, 3467, 295, 4421, 1040, 2316, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1261208707636053, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.00031481310725212097}, {"id": 8, "seek": 2950, "start": 42.76, "end": 47.92, "text": " So the problem that I'm going to be working through is the Titanic problem, which if you", "tokens": [407, 264, 1154, 300, 286, 478, 516, 281, 312, 1364, 807, 307, 264, 42183, 1154, 11, 597, 498, 291], "temperature": 0.0, "avg_logprob": -0.1261208707636053, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.00031481310725212097}, {"id": 9, "seek": 2950, "start": 47.92, "end": 54.5, "text": " remember back a couple of weeks is the data set that we looked at on Microsoft Excel.", "tokens": [1604, 646, 257, 1916, 295, 3259, 307, 264, 1412, 992, 300, 321, 2956, 412, 322, 8116, 19060, 13], "temperature": 0.0, "avg_logprob": -0.1261208707636053, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.00031481310725212097}, {"id": 10, "seek": 2950, "start": 54.5, "end": 58.72, "text": " And it has each row is one passenger on the Titanic.", "tokens": [400, 309, 575, 1184, 5386, 307, 472, 18707, 322, 264, 42183, 13], "temperature": 0.0, "avg_logprob": -0.1261208707636053, "compression_ratio": 1.8445378151260505, "no_speech_prob": 0.00031481310725212097}, {"id": 11, "seek": 5872, "start": 58.72, "end": 65.2, "text": " This is a real world data set, historic data set, tells you both of that passenger survived,", "tokens": [639, 307, 257, 957, 1002, 1412, 992, 11, 13236, 1412, 992, 11, 5112, 291, 1293, 295, 300, 18707, 14433, 11], "temperature": 0.0, "avg_logprob": -0.14151742723253039, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.840402339643333e-05}, {"id": 12, "seek": 5872, "start": 65.2, "end": 69.26, "text": " what class they were on in the ship, their sex, age, how many siblings, how many other", "tokens": [437, 1508, 436, 645, 322, 294, 264, 5374, 11, 641, 3260, 11, 3205, 11, 577, 867, 20571, 11, 577, 867, 661], "temperature": 0.0, "avg_logprob": -0.14151742723253039, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.840402339643333e-05}, {"id": 13, "seek": 5872, "start": 69.26, "end": 73.72, "text": " family members, how much they spent in the fair and whereabouts they embarked, one of", "tokens": [1605, 2679, 11, 577, 709, 436, 4418, 294, 264, 3143, 293, 689, 41620, 436, 29832, 292, 11, 472, 295], "temperature": 0.0, "avg_logprob": -0.14151742723253039, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.840402339643333e-05}, {"id": 14, "seek": 5872, "start": 73.72, "end": 76.14, "text": " three different cities.", "tokens": [1045, 819, 6486, 13], "temperature": 0.0, "avg_logprob": -0.14151742723253039, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.840402339643333e-05}, {"id": 15, "seek": 5872, "start": 76.14, "end": 80.64, "text": " And you might remember that we built a linear model.", "tokens": [400, 291, 1062, 1604, 300, 321, 3094, 257, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14151742723253039, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.840402339643333e-05}, {"id": 16, "seek": 5872, "start": 80.64, "end": 88.03999999999999, "text": " We then did the same thing using matrix multiplication and we also created a very, very simple neural", "tokens": [492, 550, 630, 264, 912, 551, 1228, 8141, 27290, 293, 321, 611, 2942, 257, 588, 11, 588, 2199, 18161], "temperature": 0.0, "avg_logprob": -0.14151742723253039, "compression_ratio": 1.6818181818181819, "no_speech_prob": 2.840402339643333e-05}, {"id": 17, "seek": 8804, "start": 88.04, "end": 92.36000000000001, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.14608910319569346, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.0782386198116e-05}, {"id": 18, "seek": 8804, "start": 92.36000000000001, "end": 98.84, "text": " You know, Excel can do nearly everything we need, as you saw, to build a neural network,", "tokens": [509, 458, 11, 19060, 393, 360, 6217, 1203, 321, 643, 11, 382, 291, 1866, 11, 281, 1322, 257, 18161, 3209, 11], "temperature": 0.0, "avg_logprob": -0.14608910319569346, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.0782386198116e-05}, {"id": 19, "seek": 8804, "start": 98.84, "end": 101.72, "text": " but it starts to get unwieldy.", "tokens": [457, 309, 3719, 281, 483, 14853, 1789, 88, 13], "temperature": 0.0, "avg_logprob": -0.14608910319569346, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.0782386198116e-05}, {"id": 20, "seek": 8804, "start": 101.72, "end": 107.0, "text": " And so that's why people don't use Excel for neural networks in practice.", "tokens": [400, 370, 300, 311, 983, 561, 500, 380, 764, 19060, 337, 18161, 9590, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.14608910319569346, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.0782386198116e-05}, {"id": 21, "seek": 8804, "start": 107.0, "end": 110.72, "text": " Instead we use a programming language like Python.", "tokens": [7156, 321, 764, 257, 9410, 2856, 411, 15329, 13], "temperature": 0.0, "avg_logprob": -0.14608910319569346, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.0782386198116e-05}, {"id": 22, "seek": 8804, "start": 110.72, "end": 118.0, "text": " So what we're going to do today is we're going to do the same thing with Python.", "tokens": [407, 437, 321, 434, 516, 281, 360, 965, 307, 321, 434, 516, 281, 360, 264, 912, 551, 365, 15329, 13], "temperature": 0.0, "avg_logprob": -0.14608910319569346, "compression_ratio": 1.5829383886255923, "no_speech_prob": 1.0782386198116e-05}, {"id": 23, "seek": 11800, "start": 118.0, "end": 124.56, "text": " So we're going to start working through the linear model and neural net from scratch notebook,", "tokens": [407, 321, 434, 516, 281, 722, 1364, 807, 264, 8213, 2316, 293, 18161, 2533, 490, 8459, 21060, 11], "temperature": 0.0, "avg_logprob": -0.12541371513815486, "compression_ratio": 1.7395833333333333, "no_speech_prob": 5.4220613492361736e-06}, {"id": 24, "seek": 11800, "start": 124.56, "end": 130.76, "text": " which you can find on Kaggle or on the course repository.", "tokens": [597, 291, 393, 915, 322, 48751, 22631, 420, 322, 264, 1164, 25841, 13], "temperature": 0.0, "avg_logprob": -0.12541371513815486, "compression_ratio": 1.7395833333333333, "no_speech_prob": 5.4220613492361736e-06}, {"id": 25, "seek": 11800, "start": 130.76, "end": 133.6, "text": " And today what we're going to do is we're going to work through the one in the clean", "tokens": [400, 965, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 589, 807, 264, 472, 294, 264, 2541], "temperature": 0.0, "avg_logprob": -0.12541371513815486, "compression_ratio": 1.7395833333333333, "no_speech_prob": 5.4220613492361736e-06}, {"id": 26, "seek": 11800, "start": 133.6, "end": 134.6, "text": " folder.", "tokens": [10820, 13], "temperature": 0.0, "avg_logprob": -0.12541371513815486, "compression_ratio": 1.7395833333333333, "no_speech_prob": 5.4220613492361736e-06}, {"id": 27, "seek": 11800, "start": 134.6, "end": 144.66, "text": " So both for fast book, the book, and course 22, these lessons, the clean folder contains", "tokens": [407, 1293, 337, 2370, 1446, 11, 264, 1446, 11, 293, 1164, 5853, 11, 613, 8820, 11, 264, 2541, 10820, 8306], "temperature": 0.0, "avg_logprob": -0.12541371513815486, "compression_ratio": 1.7395833333333333, "no_speech_prob": 5.4220613492361736e-06}, {"id": 28, "seek": 14466, "start": 144.66, "end": 152.04, "text": " all of our notebooks, but without any pros or any outputs.", "tokens": [439, 295, 527, 43782, 11, 457, 1553, 604, 6267, 420, 604, 23930, 13], "temperature": 0.0, "avg_logprob": -0.15330288733965086, "compression_ratio": 1.5051020408163265, "no_speech_prob": 4.399261160870083e-05}, {"id": 29, "seek": 14466, "start": 152.04, "end": 155.96, "text": " So here's what it looks like when I open up the linear model and neural net from scratch", "tokens": [407, 510, 311, 437, 309, 1542, 411, 562, 286, 1269, 493, 264, 8213, 2316, 293, 18161, 2533, 490, 8459], "temperature": 0.0, "avg_logprob": -0.15330288733965086, "compression_ratio": 1.5051020408163265, "no_speech_prob": 4.399261160870083e-05}, {"id": 30, "seek": 14466, "start": 155.96, "end": 159.88, "text": " in Jupyter.", "tokens": [294, 22125, 88, 391, 13], "temperature": 0.0, "avg_logprob": -0.15330288733965086, "compression_ratio": 1.5051020408163265, "no_speech_prob": 4.399261160870083e-05}, {"id": 31, "seek": 14466, "start": 159.88, "end": 167.16, "text": " What I'm using here is a paper space gradient, which as I mentioned a couple of weeks ago", "tokens": [708, 286, 478, 1228, 510, 307, 257, 3035, 1901, 16235, 11, 597, 382, 286, 2835, 257, 1916, 295, 3259, 2057], "temperature": 0.0, "avg_logprob": -0.15330288733965086, "compression_ratio": 1.5051020408163265, "no_speech_prob": 4.399261160870083e-05}, {"id": 32, "seek": 14466, "start": 167.16, "end": 169.44, "text": " is what I'm going to be doing most things in.", "tokens": [307, 437, 286, 478, 516, 281, 312, 884, 881, 721, 294, 13], "temperature": 0.0, "avg_logprob": -0.15330288733965086, "compression_ratio": 1.5051020408163265, "no_speech_prob": 4.399261160870083e-05}, {"id": 33, "seek": 16944, "start": 169.44, "end": 178.16, "text": " It looks a little bit different to the normal paper space gradient", "tokens": [467, 1542, 257, 707, 857, 819, 281, 264, 2710, 3035, 1901, 16235], "temperature": 0.0, "avg_logprob": -0.11184501647949219, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.545011739130132e-05}, {"id": 34, "seek": 16944, "start": 178.16, "end": 187.84, "text": " because the default view for paper space gradient, at least as I do this course, is their rather", "tokens": [570, 264, 7576, 1910, 337, 3035, 1901, 16235, 11, 412, 1935, 382, 286, 360, 341, 1164, 11, 307, 641, 2831], "temperature": 0.0, "avg_logprob": -0.11184501647949219, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.545011739130132e-05}, {"id": 35, "seek": 16944, "start": 187.84, "end": 199.24, "text": " awkward notebook editor, which at first glance has the same features as the real Jupyter", "tokens": [11411, 21060, 9839, 11, 597, 412, 700, 21094, 575, 264, 912, 4122, 382, 264, 957, 22125, 88, 391], "temperature": 0.0, "avg_logprob": -0.11184501647949219, "compression_ratio": 1.5555555555555556, "no_speech_prob": 2.545011739130132e-05}, {"id": 36, "seek": 19924, "start": 199.24, "end": 206.12, "text": " notebooks and Jupyter lab environments, but in practice are actually missing lots of things.", "tokens": [43782, 293, 22125, 88, 391, 2715, 12388, 11, 457, 294, 3124, 366, 767, 5361, 3195, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.14919867807505083, "compression_ratio": 1.6528925619834711, "no_speech_prob": 2.8406826459104195e-05}, {"id": 37, "seek": 19924, "start": 206.12, "end": 208.76000000000002, "text": " So this is the normal paper space.", "tokens": [407, 341, 307, 264, 2710, 3035, 1901, 13], "temperature": 0.0, "avg_logprob": -0.14919867807505083, "compression_ratio": 1.6528925619834711, "no_speech_prob": 2.8406826459104195e-05}, {"id": 38, "seek": 19924, "start": 208.76000000000002, "end": 212.44, "text": " So remember you have to click this button.", "tokens": [407, 1604, 291, 362, 281, 2052, 341, 2960, 13], "temperature": 0.0, "avg_logprob": -0.14919867807505083, "compression_ratio": 1.6528925619834711, "no_speech_prob": 2.8406826459104195e-05}, {"id": 39, "seek": 19924, "start": 212.44, "end": 217.32000000000002, "text": " And the only reason you might keep this window running is then you might go over here to", "tokens": [400, 264, 787, 1778, 291, 1062, 1066, 341, 4910, 2614, 307, 550, 291, 1062, 352, 670, 510, 281], "temperature": 0.0, "avg_logprob": -0.14919867807505083, "compression_ratio": 1.6528925619834711, "no_speech_prob": 2.8406826459104195e-05}, {"id": 40, "seek": 19924, "start": 217.32000000000002, "end": 223.32000000000002, "text": " the machine to remind yourself when you close the other tab to click stop machine.", "tokens": [264, 3479, 281, 4160, 1803, 562, 291, 1998, 264, 661, 4421, 281, 2052, 1590, 3479, 13], "temperature": 0.0, "avg_logprob": -0.14919867807505083, "compression_ratio": 1.6528925619834711, "no_speech_prob": 2.8406826459104195e-05}, {"id": 41, "seek": 19924, "start": 223.32000000000002, "end": 227.04000000000002, "text": " If you're using the free one, it doesn't matter too much.", "tokens": [759, 291, 434, 1228, 264, 1737, 472, 11, 309, 1177, 380, 1871, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.14919867807505083, "compression_ratio": 1.6528925619834711, "no_speech_prob": 2.8406826459104195e-05}, {"id": 42, "seek": 22704, "start": 227.04, "end": 231.28, "text": " And also when I started, I make sure I've got something to shut down automatically in", "tokens": [400, 611, 562, 286, 1409, 11, 286, 652, 988, 286, 600, 658, 746, 281, 5309, 760, 6772, 294], "temperature": 0.0, "avg_logprob": -0.19900624816482132, "compression_ratio": 1.461111111111111, "no_speech_prob": 4.637532583728898e-06}, {"id": 43, "seek": 22704, "start": 231.28, "end": 236.16, "text": " case I forget.", "tokens": [1389, 286, 2870, 13], "temperature": 0.0, "avg_logprob": -0.19900624816482132, "compression_ratio": 1.461111111111111, "no_speech_prob": 4.637532583728898e-06}, {"id": 44, "seek": 22704, "start": 236.16, "end": 238.48, "text": " So other than that, we can stay in this tab.", "tokens": [407, 661, 813, 300, 11, 321, 393, 1754, 294, 341, 4421, 13], "temperature": 0.0, "avg_logprob": -0.19900624816482132, "compression_ratio": 1.461111111111111, "no_speech_prob": 4.637532583728898e-06}, {"id": 45, "seek": 22704, "start": 238.48, "end": 248.44, "text": " And because this is Jupyter lab that runs, and you can always switch over to classic", "tokens": [400, 570, 341, 307, 22125, 88, 391, 2715, 300, 6676, 11, 293, 291, 393, 1009, 3679, 670, 281, 7230], "temperature": 0.0, "avg_logprob": -0.19900624816482132, "compression_ratio": 1.461111111111111, "no_speech_prob": 4.637532583728898e-06}, {"id": 46, "seek": 22704, "start": 248.44, "end": 256.84, "text": " Jupyter notebook if you want to.", "tokens": [22125, 88, 391, 21060, 498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.19900624816482132, "compression_ratio": 1.461111111111111, "no_speech_prob": 4.637532583728898e-06}, {"id": 47, "seek": 25684, "start": 256.84, "end": 262.28, "text": " So given that they've got tabs inside tabs, I normally maximize it at this point.", "tokens": [407, 2212, 300, 436, 600, 658, 20743, 1854, 20743, 11, 286, 5646, 19874, 309, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.1763831984322026, "compression_ratio": 1.6102941176470589, "no_speech_prob": 3.120022302027792e-05}, {"id": 48, "seek": 25684, "start": 262.28, "end": 265.0, "text": " And it's really helpful to know the keyboard shortcuts.", "tokens": [400, 309, 311, 534, 4961, 281, 458, 264, 10186, 34620, 13], "temperature": 0.0, "avg_logprob": -0.1763831984322026, "compression_ratio": 1.6102941176470589, "no_speech_prob": 3.120022302027792e-05}, {"id": 49, "seek": 25684, "start": 265.0, "end": 268.91999999999996, "text": " So control shift square bracket right and left switch between tabs.", "tokens": [407, 1969, 5513, 3732, 16904, 558, 293, 1411, 3679, 1296, 20743, 13], "temperature": 0.0, "avg_logprob": -0.1763831984322026, "compression_ratio": 1.6102941176470589, "no_speech_prob": 3.120022302027792e-05}, {"id": 50, "seek": 25684, "start": 268.91999999999996, "end": 270.84, "text": " That's one of the key things to know about.", "tokens": [663, 311, 472, 295, 264, 2141, 721, 281, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.1763831984322026, "compression_ratio": 1.6102941176470589, "no_speech_prob": 3.120022302027792e-05}, {"id": 51, "seek": 25684, "start": 270.84, "end": 271.84, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1763831984322026, "compression_ratio": 1.6102941176470589, "no_speech_prob": 3.120022302027792e-05}, {"id": 52, "seek": 25684, "start": 271.84, "end": 279.34, "text": " So I've opened up the clean version of the linear model and neural net from scratch notebook.", "tokens": [407, 286, 600, 5625, 493, 264, 2541, 3037, 295, 264, 8213, 2316, 293, 18161, 2533, 490, 8459, 21060, 13], "temperature": 0.0, "avg_logprob": -0.1763831984322026, "compression_ratio": 1.6102941176470589, "no_speech_prob": 3.120022302027792e-05}, {"id": 53, "seek": 25684, "start": 279.34, "end": 286.15999999999997, "text": " And so remember when you go back through the video the second time or through the notebook", "tokens": [400, 370, 1604, 562, 291, 352, 646, 807, 264, 960, 264, 1150, 565, 420, 807, 264, 21060], "temperature": 0.0, "avg_logprob": -0.1763831984322026, "compression_ratio": 1.6102941176470589, "no_speech_prob": 3.120022302027792e-05}, {"id": 54, "seek": 28616, "start": 286.16, "end": 290.12, "text": " a second time, this is generally what you want to be doing is going through the clean", "tokens": [257, 1150, 565, 11, 341, 307, 5101, 437, 291, 528, 281, 312, 884, 307, 516, 807, 264, 2541], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 55, "seek": 28616, "start": 290.12, "end": 294.40000000000003, "text": " notebook and before you run each cell, try to think about like, oh, what Jeremy say?", "tokens": [21060, 293, 949, 291, 1190, 1184, 2815, 11, 853, 281, 519, 466, 411, 11, 1954, 11, 437, 17809, 584, 30], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 56, "seek": 28616, "start": 294.40000000000003, "end": 297.16, "text": " Why are we doing this?", "tokens": [1545, 366, 321, 884, 341, 30], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 57, "seek": 28616, "start": 297.16, "end": 298.52000000000004, "text": " What output would I expect?", "tokens": [708, 5598, 576, 286, 2066, 30], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 58, "seek": 28616, "start": 298.52000000000004, "end": 300.62, "text": " Make sure you get the output you'd expect.", "tokens": [4387, 988, 291, 483, 264, 5598, 291, 1116, 2066, 13], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 59, "seek": 28616, "start": 300.62, "end": 306.0, "text": " And if you're not sure why something is the way it is, try changing it and see what happens.", "tokens": [400, 498, 291, 434, 406, 988, 983, 746, 307, 264, 636, 309, 307, 11, 853, 4473, 309, 293, 536, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 60, "seek": 28616, "start": 306.0, "end": 309.68, "text": " And then if you're still not sure, well, why did that thing not work the way I expect?", "tokens": [400, 550, 498, 291, 434, 920, 406, 988, 11, 731, 11, 983, 630, 300, 551, 406, 589, 264, 636, 286, 2066, 30], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 61, "seek": 28616, "start": 309.68, "end": 313.44000000000005, "text": " You know, search the forums if anybody's asked that question before, and you can ask the", "tokens": [509, 458, 11, 3164, 264, 26998, 498, 4472, 311, 2351, 300, 1168, 949, 11, 293, 291, 393, 1029, 264], "temperature": 0.0, "avg_logprob": -0.18336902104370983, "compression_ratio": 1.7826086956521738, "no_speech_prob": 3.5353881685296074e-05}, {"id": 62, "seek": 31344, "start": 313.44, "end": 318.36, "text": " question on the forum yourself if you're still not sure.", "tokens": [1168, 322, 264, 17542, 1803, 498, 291, 434, 920, 406, 988, 13], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 63, "seek": 31344, "start": 318.36, "end": 323.88, "text": " So as I think we've mentioned briefly before, I find it really nice to be able to use the", "tokens": [407, 382, 286, 519, 321, 600, 2835, 10515, 949, 11, 286, 915, 309, 534, 1481, 281, 312, 1075, 281, 764, 264], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 64, "seek": 31344, "start": 323.88, "end": 326.8, "text": " same notebook both on Kaggle and off Kaggle.", "tokens": [912, 21060, 1293, 322, 48751, 22631, 293, 766, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 65, "seek": 31344, "start": 326.8, "end": 331.08, "text": " So most of my notebooks start with basically the same cell, which is something that just", "tokens": [407, 881, 295, 452, 43782, 722, 365, 1936, 264, 912, 2815, 11, 597, 307, 746, 300, 445], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 66, "seek": 31344, "start": 331.08, "end": 333.0, "text": " checks whether we're on Kaggle.", "tokens": [13834, 1968, 321, 434, 322, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 67, "seek": 31344, "start": 333.0, "end": 336.32, "text": " So Kaggle sets an environment variable.", "tokens": [407, 48751, 22631, 6352, 364, 2823, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 68, "seek": 31344, "start": 336.32, "end": 337.32, "text": " So we can just check for it.", "tokens": [407, 321, 393, 445, 1520, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 69, "seek": 31344, "start": 337.32, "end": 339.42, "text": " And that way we know if we're on Kaggle.", "tokens": [400, 300, 636, 321, 458, 498, 321, 434, 322, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.1036673006804093, "compression_ratio": 1.7154471544715446, "no_speech_prob": 3.0236615202738903e-05}, {"id": 70, "seek": 33942, "start": 339.42, "end": 343.84000000000003, "text": " And so then if we are on Kaggle, a notebook that's part of a competition will already", "tokens": [400, 370, 550, 498, 321, 366, 322, 48751, 22631, 11, 257, 21060, 300, 311, 644, 295, 257, 6211, 486, 1217], "temperature": 0.0, "avg_logprob": -0.14661699860960573, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.8057091438095085e-05}, {"id": 71, "seek": 33942, "start": 343.84000000000003, "end": 347.36, "text": " have the data downloaded and unzipped for you.", "tokens": [362, 264, 1412, 21748, 293, 517, 89, 5529, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14661699860960573, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.8057091438095085e-05}, {"id": 72, "seek": 33942, "start": 347.36, "end": 352.6, "text": " Otherwise, if I haven't downloaded the data before, then I need to download it and unzip", "tokens": [10328, 11, 498, 286, 2378, 380, 21748, 264, 1412, 949, 11, 550, 286, 643, 281, 5484, 309, 293, 517, 27268], "temperature": 0.0, "avg_logprob": -0.14661699860960573, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.8057091438095085e-05}, {"id": 73, "seek": 33942, "start": 352.6, "end": 353.6, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.14661699860960573, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.8057091438095085e-05}, {"id": 74, "seek": 33942, "start": 353.6, "end": 354.6, "text": " OK?", "tokens": [2264, 30], "temperature": 0.0, "avg_logprob": -0.14661699860960573, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.8057091438095085e-05}, {"id": 75, "seek": 33942, "start": 354.6, "end": 358.8, "text": " So Kaggle is a PIP installable module.", "tokens": [407, 48751, 22631, 307, 257, 430, 9139, 3625, 712, 10088, 13], "temperature": 0.0, "avg_logprob": -0.14661699860960573, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.8057091438095085e-05}, {"id": 76, "seek": 33942, "start": 358.8, "end": 363.36, "text": " So you type PIP install Kaggle.", "tokens": [407, 291, 2010, 430, 9139, 3625, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.14661699860960573, "compression_ratio": 1.5957446808510638, "no_speech_prob": 1.8057091438095085e-05}, {"id": 77, "seek": 36336, "start": 363.36, "end": 370.08000000000004, "text": " If you're not sure how to do that, you should check our deep dive lessons to see exactly", "tokens": [759, 291, 434, 406, 988, 577, 281, 360, 300, 11, 291, 820, 1520, 527, 2452, 9192, 8820, 281, 536, 2293], "temperature": 0.0, "avg_logprob": -0.13690528869628907, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.139125384332146e-06}, {"id": 78, "seek": 36336, "start": 370.08000000000004, "end": 371.08000000000004, "text": " the steps.", "tokens": [264, 4439, 13], "temperature": 0.0, "avg_logprob": -0.13690528869628907, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.139125384332146e-06}, {"id": 79, "seek": 36336, "start": 371.08000000000004, "end": 379.04, "text": " But roughly speaking, you can use your console, PIP install, and whatever you want to install.", "tokens": [583, 9810, 4124, 11, 291, 393, 764, 428, 11076, 11, 430, 9139, 3625, 11, 293, 2035, 291, 528, 281, 3625, 13], "temperature": 0.0, "avg_logprob": -0.13690528869628907, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.139125384332146e-06}, {"id": 80, "seek": 36336, "start": 379.04, "end": 384.8, "text": " Or as we've seen before, you can do it directly in a notebook by putting an explanation mark", "tokens": [1610, 382, 321, 600, 1612, 949, 11, 291, 393, 360, 309, 3838, 294, 257, 21060, 538, 3372, 364, 10835, 1491], "temperature": 0.0, "avg_logprob": -0.13690528869628907, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.139125384332146e-06}, {"id": 81, "seek": 36336, "start": 384.8, "end": 387.12, "text": " at the start.", "tokens": [412, 264, 722, 13], "temperature": 0.0, "avg_logprob": -0.13690528869628907, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.139125384332146e-06}, {"id": 82, "seek": 36336, "start": 387.12, "end": 391.16, "text": " So that's going to run not Python but a shell command.", "tokens": [407, 300, 311, 516, 281, 1190, 406, 15329, 457, 257, 8720, 5622, 13], "temperature": 0.0, "avg_logprob": -0.13690528869628907, "compression_ratio": 1.5344827586206897, "no_speech_prob": 8.139125384332146e-06}, {"id": 83, "seek": 39116, "start": 391.16, "end": 398.84000000000003, "text": " OK, so that's enough to ensure that we have the data downloaded and a variable called", "tokens": [2264, 11, 370, 300, 311, 1547, 281, 5586, 300, 321, 362, 264, 1412, 21748, 293, 257, 7006, 1219], "temperature": 0.0, "avg_logprob": -0.12602137435566296, "compression_ratio": 1.5215311004784688, "no_speech_prob": 9.080078598344699e-06}, {"id": 84, "seek": 39116, "start": 398.84000000000003, "end": 405.16, "text": " path that's pointing at it.", "tokens": [3100, 300, 311, 12166, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.12602137435566296, "compression_ratio": 1.5215311004784688, "no_speech_prob": 9.080078598344699e-06}, {"id": 85, "seek": 39116, "start": 405.16, "end": 408.56, "text": " Most of the time we're going to be using at least PyTorch and NumPy.", "tokens": [4534, 295, 264, 565, 321, 434, 516, 281, 312, 1228, 412, 1935, 9953, 51, 284, 339, 293, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.12602137435566296, "compression_ratio": 1.5215311004784688, "no_speech_prob": 9.080078598344699e-06}, {"id": 86, "seek": 39116, "start": 408.56, "end": 412.24, "text": " So we import those so they're available to Python.", "tokens": [407, 321, 974, 729, 370, 436, 434, 2435, 281, 15329, 13], "temperature": 0.0, "avg_logprob": -0.12602137435566296, "compression_ratio": 1.5215311004784688, "no_speech_prob": 9.080078598344699e-06}, {"id": 87, "seek": 39116, "start": 412.24, "end": 416.86, "text": " And when we're working with tabular data, as we talked about before, we're generally", "tokens": [400, 562, 321, 434, 1364, 365, 4421, 1040, 1412, 11, 382, 321, 2825, 466, 949, 11, 321, 434, 5101], "temperature": 0.0, "avg_logprob": -0.12602137435566296, "compression_ratio": 1.5215311004784688, "no_speech_prob": 9.080078598344699e-06}, {"id": 88, "seek": 41686, "start": 416.86, "end": 421.96000000000004, "text": " also going to want to use pandas, and it's really important that you're somewhat familiar", "tokens": [611, 516, 281, 528, 281, 764, 4565, 296, 11, 293, 309, 311, 534, 1021, 300, 291, 434, 8344, 4963], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 89, "seek": 41686, "start": 421.96000000000004, "end": 426.32, "text": " with the kind of basic API of these three libraries.", "tokens": [365, 264, 733, 295, 3875, 9362, 295, 613, 1045, 15148, 13], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 90, "seek": 41686, "start": 426.32, "end": 432.44, "text": " And I've recommended Wes McKinney's book before, particularly for these ones.", "tokens": [400, 286, 600, 9628, 23843, 21765, 259, 2397, 311, 1446, 949, 11, 4098, 337, 613, 2306, 13], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 91, "seek": 41686, "start": 432.44, "end": 436.6, "text": " One thing just by the way is that these things tend to assume you've got a very narrow screen,", "tokens": [1485, 551, 445, 538, 264, 636, 307, 300, 613, 721, 3928, 281, 6552, 291, 600, 658, 257, 588, 9432, 2568, 11], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 92, "seek": 41686, "start": 436.6, "end": 438.32, "text": " which is really annoying because it always wraps things.", "tokens": [597, 307, 534, 11304, 570, 309, 1009, 25831, 721, 13], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 93, "seek": 41686, "start": 438.32, "end": 441.36, "text": " So if you want to put these three lines as well, then it just makes sure that everything", "tokens": [407, 498, 291, 528, 281, 829, 613, 1045, 3876, 382, 731, 11, 550, 309, 445, 1669, 988, 300, 1203], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 94, "seek": 41686, "start": 441.36, "end": 443.96000000000004, "text": " is going to use up the screen properly.", "tokens": [307, 516, 281, 764, 493, 264, 2568, 6108, 13], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 95, "seek": 41686, "start": 443.96000000000004, "end": 444.96000000000004, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.12674294577704537, "compression_ratio": 1.6777408637873754, "no_speech_prob": 2.6685122065828182e-05}, {"id": 96, "seek": 44496, "start": 444.96, "end": 450.52, "text": " So as we've seen before, you can read a common separated values file with pandas and you", "tokens": [407, 382, 321, 600, 1612, 949, 11, 291, 393, 1401, 257, 2689, 12005, 4190, 3991, 365, 4565, 296, 293, 291], "temperature": 0.0, "avg_logprob": -0.17199206352233887, "compression_ratio": 1.6298342541436464, "no_speech_prob": 2.840597699105274e-05}, {"id": 97, "seek": 44496, "start": 450.52, "end": 454.64, "text": " can take a look at the first two lines and the last two lines and how big it is.", "tokens": [393, 747, 257, 574, 412, 264, 700, 732, 3876, 293, 264, 1036, 732, 3876, 293, 577, 955, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17199206352233887, "compression_ratio": 1.6298342541436464, "no_speech_prob": 2.840597699105274e-05}, {"id": 98, "seek": 44496, "start": 454.64, "end": 459.08, "text": " And so here's the same thing as our spreadsheet.", "tokens": [400, 370, 510, 311, 264, 912, 551, 382, 527, 27733, 13], "temperature": 0.0, "avg_logprob": -0.17199206352233887, "compression_ratio": 1.6298342541436464, "no_speech_prob": 2.840597699105274e-05}, {"id": 99, "seek": 44496, "start": 459.08, "end": 469.08, "text": " OK, so there's our data from the spreadsheet and here it is as a data frame.", "tokens": [2264, 11, 370, 456, 311, 527, 1412, 490, 264, 27733, 293, 510, 309, 307, 382, 257, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.17199206352233887, "compression_ratio": 1.6298342541436464, "no_speech_prob": 2.840597699105274e-05}, {"id": 100, "seek": 46908, "start": 469.08, "end": 485.71999999999997, "text": " So if we go data frame dot is and a that returns a new data frame in which every column it", "tokens": [407, 498, 321, 352, 1412, 3920, 5893, 307, 293, 257, 300, 11247, 257, 777, 1412, 3920, 294, 597, 633, 7738, 309], "temperature": 0.0, "avg_logprob": -0.2681468725204468, "compression_ratio": 1.518987341772152, "no_speech_prob": 4.710738267021952e-06}, {"id": 101, "seek": 46908, "start": 485.71999999999997, "end": 490.71999999999997, "text": " tells us whether or not that particular value is nan.", "tokens": [5112, 505, 1968, 420, 406, 300, 1729, 2158, 307, 14067, 13], "temperature": 0.0, "avg_logprob": -0.2681468725204468, "compression_ratio": 1.518987341772152, "no_speech_prob": 4.710738267021952e-06}, {"id": 102, "seek": 46908, "start": 490.71999999999997, "end": 493.15999999999997, "text": " So nan is not a number.", "tokens": [407, 14067, 307, 406, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.2681468725204468, "compression_ratio": 1.518987341772152, "no_speech_prob": 4.710738267021952e-06}, {"id": 103, "seek": 46908, "start": 493.15999999999997, "end": 496.76, "text": " And most the most common reason you get that is because it was missing.", "tokens": [400, 881, 264, 881, 2689, 1778, 291, 483, 300, 307, 570, 309, 390, 5361, 13], "temperature": 0.0, "avg_logprob": -0.2681468725204468, "compression_ratio": 1.518987341772152, "no_speech_prob": 4.710738267021952e-06}, {"id": 104, "seek": 49676, "start": 496.76, "end": 501.74, "text": " OK, so a missing value is obviously not a number.", "tokens": [2264, 11, 370, 257, 5361, 2158, 307, 2745, 406, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 105, "seek": 49676, "start": 501.74, "end": 507.0, "text": " So we in the Excel version, we did something you should never usually do.", "tokens": [407, 321, 294, 264, 19060, 3037, 11, 321, 630, 746, 291, 820, 1128, 2673, 360, 13], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 106, "seek": 49676, "start": 507.0, "end": 513.0, "text": " We deleted all the rows with missing data just because in Excel it's a little bit harder", "tokens": [492, 22981, 439, 264, 13241, 365, 5361, 1412, 445, 570, 294, 19060, 309, 311, 257, 707, 857, 6081], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 107, "seek": 49676, "start": 513.0, "end": 514.0, "text": " to work with.", "tokens": [281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 108, "seek": 49676, "start": 514.0, "end": 516.64, "text": " In pandas, it's very easy to work with.", "tokens": [682, 4565, 296, 11, 309, 311, 588, 1858, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 109, "seek": 49676, "start": 516.64, "end": 521.52, "text": " First of all, we can just sum up what I just showed you.", "tokens": [2386, 295, 439, 11, 321, 393, 445, 2408, 493, 437, 286, 445, 4712, 291, 13], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 110, "seek": 49676, "start": 521.52, "end": 525.4, "text": " Now, if you call sum on a data frame, it sums up each column.", "tokens": [823, 11, 498, 291, 818, 2408, 322, 257, 1412, 3920, 11, 309, 34499, 493, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 111, "seek": 49676, "start": 525.4, "end": 526.4, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.15857223042270593, "compression_ratio": 1.5934959349593496, "no_speech_prob": 1.6182830222533084e-05}, {"id": 112, "seek": 52640, "start": 526.4, "end": 534.48, "text": " So you can see that there's kind of some small foundational concepts in pandas, which when", "tokens": [407, 291, 393, 536, 300, 456, 311, 733, 295, 512, 1359, 32195, 10392, 294, 4565, 296, 11, 597, 562], "temperature": 0.0, "avg_logprob": -0.10309887807303612, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.6685802367865108e-05}, {"id": 113, "seek": 52640, "start": 534.48, "end": 536.36, "text": " you put them together, take you a long way.", "tokens": [291, 829, 552, 1214, 11, 747, 291, 257, 938, 636, 13], "temperature": 0.0, "avg_logprob": -0.10309887807303612, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.6685802367865108e-05}, {"id": 114, "seek": 52640, "start": 536.36, "end": 540.9599999999999, "text": " So one idea is this idea that you can call a method on a data frame and it calls it on", "tokens": [407, 472, 1558, 307, 341, 1558, 300, 291, 393, 818, 257, 3170, 322, 257, 1412, 3920, 293, 309, 5498, 309, 322], "temperature": 0.0, "avg_logprob": -0.10309887807303612, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.6685802367865108e-05}, {"id": 115, "seek": 52640, "start": 540.9599999999999, "end": 542.4399999999999, "text": " every row.", "tokens": [633, 5386, 13], "temperature": 0.0, "avg_logprob": -0.10309887807303612, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.6685802367865108e-05}, {"id": 116, "seek": 52640, "start": 542.4399999999999, "end": 548.1999999999999, "text": " And then you can call a reduction on that and it reduces each column.", "tokens": [400, 550, 291, 393, 818, 257, 11004, 322, 300, 293, 309, 18081, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.10309887807303612, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.6685802367865108e-05}, {"id": 117, "seek": 52640, "start": 548.1999999999999, "end": 554.76, "text": " And so now we've got the total and in Python and pandas and NumPy and PyTorch, you can", "tokens": [400, 370, 586, 321, 600, 658, 264, 3217, 293, 294, 15329, 293, 4565, 296, 293, 22592, 47, 88, 293, 9953, 51, 284, 339, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.10309887807303612, "compression_ratio": 1.6986899563318778, "no_speech_prob": 2.6685802367865108e-05}, {"id": 118, "seek": 55476, "start": 554.76, "end": 558.48, "text": " treat a Boolean as a number and true will be one, false will be zero.", "tokens": [2387, 257, 23351, 28499, 382, 257, 1230, 293, 2074, 486, 312, 472, 11, 7908, 486, 312, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1446429279363044, "compression_ratio": 1.5690376569037656, "no_speech_prob": 7.64641572459368e-06}, {"id": 119, "seek": 55476, "start": 558.48, "end": 563.12, "text": " So this is the number of missing values in each column.", "tokens": [407, 341, 307, 264, 1230, 295, 5361, 4190, 294, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.1446429279363044, "compression_ratio": 1.5690376569037656, "no_speech_prob": 7.64641572459368e-06}, {"id": 120, "seek": 55476, "start": 563.12, "end": 569.0, "text": " So we can see that cabin out of 891 rows, it's nearly always empty.", "tokens": [407, 321, 393, 536, 300, 9401, 484, 295, 1649, 29925, 13241, 11, 309, 311, 6217, 1009, 6707, 13], "temperature": 0.0, "avg_logprob": -0.1446429279363044, "compression_ratio": 1.5690376569037656, "no_speech_prob": 7.64641572459368e-06}, {"id": 121, "seek": 55476, "start": 569.0, "end": 570.48, "text": " Age is empty a bit of the time.", "tokens": [16280, 307, 6707, 257, 857, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.1446429279363044, "compression_ratio": 1.5690376569037656, "no_speech_prob": 7.64641572459368e-06}, {"id": 122, "seek": 55476, "start": 570.48, "end": 572.76, "text": " Embarked is almost never empty.", "tokens": [24234, 809, 292, 307, 1920, 1128, 6707, 13], "temperature": 0.0, "avg_logprob": -0.1446429279363044, "compression_ratio": 1.5690376569037656, "no_speech_prob": 7.64641572459368e-06}, {"id": 123, "seek": 55476, "start": 572.76, "end": 580.34, "text": " So if you remember from Excel, we need to multiply a coefficient by each column.", "tokens": [407, 498, 291, 1604, 490, 19060, 11, 321, 643, 281, 12972, 257, 17619, 538, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.1446429279363044, "compression_ratio": 1.5690376569037656, "no_speech_prob": 7.64641572459368e-06}, {"id": 124, "seek": 55476, "start": 580.34, "end": 581.92, "text": " That's how we create a linear model.", "tokens": [663, 311, 577, 321, 1884, 257, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1446429279363044, "compression_ratio": 1.5690376569037656, "no_speech_prob": 7.64641572459368e-06}, {"id": 125, "seek": 58192, "start": 581.92, "end": 584.9599999999999, "text": " So how would you multiply a coefficient by a missing value?", "tokens": [407, 577, 576, 291, 12972, 257, 17619, 538, 257, 5361, 2158, 30], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 126, "seek": 58192, "start": 584.9599999999999, "end": 587.16, "text": " You can't.", "tokens": [509, 393, 380, 13], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 127, "seek": 58192, "start": 587.16, "end": 591.3199999999999, "text": " There's lots of ways of it's called imputing missing values or replacing missing value", "tokens": [821, 311, 3195, 295, 2098, 295, 309, 311, 1219, 704, 10861, 5361, 4190, 420, 19139, 5361, 2158], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 128, "seek": 58192, "start": 591.3199999999999, "end": 593.68, "text": " with a number.", "tokens": [365, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 129, "seek": 58192, "start": 593.68, "end": 599.7199999999999, "text": " The easiest, which always works is to replace missing values with the mode of a column.", "tokens": [440, 12889, 11, 597, 1009, 1985, 307, 281, 7406, 5361, 4190, 365, 264, 4391, 295, 257, 7738, 13], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 130, "seek": 58192, "start": 599.7199999999999, "end": 603.88, "text": " The mode is the most common value that works both the categorical variables.", "tokens": [440, 4391, 307, 264, 881, 2689, 2158, 300, 1985, 1293, 264, 19250, 804, 9102, 13], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 131, "seek": 58192, "start": 603.88, "end": 607.3199999999999, "text": " It's the most common category and continuous variables.", "tokens": [467, 311, 264, 881, 2689, 7719, 293, 10957, 9102, 13], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 132, "seek": 58192, "start": 607.3199999999999, "end": 611.0799999999999, "text": " That's the most common number.", "tokens": [663, 311, 264, 881, 2689, 1230, 13], "temperature": 0.0, "avg_logprob": -0.14471854255312966, "compression_ratio": 1.8928571428571428, "no_speech_prob": 2.4298258722410537e-05}, {"id": 133, "seek": 61108, "start": 611.08, "end": 617.6, "text": " So you can get the mode by calling df.mode.", "tokens": [407, 291, 393, 483, 264, 4391, 538, 5141, 274, 69, 13, 76, 1429, 13], "temperature": 0.0, "avg_logprob": -0.07543654166735135, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.7502206901554018e-05}, {"id": 134, "seek": 61108, "start": 617.6, "end": 622.88, "text": " One thing that's a bit awkward is that if there's a tie for the mode, so there's more", "tokens": [1485, 551, 300, 311, 257, 857, 11411, 307, 300, 498, 456, 311, 257, 7582, 337, 264, 4391, 11, 370, 456, 311, 544], "temperature": 0.0, "avg_logprob": -0.07543654166735135, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.7502206901554018e-05}, {"id": 135, "seek": 61108, "start": 622.88, "end": 627.26, "text": " than one thing that's the most common, it's going to return multiple rows.", "tokens": [813, 472, 551, 300, 311, 264, 881, 2689, 11, 309, 311, 516, 281, 2736, 3866, 13241, 13], "temperature": 0.0, "avg_logprob": -0.07543654166735135, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.7502206901554018e-05}, {"id": 136, "seek": 61108, "start": 627.26, "end": 630.5600000000001, "text": " So I need to return the zeroth row.", "tokens": [407, 286, 643, 281, 2736, 264, 44746, 900, 5386, 13], "temperature": 0.0, "avg_logprob": -0.07543654166735135, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.7502206901554018e-05}, {"id": 137, "seek": 61108, "start": 630.5600000000001, "end": 633.08, "text": " So here is the mode of every column.", "tokens": [407, 510, 307, 264, 4391, 295, 633, 7738, 13], "temperature": 0.0, "avg_logprob": -0.07543654166735135, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.7502206901554018e-05}, {"id": 138, "seek": 61108, "start": 633.08, "end": 638.6800000000001, "text": " So we can replace the missing values for age with 24 and the missing values for cabin with", "tokens": [407, 321, 393, 7406, 264, 5361, 4190, 337, 3205, 365, 4022, 293, 264, 5361, 4190, 337, 9401, 365], "temperature": 0.0, "avg_logprob": -0.07543654166735135, "compression_ratio": 1.7777777777777777, "no_speech_prob": 1.7502206901554018e-05}, {"id": 139, "seek": 63868, "start": 638.68, "end": 641.92, "text": " b96, b98, and embarked with s.", "tokens": [272, 22962, 11, 272, 22516, 11, 293, 29832, 292, 365, 262, 13], "temperature": 0.0, "avg_logprob": -0.1435293501073664, "compression_ratio": 1.537313432835821, "no_speech_prob": 2.8409369406290352e-05}, {"id": 140, "seek": 63868, "start": 641.92, "end": 652.0, "text": " I'll just mention in passing, I am not going to describe every single method we call and", "tokens": [286, 603, 445, 2152, 294, 8437, 11, 286, 669, 406, 516, 281, 6786, 633, 2167, 3170, 321, 818, 293], "temperature": 0.0, "avg_logprob": -0.1435293501073664, "compression_ratio": 1.537313432835821, "no_speech_prob": 2.8409369406290352e-05}, {"id": 141, "seek": 63868, "start": 652.0, "end": 654.4399999999999, "text": " every single function we use.", "tokens": [633, 2167, 2445, 321, 764, 13], "temperature": 0.0, "avg_logprob": -0.1435293501073664, "compression_ratio": 1.537313432835821, "no_speech_prob": 2.8409369406290352e-05}, {"id": 142, "seek": 63868, "start": 654.4399999999999, "end": 658.92, "text": " And that is not because you're an idiot if you don't already know them.", "tokens": [400, 300, 307, 406, 570, 291, 434, 364, 14270, 498, 291, 500, 380, 1217, 458, 552, 13], "temperature": 0.0, "avg_logprob": -0.1435293501073664, "compression_ratio": 1.537313432835821, "no_speech_prob": 2.8409369406290352e-05}, {"id": 143, "seek": 63868, "start": 658.92, "end": 660.8399999999999, "text": " Nobody knows them all.", "tokens": [9297, 3255, 552, 439, 13], "temperature": 0.0, "avg_logprob": -0.1435293501073664, "compression_ratio": 1.537313432835821, "no_speech_prob": 2.8409369406290352e-05}, {"id": 144, "seek": 63868, "start": 660.8399999999999, "end": 665.8399999999999, "text": " But I don't know which particular subset of them you don't know.", "tokens": [583, 286, 500, 380, 458, 597, 1729, 25993, 295, 552, 291, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.1435293501073664, "compression_ratio": 1.537313432835821, "no_speech_prob": 2.8409369406290352e-05}, {"id": 145, "seek": 66584, "start": 665.84, "end": 672.76, "text": " So let's assume just to pick a number at random that the average fast AI student knows 80%", "tokens": [407, 718, 311, 6552, 445, 281, 1888, 257, 1230, 412, 4974, 300, 264, 4274, 2370, 7318, 3107, 3255, 4688, 4], "temperature": 0.0, "avg_logprob": -0.14653824596870235, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.5070878109545447e-05}, {"id": 146, "seek": 66584, "start": 672.76, "end": 676.36, "text": " of the functions we call.", "tokens": [295, 264, 6828, 321, 818, 13], "temperature": 0.0, "avg_logprob": -0.14653824596870235, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.5070878109545447e-05}, {"id": 147, "seek": 66584, "start": 676.36, "end": 683.12, "text": " Then I could tell you what every function is, in which case 80% of the time I'm wasting", "tokens": [1396, 286, 727, 980, 291, 437, 633, 2445, 307, 11, 294, 597, 1389, 4688, 4, 295, 264, 565, 286, 478, 20457], "temperature": 0.0, "avg_logprob": -0.14653824596870235, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.5070878109545447e-05}, {"id": 148, "seek": 66584, "start": 683.12, "end": 686.6800000000001, "text": " your time because you already know.", "tokens": [428, 565, 570, 291, 1217, 458, 13], "temperature": 0.0, "avg_logprob": -0.14653824596870235, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.5070878109545447e-05}, {"id": 149, "seek": 66584, "start": 686.6800000000001, "end": 690.12, "text": " Or I could pick 20% of them at random, in which case I'm still not helping because most", "tokens": [1610, 286, 727, 1888, 945, 4, 295, 552, 412, 4974, 11, 294, 597, 1389, 286, 478, 920, 406, 4315, 570, 881], "temperature": 0.0, "avg_logprob": -0.14653824596870235, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.5070878109545447e-05}, {"id": 150, "seek": 66584, "start": 690.12, "end": 692.2800000000001, "text": " of the time it's not the ones you don't know.", "tokens": [295, 264, 565, 309, 311, 406, 264, 2306, 291, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.14653824596870235, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.5070878109545447e-05}, {"id": 151, "seek": 66584, "start": 692.2800000000001, "end": 695.2, "text": " My approach is that for the ones that are pretty common, I'm just not going to mention", "tokens": [1222, 3109, 307, 300, 337, 264, 2306, 300, 366, 1238, 2689, 11, 286, 478, 445, 406, 516, 281, 2152], "temperature": 0.0, "avg_logprob": -0.14653824596870235, "compression_ratio": 1.739622641509434, "no_speech_prob": 2.5070878109545447e-05}, {"id": 152, "seek": 69520, "start": 695.2, "end": 697.76, "text": " it at all because I'm assuming that you'll Google it.", "tokens": [309, 412, 439, 570, 286, 478, 11926, 300, 291, 603, 3329, 309, 13], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 153, "seek": 69520, "start": 697.76, "end": 699.32, "text": " So it's really important to know.", "tokens": [407, 309, 311, 534, 1021, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 154, "seek": 69520, "start": 699.32, "end": 702.8000000000001, "text": " So for example, if you don't know what iloc is, that's not a problem.", "tokens": [407, 337, 1365, 11, 498, 291, 500, 380, 458, 437, 1930, 905, 307, 11, 300, 311, 406, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 155, "seek": 69520, "start": 702.8000000000001, "end": 704.4000000000001, "text": " It doesn't mean you're stupid.", "tokens": [467, 1177, 380, 914, 291, 434, 6631, 13], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 156, "seek": 69520, "start": 704.4000000000001, "end": 708.8000000000001, "text": " It just means you haven't used it yet and you should Google it.", "tokens": [467, 445, 1355, 291, 2378, 380, 1143, 309, 1939, 293, 291, 820, 3329, 309, 13], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 157, "seek": 69520, "start": 708.8000000000001, "end": 714.5600000000001, "text": " So I'll mention in this particular case, this is one of the most important pandas, methods,", "tokens": [407, 286, 603, 2152, 294, 341, 1729, 1389, 11, 341, 307, 472, 295, 264, 881, 1021, 4565, 296, 11, 7150, 11], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 158, "seek": 69520, "start": 714.5600000000001, "end": 721.2, "text": " because it gives you the row located at this index, i for index and loc for location.", "tokens": [570, 309, 2709, 291, 264, 5386, 6870, 412, 341, 8186, 11, 741, 337, 8186, 293, 1628, 337, 4914, 13], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 159, "seek": 69520, "start": 721.2, "end": 723.32, "text": " So this is the zeroth row.", "tokens": [407, 341, 307, 264, 44746, 900, 5386, 13], "temperature": 0.0, "avg_logprob": -0.19952085935152494, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.00011234622070332989}, {"id": 160, "seek": 72332, "start": 723.32, "end": 729.32, "text": " But yeah, I do kind of go through things a little bit quickly on the assumption that", "tokens": [583, 1338, 11, 286, 360, 733, 295, 352, 807, 721, 257, 707, 857, 2661, 322, 264, 15302, 300], "temperature": 0.0, "avg_logprob": -0.19282537403673228, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.140493107726797e-05}, {"id": 161, "seek": 72332, "start": 729.32, "end": 735.2, "text": " students, fast AI students, are proactive, curious people.", "tokens": [1731, 11, 2370, 7318, 1731, 11, 366, 28028, 11, 6369, 561, 13], "temperature": 0.0, "avg_logprob": -0.19282537403673228, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.140493107726797e-05}, {"id": 162, "seek": 72332, "start": 735.2, "end": 738.24, "text": " And if you're not a proactive, curious person, then you could either decide to become one", "tokens": [400, 498, 291, 434, 406, 257, 28028, 11, 6369, 954, 11, 550, 291, 727, 2139, 4536, 281, 1813, 472], "temperature": 0.0, "avg_logprob": -0.19282537403673228, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.140493107726797e-05}, {"id": 163, "seek": 72332, "start": 738.24, "end": 743.4000000000001, "text": " for the purpose of this course, or maybe this course isn't for you.", "tokens": [337, 264, 4334, 295, 341, 1164, 11, 420, 1310, 341, 1164, 1943, 380, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.19282537403673228, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.140493107726797e-05}, {"id": 164, "seek": 72332, "start": 743.4000000000001, "end": 744.4000000000001, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.19282537403673228, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.140493107726797e-05}, {"id": 165, "seek": 72332, "start": 744.4000000000001, "end": 751.1400000000001, "text": " So a data frame has a very convenient method called fillNA, and that's going to replace", "tokens": [407, 257, 1412, 3920, 575, 257, 588, 10851, 3170, 1219, 2836, 5321, 11, 293, 300, 311, 516, 281, 7406], "temperature": 0.0, "avg_logprob": -0.19282537403673228, "compression_ratio": 1.6129032258064515, "no_speech_prob": 7.140493107726797e-05}, {"id": 166, "seek": 75114, "start": 751.14, "end": 755.64, "text": " the not a numbers with whatever I put here.", "tokens": [264, 406, 257, 3547, 365, 2035, 286, 829, 510, 13], "temperature": 0.0, "avg_logprob": -0.12989683058655377, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.6700407286407426e-05}, {"id": 167, "seek": 75114, "start": 755.64, "end": 759.64, "text": " And the nice thing about pandas is it kind of has this understanding that columns match", "tokens": [400, 264, 1481, 551, 466, 4565, 296, 307, 309, 733, 295, 575, 341, 3701, 300, 13766, 2995], "temperature": 0.0, "avg_logprob": -0.12989683058655377, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.6700407286407426e-05}, {"id": 168, "seek": 75114, "start": 759.64, "end": 760.64, "text": " to columns.", "tokens": [281, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12989683058655377, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.6700407286407426e-05}, {"id": 169, "seek": 75114, "start": 760.64, "end": 767.4, "text": " So it's going to take the mode from each column and match it to the same column in the data", "tokens": [407, 309, 311, 516, 281, 747, 264, 4391, 490, 1184, 7738, 293, 2995, 309, 281, 264, 912, 7738, 294, 264, 1412], "temperature": 0.0, "avg_logprob": -0.12989683058655377, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.6700407286407426e-05}, {"id": 170, "seek": 75114, "start": 767.4, "end": 771.0, "text": " frame and fill in those missing values.", "tokens": [3920, 293, 2836, 294, 729, 5361, 4190, 13], "temperature": 0.0, "avg_logprob": -0.12989683058655377, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.6700407286407426e-05}, {"id": 171, "seek": 75114, "start": 771.0, "end": 775.42, "text": " Normally, that would return a new data frame.", "tokens": [17424, 11, 300, 576, 2736, 257, 777, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.12989683058655377, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.6700407286407426e-05}, {"id": 172, "seek": 75114, "start": 775.42, "end": 779.1, "text": " Many things, including this one in pandas, have an in place argument that says actually", "tokens": [5126, 721, 11, 3009, 341, 472, 294, 4565, 296, 11, 362, 364, 294, 1081, 6770, 300, 1619, 767], "temperature": 0.0, "avg_logprob": -0.12989683058655377, "compression_ratio": 1.683127572016461, "no_speech_prob": 1.6700407286407426e-05}, {"id": 173, "seek": 77910, "start": 779.1, "end": 781.84, "text": " modify the original one.", "tokens": [16927, 264, 3380, 472, 13], "temperature": 0.0, "avg_logprob": -0.16117965607416063, "compression_ratio": 1.470899470899471, "no_speech_prob": 6.85414352119551e-06}, {"id": 174, "seek": 77910, "start": 781.84, "end": 789.44, "text": " And so if I run that, now if I call.isNA.sum, they're all zero.", "tokens": [400, 370, 498, 286, 1190, 300, 11, 586, 498, 286, 818, 2411, 271, 5321, 13, 82, 449, 11, 436, 434, 439, 4018, 13], "temperature": 0.0, "avg_logprob": -0.16117965607416063, "compression_ratio": 1.470899470899471, "no_speech_prob": 6.85414352119551e-06}, {"id": 175, "seek": 77910, "start": 789.44, "end": 793.52, "text": " So that's like the world's simplest way to get rid of missing values.", "tokens": [407, 300, 311, 411, 264, 1002, 311, 22811, 636, 281, 483, 3973, 295, 5361, 4190, 13], "temperature": 0.0, "avg_logprob": -0.16117965607416063, "compression_ratio": 1.470899470899471, "no_speech_prob": 6.85414352119551e-06}, {"id": 176, "seek": 77910, "start": 793.52, "end": 799.96, "text": " OK, so why did we do it the world's simplest way?", "tokens": [2264, 11, 370, 983, 630, 321, 360, 309, 264, 1002, 311, 22811, 636, 30], "temperature": 0.0, "avg_logprob": -0.16117965607416063, "compression_ratio": 1.470899470899471, "no_speech_prob": 6.85414352119551e-06}, {"id": 177, "seek": 77910, "start": 799.96, "end": 806.4, "text": " Because honestly, this doesn't make much difference most of the time.", "tokens": [1436, 6095, 11, 341, 1177, 380, 652, 709, 2649, 881, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.16117965607416063, "compression_ratio": 1.470899470899471, "no_speech_prob": 6.85414352119551e-06}, {"id": 178, "seek": 80640, "start": 806.4, "end": 813.36, "text": " And so I'm not going to spend time the first time I go through and build a baseline model", "tokens": [400, 370, 286, 478, 406, 516, 281, 3496, 565, 264, 700, 565, 286, 352, 807, 293, 1322, 257, 20518, 2316], "temperature": 0.0, "avg_logprob": -0.11567303628632516, "compression_ratio": 1.652, "no_speech_prob": 5.0935250328620896e-06}, {"id": 179, "seek": 80640, "start": 813.36, "end": 820.14, "text": " doing complicated things when I don't necessarily know that I need complicated things.", "tokens": [884, 6179, 721, 562, 286, 500, 380, 4725, 458, 300, 286, 643, 6179, 721, 13], "temperature": 0.0, "avg_logprob": -0.11567303628632516, "compression_ratio": 1.652, "no_speech_prob": 5.0935250328620896e-06}, {"id": 180, "seek": 80640, "start": 820.14, "end": 825.56, "text": " And so imputing missing values is an example of something that most of the time, this dumb", "tokens": [400, 370, 704, 10861, 5361, 4190, 307, 364, 1365, 295, 746, 300, 881, 295, 264, 565, 11, 341, 10316], "temperature": 0.0, "avg_logprob": -0.11567303628632516, "compression_ratio": 1.652, "no_speech_prob": 5.0935250328620896e-06}, {"id": 181, "seek": 80640, "start": 825.56, "end": 831.6, "text": " way, which always works without even thinking about it, will be quite good enough for nearly", "tokens": [636, 11, 597, 1009, 1985, 1553, 754, 1953, 466, 309, 11, 486, 312, 1596, 665, 1547, 337, 6217], "temperature": 0.0, "avg_logprob": -0.11567303628632516, "compression_ratio": 1.652, "no_speech_prob": 5.0935250328620896e-06}, {"id": 182, "seek": 80640, "start": 831.6, "end": 832.8, "text": " all the time.", "tokens": [439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.11567303628632516, "compression_ratio": 1.652, "no_speech_prob": 5.0935250328620896e-06}, {"id": 183, "seek": 80640, "start": 832.8, "end": 835.0, "text": " So we keep things simple where we can.", "tokens": [407, 321, 1066, 721, 2199, 689, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.11567303628632516, "compression_ratio": 1.652, "no_speech_prob": 5.0935250328620896e-06}, {"id": 184, "seek": 83500, "start": 835.0, "end": 836.56, "text": " John, question.", "tokens": [2619, 11, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18456065326655677, "compression_ratio": 1.5687022900763359, "no_speech_prob": 3.6465156881604344e-05}, {"id": 185, "seek": 83500, "start": 836.56, "end": 839.88, "text": " Jeremy, we've got a question on this topic.", "tokens": [17809, 11, 321, 600, 658, 257, 1168, 322, 341, 4829, 13], "temperature": 0.0, "avg_logprob": -0.18456065326655677, "compression_ratio": 1.5687022900763359, "no_speech_prob": 3.6465156881604344e-05}, {"id": 186, "seek": 83500, "start": 839.88, "end": 846.16, "text": " Javier is sort of commenting on the assumption involved in substituting with the mode.", "tokens": [508, 25384, 307, 1333, 295, 29590, 322, 264, 15302, 3288, 294, 26441, 10861, 365, 264, 4391, 13], "temperature": 0.0, "avg_logprob": -0.18456065326655677, "compression_ratio": 1.5687022900763359, "no_speech_prob": 3.6465156881604344e-05}, {"id": 187, "seek": 83500, "start": 846.16, "end": 850.84, "text": " And he's asking, in your experience, what are the pros and cons of doing this versus,", "tokens": [400, 415, 311, 3365, 11, 294, 428, 1752, 11, 437, 366, 264, 6267, 293, 1014, 295, 884, 341, 5717, 11], "temperature": 0.0, "avg_logprob": -0.18456065326655677, "compression_ratio": 1.5687022900763359, "no_speech_prob": 3.6465156881604344e-05}, {"id": 188, "seek": 83500, "start": 850.84, "end": 856.08, "text": " for example, discarding cabin or age as fields that we even train the model on?", "tokens": [337, 1365, 11, 31597, 278, 9401, 420, 3205, 382, 7909, 300, 321, 754, 3847, 264, 2316, 322, 30], "temperature": 0.0, "avg_logprob": -0.18456065326655677, "compression_ratio": 1.5687022900763359, "no_speech_prob": 3.6465156881604344e-05}, {"id": 189, "seek": 83500, "start": 856.08, "end": 860.4, "text": " Yeah, so I would certainly never throw them out, right?", "tokens": [865, 11, 370, 286, 576, 3297, 1128, 3507, 552, 484, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18456065326655677, "compression_ratio": 1.5687022900763359, "no_speech_prob": 3.6465156881604344e-05}, {"id": 190, "seek": 83500, "start": 860.4, "end": 863.38, "text": " There's just no reason to throw away data.", "tokens": [821, 311, 445, 572, 1778, 281, 3507, 1314, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18456065326655677, "compression_ratio": 1.5687022900763359, "no_speech_prob": 3.6465156881604344e-05}, {"id": 191, "seek": 86338, "start": 863.38, "end": 865.84, "text": " And there's lots of reasons to not throw away data.", "tokens": [400, 456, 311, 3195, 295, 4112, 281, 406, 3507, 1314, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11023650345978914, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129305337613914e-05}, {"id": 192, "seek": 86338, "start": 865.84, "end": 871.0, "text": " So for example, when we use the Fast AI library, which we'll use later, one of the things it", "tokens": [407, 337, 1365, 11, 562, 321, 764, 264, 15968, 7318, 6405, 11, 597, 321, 603, 764, 1780, 11, 472, 295, 264, 721, 309], "temperature": 0.0, "avg_logprob": -0.11023650345978914, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129305337613914e-05}, {"id": 193, "seek": 86338, "start": 871.0, "end": 875.64, "text": " does, which is actually a really good idea, is it creates a new column for everything", "tokens": [775, 11, 597, 307, 767, 257, 534, 665, 1558, 11, 307, 309, 7829, 257, 777, 7738, 337, 1203], "temperature": 0.0, "avg_logprob": -0.11023650345978914, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129305337613914e-05}, {"id": 194, "seek": 86338, "start": 875.64, "end": 880.38, "text": " that's got missing values, which is Boolean, which is, did that column have a missing value", "tokens": [300, 311, 658, 5361, 4190, 11, 597, 307, 23351, 28499, 11, 597, 307, 11, 630, 300, 7738, 362, 257, 5361, 2158], "temperature": 0.0, "avg_logprob": -0.11023650345978914, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129305337613914e-05}, {"id": 195, "seek": 86338, "start": 880.38, "end": 881.68, "text": " for this row?", "tokens": [337, 341, 5386, 30], "temperature": 0.0, "avg_logprob": -0.11023650345978914, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129305337613914e-05}, {"id": 196, "seek": 86338, "start": 881.68, "end": 887.66, "text": " And so maybe it turns out that cabin being empty is a great predictor.", "tokens": [400, 370, 1310, 309, 4523, 484, 300, 9401, 885, 6707, 307, 257, 869, 6069, 284, 13], "temperature": 0.0, "avg_logprob": -0.11023650345978914, "compression_ratio": 1.6818181818181819, "no_speech_prob": 1.0129305337613914e-05}, {"id": 197, "seek": 88766, "start": 887.66, "end": 893.8399999999999, "text": " So yeah, I don't throw out rows and I don't throw out columns.", "tokens": [407, 1338, 11, 286, 500, 380, 3507, 484, 13241, 293, 286, 500, 380, 3507, 484, 13766, 13], "temperature": 0.0, "avg_logprob": -0.21078665859727974, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.862647867616033e-06}, {"id": 198, "seek": 88766, "start": 893.8399999999999, "end": 895.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21078665859727974, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.862647867616033e-06}, {"id": 199, "seek": 88766, "start": 895.52, "end": 904.68, "text": " So it's helpful to understand a bit more about our data set and a really helpful, I've already", "tokens": [407, 309, 311, 4961, 281, 1223, 257, 857, 544, 466, 527, 1412, 992, 293, 257, 534, 4961, 11, 286, 600, 1217], "temperature": 0.0, "avg_logprob": -0.21078665859727974, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.862647867616033e-06}, {"id": 200, "seek": 88766, "start": 904.68, "end": 909.36, "text": " imported this, a really helpful quick method.", "tokens": [25524, 341, 11, 257, 534, 4961, 1702, 3170, 13], "temperature": 0.0, "avg_logprob": -0.21078665859727974, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.862647867616033e-06}, {"id": 201, "seek": 88766, "start": 909.36, "end": 915.04, "text": " And again, it's kind of nice to know a few quick things you can do to get a picture of", "tokens": [400, 797, 11, 309, 311, 733, 295, 1481, 281, 458, 257, 1326, 1702, 721, 291, 393, 360, 281, 483, 257, 3036, 295], "temperature": 0.0, "avg_logprob": -0.21078665859727974, "compression_ratio": 1.574468085106383, "no_speech_prob": 5.862647867616033e-06}, {"id": 202, "seek": 91504, "start": 915.04, "end": 918.28, "text": " what's happening in your data is describe.", "tokens": [437, 311, 2737, 294, 428, 1412, 307, 6786, 13], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 203, "seek": 91504, "start": 918.28, "end": 923.7199999999999, "text": " And so describe, you can say, okay, describe all the numeric variables.", "tokens": [400, 370, 6786, 11, 291, 393, 584, 11, 1392, 11, 6786, 439, 264, 7866, 299, 9102, 13], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 204, "seek": 91504, "start": 923.7199999999999, "end": 927.3199999999999, "text": " And that gives me a quick sense of what's going on here.", "tokens": [400, 300, 2709, 385, 257, 1702, 2020, 295, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 205, "seek": 91504, "start": 927.3199999999999, "end": 932.76, "text": " So we can see survive clearly is just zeros and ones, because all of the quartiles are", "tokens": [407, 321, 393, 536, 7867, 4448, 307, 445, 35193, 293, 2306, 11, 570, 439, 295, 264, 20837, 4680, 366], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 206, "seek": 91504, "start": 932.76, "end": 934.28, "text": " zeros and ones.", "tokens": [35193, 293, 2306, 13], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 207, "seek": 91504, "start": 934.28, "end": 940.68, "text": " Looks like p class is one, two, three.", "tokens": [10027, 411, 280, 1508, 307, 472, 11, 732, 11, 1045, 13], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 208, "seek": 91504, "start": 940.68, "end": 941.68, "text": " What else do we see?", "tokens": [708, 1646, 360, 321, 536, 30], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 209, "seek": 91504, "start": 941.68, "end": 944.0999999999999, "text": " Fair is an interesting one, right?", "tokens": [12157, 307, 364, 1880, 472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17578677411349314, "compression_ratio": 1.6043478260869566, "no_speech_prob": 1.28047468024306e-05}, {"id": 210, "seek": 94410, "start": 944.1, "end": 947.36, "text": " Lots of smallish numbers and one really big number, so probably long tailed.", "tokens": [15908, 295, 1359, 742, 3547, 293, 472, 534, 955, 1230, 11, 370, 1391, 938, 6838, 292, 13], "temperature": 0.0, "avg_logprob": -0.16001027317370398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 6.814312655478716e-05}, {"id": 211, "seek": 94410, "start": 947.36, "end": 953.44, "text": " So yeah, good to have a look at this to see what's going on for your numeric variables.", "tokens": [407, 1338, 11, 665, 281, 362, 257, 574, 412, 341, 281, 536, 437, 311, 516, 322, 337, 428, 7866, 299, 9102, 13], "temperature": 0.0, "avg_logprob": -0.16001027317370398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 6.814312655478716e-05}, {"id": 212, "seek": 94410, "start": 953.44, "end": 956.96, "text": " So as I said, fair looks kind of interesting.", "tokens": [407, 382, 286, 848, 11, 3143, 1542, 733, 295, 1880, 13], "temperature": 0.0, "avg_logprob": -0.16001027317370398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 6.814312655478716e-05}, {"id": 213, "seek": 94410, "start": 956.96, "end": 960.9200000000001, "text": " To find out what's going on there, I would generally go with a histogram.", "tokens": [1407, 915, 484, 437, 311, 516, 322, 456, 11, 286, 576, 5101, 352, 365, 257, 49816, 13], "temperature": 0.0, "avg_logprob": -0.16001027317370398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 6.814312655478716e-05}, {"id": 214, "seek": 94410, "start": 960.9200000000001, "end": 964.5600000000001, "text": " So if you can't quite remember what a histogram is, again, Google it.", "tokens": [407, 498, 291, 393, 380, 1596, 1604, 437, 257, 49816, 307, 11, 797, 11, 3329, 309, 13], "temperature": 0.0, "avg_logprob": -0.16001027317370398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 6.814312655478716e-05}, {"id": 215, "seek": 94410, "start": 964.5600000000001, "end": 970.6, "text": " But in short, it shows you for each amount of fair, how often does that fair appear.", "tokens": [583, 294, 2099, 11, 309, 3110, 291, 337, 1184, 2372, 295, 3143, 11, 577, 2049, 775, 300, 3143, 4204, 13], "temperature": 0.0, "avg_logprob": -0.16001027317370398, "compression_ratio": 1.6566037735849057, "no_speech_prob": 6.814312655478716e-05}, {"id": 216, "seek": 97060, "start": 970.6, "end": 975.32, "text": " It shows me here that the vast majority of fairs are less than $50, but there's a few", "tokens": [467, 3110, 385, 510, 300, 264, 8369, 6286, 295, 3143, 82, 366, 1570, 813, 1848, 2803, 11, 457, 456, 311, 257, 1326], "temperature": 0.0, "avg_logprob": -0.16807625180199032, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.854193998151459e-06}, {"id": 217, "seek": 97060, "start": 975.32, "end": 977.9200000000001, "text": " right up here to 500.", "tokens": [558, 493, 510, 281, 5923, 13], "temperature": 0.0, "avg_logprob": -0.16807625180199032, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.854193998151459e-06}, {"id": 218, "seek": 97060, "start": 977.9200000000001, "end": 984.1, "text": " So this is what we call a long tailed distribution, a small number of really big values and lots", "tokens": [407, 341, 307, 437, 321, 818, 257, 938, 6838, 292, 7316, 11, 257, 1359, 1230, 295, 534, 955, 4190, 293, 3195], "temperature": 0.0, "avg_logprob": -0.16807625180199032, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.854193998151459e-06}, {"id": 219, "seek": 97060, "start": 984.1, "end": 987.96, "text": " of small ones.", "tokens": [295, 1359, 2306, 13], "temperature": 0.0, "avg_logprob": -0.16807625180199032, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.854193998151459e-06}, {"id": 220, "seek": 97060, "start": 987.96, "end": 994.0400000000001, "text": " There are some types of model which do not like long tailed distributions.", "tokens": [821, 366, 512, 3467, 295, 2316, 597, 360, 406, 411, 938, 6838, 292, 37870, 13], "temperature": 0.0, "avg_logprob": -0.16807625180199032, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.854193998151459e-06}, {"id": 221, "seek": 97060, "start": 994.0400000000001, "end": 996.76, "text": " Linear models is certainly one of them.", "tokens": [14670, 289, 5245, 307, 3297, 472, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.16807625180199032, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.854193998151459e-06}, {"id": 222, "seek": 97060, "start": 996.76, "end": 1000.08, "text": " And neural nets are generally better behaved without them as well.", "tokens": [400, 18161, 36170, 366, 5101, 1101, 48249, 1553, 552, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16807625180199032, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.854193998151459e-06}, {"id": 223, "seek": 100008, "start": 1000.08, "end": 1005.5600000000001, "text": " Luckily there's an almost surefire way to turn a long tailed distribution into a more", "tokens": [19726, 456, 311, 364, 1920, 988, 12037, 636, 281, 1261, 257, 938, 6838, 292, 7316, 666, 257, 544], "temperature": 0.0, "avg_logprob": -0.11421872269023549, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.0952635420835577e-05}, {"id": 224, "seek": 100008, "start": 1005.5600000000001, "end": 1011.6, "text": " reasonably centered distribution, and that is to take the log.", "tokens": [23551, 18988, 7316, 11, 293, 300, 307, 281, 747, 264, 3565, 13], "temperature": 0.0, "avg_logprob": -0.11421872269023549, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.0952635420835577e-05}, {"id": 225, "seek": 100008, "start": 1011.6, "end": 1015.88, "text": " We use logs a lot in machine learning.", "tokens": [492, 764, 20820, 257, 688, 294, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.11421872269023549, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.0952635420835577e-05}, {"id": 226, "seek": 100008, "start": 1015.88, "end": 1023.2800000000001, "text": " For those of you that haven't touched them since year 10 math, it would be a very good", "tokens": [1171, 729, 295, 291, 300, 2378, 380, 9828, 552, 1670, 1064, 1266, 5221, 11, 309, 576, 312, 257, 588, 665], "temperature": 0.0, "avg_logprob": -0.11421872269023549, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.0952635420835577e-05}, {"id": 227, "seek": 100008, "start": 1023.2800000000001, "end": 1028.1200000000001, "text": " time to go to Khan Academy or something and remind yourself about what logs are and what", "tokens": [565, 281, 352, 281, 18136, 11735, 420, 746, 293, 4160, 1803, 466, 437, 20820, 366, 293, 437], "temperature": 0.0, "avg_logprob": -0.11421872269023549, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.0952635420835577e-05}, {"id": 228, "seek": 102812, "start": 1028.12, "end": 1033.2399999999998, "text": " they look like because they're actually really, really important.", "tokens": [436, 574, 411, 570, 436, 434, 767, 534, 11, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.16124579845330653, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4970712982176337e-05}, {"id": 229, "seek": 102812, "start": 1033.2399999999998, "end": 1039.84, "text": " But the basic shape of the log curve causes it to make really big numbers less really", "tokens": [583, 264, 3875, 3909, 295, 264, 3565, 7605, 7700, 309, 281, 652, 534, 955, 3547, 1570, 534], "temperature": 0.0, "avg_logprob": -0.16124579845330653, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4970712982176337e-05}, {"id": 230, "seek": 102812, "start": 1039.84, "end": 1044.1999999999998, "text": " big and doesn't change really small numbers very much at all.", "tokens": [955, 293, 1177, 380, 1319, 534, 1359, 3547, 588, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.16124579845330653, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4970712982176337e-05}, {"id": 231, "seek": 102812, "start": 1044.1999999999998, "end": 1053.08, "text": " So if we take the log, now log of 0 is nan, so a useful trick is to just do log plus 1.", "tokens": [407, 498, 321, 747, 264, 3565, 11, 586, 3565, 295, 1958, 307, 14067, 11, 370, 257, 4420, 4282, 307, 281, 445, 360, 3565, 1804, 502, 13], "temperature": 0.0, "avg_logprob": -0.16124579845330653, "compression_ratio": 1.5925925925925926, "no_speech_prob": 1.4970712982176337e-05}, {"id": 232, "seek": 105308, "start": 1053.08, "end": 1058.56, "text": " And in fact there is a log p1 if you want to do that, it does the same thing.", "tokens": [400, 294, 1186, 456, 307, 257, 3565, 280, 16, 498, 291, 528, 281, 360, 300, 11, 309, 775, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.16479749457780704, "compression_ratio": 1.5078534031413613, "no_speech_prob": 1.0615701285132673e-05}, {"id": 233, "seek": 105308, "start": 1058.56, "end": 1065.0, "text": " So if we look at the histogram of that, you can see it's much more sensible now.", "tokens": [407, 498, 321, 574, 412, 264, 49816, 295, 300, 11, 291, 393, 536, 309, 311, 709, 544, 25380, 586, 13], "temperature": 0.0, "avg_logprob": -0.16479749457780704, "compression_ratio": 1.5078534031413613, "no_speech_prob": 1.0615701285132673e-05}, {"id": 234, "seek": 105308, "start": 1065.0, "end": 1068.84, "text": " It's kind of centered and it doesn't have this big long tail.", "tokens": [467, 311, 733, 295, 18988, 293, 309, 1177, 380, 362, 341, 955, 938, 6838, 13], "temperature": 0.0, "avg_logprob": -0.16479749457780704, "compression_ratio": 1.5078534031413613, "no_speech_prob": 1.0615701285132673e-05}, {"id": 235, "seek": 105308, "start": 1068.84, "end": 1069.84, "text": " So that's pretty good.", "tokens": [407, 300, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.16479749457780704, "compression_ratio": 1.5078534031413613, "no_speech_prob": 1.0615701285132673e-05}, {"id": 236, "seek": 105308, "start": 1069.84, "end": 1074.12, "text": " So we'll be using that column in the future.", "tokens": [407, 321, 603, 312, 1228, 300, 7738, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.16479749457780704, "compression_ratio": 1.5078534031413613, "no_speech_prob": 1.0615701285132673e-05}, {"id": 237, "seek": 107412, "start": 1074.12, "end": 1083.8799999999999, "text": " As a rule of thumb, stuff like money or population, things that can grow exponentially, you very", "tokens": [1018, 257, 4978, 295, 9298, 11, 1507, 411, 1460, 420, 4415, 11, 721, 300, 393, 1852, 37330, 11, 291, 588], "temperature": 0.0, "avg_logprob": -0.1369334367605356, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.1443966033984907e-05}, {"id": 238, "seek": 107412, "start": 1083.8799999999999, "end": 1085.2399999999998, "text": " often want to take the log of.", "tokens": [2049, 528, 281, 747, 264, 3565, 295, 13], "temperature": 0.0, "avg_logprob": -0.1369334367605356, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.1443966033984907e-05}, {"id": 239, "seek": 107412, "start": 1085.2399999999998, "end": 1089.12, "text": " So if you have a column with a dollar sign on it, that's a good sign it might be something", "tokens": [407, 498, 291, 362, 257, 7738, 365, 257, 7241, 1465, 322, 309, 11, 300, 311, 257, 665, 1465, 309, 1062, 312, 746], "temperature": 0.0, "avg_logprob": -0.1369334367605356, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.1443966033984907e-05}, {"id": 240, "seek": 107412, "start": 1089.12, "end": 1092.1999999999998, "text": " to take the log of.", "tokens": [281, 747, 264, 3565, 295, 13], "temperature": 0.0, "avg_logprob": -0.1369334367605356, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.1443966033984907e-05}, {"id": 241, "seek": 107412, "start": 1092.1999999999998, "end": 1096.0, "text": " So there was another one here which is we had a numeric, which actually doesn't look", "tokens": [407, 456, 390, 1071, 472, 510, 597, 307, 321, 632, 257, 7866, 299, 11, 597, 767, 1177, 380, 574], "temperature": 0.0, "avg_logprob": -0.1369334367605356, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.1443966033984907e-05}, {"id": 242, "seek": 107412, "start": 1096.0, "end": 1097.0, "text": " numeric at all.", "tokens": [7866, 299, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1369334367605356, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.1443966033984907e-05}, {"id": 243, "seek": 107412, "start": 1097.0, "end": 1100.7199999999998, "text": " It looks like it's actually categories.", "tokens": [467, 1542, 411, 309, 311, 767, 10479, 13], "temperature": 0.0, "avg_logprob": -0.1369334367605356, "compression_ratio": 1.6844444444444444, "no_speech_prob": 2.1443966033984907e-05}, {"id": 244, "seek": 110072, "start": 1100.72, "end": 1106.88, "text": " So pandas gives us a.unique and so we can see, yep, they're just 1, 2 and 3 are all", "tokens": [407, 4565, 296, 2709, 505, 257, 2411, 409, 1925, 293, 370, 321, 393, 536, 11, 18633, 11, 436, 434, 445, 502, 11, 568, 293, 805, 366, 439], "temperature": 0.0, "avg_logprob": -0.2264043688774109, "compression_ratio": 1.6225490196078431, "no_speech_prob": 4.1983090341091156e-05}, {"id": 245, "seek": 110072, "start": 1106.88, "end": 1109.04, "text": " the levels of p class.", "tokens": [264, 4358, 295, 280, 1508, 13], "temperature": 0.0, "avg_logprob": -0.2264043688774109, "compression_ratio": 1.6225490196078431, "no_speech_prob": 4.1983090341091156e-05}, {"id": 246, "seek": 110072, "start": 1109.04, "end": 1113.28, "text": " That's their first class, second class or third class.", "tokens": [663, 311, 641, 700, 1508, 11, 1150, 1508, 420, 2636, 1508, 13], "temperature": 0.0, "avg_logprob": -0.2264043688774109, "compression_ratio": 1.6225490196078431, "no_speech_prob": 4.1983090341091156e-05}, {"id": 247, "seek": 110072, "start": 1113.28, "end": 1117.68, "text": " We can also describe all the non-numeric variables.", "tokens": [492, 393, 611, 6786, 439, 264, 2107, 12, 77, 15583, 299, 9102, 13], "temperature": 0.0, "avg_logprob": -0.2264043688774109, "compression_ratio": 1.6225490196078431, "no_speech_prob": 4.1983090341091156e-05}, {"id": 248, "seek": 110072, "start": 1117.68, "end": 1122.44, "text": " And so we can see here that not surprisingly names are unique because the count of names", "tokens": [400, 370, 321, 393, 536, 510, 300, 406, 17600, 5288, 366, 3845, 570, 264, 1207, 295, 5288], "temperature": 0.0, "avg_logprob": -0.2264043688774109, "compression_ratio": 1.6225490196078431, "no_speech_prob": 4.1983090341091156e-05}, {"id": 249, "seek": 110072, "start": 1122.44, "end": 1123.76, "text": " is the same as count unique.", "tokens": [307, 264, 912, 382, 1207, 3845, 13], "temperature": 0.0, "avg_logprob": -0.2264043688774109, "compression_ratio": 1.6225490196078431, "no_speech_prob": 4.1983090341091156e-05}, {"id": 250, "seek": 112376, "start": 1123.76, "end": 1140.28, "text": " There's two sexes, 681 different tickets, 147 different cabins and three levels of embarked.", "tokens": [821, 311, 732, 3260, 279, 11, 23317, 16, 819, 12628, 11, 3499, 22, 819, 5487, 1292, 293, 1045, 4358, 295, 29832, 292, 13], "temperature": 0.0, "avg_logprob": -0.1236652621516475, "compression_ratio": 1.0823529411764705, "no_speech_prob": 4.289230673748534e-06}, {"id": 251, "seek": 114028, "start": 1140.28, "end": 1157.02, "text": " So we cannot multiply the letter S by a coefficient or the word male by a coefficient.", "tokens": [407, 321, 2644, 12972, 264, 5063, 318, 538, 257, 17619, 420, 264, 1349, 7133, 538, 257, 17619, 13], "temperature": 0.0, "avg_logprob": -0.106677022091178, "compression_ratio": 1.3846153846153846, "no_speech_prob": 9.66511379374424e-06}, {"id": 252, "seek": 114028, "start": 1157.02, "end": 1160.6399999999999, "text": " So what do we do?", "tokens": [407, 437, 360, 321, 360, 30], "temperature": 0.0, "avg_logprob": -0.106677022091178, "compression_ratio": 1.3846153846153846, "no_speech_prob": 9.66511379374424e-06}, {"id": 253, "seek": 114028, "start": 1160.6399999999999, "end": 1166.8, "text": " What we do is we create something called dummy variables.", "tokens": [708, 321, 360, 307, 321, 1884, 746, 1219, 35064, 9102, 13], "temperature": 0.0, "avg_logprob": -0.106677022091178, "compression_ratio": 1.3846153846153846, "no_speech_prob": 9.66511379374424e-06}, {"id": 254, "seek": 116680, "start": 1166.8, "end": 1177.6, "text": " Dummy variables are, we can just go get dummies, a column that says for example is sex female,", "tokens": [413, 8620, 9102, 366, 11, 321, 393, 445, 352, 483, 16784, 38374, 11, 257, 7738, 300, 1619, 337, 1365, 307, 3260, 6556, 11], "temperature": 0.0, "avg_logprob": -0.15186061499253758, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.1842780622828286e-05}, {"id": 255, "seek": 116680, "start": 1177.6, "end": 1181.32, "text": " is sex male, is p class 1, is p class 2, is p class 3.", "tokens": [307, 3260, 7133, 11, 307, 280, 1508, 502, 11, 307, 280, 1508, 568, 11, 307, 280, 1508, 805, 13], "temperature": 0.0, "avg_logprob": -0.15186061499253758, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.1842780622828286e-05}, {"id": 256, "seek": 116680, "start": 1181.32, "end": 1186.24, "text": " So for every possible level of every possible categorical variable is a Boolean column of", "tokens": [407, 337, 633, 1944, 1496, 295, 633, 1944, 19250, 804, 7006, 307, 257, 23351, 28499, 7738, 295], "temperature": 0.0, "avg_logprob": -0.15186061499253758, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.1842780622828286e-05}, {"id": 257, "seek": 116680, "start": 1186.24, "end": 1190.9199999999998, "text": " did that row have that value of that column.", "tokens": [630, 300, 5386, 362, 300, 2158, 295, 300, 7738, 13], "temperature": 0.0, "avg_logprob": -0.15186061499253758, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.1842780622828286e-05}, {"id": 258, "seek": 116680, "start": 1190.9199999999998, "end": 1193.6399999999999, "text": " So I think we've briefly talked about this before that there's a couple of different", "tokens": [407, 286, 519, 321, 600, 10515, 2825, 466, 341, 949, 300, 456, 311, 257, 1916, 295, 819], "temperature": 0.0, "avg_logprob": -0.15186061499253758, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.1842780622828286e-05}, {"id": 259, "seek": 116680, "start": 1193.6399999999999, "end": 1195.36, "text": " ways we can do this.", "tokens": [2098, 321, 393, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.15186061499253758, "compression_ratio": 1.7410714285714286, "no_speech_prob": 1.1842780622828286e-05}, {"id": 260, "seek": 119536, "start": 1195.36, "end": 1201.9599999999998, "text": " One is that for an n level categorical variable we could use n minus 1 levels, in which case", "tokens": [1485, 307, 300, 337, 364, 297, 1496, 19250, 804, 7006, 321, 727, 764, 297, 3175, 502, 4358, 11, 294, 597, 1389], "temperature": 0.0, "avg_logprob": -0.15891955761199303, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.3551794583909214e-05}, {"id": 261, "seek": 119536, "start": 1201.9599999999998, "end": 1205.56, "text": " we also need a constant term in our model.", "tokens": [321, 611, 643, 257, 5754, 1433, 294, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15891955761199303, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.3551794583909214e-05}, {"id": 262, "seek": 119536, "start": 1205.56, "end": 1213.7199999999998, "text": " Pandas by default shows all n levels, although you can pass an argument to change that if", "tokens": [16995, 296, 538, 7576, 3110, 439, 297, 4358, 11, 4878, 291, 393, 1320, 364, 6770, 281, 1319, 300, 498], "temperature": 0.0, "avg_logprob": -0.15891955761199303, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.3551794583909214e-05}, {"id": 263, "seek": 119536, "start": 1213.7199999999998, "end": 1214.7199999999998, "text": " you want.", "tokens": [291, 528, 13], "temperature": 0.0, "avg_logprob": -0.15891955761199303, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.3551794583909214e-05}, {"id": 264, "seek": 119536, "start": 1214.7199999999998, "end": 1219.52, "text": " There we are, dropped first.", "tokens": [821, 321, 366, 11, 8119, 700, 13], "temperature": 0.0, "avg_logprob": -0.15891955761199303, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.3551794583909214e-05}, {"id": 265, "seek": 119536, "start": 1219.52, "end": 1222.9199999999998, "text": " I kind of like having all of them sometimes because then you don't have to put in a constant", "tokens": [286, 733, 295, 411, 1419, 439, 295, 552, 2171, 570, 550, 291, 500, 380, 362, 281, 829, 294, 257, 5754], "temperature": 0.0, "avg_logprob": -0.15891955761199303, "compression_ratio": 1.5657894736842106, "no_speech_prob": 2.3551794583909214e-05}, {"id": 266, "seek": 122292, "start": 1222.92, "end": 1229.8000000000002, "text": " term and it's a bit less annoying and it can be a bit easier to interpret, but I don't", "tokens": [1433, 293, 309, 311, 257, 857, 1570, 11304, 293, 309, 393, 312, 257, 857, 3571, 281, 7302, 11, 457, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.1779316550806949, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.241131940332707e-05}, {"id": 267, "seek": 122292, "start": 1229.8000000000002, "end": 1232.76, "text": " feel strongly about it either way.", "tokens": [841, 10613, 466, 309, 2139, 636, 13], "temperature": 0.0, "avg_logprob": -0.1779316550806949, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.241131940332707e-05}, {"id": 268, "seek": 122292, "start": 1232.76, "end": 1239.44, "text": " Okay, so here's a list of all of the columns that pandas added.", "tokens": [1033, 11, 370, 510, 311, 257, 1329, 295, 439, 295, 264, 13766, 300, 4565, 296, 3869, 13], "temperature": 0.0, "avg_logprob": -0.1779316550806949, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.241131940332707e-05}, {"id": 269, "seek": 122292, "start": 1239.44, "end": 1242.8400000000001, "text": " I guess strictly speaking I probably should have automated that, but never mind, I just", "tokens": [286, 2041, 20792, 4124, 286, 1391, 820, 362, 18473, 300, 11, 457, 1128, 1575, 11, 286, 445], "temperature": 0.0, "avg_logprob": -0.1779316550806949, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.241131940332707e-05}, {"id": 270, "seek": 122292, "start": 1242.8400000000001, "end": 1246.2, "text": " copied and pasted them.", "tokens": [25365, 293, 1791, 292, 552, 13], "temperature": 0.0, "avg_logprob": -0.1779316550806949, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.241131940332707e-05}, {"id": 271, "seek": 122292, "start": 1246.2, "end": 1250.3600000000001, "text": " And so here are a few examples of the added columns.", "tokens": [400, 370, 510, 366, 257, 1326, 5110, 295, 264, 3869, 13766, 13], "temperature": 0.0, "avg_logprob": -0.1779316550806949, "compression_ratio": 1.5981735159817352, "no_speech_prob": 1.241131940332707e-05}, {"id": 272, "seek": 125036, "start": 1250.36, "end": 1255.36, "text": " In Unix, pandas, lots of things like that, head means the first few rows or the first", "tokens": [682, 1156, 970, 11, 4565, 296, 11, 3195, 295, 721, 411, 300, 11, 1378, 1355, 264, 700, 1326, 13241, 420, 264, 700], "temperature": 0.0, "avg_logprob": -0.18295361529821638, "compression_ratio": 1.6062176165803108, "no_speech_prob": 1.834159775171429e-05}, {"id": 273, "seek": 125036, "start": 1255.36, "end": 1256.36, "text": " few lines.", "tokens": [1326, 3876, 13], "temperature": 0.0, "avg_logprob": -0.18295361529821638, "compression_ratio": 1.6062176165803108, "no_speech_prob": 1.834159775171429e-05}, {"id": 274, "seek": 125036, "start": 1256.36, "end": 1263.12, "text": " So 5 by default in pandas, so here you can see they're never both male and female, they're", "tokens": [407, 1025, 538, 7576, 294, 4565, 296, 11, 370, 510, 291, 393, 536, 436, 434, 1128, 1293, 7133, 293, 6556, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.18295361529821638, "compression_ratio": 1.6062176165803108, "no_speech_prob": 1.834159775171429e-05}, {"id": 275, "seek": 125036, "start": 1263.12, "end": 1267.4799999999998, "text": " never neither, they're always one or the other.", "tokens": [1128, 9662, 11, 436, 434, 1009, 472, 420, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.18295361529821638, "compression_ratio": 1.6062176165803108, "no_speech_prob": 1.834159775171429e-05}, {"id": 276, "seek": 125036, "start": 1267.4799999999998, "end": 1275.32, "text": " So with that now we've got numbers, which we can multiply by coefficients.", "tokens": [407, 365, 300, 586, 321, 600, 658, 3547, 11, 597, 321, 393, 12972, 538, 31994, 13], "temperature": 0.0, "avg_logprob": -0.18295361529821638, "compression_ratio": 1.6062176165803108, "no_speech_prob": 1.834159775171429e-05}, {"id": 277, "seek": 127532, "start": 1275.32, "end": 1283.84, "text": " It's not going to work for name, obviously, because we'd have 891 columns and all of them", "tokens": [467, 311, 406, 516, 281, 589, 337, 1315, 11, 2745, 11, 570, 321, 1116, 362, 1649, 29925, 13766, 293, 439, 295, 552], "temperature": 0.0, "avg_logprob": -0.16614599716969025, "compression_ratio": 1.4414893617021276, "no_speech_prob": 1.3845590729033574e-05}, {"id": 278, "seek": 127532, "start": 1283.84, "end": 1285.6, "text": " would be unique.", "tokens": [576, 312, 3845, 13], "temperature": 0.0, "avg_logprob": -0.16614599716969025, "compression_ratio": 1.4414893617021276, "no_speech_prob": 1.3845590729033574e-05}, {"id": 279, "seek": 127532, "start": 1285.6, "end": 1288.4399999999998, "text": " So we'll ignore that for now.", "tokens": [407, 321, 603, 11200, 300, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.16614599716969025, "compression_ratio": 1.4414893617021276, "no_speech_prob": 1.3845590729033574e-05}, {"id": 280, "seek": 127532, "start": 1288.4399999999998, "end": 1292.04, "text": " That doesn't mean it's have to always ignore it.", "tokens": [663, 1177, 380, 914, 309, 311, 362, 281, 1009, 11200, 309, 13], "temperature": 0.0, "avg_logprob": -0.16614599716969025, "compression_ratio": 1.4414893617021276, "no_speech_prob": 1.3845590729033574e-05}, {"id": 281, "seek": 127532, "start": 1292.04, "end": 1301.32, "text": " And in fact something I did do on the forum topic, because I made a list of some nice", "tokens": [400, 294, 1186, 746, 286, 630, 360, 322, 264, 17542, 4829, 11, 570, 286, 1027, 257, 1329, 295, 512, 1481], "temperature": 0.0, "avg_logprob": -0.16614599716969025, "compression_ratio": 1.4414893617021276, "no_speech_prob": 1.3845590729033574e-05}, {"id": 282, "seek": 130132, "start": 1301.32, "end": 1309.32, "text": " Titanic notebooks that I found, and quite a few of them really go hard on this name", "tokens": [42183, 43782, 300, 286, 1352, 11, 293, 1596, 257, 1326, 295, 552, 534, 352, 1152, 322, 341, 1315], "temperature": 0.0, "avg_logprob": -0.18196530177675443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.132891626795754e-05}, {"id": 283, "seek": 130132, "start": 1309.32, "end": 1310.32, "text": " column.", "tokens": [7738, 13], "temperature": 0.0, "avg_logprob": -0.18196530177675443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.132891626795754e-05}, {"id": 284, "seek": 130132, "start": 1310.32, "end": 1319.84, "text": " And in fact one of them, yeah this one, in what I believe is, yes, Krestyot's first ever", "tokens": [400, 294, 1186, 472, 295, 552, 11, 1338, 341, 472, 11, 294, 437, 286, 1697, 307, 11, 2086, 11, 591, 4149, 88, 310, 311, 700, 1562], "temperature": 0.0, "avg_logprob": -0.18196530177675443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.132891626795754e-05}, {"id": 285, "seek": 130132, "start": 1319.84, "end": 1320.84, "text": " Kaggle notebook.", "tokens": [48751, 22631, 21060, 13], "temperature": 0.0, "avg_logprob": -0.18196530177675443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.132891626795754e-05}, {"id": 286, "seek": 130132, "start": 1320.84, "end": 1324.8, "text": " He's now the number one ranked Kaggle notebook person in the world.", "tokens": [634, 311, 586, 264, 1230, 472, 20197, 48751, 22631, 21060, 954, 294, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.18196530177675443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.132891626795754e-05}, {"id": 287, "seek": 130132, "start": 1324.8, "end": 1327.3999999999999, "text": " So this is a very good start.", "tokens": [407, 341, 307, 257, 588, 665, 722, 13], "temperature": 0.0, "avg_logprob": -0.18196530177675443, "compression_ratio": 1.5364583333333333, "no_speech_prob": 4.132891626795754e-05}, {"id": 288, "seek": 132740, "start": 1327.4, "end": 1331.4, "text": " And he got a much better score than any model that we're going to create in this course,", "tokens": [400, 415, 658, 257, 709, 1101, 6175, 813, 604, 2316, 300, 321, 434, 516, 281, 1884, 294, 341, 1164, 11], "temperature": 0.0, "avg_logprob": -0.13133789772211119, "compression_ratio": 1.599056603773585, "no_speech_prob": 8.663640983286314e-06}, {"id": 289, "seek": 132740, "start": 1331.4, "end": 1334.72, "text": " using only that column name.", "tokens": [1228, 787, 300, 7738, 1315, 13], "temperature": 0.0, "avg_logprob": -0.13133789772211119, "compression_ratio": 1.599056603773585, "no_speech_prob": 8.663640983286314e-06}, {"id": 290, "seek": 132740, "start": 1334.72, "end": 1343.44, "text": " And basically, yeah, he came up with this simple little decision tree by recognizing", "tokens": [400, 1936, 11, 1338, 11, 415, 1361, 493, 365, 341, 2199, 707, 3537, 4230, 538, 18538], "temperature": 0.0, "avg_logprob": -0.13133789772211119, "compression_ratio": 1.599056603773585, "no_speech_prob": 8.663640983286314e-06}, {"id": 291, "seek": 132740, "start": 1343.44, "end": 1348.3400000000001, "text": " all of the information that's in a name column.", "tokens": [439, 295, 264, 1589, 300, 311, 294, 257, 1315, 7738, 13], "temperature": 0.0, "avg_logprob": -0.13133789772211119, "compression_ratio": 1.599056603773585, "no_speech_prob": 8.663640983286314e-06}, {"id": 292, "seek": 132740, "start": 1348.3400000000001, "end": 1356.76, "text": " So yeah, we don't have to treat a big string of letters like this as a random big string", "tokens": [407, 1338, 11, 321, 500, 380, 362, 281, 2387, 257, 955, 6798, 295, 7825, 411, 341, 382, 257, 4974, 955, 6798], "temperature": 0.0, "avg_logprob": -0.13133789772211119, "compression_ratio": 1.599056603773585, "no_speech_prob": 8.663640983286314e-06}, {"id": 293, "seek": 135676, "start": 1356.76, "end": 1357.76, "text": " of letters.", "tokens": [295, 7825, 13], "temperature": 0.0, "avg_logprob": -0.1410485795400675, "compression_ratio": 1.6300813008130082, "no_speech_prob": 3.647157791419886e-05}, {"id": 294, "seek": 135676, "start": 1357.76, "end": 1363.08, "text": " We can use our domain expertise to recognize that things like Mr. have meaning, and that", "tokens": [492, 393, 764, 527, 9274, 11769, 281, 5521, 300, 721, 411, 2221, 13, 362, 3620, 11, 293, 300], "temperature": 0.0, "avg_logprob": -0.1410485795400675, "compression_ratio": 1.6300813008130082, "no_speech_prob": 3.647157791419886e-05}, {"id": 295, "seek": 135676, "start": 1363.08, "end": 1369.24, "text": " people with the same surname might be in the same family, and actually figure out quite", "tokens": [561, 365, 264, 912, 50152, 1062, 312, 294, 264, 912, 1605, 11, 293, 767, 2573, 484, 1596], "temperature": 0.0, "avg_logprob": -0.1410485795400675, "compression_ratio": 1.6300813008130082, "no_speech_prob": 3.647157791419886e-05}, {"id": 296, "seek": 135676, "start": 1369.24, "end": 1372.16, "text": " a lot from that.", "tokens": [257, 688, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.1410485795400675, "compression_ratio": 1.6300813008130082, "no_speech_prob": 3.647157791419886e-05}, {"id": 297, "seek": 135676, "start": 1372.16, "end": 1374.08, "text": " But that's not something I'm going to do.", "tokens": [583, 300, 311, 406, 746, 286, 478, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1410485795400675, "compression_ratio": 1.6300813008130082, "no_speech_prob": 3.647157791419886e-05}, {"id": 298, "seek": 135676, "start": 1374.08, "end": 1377.96, "text": " I'll let you look at those notebooks if you're interested in the feature engineering.", "tokens": [286, 603, 718, 291, 574, 412, 729, 43782, 498, 291, 434, 3102, 294, 264, 4111, 7043, 13], "temperature": 0.0, "avg_logprob": -0.1410485795400675, "compression_ratio": 1.6300813008130082, "no_speech_prob": 3.647157791419886e-05}, {"id": 299, "seek": 135676, "start": 1377.96, "end": 1382.16, "text": " And I do think that they're very interesting, so do check them out.", "tokens": [400, 286, 360, 519, 300, 436, 434, 588, 1880, 11, 370, 360, 1520, 552, 484, 13], "temperature": 0.0, "avg_logprob": -0.1410485795400675, "compression_ratio": 1.6300813008130082, "no_speech_prob": 3.647157791419886e-05}, {"id": 300, "seek": 138216, "start": 1382.16, "end": 1388.0, "text": " Our focus today is on building a linear model in a neural net from scratch, not on tabular", "tokens": [2621, 1879, 965, 307, 322, 2390, 257, 8213, 2316, 294, 257, 18161, 2533, 490, 8459, 11, 406, 322, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.16748852199978298, "compression_ratio": 1.5072463768115942, "no_speech_prob": 1.1478238775453065e-05}, {"id": 301, "seek": 138216, "start": 1388.0, "end": 1392.4, "text": " feature engineering, even though that's also a very important subject.", "tokens": [4111, 7043, 11, 754, 1673, 300, 311, 611, 257, 588, 1021, 3983, 13], "temperature": 0.0, "avg_logprob": -0.16748852199978298, "compression_ratio": 1.5072463768115942, "no_speech_prob": 1.1478238775453065e-05}, {"id": 302, "seek": 138216, "start": 1392.4, "end": 1403.7, "text": " OK, so we talked about how matrix multiplication makes linear models much easier.", "tokens": [2264, 11, 370, 321, 2825, 466, 577, 8141, 27290, 1669, 8213, 5245, 709, 3571, 13], "temperature": 0.0, "avg_logprob": -0.16748852199978298, "compression_ratio": 1.5072463768115942, "no_speech_prob": 1.1478238775453065e-05}, {"id": 303, "seek": 138216, "start": 1403.7, "end": 1407.2, "text": " And the other thing we did in Excel was element-wise multiplication.", "tokens": [400, 264, 661, 551, 321, 630, 294, 19060, 390, 4478, 12, 3711, 27290, 13], "temperature": 0.0, "avg_logprob": -0.16748852199978298, "compression_ratio": 1.5072463768115942, "no_speech_prob": 1.1478238775453065e-05}, {"id": 304, "seek": 140720, "start": 1407.2, "end": 1412.96, "text": " Both of those things are much easier if we use PyTorch instead of plain Python.", "tokens": [6767, 295, 729, 721, 366, 709, 3571, 498, 321, 764, 9953, 51, 284, 339, 2602, 295, 11121, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 305, "seek": 140720, "start": 1412.96, "end": 1413.96, "text": " Or we could use NumPy.", "tokens": [1610, 321, 727, 764, 22592, 47, 88, 13], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 306, "seek": 140720, "start": 1413.96, "end": 1418.0, "text": " But I tend to just stick with PyTorch when I can, because it's easier to learn one library", "tokens": [583, 286, 3928, 281, 445, 2897, 365, 9953, 51, 284, 339, 562, 286, 393, 11, 570, 309, 311, 3571, 281, 1466, 472, 6405], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 307, "seek": 140720, "start": 1418.0, "end": 1419.0, "text": " than two.", "tokens": [813, 732, 13], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 308, "seek": 140720, "start": 1419.0, "end": 1421.8, "text": " So I just do everything in PyTorch.", "tokens": [407, 286, 445, 360, 1203, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 309, "seek": 140720, "start": 1421.8, "end": 1424.2, "text": " I almost never touch NumPy nowadays.", "tokens": [286, 1920, 1128, 2557, 22592, 47, 88, 13434, 13], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 310, "seek": 140720, "start": 1424.2, "end": 1429.3600000000001, "text": " They're both great, but they do everything each other does, except PyTorch also does", "tokens": [814, 434, 1293, 869, 11, 457, 436, 360, 1203, 1184, 661, 775, 11, 3993, 9953, 51, 284, 339, 611, 775], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 311, "seek": 140720, "start": 1429.3600000000001, "end": 1434.2, "text": " differentiation and GPUs, so why not just learn PyTorch?", "tokens": [38902, 293, 18407, 82, 11, 370, 983, 406, 445, 1466, 9953, 51, 284, 339, 30], "temperature": 0.0, "avg_logprob": -0.1064814461602105, "compression_ratio": 1.6076923076923078, "no_speech_prob": 2.6677136702346615e-05}, {"id": 312, "seek": 143420, "start": 1434.2, "end": 1444.96, "text": " So to turn a column into something that I can do PyTorch calculations on, I have to", "tokens": [407, 281, 1261, 257, 7738, 666, 746, 300, 286, 393, 360, 9953, 51, 284, 339, 20448, 322, 11, 286, 362, 281], "temperature": 0.0, "avg_logprob": -0.11973706632852554, "compression_ratio": 1.4437086092715232, "no_speech_prob": 2.0784384105354548e-05}, {"id": 313, "seek": 143420, "start": 1444.96, "end": 1446.96, "text": " turn it into a tensor.", "tokens": [1261, 309, 666, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.11973706632852554, "compression_ratio": 1.4437086092715232, "no_speech_prob": 2.0784384105354548e-05}, {"id": 314, "seek": 143420, "start": 1446.96, "end": 1452.64, "text": " So a tensor is just what NumPy calls an array.", "tokens": [407, 257, 40863, 307, 445, 437, 22592, 47, 88, 5498, 364, 10225, 13], "temperature": 0.0, "avg_logprob": -0.11973706632852554, "compression_ratio": 1.4437086092715232, "no_speech_prob": 2.0784384105354548e-05}, {"id": 315, "seek": 143420, "start": 1452.64, "end": 1457.6000000000001, "text": " It's what mathematicians would call either a vector or a matrix.", "tokens": [467, 311, 437, 32811, 2567, 576, 818, 2139, 257, 8062, 420, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.11973706632852554, "compression_ratio": 1.4437086092715232, "no_speech_prob": 2.0784384105354548e-05}, {"id": 316, "seek": 145760, "start": 1457.6, "end": 1464.32, "text": " Or once we go to higher ranks, mathematicians and physicists just call them tensors.", "tokens": [1610, 1564, 321, 352, 281, 2946, 21406, 11, 32811, 2567, 293, 48716, 445, 818, 552, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.10540868798080756, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4509450920741074e-05}, {"id": 317, "seek": 145760, "start": 1464.32, "end": 1470.1599999999999, "text": " In fact, this idea originally in computer science came from a notation developed in", "tokens": [682, 1186, 11, 341, 1558, 7993, 294, 3820, 3497, 1361, 490, 257, 24657, 4743, 294], "temperature": 0.0, "avg_logprob": -0.10540868798080756, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4509450920741074e-05}, {"id": 318, "seek": 145760, "start": 1470.1599999999999, "end": 1474.84, "text": " the 50s called APL, which has turned into a programming language in the 60s by a guy", "tokens": [264, 2625, 82, 1219, 5372, 43, 11, 597, 575, 3574, 666, 257, 9410, 2856, 294, 264, 4060, 82, 538, 257, 2146], "temperature": 0.0, "avg_logprob": -0.10540868798080756, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4509450920741074e-05}, {"id": 319, "seek": 145760, "start": 1474.84, "end": 1475.84, "text": " called Ken Iverson.", "tokens": [1219, 8273, 286, 840, 266, 13], "temperature": 0.0, "avg_logprob": -0.10540868798080756, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4509450920741074e-05}, {"id": 320, "seek": 145760, "start": 1475.84, "end": 1482.6799999999998, "text": " And Ken Iverson actually came up with this idea from, he said, his time doing tensor", "tokens": [400, 8273, 286, 840, 266, 767, 1361, 493, 365, 341, 1558, 490, 11, 415, 848, 11, 702, 565, 884, 40863], "temperature": 0.0, "avg_logprob": -0.10540868798080756, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4509450920741074e-05}, {"id": 321, "seek": 145760, "start": 1482.6799999999998, "end": 1484.8799999999999, "text": " analysis in physics.", "tokens": [5215, 294, 10649, 13], "temperature": 0.0, "avg_logprob": -0.10540868798080756, "compression_ratio": 1.5924369747899159, "no_speech_prob": 1.4509450920741074e-05}, {"id": 322, "seek": 148488, "start": 1484.88, "end": 1488.94, "text": " So these areas are very related.", "tokens": [407, 613, 3179, 366, 588, 4077, 13], "temperature": 0.0, "avg_logprob": -0.15033245086669922, "compression_ratio": 1.6625, "no_speech_prob": 9.515947567706462e-06}, {"id": 323, "seek": 148488, "start": 1488.94, "end": 1493.8000000000002, "text": " So we can turn the survived column into a tensor, and we'll call that tensor our dependent", "tokens": [407, 321, 393, 1261, 264, 14433, 7738, 666, 257, 40863, 11, 293, 321, 603, 818, 300, 40863, 527, 12334], "temperature": 0.0, "avg_logprob": -0.15033245086669922, "compression_ratio": 1.6625, "no_speech_prob": 9.515947567706462e-06}, {"id": 324, "seek": 148488, "start": 1493.8000000000002, "end": 1494.8000000000002, "text": " variable.", "tokens": [7006, 13], "temperature": 0.0, "avg_logprob": -0.15033245086669922, "compression_ratio": 1.6625, "no_speech_prob": 9.515947567706462e-06}, {"id": 325, "seek": 148488, "start": 1494.8000000000002, "end": 1497.92, "text": " That's the thing we're trying to predict.", "tokens": [663, 311, 264, 551, 321, 434, 1382, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.15033245086669922, "compression_ratio": 1.6625, "no_speech_prob": 9.515947567706462e-06}, {"id": 326, "seek": 148488, "start": 1497.92, "end": 1499.6200000000001, "text": " So now we need some independent variables.", "tokens": [407, 586, 321, 643, 512, 6695, 9102, 13], "temperature": 0.0, "avg_logprob": -0.15033245086669922, "compression_ratio": 1.6625, "no_speech_prob": 9.515947567706462e-06}, {"id": 327, "seek": 148488, "start": 1499.6200000000001, "end": 1506.8000000000002, "text": " So our independent variables are age, siblings.", "tokens": [407, 527, 6695, 9102, 366, 3205, 11, 20571, 13], "temperature": 0.0, "avg_logprob": -0.15033245086669922, "compression_ratio": 1.6625, "no_speech_prob": 9.515947567706462e-06}, {"id": 328, "seek": 150680, "start": 1506.8, "end": 1515.8, "text": " That one is, oh, yeah, number of other family members.", "tokens": [663, 472, 307, 11, 1954, 11, 1338, 11, 1230, 295, 661, 1605, 2679, 13], "temperature": 0.0, "avg_logprob": -0.1573513851890081, "compression_ratio": 1.5, "no_speech_prob": 1.1842773346870672e-05}, {"id": 329, "seek": 150680, "start": 1515.8, "end": 1520.68, "text": " The log of fare that we just created plus all of those dummy columns we added.", "tokens": [440, 3565, 295, 11994, 300, 321, 445, 2942, 1804, 439, 295, 729, 35064, 13766, 321, 3869, 13], "temperature": 0.0, "avg_logprob": -0.1573513851890081, "compression_ratio": 1.5, "no_speech_prob": 1.1842773346870672e-05}, {"id": 330, "seek": 150680, "start": 1520.68, "end": 1529.04, "text": " And so we can now grab those values and turn them into a tensor.", "tokens": [400, 370, 321, 393, 586, 4444, 729, 4190, 293, 1261, 552, 666, 257, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1573513851890081, "compression_ratio": 1.5, "no_speech_prob": 1.1842773346870672e-05}, {"id": 331, "seek": 150680, "start": 1529.04, "end": 1531.6399999999999, "text": " And we have to make sure they're floats.", "tokens": [400, 321, 362, 281, 652, 988, 436, 434, 37878, 13], "temperature": 0.0, "avg_logprob": -0.1573513851890081, "compression_ratio": 1.5, "no_speech_prob": 1.1842773346870672e-05}, {"id": 332, "seek": 150680, "start": 1531.6399999999999, "end": 1533.6, "text": " We want them all to be the same data type.", "tokens": [492, 528, 552, 439, 281, 312, 264, 912, 1412, 2010, 13], "temperature": 0.0, "avg_logprob": -0.1573513851890081, "compression_ratio": 1.5, "no_speech_prob": 1.1842773346870672e-05}, {"id": 333, "seek": 153360, "start": 1533.6, "end": 1539.9199999999998, "text": " And PyTorch wants things to be floats if you're going to multiply things together.", "tokens": [400, 9953, 51, 284, 339, 2738, 721, 281, 312, 37878, 498, 291, 434, 516, 281, 12972, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10227104028065999, "compression_ratio": 1.6091954022988506, "no_speech_prob": 2.726430693655857e-06}, {"id": 334, "seek": 153360, "start": 1539.9199999999998, "end": 1544.9199999999998, "text": " So there we are.", "tokens": [407, 456, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.10227104028065999, "compression_ratio": 1.6091954022988506, "no_speech_prob": 2.726430693655857e-06}, {"id": 335, "seek": 153360, "start": 1544.9199999999998, "end": 1551.08, "text": " And so one of the most important attributes of a tensor, probably the most important attribute,", "tokens": [400, 370, 472, 295, 264, 881, 1021, 17212, 295, 257, 40863, 11, 1391, 264, 881, 1021, 19667, 11], "temperature": 0.0, "avg_logprob": -0.10227104028065999, "compression_ratio": 1.6091954022988506, "no_speech_prob": 2.726430693655857e-06}, {"id": 336, "seek": 153360, "start": 1551.08, "end": 1558.24, "text": " is its shape, which is how many rows does it have and how many columns does it have.", "tokens": [307, 1080, 3909, 11, 597, 307, 577, 867, 13241, 775, 309, 362, 293, 577, 867, 13766, 775, 309, 362, 13], "temperature": 0.0, "avg_logprob": -0.10227104028065999, "compression_ratio": 1.6091954022988506, "no_speech_prob": 2.726430693655857e-06}, {"id": 337, "seek": 155824, "start": 1558.24, "end": 1565.6, "text": " The length of the shape is called its rank.", "tokens": [440, 4641, 295, 264, 3909, 307, 1219, 1080, 6181, 13], "temperature": 0.0, "avg_logprob": -0.1160097424946134, "compression_ratio": 1.5255474452554745, "no_speech_prob": 3.905426183337113e-06}, {"id": 338, "seek": 155824, "start": 1565.6, "end": 1566.6, "text": " That's the rank of the tensor.", "tokens": [663, 311, 264, 6181, 295, 264, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1160097424946134, "compression_ratio": 1.5255474452554745, "no_speech_prob": 3.905426183337113e-06}, {"id": 339, "seek": 155824, "start": 1566.6, "end": 1571.16, "text": " It's the number of dimensions or axes that it has.", "tokens": [467, 311, 264, 1230, 295, 12819, 420, 35387, 300, 309, 575, 13], "temperature": 0.0, "avg_logprob": -0.1160097424946134, "compression_ratio": 1.5255474452554745, "no_speech_prob": 3.905426183337113e-06}, {"id": 340, "seek": 155824, "start": 1571.16, "end": 1586.0, "text": " So a vector is rank one, a matrix is rank two, a scalar is rank zero, and so forth.", "tokens": [407, 257, 8062, 307, 6181, 472, 11, 257, 8141, 307, 6181, 732, 11, 257, 39684, 307, 6181, 4018, 11, 293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1160097424946134, "compression_ratio": 1.5255474452554745, "no_speech_prob": 3.905426183337113e-06}, {"id": 341, "seek": 158600, "start": 1586.0, "end": 1591.16, "text": " I try not to use too much jargon, but there's some pieces of jargon that are really important", "tokens": [286, 853, 406, 281, 764, 886, 709, 15181, 10660, 11, 457, 456, 311, 512, 3755, 295, 15181, 10660, 300, 366, 534, 1021], "temperature": 0.0, "avg_logprob": -0.15000433670847038, "compression_ratio": 1.5742574257425743, "no_speech_prob": 9.368345672555733e-06}, {"id": 342, "seek": 158600, "start": 1591.16, "end": 1596.2, "text": " because otherwise you're going to have to say the length of the shape again and again.", "tokens": [570, 5911, 291, 434, 516, 281, 362, 281, 584, 264, 4641, 295, 264, 3909, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.15000433670847038, "compression_ratio": 1.5742574257425743, "no_speech_prob": 9.368345672555733e-06}, {"id": 343, "seek": 158600, "start": 1596.2, "end": 1597.88, "text": " It's much easier to say rank.", "tokens": [467, 311, 709, 3571, 281, 584, 6181, 13], "temperature": 0.0, "avg_logprob": -0.15000433670847038, "compression_ratio": 1.5742574257425743, "no_speech_prob": 9.368345672555733e-06}, {"id": 344, "seek": 158600, "start": 1597.88, "end": 1601.92, "text": " So we'll use that word a lot.", "tokens": [407, 321, 603, 764, 300, 1349, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.15000433670847038, "compression_ratio": 1.5742574257425743, "no_speech_prob": 9.368345672555733e-06}, {"id": 345, "seek": 158600, "start": 1601.92, "end": 1605.64, "text": " So a table is a rank two tensor.", "tokens": [407, 257, 3199, 307, 257, 6181, 732, 40863, 13], "temperature": 0.0, "avg_logprob": -0.15000433670847038, "compression_ratio": 1.5742574257425743, "no_speech_prob": 9.368345672555733e-06}, {"id": 346, "seek": 158600, "start": 1605.64, "end": 1607.24, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.15000433670847038, "compression_ratio": 1.5742574257425743, "no_speech_prob": 9.368345672555733e-06}, {"id": 347, "seek": 158600, "start": 1607.24, "end": 1611.52, "text": " So we've now got the data in good shape.", "tokens": [407, 321, 600, 586, 658, 264, 1412, 294, 665, 3909, 13], "temperature": 0.0, "avg_logprob": -0.15000433670847038, "compression_ratio": 1.5742574257425743, "no_speech_prob": 9.368345672555733e-06}, {"id": 348, "seek": 161152, "start": 1611.52, "end": 1616.16, "text": " These are independent variables and we've got our dependent variable.", "tokens": [1981, 366, 6695, 9102, 293, 321, 600, 658, 527, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1441211191813151, "compression_ratio": 1.605263157894737, "no_speech_prob": 7.889158041507471e-06}, {"id": 349, "seek": 161152, "start": 1616.16, "end": 1624.48, "text": " So we can now go ahead and do exactly what we did in Excel, which is to multiply our", "tokens": [407, 321, 393, 586, 352, 2286, 293, 360, 2293, 437, 321, 630, 294, 19060, 11, 597, 307, 281, 12972, 527], "temperature": 0.0, "avg_logprob": -0.1441211191813151, "compression_ratio": 1.605263157894737, "no_speech_prob": 7.889158041507471e-06}, {"id": 350, "seek": 161152, "start": 1624.48, "end": 1629.36, "text": " rows of data by some coefficients.", "tokens": [13241, 295, 1412, 538, 512, 31994, 13], "temperature": 0.0, "avg_logprob": -0.1441211191813151, "compression_ratio": 1.605263157894737, "no_speech_prob": 7.889158041507471e-06}, {"id": 351, "seek": 161152, "start": 1629.36, "end": 1634.78, "text": " And remember, to start with, we create random coefficients.", "tokens": [400, 1604, 11, 281, 722, 365, 11, 321, 1884, 4974, 31994, 13], "temperature": 0.0, "avg_logprob": -0.1441211191813151, "compression_ratio": 1.605263157894737, "no_speech_prob": 7.889158041507471e-06}, {"id": 352, "seek": 161152, "start": 1634.78, "end": 1638.32, "text": " So we're going to need one coefficient for each column.", "tokens": [407, 321, 434, 516, 281, 643, 472, 17619, 337, 1184, 7738, 13], "temperature": 0.0, "avg_logprob": -0.1441211191813151, "compression_ratio": 1.605263157894737, "no_speech_prob": 7.889158041507471e-06}, {"id": 353, "seek": 163832, "start": 1638.32, "end": 1644.9199999999998, "text": " Now in Excel we also had a constant, but in our case now we've got every column, every", "tokens": [823, 294, 19060, 321, 611, 632, 257, 5754, 11, 457, 294, 527, 1389, 586, 321, 600, 658, 633, 7738, 11, 633], "temperature": 0.0, "avg_logprob": -0.11835581117922121, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.2411062016326468e-05}, {"id": 354, "seek": 163832, "start": 1644.9199999999998, "end": 1647.84, "text": " level in our dummy variables, so we don't need a constant.", "tokens": [1496, 294, 527, 35064, 9102, 11, 370, 321, 500, 380, 643, 257, 5754, 13], "temperature": 0.0, "avg_logprob": -0.11835581117922121, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.2411062016326468e-05}, {"id": 355, "seek": 163832, "start": 1647.84, "end": 1653.04, "text": " So the number of coefficients we need is equal to the shape of the independent variables", "tokens": [407, 264, 1230, 295, 31994, 321, 643, 307, 2681, 281, 264, 3909, 295, 264, 6695, 9102], "temperature": 0.0, "avg_logprob": -0.11835581117922121, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.2411062016326468e-05}, {"id": 356, "seek": 163832, "start": 1653.04, "end": 1656.4399999999998, "text": " and it's the index one element.", "tokens": [293, 309, 311, 264, 8186, 472, 4478, 13], "temperature": 0.0, "avg_logprob": -0.11835581117922121, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.2411062016326468e-05}, {"id": 357, "seek": 163832, "start": 1656.4399999999998, "end": 1658.04, "text": " That's the number of columns.", "tokens": [663, 311, 264, 1230, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.11835581117922121, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.2411062016326468e-05}, {"id": 358, "seek": 163832, "start": 1658.04, "end": 1660.6399999999999, "text": " That's how many coefficients we want.", "tokens": [663, 311, 577, 867, 31994, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.11835581117922121, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.2411062016326468e-05}, {"id": 359, "seek": 163832, "start": 1660.6399999999999, "end": 1668.04, "text": " So we can now ask PyTorch to give us some random numbers and coef of them.", "tokens": [407, 321, 393, 586, 1029, 9953, 51, 284, 339, 281, 976, 505, 512, 4974, 3547, 293, 598, 5666, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.11835581117922121, "compression_ratio": 1.7705627705627707, "no_speech_prob": 1.2411062016326468e-05}, {"id": 360, "seek": 166804, "start": 1668.04, "end": 1669.78, "text": " They're between zero and one.", "tokens": [814, 434, 1296, 4018, 293, 472, 13], "temperature": 0.0, "avg_logprob": -0.16075921640163515, "compression_ratio": 1.53, "no_speech_prob": 4.1983690607594326e-05}, {"id": 361, "seek": 166804, "start": 1669.78, "end": 1673.76, "text": " So if we subtract a half, then they'll be centered.", "tokens": [407, 498, 321, 16390, 257, 1922, 11, 550, 436, 603, 312, 18988, 13], "temperature": 0.0, "avg_logprob": -0.16075921640163515, "compression_ratio": 1.53, "no_speech_prob": 4.1983690607594326e-05}, {"id": 362, "seek": 166804, "start": 1673.76, "end": 1674.76, "text": " And there we go.", "tokens": [400, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.16075921640163515, "compression_ratio": 1.53, "no_speech_prob": 4.1983690607594326e-05}, {"id": 363, "seek": 166804, "start": 1674.76, "end": 1679.28, "text": " Before I do that, I set the seed.", "tokens": [4546, 286, 360, 300, 11, 286, 992, 264, 8871, 13], "temperature": 0.0, "avg_logprob": -0.16075921640163515, "compression_ratio": 1.53, "no_speech_prob": 4.1983690607594326e-05}, {"id": 364, "seek": 166804, "start": 1679.28, "end": 1688.44, "text": " What that means is in computers, computers in general cannot create truly random numbers.", "tokens": [708, 300, 1355, 307, 294, 10807, 11, 10807, 294, 2674, 2644, 1884, 4908, 4974, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16075921640163515, "compression_ratio": 1.53, "no_speech_prob": 4.1983690607594326e-05}, {"id": 365, "seek": 166804, "start": 1688.44, "end": 1696.18, "text": " Instead, they can calculate a sequence of numbers that behave in a random like way.", "tokens": [7156, 11, 436, 393, 8873, 257, 8310, 295, 3547, 300, 15158, 294, 257, 4974, 411, 636, 13], "temperature": 0.0, "avg_logprob": -0.16075921640163515, "compression_ratio": 1.53, "no_speech_prob": 4.1983690607594326e-05}, {"id": 366, "seek": 169618, "start": 1696.18, "end": 1699.64, "text": " That's actually good for us because often in my teaching, I like to be able to say,", "tokens": [663, 311, 767, 665, 337, 505, 570, 2049, 294, 452, 4571, 11, 286, 411, 281, 312, 1075, 281, 584, 11], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 367, "seek": 169618, "start": 1699.64, "end": 1702.2, "text": " you know, in the pros, oh, look, that was two.", "tokens": [291, 458, 11, 294, 264, 6267, 11, 1954, 11, 574, 11, 300, 390, 732, 13], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 368, "seek": 169618, "start": 1702.2, "end": 1704.0, "text": " Now it's three or whatever.", "tokens": [823, 309, 311, 1045, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 369, "seek": 169618, "start": 1704.0, "end": 1708.76, "text": " And if I was using really random numbers, then I couldn't do that because it'd be different", "tokens": [400, 498, 286, 390, 1228, 534, 4974, 3547, 11, 550, 286, 2809, 380, 360, 300, 570, 309, 1116, 312, 819], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 370, "seek": 169618, "start": 1708.76, "end": 1709.76, "text": " each time.", "tokens": [1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 371, "seek": 169618, "start": 1709.76, "end": 1713.72, "text": " So this makes my results reproducible.", "tokens": [407, 341, 1669, 452, 3542, 11408, 32128, 13], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 372, "seek": 169618, "start": 1713.72, "end": 1719.76, "text": " That means if you run it, you'll get the same random numbers as I do by saying start the", "tokens": [663, 1355, 498, 291, 1190, 309, 11, 291, 603, 483, 264, 912, 4974, 3547, 382, 286, 360, 538, 1566, 722, 264], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 373, "seek": 169618, "start": 1719.76, "end": 1725.72, "text": " pseudo random sequence with this number.", "tokens": [35899, 4974, 8310, 365, 341, 1230, 13], "temperature": 0.0, "avg_logprob": -0.13886655278566504, "compression_ratio": 1.628787878787879, "no_speech_prob": 7.527571597165661e-06}, {"id": 374, "seek": 172572, "start": 1725.72, "end": 1731.56, "text": " I mentioned in passing, a lot of people are very, very into reproducible results.", "tokens": [286, 2835, 294, 8437, 11, 257, 688, 295, 561, 366, 588, 11, 588, 666, 11408, 32128, 3542, 13], "temperature": 0.0, "avg_logprob": -0.08980524941776576, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.3630705325340386e-05}, {"id": 375, "seek": 172572, "start": 1731.56, "end": 1735.64, "text": " They think it's really important to always do this.", "tokens": [814, 519, 309, 311, 534, 1021, 281, 1009, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.08980524941776576, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.3630705325340386e-05}, {"id": 376, "seek": 172572, "start": 1735.64, "end": 1738.18, "text": " I strongly disagree with that.", "tokens": [286, 10613, 14091, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.08980524941776576, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.3630705325340386e-05}, {"id": 377, "seek": 172572, "start": 1738.18, "end": 1743.72, "text": " In my opinion, an important part of understanding your data is understanding how much it varies", "tokens": [682, 452, 4800, 11, 364, 1021, 644, 295, 3701, 428, 1412, 307, 3701, 577, 709, 309, 21716], "temperature": 0.0, "avg_logprob": -0.08980524941776576, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.3630705325340386e-05}, {"id": 378, "seek": 172572, "start": 1743.72, "end": 1745.04, "text": " from run to run.", "tokens": [490, 1190, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.08980524941776576, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.3630705325340386e-05}, {"id": 379, "seek": 172572, "start": 1745.04, "end": 1751.16, "text": " So if I'm not teaching and wanting to be able to write things about these pseudo random", "tokens": [407, 498, 286, 478, 406, 4571, 293, 7935, 281, 312, 1075, 281, 2464, 721, 466, 613, 35899, 4974], "temperature": 0.0, "avg_logprob": -0.08980524941776576, "compression_ratio": 1.5938864628820961, "no_speech_prob": 1.3630705325340386e-05}, {"id": 380, "seek": 175116, "start": 1751.16, "end": 1756.1200000000001, "text": " numbers, I almost never use a manual seed.", "tokens": [3547, 11, 286, 1920, 1128, 764, 257, 9688, 8871, 13], "temperature": 0.0, "avg_logprob": -0.1733380769428454, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.8924003597931005e-05}, {"id": 381, "seek": 175116, "start": 1756.1200000000001, "end": 1759.72, "text": " Instead I like to run things a few times and get an intuitive sense of like, oh, this is", "tokens": [7156, 286, 411, 281, 1190, 721, 257, 1326, 1413, 293, 483, 364, 21769, 2020, 295, 411, 11, 1954, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.1733380769428454, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.8924003597931005e-05}, {"id": 382, "seek": 175116, "start": 1759.72, "end": 1761.88, "text": " like very, very stable.", "tokens": [411, 588, 11, 588, 8351, 13], "temperature": 0.0, "avg_logprob": -0.1733380769428454, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.8924003597931005e-05}, {"id": 383, "seek": 175116, "start": 1761.88, "end": 1763.3600000000001, "text": " Or oh, this is all over the place.", "tokens": [1610, 1954, 11, 341, 307, 439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1733380769428454, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.8924003597931005e-05}, {"id": 384, "seek": 175116, "start": 1763.3600000000001, "end": 1768.24, "text": " Getting an intuitive understanding of how your data behaves and your model behaves is", "tokens": [13674, 364, 21769, 3701, 295, 577, 428, 1412, 36896, 293, 428, 2316, 36896, 307], "temperature": 0.0, "avg_logprob": -0.1733380769428454, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.8924003597931005e-05}, {"id": 385, "seek": 175116, "start": 1768.24, "end": 1770.92, "text": " really important.", "tokens": [534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1733380769428454, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.8924003597931005e-05}, {"id": 386, "seek": 175116, "start": 1770.92, "end": 1777.6000000000001, "text": " Now here's one of the coolest lines of code you'll ever see.", "tokens": [823, 510, 311, 472, 295, 264, 22013, 3876, 295, 3089, 291, 603, 1562, 536, 13], "temperature": 0.0, "avg_logprob": -0.1733380769428454, "compression_ratio": 1.6210045662100456, "no_speech_prob": 1.8924003597931005e-05}, {"id": 387, "seek": 177760, "start": 1777.6, "end": 1784.0, "text": " I know it doesn't look like much, but think about what it's doing.", "tokens": [286, 458, 309, 1177, 380, 574, 411, 709, 11, 457, 519, 466, 437, 309, 311, 884, 13], "temperature": 0.0, "avg_logprob": -0.22388026827857607, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3211384612077381e-05}, {"id": 388, "seek": 177760, "start": 1784.0, "end": 1787.32, "text": " Yeah, that'll do.", "tokens": [865, 11, 300, 603, 360, 13], "temperature": 0.0, "avg_logprob": -0.22388026827857607, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3211384612077381e-05}, {"id": 389, "seek": 177760, "start": 1787.32, "end": 1795.32, "text": " Okay, so we've multiplied a matrix by a vector.", "tokens": [1033, 11, 370, 321, 600, 17207, 257, 8141, 538, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.22388026827857607, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3211384612077381e-05}, {"id": 390, "seek": 177760, "start": 1795.32, "end": 1796.32, "text": " Now that's pretty interesting.", "tokens": [823, 300, 311, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22388026827857607, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3211384612077381e-05}, {"id": 391, "seek": 177760, "start": 1796.32, "end": 1801.9599999999998, "text": " Now mathematicians amongst you will know that you can certainly do a matrix vector product,", "tokens": [823, 32811, 2567, 12918, 291, 486, 458, 300, 291, 393, 3297, 360, 257, 8141, 8062, 1674, 11], "temperature": 0.0, "avg_logprob": -0.22388026827857607, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3211384612077381e-05}, {"id": 392, "seek": 177760, "start": 1801.9599999999998, "end": 1806.6, "text": " but that's not what we've done here at all.", "tokens": [457, 300, 311, 406, 437, 321, 600, 1096, 510, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.22388026827857607, "compression_ratio": 1.5333333333333334, "no_speech_prob": 1.3211384612077381e-05}, {"id": 393, "seek": 180660, "start": 1806.6, "end": 1809.08, "text": " We've used element-wise multiplication.", "tokens": [492, 600, 1143, 4478, 12, 3711, 27290, 13], "temperature": 0.0, "avg_logprob": -0.14343687283095494, "compression_ratio": 1.709090909090909, "no_speech_prob": 5.307051833369769e-05}, {"id": 394, "seek": 180660, "start": 1809.08, "end": 1815.8799999999999, "text": " So normally if we did the element-wise multiplication of two vectors, it would multiply element one", "tokens": [407, 5646, 498, 321, 630, 264, 4478, 12, 3711, 27290, 295, 732, 18875, 11, 309, 576, 12972, 4478, 472], "temperature": 0.0, "avg_logprob": -0.14343687283095494, "compression_ratio": 1.709090909090909, "no_speech_prob": 5.307051833369769e-05}, {"id": 395, "seek": 180660, "start": 1815.8799999999999, "end": 1820.48, "text": " with element one, element two with element two, and so forth, and create a vector of", "tokens": [365, 4478, 472, 11, 4478, 732, 365, 4478, 732, 11, 293, 370, 5220, 11, 293, 1884, 257, 8062, 295], "temperature": 0.0, "avg_logprob": -0.14343687283095494, "compression_ratio": 1.709090909090909, "no_speech_prob": 5.307051833369769e-05}, {"id": 396, "seek": 180660, "start": 1820.48, "end": 1823.48, "text": " the same size output.", "tokens": [264, 912, 2744, 5598, 13], "temperature": 0.0, "avg_logprob": -0.14343687283095494, "compression_ratio": 1.709090909090909, "no_speech_prob": 5.307051833369769e-05}, {"id": 397, "seek": 180660, "start": 1823.48, "end": 1826.9399999999998, "text": " But here we've done a matrix times a vector.", "tokens": [583, 510, 321, 600, 1096, 257, 8141, 1413, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.14343687283095494, "compression_ratio": 1.709090909090909, "no_speech_prob": 5.307051833369769e-05}, {"id": 398, "seek": 180660, "start": 1826.9399999999998, "end": 1829.58, "text": " How does that work?", "tokens": [1012, 775, 300, 589, 30], "temperature": 0.0, "avg_logprob": -0.14343687283095494, "compression_ratio": 1.709090909090909, "no_speech_prob": 5.307051833369769e-05}, {"id": 399, "seek": 180660, "start": 1829.58, "end": 1833.8, "text": " This is using the incredibly powerful technique of broadcasting.", "tokens": [639, 307, 1228, 264, 6252, 4005, 6532, 295, 30024, 13], "temperature": 0.0, "avg_logprob": -0.14343687283095494, "compression_ratio": 1.709090909090909, "no_speech_prob": 5.307051833369769e-05}, {"id": 400, "seek": 183380, "start": 1833.8, "end": 1839.44, "text": " And broadcasting again comes from APL, a notation invented in the 50s and a programming language", "tokens": [400, 30024, 797, 1487, 490, 5372, 43, 11, 257, 24657, 14479, 294, 264, 2625, 82, 293, 257, 9410, 2856], "temperature": 0.0, "avg_logprob": -0.08567637613374893, "compression_ratio": 1.470899470899471, "no_speech_prob": 4.289277967473026e-06}, {"id": 401, "seek": 183380, "start": 1839.44, "end": 1843.08, "text": " developed in the 60s.", "tokens": [4743, 294, 264, 4060, 82, 13], "temperature": 0.0, "avg_logprob": -0.08567637613374893, "compression_ratio": 1.470899470899471, "no_speech_prob": 4.289277967473026e-06}, {"id": 402, "seek": 183380, "start": 1843.08, "end": 1845.56, "text": " And it's got a number of benefits.", "tokens": [400, 309, 311, 658, 257, 1230, 295, 5311, 13], "temperature": 0.0, "avg_logprob": -0.08567637613374893, "compression_ratio": 1.470899470899471, "no_speech_prob": 4.289277967473026e-06}, {"id": 403, "seek": 183380, "start": 1845.56, "end": 1850.84, "text": " Basically what it's going to do is it's going to take each coefficient and multiply them", "tokens": [8537, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 747, 1184, 17619, 293, 12972, 552], "temperature": 0.0, "avg_logprob": -0.08567637613374893, "compression_ratio": 1.470899470899471, "no_speech_prob": 4.289277967473026e-06}, {"id": 404, "seek": 183380, "start": 1850.84, "end": 1855.52, "text": " in turn by every row in our matrix.", "tokens": [294, 1261, 538, 633, 5386, 294, 527, 8141, 13], "temperature": 0.0, "avg_logprob": -0.08567637613374893, "compression_ratio": 1.470899470899471, "no_speech_prob": 4.289277967473026e-06}, {"id": 405, "seek": 185552, "start": 1855.52, "end": 1866.16, "text": " So if you look at the shape of our independent variable and the shape of our coefficients,", "tokens": [407, 498, 291, 574, 412, 264, 3909, 295, 527, 6695, 7006, 293, 264, 3909, 295, 527, 31994, 11], "temperature": 0.0, "avg_logprob": -0.12008061347069679, "compression_ratio": 1.6032608695652173, "no_speech_prob": 2.684175342437811e-06}, {"id": 406, "seek": 185552, "start": 1866.16, "end": 1872.6399999999999, "text": " you can see that each one of these coefficients can be multiplied by each of these 891 values", "tokens": [291, 393, 536, 300, 1184, 472, 295, 613, 31994, 393, 312, 17207, 538, 1184, 295, 613, 1649, 29925, 4190], "temperature": 0.0, "avg_logprob": -0.12008061347069679, "compression_ratio": 1.6032608695652173, "no_speech_prob": 2.684175342437811e-06}, {"id": 407, "seek": 185552, "start": 1872.6399999999999, "end": 1875.46, "text": " in turn.", "tokens": [294, 1261, 13], "temperature": 0.0, "avg_logprob": -0.12008061347069679, "compression_ratio": 1.6032608695652173, "no_speech_prob": 2.684175342437811e-06}, {"id": 408, "seek": 185552, "start": 1875.46, "end": 1880.76, "text": " And so the reason we call it broadcasting is it's as if this is 891 columns by 12 rows", "tokens": [400, 370, 264, 1778, 321, 818, 309, 30024, 307, 309, 311, 382, 498, 341, 307, 1649, 29925, 13766, 538, 2272, 13241], "temperature": 0.0, "avg_logprob": -0.12008061347069679, "compression_ratio": 1.6032608695652173, "no_speech_prob": 2.684175342437811e-06}, {"id": 409, "seek": 185552, "start": 1880.76, "end": 1882.16, "text": " by 12 columns.", "tokens": [538, 2272, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12008061347069679, "compression_ratio": 1.6032608695652173, "no_speech_prob": 2.684175342437811e-06}, {"id": 410, "seek": 188216, "start": 1882.16, "end": 1887.92, "text": " It's as if this was broadcast 891 times.", "tokens": [467, 311, 382, 498, 341, 390, 9975, 1649, 29925, 1413, 13], "temperature": 0.0, "avg_logprob": -0.19003096080961682, "compression_ratio": 1.7134831460674158, "no_speech_prob": 9.36839478526963e-06}, {"id": 411, "seek": 188216, "start": 1887.92, "end": 1893.76, "text": " It's as if we had a loop looping 891 times and doing coefficients times row zero, coefficients", "tokens": [467, 311, 382, 498, 321, 632, 257, 6367, 6367, 278, 1649, 29925, 1413, 293, 884, 31994, 1413, 5386, 4018, 11, 31994], "temperature": 0.0, "avg_logprob": -0.19003096080961682, "compression_ratio": 1.7134831460674158, "no_speech_prob": 9.36839478526963e-06}, {"id": 412, "seek": 188216, "start": 1893.76, "end": 1898.24, "text": " times row one, coefficients times row zero, two, and so forth, which is exactly what we", "tokens": [1413, 5386, 472, 11, 31994, 1413, 5386, 4018, 11, 732, 11, 293, 370, 5220, 11, 597, 307, 2293, 437, 321], "temperature": 0.0, "avg_logprob": -0.19003096080961682, "compression_ratio": 1.7134831460674158, "no_speech_prob": 9.36839478526963e-06}, {"id": 413, "seek": 188216, "start": 1898.24, "end": 1900.0400000000002, "text": " want.", "tokens": [528, 13], "temperature": 0.0, "avg_logprob": -0.19003096080961682, "compression_ratio": 1.7134831460674158, "no_speech_prob": 9.36839478526963e-06}, {"id": 414, "seek": 188216, "start": 1900.0400000000002, "end": 1904.0400000000002, "text": " Now, reasons to use broadcasting.", "tokens": [823, 11, 4112, 281, 764, 30024, 13], "temperature": 0.0, "avg_logprob": -0.19003096080961682, "compression_ratio": 1.7134831460674158, "no_speech_prob": 9.36839478526963e-06}, {"id": 415, "seek": 188216, "start": 1904.0400000000002, "end": 1908.24, "text": " Obviously, the code is much more concise.", "tokens": [7580, 11, 264, 3089, 307, 709, 544, 44882, 13], "temperature": 0.0, "avg_logprob": -0.19003096080961682, "compression_ratio": 1.7134831460674158, "no_speech_prob": 9.36839478526963e-06}, {"id": 416, "seek": 190824, "start": 1908.24, "end": 1913.92, "text": " It looks more like math rather than flunky programming with lots of boilerplate.", "tokens": [467, 1542, 544, 411, 5221, 2831, 813, 932, 25837, 9410, 365, 3195, 295, 39228, 37008, 13], "temperature": 0.0, "avg_logprob": -0.15498355649552256, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.09286121794139e-06}, {"id": 417, "seek": 190824, "start": 1913.92, "end": 1915.96, "text": " So that's good.", "tokens": [407, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.15498355649552256, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.09286121794139e-06}, {"id": 418, "seek": 190824, "start": 1915.96, "end": 1922.4, "text": " Also that broadcasting all happened in optimized C code.", "tokens": [2743, 300, 30024, 439, 2011, 294, 26941, 383, 3089, 13], "temperature": 0.0, "avg_logprob": -0.15498355649552256, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.09286121794139e-06}, {"id": 419, "seek": 190824, "start": 1922.4, "end": 1927.84, "text": " And if in fact it's being done on a GPU, it's being done in optimized GPU assembler, CUDA", "tokens": [400, 498, 294, 1186, 309, 311, 885, 1096, 322, 257, 18407, 11, 309, 311, 885, 1096, 294, 26941, 18407, 8438, 1918, 11, 29777, 7509], "temperature": 0.0, "avg_logprob": -0.15498355649552256, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.09286121794139e-06}, {"id": 420, "seek": 190824, "start": 1927.84, "end": 1928.84, "text": " code.", "tokens": [3089, 13], "temperature": 0.0, "avg_logprob": -0.15498355649552256, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.09286121794139e-06}, {"id": 421, "seek": 190824, "start": 1928.84, "end": 1932.52, "text": " It's going to run very, very fast indeed.", "tokens": [467, 311, 516, 281, 1190, 588, 11, 588, 2370, 6451, 13], "temperature": 0.0, "avg_logprob": -0.15498355649552256, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.09286121794139e-06}, {"id": 422, "seek": 190824, "start": 1932.52, "end": 1938.22, "text": " And this is a trick of why we can use a so-called slow language like Python to do very fast", "tokens": [400, 341, 307, 257, 4282, 295, 983, 321, 393, 764, 257, 370, 12, 11880, 2964, 2856, 411, 15329, 281, 360, 588, 2370], "temperature": 0.0, "avg_logprob": -0.15498355649552256, "compression_ratio": 1.5892116182572613, "no_speech_prob": 4.09286121794139e-06}, {"id": 423, "seek": 193822, "start": 1938.22, "end": 1944.88, "text": " big models is because a single line of code like this can run very quickly on optimized", "tokens": [955, 5245, 307, 570, 257, 2167, 1622, 295, 3089, 411, 341, 393, 1190, 588, 2661, 322, 26941], "temperature": 0.0, "avg_logprob": -0.08754265562016914, "compression_ratio": 1.4028776978417266, "no_speech_prob": 6.14391365161282e-06}, {"id": 424, "seek": 193822, "start": 1944.88, "end": 1949.24, "text": " hardware on lots and lots of data.", "tokens": [8837, 322, 3195, 293, 3195, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08754265562016914, "compression_ratio": 1.4028776978417266, "no_speech_prob": 6.14391365161282e-06}, {"id": 425, "seek": 193822, "start": 1949.24, "end": 1955.92, "text": " The rules of broadcasting are a little bit subtle and important to know.", "tokens": [440, 4474, 295, 30024, 366, 257, 707, 857, 13743, 293, 1021, 281, 458, 13], "temperature": 0.0, "avg_logprob": -0.08754265562016914, "compression_ratio": 1.4028776978417266, "no_speech_prob": 6.14391365161282e-06}, {"id": 426, "seek": 195592, "start": 1955.92, "end": 1971.5600000000002, "text": " And so I would strongly encourage you to Google NumPy broadcasting rules and see", "tokens": [400, 370, 286, 576, 10613, 5373, 291, 281, 3329, 22592, 47, 88, 30024, 4474, 293, 536], "temperature": 0.0, "avg_logprob": -0.16248953342437744, "compression_ratio": 1.4752475247524752, "no_speech_prob": 1.0782834579003975e-05}, {"id": 427, "seek": 195592, "start": 1971.5600000000002, "end": 1973.04, "text": " exactly how they work.", "tokens": [2293, 577, 436, 589, 13], "temperature": 0.0, "avg_logprob": -0.16248953342437744, "compression_ratio": 1.4752475247524752, "no_speech_prob": 1.0782834579003975e-05}, {"id": 428, "seek": 195592, "start": 1973.04, "end": 1976.28, "text": " But the kind of intuitive understanding of them hopefully you'll get pretty quickly,", "tokens": [583, 264, 733, 295, 21769, 3701, 295, 552, 4696, 291, 603, 483, 1238, 2661, 11], "temperature": 0.0, "avg_logprob": -0.16248953342437744, "compression_ratio": 1.4752475247524752, "no_speech_prob": 1.0782834579003975e-05}, {"id": 429, "seek": 195592, "start": 1976.28, "end": 1982.98, "text": " which is generally speaking, you can kind of as long as the last axes match, it'll broadcast", "tokens": [597, 307, 5101, 4124, 11, 291, 393, 733, 295, 382, 938, 382, 264, 1036, 35387, 2995, 11, 309, 603, 9975], "temperature": 0.0, "avg_logprob": -0.16248953342437744, "compression_ratio": 1.4752475247524752, "no_speech_prob": 1.0782834579003975e-05}, {"id": 430, "seek": 195592, "start": 1982.98, "end": 1984.2, "text": " over those axes.", "tokens": [670, 729, 35387, 13], "temperature": 0.0, "avg_logprob": -0.16248953342437744, "compression_ratio": 1.4752475247524752, "no_speech_prob": 1.0782834579003975e-05}, {"id": 431, "seek": 198420, "start": 1984.2, "end": 1990.0800000000002, "text": " You can broadcast a rank three thing with a rank one thing or, you know, the most simple", "tokens": [509, 393, 9975, 257, 6181, 1045, 551, 365, 257, 6181, 472, 551, 420, 11, 291, 458, 11, 264, 881, 2199], "temperature": 0.0, "avg_logprob": -0.23035263193064723, "compression_ratio": 1.4861111111111112, "no_speech_prob": 2.2825417545391247e-05}, {"id": 432, "seek": 198420, "start": 1990.0800000000002, "end": 2000.48, "text": " version would be tensor one, two, three times two.", "tokens": [3037, 576, 312, 40863, 472, 11, 732, 11, 1045, 1413, 732, 13], "temperature": 0.0, "avg_logprob": -0.23035263193064723, "compression_ratio": 1.4861111111111112, "no_speech_prob": 2.2825417545391247e-05}, {"id": 433, "seek": 198420, "start": 2000.48, "end": 2012.6000000000001, "text": " So broadcast a scalar over a vector.", "tokens": [407, 9975, 257, 39684, 670, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.23035263193064723, "compression_ratio": 1.4861111111111112, "no_speech_prob": 2.2825417545391247e-05}, {"id": 434, "seek": 198420, "start": 2012.6000000000001, "end": 2013.68, "text": " That's exactly what you would expect.", "tokens": [663, 311, 2293, 437, 291, 576, 2066, 13], "temperature": 0.0, "avg_logprob": -0.23035263193064723, "compression_ratio": 1.4861111111111112, "no_speech_prob": 2.2825417545391247e-05}, {"id": 435, "seek": 201368, "start": 2013.68, "end": 2018.24, "text": " So it's copying effectively that too into each of these spots, multiplying them together.", "tokens": [407, 309, 311, 27976, 8659, 300, 886, 666, 1184, 295, 613, 10681, 11, 30955, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.140087658597022, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.3921973479446024e-05}, {"id": 436, "seek": 201368, "start": 2018.24, "end": 2021.04, "text": " But it doesn't use up any memory to do that.", "tokens": [583, 309, 1177, 380, 764, 493, 604, 4675, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.140087658597022, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.3921973479446024e-05}, {"id": 437, "seek": 201368, "start": 2021.04, "end": 2024.1200000000001, "text": " It's kind of a virtual copying if you like.", "tokens": [467, 311, 733, 295, 257, 6374, 27976, 498, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.140087658597022, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.3921973479446024e-05}, {"id": 438, "seek": 201368, "start": 2024.1200000000001, "end": 2028.94, "text": " So this line of code, independence by coefficients, is very, very important.", "tokens": [407, 341, 1622, 295, 3089, 11, 14640, 538, 31994, 11, 307, 588, 11, 588, 1021, 13], "temperature": 0.0, "avg_logprob": -0.140087658597022, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.3921973479446024e-05}, {"id": 439, "seek": 201368, "start": 2028.94, "end": 2034.0800000000002, "text": " And it's the key step that we wanted to take, which is now we know exactly what happens", "tokens": [400, 309, 311, 264, 2141, 1823, 300, 321, 1415, 281, 747, 11, 597, 307, 586, 321, 458, 2293, 437, 2314], "temperature": 0.0, "avg_logprob": -0.140087658597022, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.3921973479446024e-05}, {"id": 440, "seek": 201368, "start": 2034.0800000000002, "end": 2037.28, "text": " when we multiply the coefficients in.", "tokens": [562, 321, 12972, 264, 31994, 294, 13], "temperature": 0.0, "avg_logprob": -0.140087658597022, "compression_ratio": 1.6008403361344539, "no_speech_prob": 2.3921973479446024e-05}, {"id": 441, "seek": 203728, "start": 2037.28, "end": 2050.32, "text": " And if you remember back to Excel, we did that product and then in Excel there's a sum", "tokens": [400, 498, 291, 1604, 646, 281, 19060, 11, 321, 630, 300, 1674, 293, 550, 294, 19060, 456, 311, 257, 2408], "temperature": 0.0, "avg_logprob": -0.2004603472622958, "compression_ratio": 1.5568862275449102, "no_speech_prob": 5.594174581347033e-06}, {"id": 442, "seek": 203728, "start": 2050.32, "end": 2054.24, "text": " product we then added it all together because that's what a linear model is.", "tokens": [1674, 321, 550, 3869, 309, 439, 1214, 570, 300, 311, 437, 257, 8213, 2316, 307, 13], "temperature": 0.0, "avg_logprob": -0.2004603472622958, "compression_ratio": 1.5568862275449102, "no_speech_prob": 5.594174581347033e-06}, {"id": 443, "seek": 203728, "start": 2054.24, "end": 2058.8, "text": " It's the coefficients times the values added together.", "tokens": [467, 311, 264, 31994, 1413, 264, 4190, 3869, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2004603472622958, "compression_ratio": 1.5568862275449102, "no_speech_prob": 5.594174581347033e-06}, {"id": 444, "seek": 203728, "start": 2058.8, "end": 2062.7, "text": " So we're now going to add those together.", "tokens": [407, 321, 434, 586, 516, 281, 909, 729, 1214, 13], "temperature": 0.0, "avg_logprob": -0.2004603472622958, "compression_ratio": 1.5568862275449102, "no_speech_prob": 5.594174581347033e-06}, {"id": 445, "seek": 206270, "start": 2062.7, "end": 2070.46, "text": " But before we do that, if we did add up this row, you can see that the very first value", "tokens": [583, 949, 321, 360, 300, 11, 498, 321, 630, 909, 493, 341, 5386, 11, 291, 393, 536, 300, 264, 588, 700, 2158], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 446, "seek": 206270, "start": 2070.46, "end": 2074.0, "text": " has a very large magnitude and all the other ones are small.", "tokens": [575, 257, 588, 2416, 15668, 293, 439, 264, 661, 2306, 366, 1359, 13], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 447, "seek": 206270, "start": 2074.0, "end": 2075.0, "text": " Same with row two.", "tokens": [10635, 365, 5386, 732, 13], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 448, "seek": 206270, "start": 2075.0, "end": 2076.0, "text": " Same with row three.", "tokens": [10635, 365, 5386, 1045, 13], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 449, "seek": 206270, "start": 2076.0, "end": 2077.8399999999997, "text": " Same with row four.", "tokens": [10635, 365, 5386, 1451, 13], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 450, "seek": 206270, "start": 2077.8399999999997, "end": 2078.8399999999997, "text": " What's going on here?", "tokens": [708, 311, 516, 322, 510, 30], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 451, "seek": 206270, "start": 2078.8399999999997, "end": 2084.8799999999997, "text": " Well, what's going on is that the very first column was age.", "tokens": [1042, 11, 437, 311, 516, 322, 307, 300, 264, 588, 700, 7738, 390, 3205, 13], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 452, "seek": 206270, "start": 2084.8799999999997, "end": 2089.98, "text": " And age is much bigger than any of the other columns.", "tokens": [400, 3205, 307, 709, 3801, 813, 604, 295, 264, 661, 13766, 13], "temperature": 0.0, "avg_logprob": -0.11997308825502301, "compression_ratio": 1.7424242424242424, "no_speech_prob": 1.4063498383620754e-05}, {"id": 453, "seek": 208998, "start": 2089.98, "end": 2092.98, "text": " It's not the end of the world, but it's not ideal, right?", "tokens": [467, 311, 406, 264, 917, 295, 264, 1002, 11, 457, 309, 311, 406, 7157, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12834678318189538, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.1125013770652004e-05}, {"id": 454, "seek": 208998, "start": 2092.98, "end": 2099.22, "text": " Because it means that a coefficient of, say, 0.5 times age means something very different", "tokens": [1436, 309, 1355, 300, 257, 17619, 295, 11, 584, 11, 1958, 13, 20, 1413, 3205, 1355, 746, 588, 819], "temperature": 0.0, "avg_logprob": -0.12834678318189538, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.1125013770652004e-05}, {"id": 455, "seek": 208998, "start": 2099.22, "end": 2104.4, "text": " to a coefficient of, say, 0.5 times log fair.", "tokens": [281, 257, 17619, 295, 11, 584, 11, 1958, 13, 20, 1413, 3565, 3143, 13], "temperature": 0.0, "avg_logprob": -0.12834678318189538, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.1125013770652004e-05}, {"id": 456, "seek": 208998, "start": 2104.4, "end": 2108.8, "text": " And that means that that random coefficient we start with, it's going to mean very different", "tokens": [400, 300, 1355, 300, 300, 4974, 17619, 321, 722, 365, 11, 309, 311, 516, 281, 914, 588, 819], "temperature": 0.0, "avg_logprob": -0.12834678318189538, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.1125013770652004e-05}, {"id": 457, "seek": 208998, "start": 2108.8, "end": 2109.8, "text": " things for different columns.", "tokens": [721, 337, 819, 13766, 13], "temperature": 0.0, "avg_logprob": -0.12834678318189538, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.1125013770652004e-05}, {"id": 458, "seek": 208998, "start": 2109.8, "end": 2112.72, "text": " And that's going to make it really hard to optimize.", "tokens": [400, 300, 311, 516, 281, 652, 309, 534, 1152, 281, 19719, 13], "temperature": 0.0, "avg_logprob": -0.12834678318189538, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.1125013770652004e-05}, {"id": 459, "seek": 208998, "start": 2112.72, "end": 2116.64, "text": " So we would like all the columns to have about the same range.", "tokens": [407, 321, 576, 411, 439, 264, 13766, 281, 362, 466, 264, 912, 3613, 13], "temperature": 0.0, "avg_logprob": -0.12834678318189538, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.1125013770652004e-05}, {"id": 460, "seek": 211664, "start": 2116.64, "end": 2126.12, "text": " So what we could do, as we did in Excel, is to divide them by the maximum.", "tokens": [407, 437, 321, 727, 360, 11, 382, 321, 630, 294, 19060, 11, 307, 281, 9845, 552, 538, 264, 6674, 13], "temperature": 0.0, "avg_logprob": -0.14963972688925387, "compression_ratio": 1.6683937823834196, "no_speech_prob": 6.747979568899609e-06}, {"id": 461, "seek": 211664, "start": 2126.12, "end": 2129.12, "text": " So the maximum, so we did it for age and we also did it for fair.", "tokens": [407, 264, 6674, 11, 370, 321, 630, 309, 337, 3205, 293, 321, 611, 630, 309, 337, 3143, 13], "temperature": 0.0, "avg_logprob": -0.14963972688925387, "compression_ratio": 1.6683937823834196, "no_speech_prob": 6.747979568899609e-06}, {"id": 462, "seek": 211664, "start": 2129.12, "end": 2134.7, "text": " In this case, I didn't use log.", "tokens": [682, 341, 1389, 11, 286, 994, 380, 764, 3565, 13], "temperature": 0.0, "avg_logprob": -0.14963972688925387, "compression_ratio": 1.6683937823834196, "no_speech_prob": 6.747979568899609e-06}, {"id": 463, "seek": 211664, "start": 2134.7, "end": 2140.06, "text": " So we can get the max of each row by calling.max.", "tokens": [407, 321, 393, 483, 264, 11469, 295, 1184, 5386, 538, 5141, 2411, 41167, 13], "temperature": 0.0, "avg_logprob": -0.14963972688925387, "compression_ratio": 1.6683937823834196, "no_speech_prob": 6.747979568899609e-06}, {"id": 464, "seek": 211664, "start": 2140.06, "end": 2141.52, "text": " And you can pass in a dimension.", "tokens": [400, 291, 393, 1320, 294, 257, 10139, 13], "temperature": 0.0, "avg_logprob": -0.14963972688925387, "compression_ratio": 1.6683937823834196, "no_speech_prob": 6.747979568899609e-06}, {"id": 465, "seek": 211664, "start": 2141.52, "end": 2145.18, "text": " Do you want the maximum of the rows or the maximum of the columns?", "tokens": [1144, 291, 528, 264, 6674, 295, 264, 13241, 420, 264, 6674, 295, 264, 13766, 30], "temperature": 0.0, "avg_logprob": -0.14963972688925387, "compression_ratio": 1.6683937823834196, "no_speech_prob": 6.747979568899609e-06}, {"id": 466, "seek": 214518, "start": 2145.18, "end": 2147.66, "text": " We want the maximum over the rows.", "tokens": [492, 528, 264, 6674, 670, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 467, "seek": 214518, "start": 2147.66, "end": 2150.54, "text": " So we pass in dimension 0.", "tokens": [407, 321, 1320, 294, 10139, 1958, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 468, "seek": 214518, "start": 2150.54, "end": 2156.66, "text": " So those different parts of the shape are called either axes or dimensions.", "tokens": [407, 729, 819, 3166, 295, 264, 3909, 366, 1219, 2139, 35387, 420, 12819, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 469, "seek": 214518, "start": 2156.66, "end": 2158.48, "text": " PyTorch calls them dimensions.", "tokens": [9953, 51, 284, 339, 5498, 552, 12819, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 470, "seek": 214518, "start": 2158.48, "end": 2161.2599999999998, "text": " So that's going to give us the maximum of each row.", "tokens": [407, 300, 311, 516, 281, 976, 505, 264, 6674, 295, 1184, 5386, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 471, "seek": 214518, "start": 2161.2599999999998, "end": 2166.06, "text": " And if you look at the docs for PyTorch's max function, it'll tell you it returns two", "tokens": [400, 498, 291, 574, 412, 264, 45623, 337, 9953, 51, 284, 339, 311, 11469, 2445, 11, 309, 603, 980, 291, 309, 11247, 732], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 472, "seek": 214518, "start": 2166.06, "end": 2167.06, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 473, "seek": 214518, "start": 2167.06, "end": 2172.62, "text": " The actual value of each maximum and the index of which row it was.", "tokens": [440, 3539, 2158, 295, 1184, 6674, 293, 264, 8186, 295, 597, 5386, 309, 390, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 474, "seek": 214518, "start": 2172.62, "end": 2174.3799999999997, "text": " We want the values.", "tokens": [492, 528, 264, 4190, 13], "temperature": 0.0, "avg_logprob": -0.15410726796025814, "compression_ratio": 1.7106382978723405, "no_speech_prob": 1.8342219846090302e-05}, {"id": 475, "seek": 217438, "start": 2174.38, "end": 2179.7400000000002, "text": " So now, thanks to broadcasting, we can just say take the independent variables and divide", "tokens": [407, 586, 11, 3231, 281, 30024, 11, 321, 393, 445, 584, 747, 264, 6695, 9102, 293, 9845], "temperature": 0.0, "avg_logprob": -0.1620633051945613, "compression_ratio": 1.5212765957446808, "no_speech_prob": 1.0615977771522012e-05}, {"id": 476, "seek": 217438, "start": 2179.7400000000002, "end": 2181.9, "text": " them by the vector of values.", "tokens": [552, 538, 264, 8062, 295, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1620633051945613, "compression_ratio": 1.5212765957446808, "no_speech_prob": 1.0615977771522012e-05}, {"id": 477, "seek": 217438, "start": 2181.9, "end": 2185.38, "text": " Again we've got a matrix and a vector.", "tokens": [3764, 321, 600, 658, 257, 8141, 293, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.1620633051945613, "compression_ratio": 1.5212765957446808, "no_speech_prob": 1.0615977771522012e-05}, {"id": 478, "seek": 217438, "start": 2185.38, "end": 2192.78, "text": " And so this is going to do an element-wise division of each row of this divided by this", "tokens": [400, 370, 341, 307, 516, 281, 360, 364, 4478, 12, 3711, 10044, 295, 1184, 5386, 295, 341, 6666, 538, 341], "temperature": 0.0, "avg_logprob": -0.1620633051945613, "compression_ratio": 1.5212765957446808, "no_speech_prob": 1.0615977771522012e-05}, {"id": 479, "seek": 217438, "start": 2192.78, "end": 2193.78, "text": " vector.", "tokens": [8062, 13], "temperature": 0.0, "avg_logprob": -0.1620633051945613, "compression_ratio": 1.5212765957446808, "no_speech_prob": 1.0615977771522012e-05}, {"id": 480, "seek": 217438, "start": 2193.78, "end": 2198.26, "text": " Again, in a very optimized way.", "tokens": [3764, 11, 294, 257, 588, 26941, 636, 13], "temperature": 0.0, "avg_logprob": -0.1620633051945613, "compression_ratio": 1.5212765957446808, "no_speech_prob": 1.0615977771522012e-05}, {"id": 481, "seek": 219826, "start": 2198.26, "end": 2205.1000000000004, "text": " So if we now look at our normalized independent variables by the coefficients, you can see", "tokens": [407, 498, 321, 586, 574, 412, 527, 48704, 6695, 9102, 538, 264, 31994, 11, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.12315442302439472, "compression_ratio": 1.6047430830039526, "no_speech_prob": 8.800896466709673e-06}, {"id": 482, "seek": 219826, "start": 2205.1000000000004, "end": 2207.3, "text": " they're all pretty similar values.", "tokens": [436, 434, 439, 1238, 2531, 4190, 13], "temperature": 0.0, "avg_logprob": -0.12315442302439472, "compression_ratio": 1.6047430830039526, "no_speech_prob": 8.800896466709673e-06}, {"id": 483, "seek": 219826, "start": 2207.3, "end": 2209.5, "text": " So that's good.", "tokens": [407, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.12315442302439472, "compression_ratio": 1.6047430830039526, "no_speech_prob": 8.800896466709673e-06}, {"id": 484, "seek": 219826, "start": 2209.5, "end": 2212.6200000000003, "text": " There's lots of different ways of normalizing, but the main ones you'll come across is either", "tokens": [821, 311, 3195, 295, 819, 2098, 295, 2710, 3319, 11, 457, 264, 2135, 2306, 291, 603, 808, 2108, 307, 2139], "temperature": 0.0, "avg_logprob": -0.12315442302439472, "compression_ratio": 1.6047430830039526, "no_speech_prob": 8.800896466709673e-06}, {"id": 485, "seek": 219826, "start": 2212.6200000000003, "end": 2219.0200000000004, "text": " dividing by the maximum or subtracting the mean and dividing by the standard deviation.", "tokens": [26764, 538, 264, 6674, 420, 16390, 278, 264, 914, 293, 26764, 538, 264, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.12315442302439472, "compression_ratio": 1.6047430830039526, "no_speech_prob": 8.800896466709673e-06}, {"id": 486, "seek": 219826, "start": 2219.0200000000004, "end": 2222.46, "text": " It normally doesn't matter too much.", "tokens": [467, 5646, 1177, 380, 1871, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.12315442302439472, "compression_ratio": 1.6047430830039526, "no_speech_prob": 8.800896466709673e-06}, {"id": 487, "seek": 219826, "start": 2222.46, "end": 2224.7400000000002, "text": " Because I'm lazy, I just pick the easier one.", "tokens": [1436, 286, 478, 14847, 11, 286, 445, 1888, 264, 3571, 472, 13], "temperature": 0.0, "avg_logprob": -0.12315442302439472, "compression_ratio": 1.6047430830039526, "no_speech_prob": 8.800896466709673e-06}, {"id": 488, "seek": 222474, "start": 2224.74, "end": 2229.02, "text": " And being lazy and picking the easier one is a very good plan in my opinion.", "tokens": [400, 885, 14847, 293, 8867, 264, 3571, 472, 307, 257, 588, 665, 1393, 294, 452, 4800, 13], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 489, "seek": 222474, "start": 2229.02, "end": 2233.5, "text": " So now that we can see that multiplying them together is working pretty well, we can now", "tokens": [407, 586, 300, 321, 393, 536, 300, 30955, 552, 1214, 307, 1364, 1238, 731, 11, 321, 393, 586], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 490, "seek": 222474, "start": 2233.5, "end": 2234.8999999999996, "text": " add them up.", "tokens": [909, 552, 493, 13], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 491, "seek": 222474, "start": 2234.8999999999996, "end": 2241.14, "text": " And now we want to add up over the columns.", "tokens": [400, 586, 321, 528, 281, 909, 493, 670, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 492, "seek": 222474, "start": 2241.14, "end": 2242.8199999999997, "text": " And that would give us predictions.", "tokens": [400, 300, 576, 976, 505, 21264, 13], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 493, "seek": 222474, "start": 2242.8199999999997, "end": 2246.9399999999996, "text": " Now obviously just like in Excel, when we started out, they're not useful predictions", "tokens": [823, 2745, 445, 411, 294, 19060, 11, 562, 321, 1409, 484, 11, 436, 434, 406, 4420, 21264], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 494, "seek": 222474, "start": 2246.9399999999996, "end": 2250.7, "text": " because they're random coefficients, but they are predictions nonetheless.", "tokens": [570, 436, 434, 4974, 31994, 11, 457, 436, 366, 21264, 26756, 13], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 495, "seek": 222474, "start": 2250.7, "end": 2254.4199999999996, "text": " And here's the first 10 of them.", "tokens": [400, 510, 311, 264, 700, 1266, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.10389309435819102, "compression_ratio": 1.6678966789667897, "no_speech_prob": 1.0782930985442363e-05}, {"id": 496, "seek": 225442, "start": 2254.42, "end": 2262.4, "text": " So then remember, we want to use gradient descent to try to make these better.", "tokens": [407, 550, 1604, 11, 321, 528, 281, 764, 16235, 23475, 281, 853, 281, 652, 613, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13120126724243164, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.3006421795580536e-05}, {"id": 497, "seek": 225442, "start": 2262.4, "end": 2265.7400000000002, "text": " So to do gradient descent, we need a loss.", "tokens": [407, 281, 360, 16235, 23475, 11, 321, 643, 257, 4470, 13], "temperature": 0.0, "avg_logprob": -0.13120126724243164, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.3006421795580536e-05}, {"id": 498, "seek": 225442, "start": 2265.7400000000002, "end": 2271.6800000000003, "text": " The loss is the measure of how good or bad are these coefficients.", "tokens": [440, 4470, 307, 264, 3481, 295, 577, 665, 420, 1578, 366, 613, 31994, 13], "temperature": 0.0, "avg_logprob": -0.13120126724243164, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.3006421795580536e-05}, {"id": 499, "seek": 225442, "start": 2271.6800000000003, "end": 2277.42, "text": " My favorite loss function as a kind of like, don't think about it, just chuck something", "tokens": [1222, 2954, 4470, 2445, 382, 257, 733, 295, 411, 11, 500, 380, 519, 466, 309, 11, 445, 20870, 746], "temperature": 0.0, "avg_logprob": -0.13120126724243164, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.3006421795580536e-05}, {"id": 500, "seek": 225442, "start": 2277.42, "end": 2280.12, "text": " out there, is the mean absolute value.", "tokens": [484, 456, 11, 307, 264, 914, 8236, 2158, 13], "temperature": 0.0, "avg_logprob": -0.13120126724243164, "compression_ratio": 1.598984771573604, "no_speech_prob": 1.3006421795580536e-05}, {"id": 501, "seek": 228012, "start": 2280.12, "end": 2290.54, "text": " And here it is, torch.absoluteValue of the error, the difference, take the mean.", "tokens": [400, 510, 309, 307, 11, 27822, 13, 17243, 401, 1169, 53, 304, 622, 295, 264, 6713, 11, 264, 2649, 11, 747, 264, 914, 13], "temperature": 0.0, "avg_logprob": -0.13880282098596747, "compression_ratio": 1.5388349514563107, "no_speech_prob": 4.092869858141057e-06}, {"id": 502, "seek": 228012, "start": 2290.54, "end": 2297.42, "text": " And often stuff like this, you'll see people will use prewritten mean absolute error functions,", "tokens": [400, 2049, 1507, 411, 341, 11, 291, 603, 536, 561, 486, 764, 659, 26859, 914, 8236, 6713, 6828, 11], "temperature": 0.0, "avg_logprob": -0.13880282098596747, "compression_ratio": 1.5388349514563107, "no_speech_prob": 4.092869858141057e-06}, {"id": 503, "seek": 228012, "start": 2297.42, "end": 2301.9, "text": " which is also fine, but I quite like to write it out because I can see exactly what's going", "tokens": [597, 307, 611, 2489, 11, 457, 286, 1596, 411, 281, 2464, 309, 484, 570, 286, 393, 536, 2293, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.13880282098596747, "compression_ratio": 1.5388349514563107, "no_speech_prob": 4.092869858141057e-06}, {"id": 504, "seek": 228012, "start": 2301.9, "end": 2302.9, "text": " on.", "tokens": [322, 13], "temperature": 0.0, "avg_logprob": -0.13880282098596747, "compression_ratio": 1.5388349514563107, "no_speech_prob": 4.092869858141057e-06}, {"id": 505, "seek": 228012, "start": 2302.9, "end": 2303.9, "text": " No confusion.", "tokens": [883, 15075, 13], "temperature": 0.0, "avg_logprob": -0.13880282098596747, "compression_ratio": 1.5388349514563107, "no_speech_prob": 4.092869858141057e-06}, {"id": 506, "seek": 228012, "start": 2303.9, "end": 2307.62, "text": " No chance of misunderstanding.", "tokens": [883, 2931, 295, 29227, 13], "temperature": 0.0, "avg_logprob": -0.13880282098596747, "compression_ratio": 1.5388349514563107, "no_speech_prob": 4.092869858141057e-06}, {"id": 507, "seek": 230762, "start": 2307.62, "end": 2315.74, "text": " So those are all the steps I'm going to need to create coefficients, run a linear model,", "tokens": [407, 729, 366, 439, 264, 4439, 286, 478, 516, 281, 643, 281, 1884, 31994, 11, 1190, 257, 8213, 2316, 11], "temperature": 0.0, "avg_logprob": -0.13140926026461416, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.5215292680513812e-06}, {"id": 508, "seek": 230762, "start": 2315.74, "end": 2317.58, "text": " and get its loss.", "tokens": [293, 483, 1080, 4470, 13], "temperature": 0.0, "avg_logprob": -0.13140926026461416, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.5215292680513812e-06}, {"id": 509, "seek": 230762, "start": 2317.58, "end": 2322.1, "text": " So what I like to do in my notebooks, like not just for teaching, but all the time, is", "tokens": [407, 437, 286, 411, 281, 360, 294, 452, 43782, 11, 411, 406, 445, 337, 4571, 11, 457, 439, 264, 565, 11, 307], "temperature": 0.0, "avg_logprob": -0.13140926026461416, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.5215292680513812e-06}, {"id": 510, "seek": 230762, "start": 2322.1, "end": 2327.62, "text": " to like do everything step by step manually, and then just copy and paste the steps into", "tokens": [281, 411, 360, 1203, 1823, 538, 1823, 16945, 11, 293, 550, 445, 5055, 293, 9163, 264, 4439, 666], "temperature": 0.0, "avg_logprob": -0.13140926026461416, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.5215292680513812e-06}, {"id": 511, "seek": 230762, "start": 2327.62, "end": 2328.62, "text": " a function.", "tokens": [257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13140926026461416, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.5215292680513812e-06}, {"id": 512, "seek": 230762, "start": 2328.62, "end": 2332.62, "text": " So here's my calc preds function is exactly what I just did.", "tokens": [407, 510, 311, 452, 2104, 66, 3852, 82, 2445, 307, 2293, 437, 286, 445, 630, 13], "temperature": 0.0, "avg_logprob": -0.13140926026461416, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.5215292680513812e-06}, {"id": 513, "seek": 230762, "start": 2332.62, "end": 2337.5, "text": " Here's my calc loss function, exactly what I just did.", "tokens": [1692, 311, 452, 2104, 66, 4470, 2445, 11, 2293, 437, 286, 445, 630, 13], "temperature": 0.0, "avg_logprob": -0.13140926026461416, "compression_ratio": 1.7903930131004366, "no_speech_prob": 2.5215292680513812e-06}, {"id": 514, "seek": 233750, "start": 2337.5, "end": 2344.34, "text": " And that way, a lot of people go back and delete all their explorations, or they do", "tokens": [400, 300, 636, 11, 257, 688, 295, 561, 352, 646, 293, 12097, 439, 641, 24765, 763, 11, 420, 436, 360], "temperature": 0.0, "avg_logprob": -0.13207836151123048, "compression_ratio": 1.7314487632508835, "no_speech_prob": 1.0129343536391389e-05}, {"id": 515, "seek": 233750, "start": 2344.34, "end": 2350.02, "text": " them in a different notebook, or they're working in an IDE, they'll go and do it in some line", "tokens": [552, 294, 257, 819, 21060, 11, 420, 436, 434, 1364, 294, 364, 40930, 11, 436, 603, 352, 293, 360, 309, 294, 512, 1622], "temperature": 0.0, "avg_logprob": -0.13207836151123048, "compression_ratio": 1.7314487632508835, "no_speech_prob": 1.0129343536391389e-05}, {"id": 516, "seek": 233750, "start": 2350.02, "end": 2351.3, "text": " oriented rep or whatever.", "tokens": [21841, 1085, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.13207836151123048, "compression_ratio": 1.7314487632508835, "no_speech_prob": 1.0129343536391389e-05}, {"id": 517, "seek": 233750, "start": 2351.3, "end": 2356.66, "text": " But if you think about the benefits of keeping it here, when you come back to it in six months,", "tokens": [583, 498, 291, 519, 466, 264, 5311, 295, 5145, 309, 510, 11, 562, 291, 808, 646, 281, 309, 294, 2309, 2493, 11], "temperature": 0.0, "avg_logprob": -0.13207836151123048, "compression_ratio": 1.7314487632508835, "no_speech_prob": 1.0129343536391389e-05}, {"id": 518, "seek": 233750, "start": 2356.66, "end": 2359.62, "text": " you'll see exactly why you did what you did and how we got there, or if you're showing", "tokens": [291, 603, 536, 2293, 983, 291, 630, 437, 291, 630, 293, 577, 321, 658, 456, 11, 420, 498, 291, 434, 4099], "temperature": 0.0, "avg_logprob": -0.13207836151123048, "compression_ratio": 1.7314487632508835, "no_speech_prob": 1.0129343536391389e-05}, {"id": 519, "seek": 233750, "start": 2359.62, "end": 2364.74, "text": " it to your boss or your colleague, you can see exactly what's happening, what does each", "tokens": [309, 281, 428, 5741, 420, 428, 13532, 11, 291, 393, 536, 2293, 437, 311, 2737, 11, 437, 775, 1184], "temperature": 0.0, "avg_logprob": -0.13207836151123048, "compression_ratio": 1.7314487632508835, "no_speech_prob": 1.0129343536391389e-05}, {"id": 520, "seek": 233750, "start": 2364.74, "end": 2366.1, "text": " step look like.", "tokens": [1823, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.13207836151123048, "compression_ratio": 1.7314487632508835, "no_speech_prob": 1.0129343536391389e-05}, {"id": 521, "seek": 236610, "start": 2366.1, "end": 2369.02, "text": " I think this is really very helpful indeed.", "tokens": [286, 519, 341, 307, 534, 588, 4961, 6451, 13], "temperature": 0.0, "avg_logprob": -0.13370251878399717, "compression_ratio": 1.6317829457364341, "no_speech_prob": 4.610766700352542e-05}, {"id": 522, "seek": 236610, "start": 2369.02, "end": 2376.98, "text": " I know not many people code that way, but I feel strongly that it's a huge productivity", "tokens": [286, 458, 406, 867, 561, 3089, 300, 636, 11, 457, 286, 841, 10613, 300, 309, 311, 257, 2603, 15604], "temperature": 0.0, "avg_logprob": -0.13370251878399717, "compression_ratio": 1.6317829457364341, "no_speech_prob": 4.610766700352542e-05}, {"id": 523, "seek": 236610, "start": 2376.98, "end": 2380.8199999999997, "text": " win to individuals and teams.", "tokens": [1942, 281, 5346, 293, 5491, 13], "temperature": 0.0, "avg_logprob": -0.13370251878399717, "compression_ratio": 1.6317829457364341, "no_speech_prob": 4.610766700352542e-05}, {"id": 524, "seek": 236610, "start": 2380.8199999999997, "end": 2387.62, "text": " So remember from our gradient descent from scratch that the one bit we don't want to", "tokens": [407, 1604, 490, 527, 16235, 23475, 490, 8459, 300, 264, 472, 857, 321, 500, 380, 528, 281], "temperature": 0.0, "avg_logprob": -0.13370251878399717, "compression_ratio": 1.6317829457364341, "no_speech_prob": 4.610766700352542e-05}, {"id": 525, "seek": 236610, "start": 2387.62, "end": 2391.98, "text": " do from scratch is calculating derivatives, because it's just menial and boring.", "tokens": [360, 490, 8459, 307, 28258, 33733, 11, 570, 309, 311, 445, 1706, 831, 293, 9989, 13], "temperature": 0.0, "avg_logprob": -0.13370251878399717, "compression_ratio": 1.6317829457364341, "no_speech_prob": 4.610766700352542e-05}, {"id": 526, "seek": 236610, "start": 2391.98, "end": 2395.2599999999998, "text": " So to get PyTorch to do it for us, you have to say, well, what things do you want derivatives", "tokens": [407, 281, 483, 9953, 51, 284, 339, 281, 360, 309, 337, 505, 11, 291, 362, 281, 584, 11, 731, 11, 437, 721, 360, 291, 528, 33733], "temperature": 0.0, "avg_logprob": -0.13370251878399717, "compression_ratio": 1.6317829457364341, "no_speech_prob": 4.610766700352542e-05}, {"id": 527, "seek": 239526, "start": 2395.26, "end": 2396.26, "text": " for?", "tokens": [337, 30], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 528, "seek": 239526, "start": 2396.26, "end": 2398.5, "text": " And of course, we want it for the coefficients.", "tokens": [400, 295, 1164, 11, 321, 528, 309, 337, 264, 31994, 13], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 529, "seek": 239526, "start": 2398.5, "end": 2401.0600000000004, "text": " So then we have to say requires grad.", "tokens": [407, 550, 321, 362, 281, 584, 7029, 2771, 13], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 530, "seek": 239526, "start": 2401.0600000000004, "end": 2405.5400000000004, "text": " And remember, very important in PyTorch, if there's an underscore at the end, that's an", "tokens": [400, 1604, 11, 588, 1021, 294, 9953, 51, 284, 339, 11, 498, 456, 311, 364, 37556, 412, 264, 917, 11, 300, 311, 364], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 531, "seek": 239526, "start": 2405.5400000000004, "end": 2406.82, "text": " in place operation.", "tokens": [294, 1081, 6916, 13], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 532, "seek": 239526, "start": 2406.82, "end": 2410.5, "text": " So this is actually going to change coefs.", "tokens": [407, 341, 307, 767, 516, 281, 1319, 598, 5666, 82, 13], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 533, "seek": 239526, "start": 2410.5, "end": 2414.34, "text": " It also returns them, right, but it also changes them in place.", "tokens": [467, 611, 11247, 552, 11, 558, 11, 457, 309, 611, 2962, 552, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 534, "seek": 239526, "start": 2414.34, "end": 2418.84, "text": " So now we've got exactly the same numbers as before, but with requires grad turned on.", "tokens": [407, 586, 321, 600, 658, 2293, 264, 912, 3547, 382, 949, 11, 457, 365, 7029, 2771, 3574, 322, 13], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 535, "seek": 239526, "start": 2418.84, "end": 2423.7400000000002, "text": " So now when we calculate our loss, that doesn't do any other calculations, but what it does", "tokens": [407, 586, 562, 321, 8873, 527, 4470, 11, 300, 1177, 380, 360, 604, 661, 20448, 11, 457, 437, 309, 775], "temperature": 0.0, "avg_logprob": -0.10508656857618645, "compression_ratio": 1.7285714285714286, "no_speech_prob": 2.840864544850774e-05}, {"id": 536, "seek": 242374, "start": 2423.74, "end": 2426.54, "text": " store is a gradient function.", "tokens": [3531, 307, 257, 16235, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1146161329178583, "compression_ratio": 1.7819148936170213, "no_speech_prob": 7.889052540122066e-06}, {"id": 537, "seek": 242374, "start": 2426.54, "end": 2431.8199999999997, "text": " It's the function that Python has remembered that it would have to do to undo those steps", "tokens": [467, 311, 264, 2445, 300, 15329, 575, 13745, 300, 309, 576, 362, 281, 360, 281, 23779, 729, 4439], "temperature": 0.0, "avg_logprob": -0.1146161329178583, "compression_ratio": 1.7819148936170213, "no_speech_prob": 7.889052540122066e-06}, {"id": 538, "seek": 242374, "start": 2431.8199999999997, "end": 2433.64, "text": " to get back to the gradient.", "tokens": [281, 483, 646, 281, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.1146161329178583, "compression_ratio": 1.7819148936170213, "no_speech_prob": 7.889052540122066e-06}, {"id": 539, "seek": 242374, "start": 2433.64, "end": 2442.2599999999998, "text": " And to say, oh, please actually call that backward gradient function, you call backward.", "tokens": [400, 281, 584, 11, 1954, 11, 1767, 767, 818, 300, 23897, 16235, 2445, 11, 291, 818, 23897, 13], "temperature": 0.0, "avg_logprob": -0.1146161329178583, "compression_ratio": 1.7819148936170213, "no_speech_prob": 7.889052540122066e-06}, {"id": 540, "seek": 242374, "start": 2442.2599999999998, "end": 2447.9399999999996, "text": " And at that point, it sticks into a.grad attribute, the coefficients, the coefficients", "tokens": [400, 412, 300, 935, 11, 309, 12518, 666, 257, 2411, 7165, 19667, 11, 264, 31994, 11, 264, 31994], "temperature": 0.0, "avg_logprob": -0.1146161329178583, "compression_ratio": 1.7819148936170213, "no_speech_prob": 7.889052540122066e-06}, {"id": 541, "seek": 242374, "start": 2447.9399999999996, "end": 2450.02, "text": " gradients.", "tokens": [2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.1146161329178583, "compression_ratio": 1.7819148936170213, "no_speech_prob": 7.889052540122066e-06}, {"id": 542, "seek": 245002, "start": 2450.02, "end": 2457.88, "text": " So this tells us that if we increased the age coefficient, the loss would go down.", "tokens": [407, 341, 5112, 505, 300, 498, 321, 6505, 264, 3205, 17619, 11, 264, 4470, 576, 352, 760, 13], "temperature": 0.0, "avg_logprob": -0.13479460021595896, "compression_ratio": 1.695, "no_speech_prob": 3.611908596212743e-06}, {"id": 543, "seek": 245002, "start": 2457.88, "end": 2461.34, "text": " So therefore we should do that.", "tokens": [407, 4412, 321, 820, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.13479460021595896, "compression_ratio": 1.695, "no_speech_prob": 3.611908596212743e-06}, {"id": 544, "seek": 245002, "start": 2461.34, "end": 2463.38, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.13479460021595896, "compression_ratio": 1.695, "no_speech_prob": 3.611908596212743e-06}, {"id": 545, "seek": 245002, "start": 2463.38, "end": 2469.64, "text": " So since negative means increasing this would decrease the loss, that means we need to,", "tokens": [407, 1670, 3671, 1355, 5662, 341, 576, 11514, 264, 4470, 11, 300, 1355, 321, 643, 281, 11], "temperature": 0.0, "avg_logprob": -0.13479460021595896, "compression_ratio": 1.695, "no_speech_prob": 3.611908596212743e-06}, {"id": 546, "seek": 245002, "start": 2469.64, "end": 2475.1, "text": " if you remember back to the gradient descent from scratch notebook, we need to subtract", "tokens": [498, 291, 1604, 646, 281, 264, 16235, 23475, 490, 8459, 21060, 11, 321, 643, 281, 16390], "temperature": 0.0, "avg_logprob": -0.13479460021595896, "compression_ratio": 1.695, "no_speech_prob": 3.611908596212743e-06}, {"id": 547, "seek": 245002, "start": 2475.1, "end": 2479.2599999999998, "text": " the coefficients times the learning rate.", "tokens": [264, 31994, 1413, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.13479460021595896, "compression_ratio": 1.695, "no_speech_prob": 3.611908596212743e-06}, {"id": 548, "seek": 247926, "start": 2479.26, "end": 2483.6800000000003, "text": " So we haven't got any particular ideas yet of how to set the learning rate.", "tokens": [407, 321, 2378, 380, 658, 604, 1729, 3487, 1939, 295, 577, 281, 992, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.18254940635279604, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.4063541129871737e-05}, {"id": 549, "seek": 247926, "start": 2483.6800000000003, "end": 2487.46, "text": " So for now I just pick, just try a few and still find out what works best.", "tokens": [407, 337, 586, 286, 445, 1888, 11, 445, 853, 257, 1326, 293, 920, 915, 484, 437, 1985, 1151, 13], "temperature": 0.0, "avg_logprob": -0.18254940635279604, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.4063541129871737e-05}, {"id": 550, "seek": 247926, "start": 2487.46, "end": 2492.0, "text": " In this case I found.1 worked pretty well.", "tokens": [682, 341, 1389, 286, 1352, 2411, 16, 2732, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.18254940635279604, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.4063541129871737e-05}, {"id": 551, "seek": 247926, "start": 2492.0, "end": 2500.5, "text": " So I now subtract, so again this is sub underscore, so subtract in place from the coefficients", "tokens": [407, 286, 586, 16390, 11, 370, 797, 341, 307, 1422, 37556, 11, 370, 16390, 294, 1081, 490, 264, 31994], "temperature": 0.0, "avg_logprob": -0.18254940635279604, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.4063541129871737e-05}, {"id": 552, "seek": 247926, "start": 2500.5, "end": 2503.5200000000004, "text": " the gradient times the learning rate.", "tokens": [264, 16235, 1413, 264, 2539, 3314, 13], "temperature": 0.0, "avg_logprob": -0.18254940635279604, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.4063541129871737e-05}, {"id": 553, "seek": 247926, "start": 2503.5200000000004, "end": 2505.94, "text": " And so the loss has gone down.", "tokens": [400, 370, 264, 4470, 575, 2780, 760, 13], "temperature": 0.0, "avg_logprob": -0.18254940635279604, "compression_ratio": 1.600896860986547, "no_speech_prob": 1.4063541129871737e-05}, {"id": 554, "seek": 250594, "start": 2505.94, "end": 2510.94, "text": " That's great, from.54 to.52.", "tokens": [663, 311, 869, 11, 490, 2411, 19563, 281, 2411, 17602, 13], "temperature": 0.0, "avg_logprob": -0.13026788960332455, "compression_ratio": 1.391025641025641, "no_speech_prob": 3.6687958981929114e-06}, {"id": 555, "seek": 250594, "start": 2510.94, "end": 2515.14, "text": " So there is one step.", "tokens": [407, 456, 307, 472, 1823, 13], "temperature": 0.0, "avg_logprob": -0.13026788960332455, "compression_ratio": 1.391025641025641, "no_speech_prob": 3.6687958981929114e-06}, {"id": 556, "seek": 250594, "start": 2515.14, "end": 2525.66, "text": " So we've now got everything we need to train a linear model.", "tokens": [407, 321, 600, 586, 658, 1203, 321, 643, 281, 3847, 257, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13026788960332455, "compression_ratio": 1.391025641025641, "no_speech_prob": 3.6687958981929114e-06}, {"id": 557, "seek": 250594, "start": 2525.66, "end": 2526.94, "text": " So let's do it.", "tokens": [407, 718, 311, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.13026788960332455, "compression_ratio": 1.391025641025641, "no_speech_prob": 3.6687958981929114e-06}, {"id": 558, "seek": 250594, "start": 2526.94, "end": 2532.82, "text": " Now as we discussed last week, to see whether your model is any good, it's important that", "tokens": [823, 382, 321, 7152, 1036, 1243, 11, 281, 536, 1968, 428, 2316, 307, 604, 665, 11, 309, 311, 1021, 300], "temperature": 0.0, "avg_logprob": -0.13026788960332455, "compression_ratio": 1.391025641025641, "no_speech_prob": 3.6687958981929114e-06}, {"id": 559, "seek": 253282, "start": 2532.82, "end": 2538.34, "text": " you split your data into training and validation.", "tokens": [291, 7472, 428, 1412, 666, 3097, 293, 24071, 13], "temperature": 0.0, "avg_logprob": -0.15543543690382833, "compression_ratio": 1.6570247933884297, "no_speech_prob": 2.977050462504849e-05}, {"id": 560, "seek": 253282, "start": 2538.34, "end": 2543.5800000000004, "text": " For the Titanic data set it's actually pretty much fine to use a random split, because back", "tokens": [1171, 264, 42183, 1412, 992, 309, 311, 767, 1238, 709, 2489, 281, 764, 257, 4974, 7472, 11, 570, 646], "temperature": 0.0, "avg_logprob": -0.15543543690382833, "compression_ratio": 1.6570247933884297, "no_speech_prob": 2.977050462504849e-05}, {"id": 561, "seek": 253282, "start": 2543.5800000000004, "end": 2547.98, "text": " when my friend Margit and I actually created this competition for Kaggle many years ago,", "tokens": [562, 452, 1277, 20000, 270, 293, 286, 767, 2942, 341, 6211, 337, 48751, 22631, 867, 924, 2057, 11], "temperature": 0.0, "avg_logprob": -0.15543543690382833, "compression_ratio": 1.6570247933884297, "no_speech_prob": 2.977050462504849e-05}, {"id": 562, "seek": 253282, "start": 2547.98, "end": 2551.48, "text": " that's basically what we did if I remember correctly.", "tokens": [300, 311, 1936, 437, 321, 630, 498, 286, 1604, 8944, 13], "temperature": 0.0, "avg_logprob": -0.15543543690382833, "compression_ratio": 1.6570247933884297, "no_speech_prob": 2.977050462504849e-05}, {"id": 563, "seek": 253282, "start": 2551.48, "end": 2557.44, "text": " So we can split them randomly into a training set and a validation set.", "tokens": [407, 321, 393, 7472, 552, 16979, 666, 257, 3097, 992, 293, 257, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.15543543690382833, "compression_ratio": 1.6570247933884297, "no_speech_prob": 2.977050462504849e-05}, {"id": 564, "seek": 253282, "start": 2557.44, "end": 2561.5800000000004, "text": " So we're just going to use Fast.ai for that.", "tokens": [407, 321, 434, 445, 516, 281, 764, 15968, 13, 1301, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.15543543690382833, "compression_ratio": 1.6570247933884297, "no_speech_prob": 2.977050462504849e-05}, {"id": 565, "seek": 256158, "start": 2561.58, "end": 2565.58, "text": " It's very easy to do it manually with NumPy or PyTorch.", "tokens": [467, 311, 588, 1858, 281, 360, 309, 16945, 365, 22592, 47, 88, 420, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.10884163514623102, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.535388532327488e-05}, {"id": 566, "seek": 256158, "start": 2565.58, "end": 2569.62, "text": " You can use scikit-learns, train test split.", "tokens": [509, 393, 764, 2180, 22681, 12, 306, 1083, 82, 11, 3847, 1500, 7472, 13], "temperature": 0.0, "avg_logprob": -0.10884163514623102, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.535388532327488e-05}, {"id": 567, "seek": 256158, "start": 2569.62, "end": 2574.54, "text": " I'm using Fast.ai here partly because it's easy just to remember one way to do things,", "tokens": [286, 478, 1228, 15968, 13, 1301, 510, 17031, 570, 309, 311, 1858, 445, 281, 1604, 472, 636, 281, 360, 721, 11], "temperature": 0.0, "avg_logprob": -0.10884163514623102, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.535388532327488e-05}, {"id": 568, "seek": 256158, "start": 2574.54, "end": 2578.1, "text": " and this works everywhere, and partly because in the next notebook we're going to be seeing", "tokens": [293, 341, 1985, 5315, 11, 293, 17031, 570, 294, 264, 958, 21060, 321, 434, 516, 281, 312, 2577], "temperature": 0.0, "avg_logprob": -0.10884163514623102, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.535388532327488e-05}, {"id": 569, "seek": 256158, "start": 2578.1, "end": 2584.62, "text": " how to do more stuff in Fast.ai, so I want to make sure we have exactly the same split.", "tokens": [577, 281, 360, 544, 1507, 294, 15968, 13, 1301, 11, 370, 286, 528, 281, 652, 988, 321, 362, 2293, 264, 912, 7472, 13], "temperature": 0.0, "avg_logprob": -0.10884163514623102, "compression_ratio": 1.5751072961373391, "no_speech_prob": 3.535388532327488e-05}, {"id": 570, "seek": 258462, "start": 2584.62, "end": 2593.58, "text": " So those are a list of the indexes of the rows that will be, for example, in the validation", "tokens": [407, 729, 366, 257, 1329, 295, 264, 8186, 279, 295, 264, 13241, 300, 486, 312, 11, 337, 1365, 11, 294, 264, 24071], "temperature": 0.0, "avg_logprob": -0.1133193454226932, "compression_ratio": 1.771604938271605, "no_speech_prob": 9.080301424546633e-06}, {"id": 571, "seek": 258462, "start": 2593.58, "end": 2595.46, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.1133193454226932, "compression_ratio": 1.771604938271605, "no_speech_prob": 9.080301424546633e-06}, {"id": 572, "seek": 258462, "start": 2595.46, "end": 2597.44, "text": " That's why I call it validation split.", "tokens": [663, 311, 983, 286, 818, 309, 24071, 7472, 13], "temperature": 0.0, "avg_logprob": -0.1133193454226932, "compression_ratio": 1.771604938271605, "no_speech_prob": 9.080301424546633e-06}, {"id": 573, "seek": 258462, "start": 2597.44, "end": 2603.66, "text": " So to create the validation independent variables you have to use those to index into the independent", "tokens": [407, 281, 1884, 264, 24071, 6695, 9102, 291, 362, 281, 764, 729, 281, 8186, 666, 264, 6695], "temperature": 0.0, "avg_logprob": -0.1133193454226932, "compression_ratio": 1.771604938271605, "no_speech_prob": 9.080301424546633e-06}, {"id": 574, "seek": 258462, "start": 2603.66, "end": 2604.66, "text": " variables.", "tokens": [9102, 13], "temperature": 0.0, "avg_logprob": -0.1133193454226932, "compression_ratio": 1.771604938271605, "no_speech_prob": 9.080301424546633e-06}, {"id": 575, "seek": 258462, "start": 2604.66, "end": 2608.06, "text": " And ditto for the dependent variables.", "tokens": [400, 274, 34924, 337, 264, 12334, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1133193454226932, "compression_ratio": 1.771604938271605, "no_speech_prob": 9.080301424546633e-06}, {"id": 576, "seek": 260806, "start": 2608.06, "end": 2617.54, "text": " And so now we've got our independent variable training set and our validation set, and we've", "tokens": [400, 370, 586, 321, 600, 658, 527, 6695, 7006, 3097, 992, 293, 527, 24071, 992, 11, 293, 321, 600], "temperature": 0.0, "avg_logprob": -0.1196039535187103, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.668779299914604e-06}, {"id": 577, "seek": 260806, "start": 2617.54, "end": 2622.14, "text": " also got the same for the dependent variables.", "tokens": [611, 658, 264, 912, 337, 264, 12334, 9102, 13], "temperature": 0.0, "avg_logprob": -0.1196039535187103, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.668779299914604e-06}, {"id": 578, "seek": 260806, "start": 2622.14, "end": 2626.98, "text": " So like I said before, I normally take stuff that I've already done in a notebook, seems", "tokens": [407, 411, 286, 848, 949, 11, 286, 5646, 747, 1507, 300, 286, 600, 1217, 1096, 294, 257, 21060, 11, 2544], "temperature": 0.0, "avg_logprob": -0.1196039535187103, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.668779299914604e-06}, {"id": 579, "seek": 260806, "start": 2626.98, "end": 2629.22, "text": " to be working, and put them into functions.", "tokens": [281, 312, 1364, 11, 293, 829, 552, 666, 6828, 13], "temperature": 0.0, "avg_logprob": -0.1196039535187103, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.668779299914604e-06}, {"id": 580, "seek": 260806, "start": 2629.22, "end": 2632.52, "text": " So here's the step which actually updates coefficients.", "tokens": [407, 510, 311, 264, 1823, 597, 767, 9205, 31994, 13], "temperature": 0.0, "avg_logprob": -0.1196039535187103, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.668779299914604e-06}, {"id": 581, "seek": 260806, "start": 2632.52, "end": 2635.2599999999998, "text": " So let's chuck that into a function.", "tokens": [407, 718, 311, 20870, 300, 666, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1196039535187103, "compression_ratio": 1.6590909090909092, "no_speech_prob": 3.668779299914604e-06}, {"id": 582, "seek": 263526, "start": 2635.26, "end": 2639.0200000000004, "text": " And then the steps that go count loss, stop backward, update coefficients, and then print", "tokens": [400, 550, 264, 4439, 300, 352, 1207, 4470, 11, 1590, 23897, 11, 5623, 31994, 11, 293, 550, 4482], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 583, "seek": 263526, "start": 2639.0200000000004, "end": 2641.5, "text": " the loss, we'll chuck that in one function.", "tokens": [264, 4470, 11, 321, 603, 20870, 300, 294, 472, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 584, "seek": 263526, "start": 2641.5, "end": 2645.6600000000003, "text": " So just copying and pasting stuff into cells here.", "tokens": [407, 445, 27976, 293, 1791, 278, 1507, 666, 5438, 510, 13], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 585, "seek": 263526, "start": 2645.6600000000003, "end": 2650.0200000000004, "text": " And then the bit on the very top of the previous section that got the random numbers, minus", "tokens": [400, 550, 264, 857, 322, 264, 588, 1192, 295, 264, 3894, 3541, 300, 658, 264, 4974, 3547, 11, 3175], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 586, "seek": 263526, "start": 2650.0200000000004, "end": 2653.7400000000002, "text": ".5 requires grad, chuck that in the function.", "tokens": [2411, 20, 7029, 2771, 11, 20870, 300, 294, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 587, "seek": 263526, "start": 2653.7400000000002, "end": 2657.78, "text": " So here we've got something that initializes coefficients, something that does one epoch", "tokens": [407, 510, 321, 600, 658, 746, 300, 5883, 5660, 31994, 11, 746, 300, 775, 472, 30992, 339], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 588, "seek": 263526, "start": 2657.78, "end": 2660.3, "text": " by updating coefficients.", "tokens": [538, 25113, 31994, 13], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 589, "seek": 263526, "start": 2660.3, "end": 2664.82, "text": " So we can put that together into something that trains the model for any epochs with", "tokens": [407, 321, 393, 829, 300, 1214, 666, 746, 300, 16329, 264, 2316, 337, 604, 30992, 28346, 365], "temperature": 0.0, "avg_logprob": -0.15516573285299634, "compression_ratio": 1.8945454545454545, "no_speech_prob": 9.080368727154564e-06}, {"id": 590, "seek": 266482, "start": 2664.82, "end": 2672.42, "text": " some learning rate by setting the manual seed, initializing the coefficients, doing one epoch", "tokens": [512, 2539, 3314, 538, 3287, 264, 9688, 8871, 11, 5883, 3319, 264, 31994, 11, 884, 472, 30992, 339], "temperature": 0.0, "avg_logprob": -0.0758950397221729, "compression_ratio": 1.7422680412371134, "no_speech_prob": 2.212523395428434e-05}, {"id": 591, "seek": 266482, "start": 2672.42, "end": 2677.46, "text": " in a loop, and then return the coefficients.", "tokens": [294, 257, 6367, 11, 293, 550, 2736, 264, 31994, 13], "temperature": 0.0, "avg_logprob": -0.0758950397221729, "compression_ratio": 1.7422680412371134, "no_speech_prob": 2.212523395428434e-05}, {"id": 592, "seek": 266482, "start": 2677.46, "end": 2681.2400000000002, "text": " So let's go ahead and run that function.", "tokens": [407, 718, 311, 352, 2286, 293, 1190, 300, 2445, 13], "temperature": 0.0, "avg_logprob": -0.0758950397221729, "compression_ratio": 1.7422680412371134, "no_speech_prob": 2.212523395428434e-05}, {"id": 593, "seek": 266482, "start": 2681.2400000000002, "end": 2685.6200000000003, "text": " So it's printing at the end of each one the loss, and you can see the loss going down", "tokens": [407, 309, 311, 14699, 412, 264, 917, 295, 1184, 472, 264, 4470, 11, 293, 291, 393, 536, 264, 4470, 516, 760], "temperature": 0.0, "avg_logprob": -0.0758950397221729, "compression_ratio": 1.7422680412371134, "no_speech_prob": 2.212523395428434e-05}, {"id": 594, "seek": 266482, "start": 2685.6200000000003, "end": 2690.7400000000002, "text": " from.53, down, down, down, down, down, to a bit under.3.", "tokens": [490, 2411, 19584, 11, 760, 11, 760, 11, 760, 11, 760, 11, 760, 11, 281, 257, 857, 833, 2411, 18, 13], "temperature": 0.0, "avg_logprob": -0.0758950397221729, "compression_ratio": 1.7422680412371134, "no_speech_prob": 2.212523395428434e-05}, {"id": 595, "seek": 266482, "start": 2690.7400000000002, "end": 2691.7400000000002, "text": " So that's good.", "tokens": [407, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.0758950397221729, "compression_ratio": 1.7422680412371134, "no_speech_prob": 2.212523395428434e-05}, {"id": 596, "seek": 269174, "start": 2691.74, "end": 2697.9799999999996, "text": " So we've successfully built and trained a linear model on a real data set.", "tokens": [407, 321, 600, 10727, 3094, 293, 8895, 257, 8213, 2316, 322, 257, 957, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 597, "seek": 269174, "start": 2697.9799999999996, "end": 2704.2599999999998, "text": " I mean, it's a Kaggle data set, but it's important to not underestimate how real Kaggle data", "tokens": [286, 914, 11, 309, 311, 257, 48751, 22631, 1412, 992, 11, 457, 309, 311, 1021, 281, 406, 35826, 577, 957, 48751, 22631, 1412], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 598, "seek": 269174, "start": 2704.2599999999998, "end": 2705.2599999999998, "text": " sets are.", "tokens": [6352, 366, 13], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 599, "seek": 269174, "start": 2705.2599999999998, "end": 2706.2599999999998, "text": " They're real data.", "tokens": [814, 434, 957, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 600, "seek": 269174, "start": 2706.2599999999998, "end": 2710.4199999999996, "text": " And this one's a playground data set, so it's not like anybody actually cares about predicting", "tokens": [400, 341, 472, 311, 257, 24646, 1412, 992, 11, 370, 309, 311, 406, 411, 4472, 767, 12310, 466, 32884], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 601, "seek": 269174, "start": 2710.4199999999996, "end": 2714.4199999999996, "text": " who survived the Titanic, because we already know.", "tokens": [567, 14433, 264, 42183, 11, 570, 321, 1217, 458, 13], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 602, "seek": 269174, "start": 2714.4199999999996, "end": 2719.2999999999997, "text": " But it has all the same features of different data types and missing values and normalization", "tokens": [583, 309, 575, 439, 264, 912, 4122, 295, 819, 1412, 3467, 293, 5361, 4190, 293, 2710, 2144], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 603, "seek": 269174, "start": 2719.2999999999997, "end": 2720.2999999999997, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.15870251862899118, "compression_ratio": 1.6423357664233578, "no_speech_prob": 1.593637716723606e-05}, {"id": 604, "seek": 272030, "start": 2720.3, "end": 2725.1800000000003, "text": " So it's a good playground.", "tokens": [407, 309, 311, 257, 665, 24646, 13], "temperature": 0.0, "avg_logprob": -0.1491916900457338, "compression_ratio": 1.5971563981042654, "no_speech_prob": 4.289286607672693e-06}, {"id": 605, "seek": 272030, "start": 2725.1800000000003, "end": 2729.5, "text": " So it would be nice to see what the coefficients are attached to each variable.", "tokens": [407, 309, 576, 312, 1481, 281, 536, 437, 264, 31994, 366, 8570, 281, 1184, 7006, 13], "temperature": 0.0, "avg_logprob": -0.1491916900457338, "compression_ratio": 1.5971563981042654, "no_speech_prob": 4.289286607672693e-06}, {"id": 606, "seek": 272030, "start": 2729.5, "end": 2734.94, "text": " So if we just zip together the independent variables and the coefficients, and we don't", "tokens": [407, 498, 321, 445, 20730, 1214, 264, 6695, 9102, 293, 264, 31994, 11, 293, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1491916900457338, "compression_ratio": 1.5971563981042654, "no_speech_prob": 4.289286607672693e-06}, {"id": 607, "seek": 272030, "start": 2734.94, "end": 2741.3, "text": " need the regret anymore, and create a dict of that, there we go.", "tokens": [643, 264, 10879, 3602, 11, 293, 1884, 257, 12569, 295, 300, 11, 456, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.1491916900457338, "compression_ratio": 1.5971563981042654, "no_speech_prob": 4.289286607672693e-06}, {"id": 608, "seek": 272030, "start": 2741.3, "end": 2747.54, "text": " So it looks like older people had less chance of surviving.", "tokens": [407, 309, 1542, 411, 4906, 561, 632, 1570, 2931, 295, 24948, 13], "temperature": 0.0, "avg_logprob": -0.1491916900457338, "compression_ratio": 1.5971563981042654, "no_speech_prob": 4.289286607672693e-06}, {"id": 609, "seek": 272030, "start": 2747.54, "end": 2749.86, "text": " That makes sense.", "tokens": [663, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1491916900457338, "compression_ratio": 1.5971563981042654, "no_speech_prob": 4.289286607672693e-06}, {"id": 610, "seek": 274986, "start": 2749.86, "end": 2752.3, "text": " Older people had less chance of surviving.", "tokens": [8633, 260, 561, 632, 1570, 2931, 295, 24948, 13], "temperature": 0.0, "avg_logprob": -0.1765911016571388, "compression_ratio": 1.4887892376681615, "no_speech_prob": 3.96691166315577e-06}, {"id": 611, "seek": 274986, "start": 2752.3, "end": 2754.54, "text": " Also makes sense.", "tokens": [2743, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1765911016571388, "compression_ratio": 1.4887892376681615, "no_speech_prob": 3.96691166315577e-06}, {"id": 612, "seek": 274986, "start": 2754.54, "end": 2762.5, "text": " So it's good to kind of eyeball these and check that they seem reasonable.", "tokens": [407, 309, 311, 665, 281, 733, 295, 38868, 613, 293, 1520, 300, 436, 1643, 10585, 13], "temperature": 0.0, "avg_logprob": -0.1765911016571388, "compression_ratio": 1.4887892376681615, "no_speech_prob": 3.96691166315577e-06}, {"id": 613, "seek": 274986, "start": 2762.5, "end": 2770.06, "text": " Now the metric for this Kaggle competition is not mean absolute error.", "tokens": [823, 264, 20678, 337, 341, 48751, 22631, 6211, 307, 406, 914, 8236, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1765911016571388, "compression_ratio": 1.4887892376681615, "no_speech_prob": 3.96691166315577e-06}, {"id": 614, "seek": 274986, "start": 2770.06, "end": 2771.06, "text": " It's accuracy.", "tokens": [467, 311, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1765911016571388, "compression_ratio": 1.4887892376681615, "no_speech_prob": 3.96691166315577e-06}, {"id": 615, "seek": 274986, "start": 2771.06, "end": 2775.34, "text": " Now, of course, we can't use accuracy as a loss function, because it doesn't have a sensible", "tokens": [823, 11, 295, 1164, 11, 321, 393, 380, 764, 14170, 382, 257, 4470, 2445, 11, 570, 309, 1177, 380, 362, 257, 25380], "temperature": 0.0, "avg_logprob": -0.1765911016571388, "compression_ratio": 1.4887892376681615, "no_speech_prob": 3.96691166315577e-06}, {"id": 616, "seek": 274986, "start": 2775.34, "end": 2778.02, "text": " gradient, really.", "tokens": [16235, 11, 534, 13], "temperature": 0.0, "avg_logprob": -0.1765911016571388, "compression_ratio": 1.4887892376681615, "no_speech_prob": 3.96691166315577e-06}, {"id": 617, "seek": 277802, "start": 2778.02, "end": 2781.58, "text": " But we should measure accuracy to see how we're doing, because that's going to tell", "tokens": [583, 321, 820, 3481, 14170, 281, 536, 577, 321, 434, 884, 11, 570, 300, 311, 516, 281, 980], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 618, "seek": 277802, "start": 2781.58, "end": 2786.58, "text": " us how we're going against the thing that the Kaggle competition cares about.", "tokens": [505, 577, 321, 434, 516, 1970, 264, 551, 300, 264, 48751, 22631, 6211, 12310, 466, 13], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 619, "seek": 277802, "start": 2786.58, "end": 2789.78, "text": " So we can calculate our predictions.", "tokens": [407, 321, 393, 8873, 527, 21264, 13], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 620, "seek": 277802, "start": 2789.78, "end": 2795.82, "text": " And we'll just say, OK, well, any time the prediction is over 0.5, we'll say that's predicting", "tokens": [400, 321, 603, 445, 584, 11, 2264, 11, 731, 11, 604, 565, 264, 17630, 307, 670, 1958, 13, 20, 11, 321, 603, 584, 300, 311, 32884], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 621, "seek": 277802, "start": 2795.82, "end": 2797.54, "text": " survival.", "tokens": [12559, 13], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 622, "seek": 277802, "start": 2797.54, "end": 2799.66, "text": " So that's our predictors of survival.", "tokens": [407, 300, 311, 527, 6069, 830, 295, 12559, 13], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 623, "seek": 277802, "start": 2799.66, "end": 2802.24, "text": " This is the actual in a validation set.", "tokens": [639, 307, 264, 3539, 294, 257, 24071, 992, 13], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 624, "seek": 277802, "start": 2802.24, "end": 2806.74, "text": " So if they're the same, then we predicted it correctly.", "tokens": [407, 498, 436, 434, 264, 912, 11, 550, 321, 19147, 309, 8944, 13], "temperature": 0.0, "avg_logprob": -0.1398557242700609, "compression_ratio": 1.7137254901960783, "no_speech_prob": 1.6963500456768088e-05}, {"id": 625, "seek": 280674, "start": 2806.74, "end": 2812.1, "text": " So here's are we right or wrong for the first 16 rows?", "tokens": [407, 510, 311, 366, 321, 558, 420, 2085, 337, 264, 700, 3165, 13241, 30], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 626, "seek": 280674, "start": 2812.1, "end": 2815.12, "text": " We're right more often than not.", "tokens": [492, 434, 558, 544, 2049, 813, 406, 13], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 627, "seek": 280674, "start": 2815.12, "end": 2820.5, "text": " So if we take the mean of those, remember true equals 1, then that's our accuracy.", "tokens": [407, 498, 321, 747, 264, 914, 295, 729, 11, 1604, 2074, 6915, 502, 11, 550, 300, 311, 527, 14170, 13], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 628, "seek": 280674, "start": 2820.5, "end": 2822.9799999999996, "text": " So we are right about 79% of the time.", "tokens": [407, 321, 366, 558, 466, 32803, 4, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 629, "seek": 280674, "start": 2822.9799999999996, "end": 2824.58, "text": " So that's not bad.", "tokens": [407, 300, 311, 406, 1578, 13], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 630, "seek": 280674, "start": 2824.58, "end": 2831.14, "text": " So we've successfully created something that's actually predicting who survived the Titanic.", "tokens": [407, 321, 600, 10727, 2942, 746, 300, 311, 767, 32884, 567, 14433, 264, 42183, 13], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 631, "seek": 280674, "start": 2831.14, "end": 2832.14, "text": " That's cool.", "tokens": [663, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 632, "seek": 280674, "start": 2832.14, "end": 2833.14, "text": " From scratch.", "tokens": [3358, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14923642861722697, "compression_ratio": 1.5398230088495575, "no_speech_prob": 3.844854290946387e-06}, {"id": 633, "seek": 283314, "start": 2833.14, "end": 2841.1, "text": " So let's create a function for that, an accuracy function that just does what I showed.", "tokens": [407, 718, 311, 1884, 257, 2445, 337, 300, 11, 364, 14170, 2445, 300, 445, 775, 437, 286, 4712, 13], "temperature": 0.0, "avg_logprob": -0.17668763796488443, "compression_ratio": 1.556701030927835, "no_speech_prob": 6.962075531191658e-06}, {"id": 634, "seek": 283314, "start": 2841.1, "end": 2842.1, "text": " And there it is.", "tokens": [400, 456, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.17668763796488443, "compression_ratio": 1.556701030927835, "no_speech_prob": 6.962075531191658e-06}, {"id": 635, "seek": 283314, "start": 2842.1, "end": 2849.9, "text": " Now, I'll say another thing like, you know, a weird coding thing for me, you know, weird", "tokens": [823, 11, 286, 603, 584, 1071, 551, 411, 11, 291, 458, 11, 257, 3657, 17720, 551, 337, 385, 11, 291, 458, 11, 3657], "temperature": 0.0, "avg_logprob": -0.17668763796488443, "compression_ratio": 1.556701030927835, "no_speech_prob": 6.962075531191658e-06}, {"id": 636, "seek": 283314, "start": 2849.9, "end": 2856.2799999999997, "text": " as it's not that common, is I use less comments than most people, because all of my code lives", "tokens": [382, 309, 311, 406, 300, 2689, 11, 307, 286, 764, 1570, 3053, 813, 881, 561, 11, 570, 439, 295, 452, 3089, 2909], "temperature": 0.0, "avg_logprob": -0.17668763796488443, "compression_ratio": 1.556701030927835, "no_speech_prob": 6.962075531191658e-06}, {"id": 637, "seek": 283314, "start": 2856.2799999999997, "end": 2857.2799999999997, "text": " in notebooks.", "tokens": [294, 43782, 13], "temperature": 0.0, "avg_logprob": -0.17668763796488443, "compression_ratio": 1.556701030927835, "no_speech_prob": 6.962075531191658e-06}, {"id": 638, "seek": 285728, "start": 2857.28, "end": 2863.5, "text": " And the real version of this notebook is full of pros.", "tokens": [400, 264, 957, 3037, 295, 341, 21060, 307, 1577, 295, 6267, 13], "temperature": 0.0, "avg_logprob": -0.21224092182360196, "compression_ratio": 1.6, "no_speech_prob": 6.643097549385857e-06}, {"id": 639, "seek": 285728, "start": 2863.5, "end": 2864.98, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.21224092182360196, "compression_ratio": 1.6, "no_speech_prob": 6.643097549385857e-06}, {"id": 640, "seek": 285728, "start": 2864.98, "end": 2870.2400000000002, "text": " So when I've taken people through a whole journey about what I've built here and why", "tokens": [407, 562, 286, 600, 2726, 561, 807, 257, 1379, 4671, 466, 437, 286, 600, 3094, 510, 293, 983], "temperature": 0.0, "avg_logprob": -0.21224092182360196, "compression_ratio": 1.6, "no_speech_prob": 6.643097549385857e-06}, {"id": 641, "seek": 285728, "start": 2870.2400000000002, "end": 2875.0600000000004, "text": " I've built it and what intermediate results are and check them along the way, the function", "tokens": [286, 600, 3094, 309, 293, 437, 19376, 3542, 366, 293, 1520, 552, 2051, 264, 636, 11, 264, 2445], "temperature": 0.0, "avg_logprob": -0.21224092182360196, "compression_ratio": 1.6, "no_speech_prob": 6.643097549385857e-06}, {"id": 642, "seek": 285728, "start": 2875.0600000000004, "end": 2880.0600000000004, "text": " itself, for me, doesn't need extensive comments.", "tokens": [2564, 11, 337, 385, 11, 1177, 380, 643, 13246, 3053, 13], "temperature": 0.0, "avg_logprob": -0.21224092182360196, "compression_ratio": 1.6, "no_speech_prob": 6.643097549385857e-06}, {"id": 643, "seek": 285728, "start": 2880.0600000000004, "end": 2884.94, "text": " I'd rather explain the thinking of how I got there and show examples of how to use it and", "tokens": [286, 1116, 2831, 2903, 264, 1953, 295, 577, 286, 658, 456, 293, 855, 5110, 295, 577, 281, 764, 309, 293], "temperature": 0.0, "avg_logprob": -0.21224092182360196, "compression_ratio": 1.6, "no_speech_prob": 6.643097549385857e-06}, {"id": 644, "seek": 288494, "start": 2884.94, "end": 2887.54, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.24968267889583812, "compression_ratio": 1.4508670520231215, "no_speech_prob": 2.6686984710977413e-05}, {"id": 645, "seek": 288494, "start": 2887.54, "end": 2889.3, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.24968267889583812, "compression_ratio": 1.4508670520231215, "no_speech_prob": 2.6686984710977413e-05}, {"id": 646, "seek": 288494, "start": 2889.3, "end": 2897.82, "text": " Now here's the first few predictions we made.", "tokens": [823, 510, 311, 264, 700, 1326, 21264, 321, 1027, 13], "temperature": 0.0, "avg_logprob": -0.24968267889583812, "compression_ratio": 1.4508670520231215, "no_speech_prob": 2.6686984710977413e-05}, {"id": 647, "seek": 288494, "start": 2897.82, "end": 2905.46, "text": " And some of the time we're predicting negatives for survival and greater than one for survival.", "tokens": [400, 512, 295, 264, 565, 321, 434, 32884, 40019, 337, 12559, 293, 5044, 813, 472, 337, 12559, 13], "temperature": 0.0, "avg_logprob": -0.24968267889583812, "compression_ratio": 1.4508670520231215, "no_speech_prob": 2.6686984710977413e-05}, {"id": 648, "seek": 288494, "start": 2905.46, "end": 2908.48, "text": " Which doesn't really make much sense, right?", "tokens": [3013, 1177, 380, 534, 652, 709, 2020, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24968267889583812, "compression_ratio": 1.4508670520231215, "no_speech_prob": 2.6686984710977413e-05}, {"id": 649, "seek": 288494, "start": 2908.48, "end": 2913.46, "text": " People either survived one or they didn't, zero.", "tokens": [3432, 2139, 14433, 472, 420, 436, 994, 380, 11, 4018, 13], "temperature": 0.0, "avg_logprob": -0.24968267889583812, "compression_ratio": 1.4508670520231215, "no_speech_prob": 2.6686984710977413e-05}, {"id": 650, "seek": 291346, "start": 2913.46, "end": 2919.98, "text": " It would be nice if we had a way to automatically squish everything between zero and one.", "tokens": [467, 576, 312, 1481, 498, 321, 632, 257, 636, 281, 6772, 31379, 1203, 1296, 4018, 293, 472, 13], "temperature": 0.0, "avg_logprob": -0.1312797296614874, "compression_ratio": 1.6116504854368932, "no_speech_prob": 1.1477869520604145e-05}, {"id": 651, "seek": 291346, "start": 2919.98, "end": 2924.34, "text": " That's going to make it much easier to optimize.", "tokens": [663, 311, 516, 281, 652, 309, 709, 3571, 281, 19719, 13], "temperature": 0.0, "avg_logprob": -0.1312797296614874, "compression_ratio": 1.6116504854368932, "no_speech_prob": 1.1477869520604145e-05}, {"id": 652, "seek": 291346, "start": 2924.34, "end": 2928.38, "text": " The optimizer doesn't have to try hard to hit exactly one or hit exactly zero, but it", "tokens": [440, 5028, 6545, 1177, 380, 362, 281, 853, 1152, 281, 2045, 2293, 472, 420, 2045, 2293, 4018, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.1312797296614874, "compression_ratio": 1.6116504854368932, "no_speech_prob": 1.1477869520604145e-05}, {"id": 653, "seek": 291346, "start": 2928.38, "end": 2933.7400000000002, "text": " can just like try to create a really big number to mean survived or a really small number", "tokens": [393, 445, 411, 853, 281, 1884, 257, 534, 955, 1230, 281, 914, 14433, 420, 257, 534, 1359, 1230], "temperature": 0.0, "avg_logprob": -0.1312797296614874, "compression_ratio": 1.6116504854368932, "no_speech_prob": 1.1477869520604145e-05}, {"id": 654, "seek": 291346, "start": 2933.7400000000002, "end": 2937.66, "text": " to mean perished.", "tokens": [281, 914, 680, 4729, 13], "temperature": 0.0, "avg_logprob": -0.1312797296614874, "compression_ratio": 1.6116504854368932, "no_speech_prob": 1.1477869520604145e-05}, {"id": 655, "seek": 293766, "start": 2937.66, "end": 2943.7799999999997, "text": " Here's a great function.", "tokens": [1692, 311, 257, 869, 2445, 13], "temperature": 0.0, "avg_logprob": -0.14988478808335856, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.7693620268109953e-06}, {"id": 656, "seek": 293766, "start": 2943.7799999999997, "end": 2954.58, "text": " Here's a function that as I increase, let's make it even bigger, range.", "tokens": [1692, 311, 257, 2445, 300, 382, 286, 3488, 11, 718, 311, 652, 309, 754, 3801, 11, 3613, 13], "temperature": 0.0, "avg_logprob": -0.14988478808335856, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.7693620268109953e-06}, {"id": 657, "seek": 293766, "start": 2954.58, "end": 2960.46, "text": " As my numbers get beyond four or five, it's asymptoting to one.", "tokens": [1018, 452, 3547, 483, 4399, 1451, 420, 1732, 11, 309, 311, 35114, 17001, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.14988478808335856, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.7693620268109953e-06}, {"id": 658, "seek": 293766, "start": 2960.46, "end": 2966.06, "text": " And on the negative side, as they get beyond negative four or five, they asymptote to zero.", "tokens": [400, 322, 264, 3671, 1252, 11, 382, 436, 483, 4399, 3671, 1451, 420, 1732, 11, 436, 35114, 1370, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.14988478808335856, "compression_ratio": 1.6363636363636365, "no_speech_prob": 2.7693620268109953e-06}, {"id": 659, "seek": 296606, "start": 2966.06, "end": 2971.54, "text": " Got to zoom in a bit.", "tokens": [5803, 281, 8863, 294, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.1598077384374475, "compression_ratio": 1.4883720930232558, "no_speech_prob": 3.966926215070998e-06}, {"id": 660, "seek": 296606, "start": 2971.54, "end": 2976.14, "text": " But then around about zero, it's pretty much a straight line.", "tokens": [583, 550, 926, 466, 4018, 11, 309, 311, 1238, 709, 257, 2997, 1622, 13], "temperature": 0.0, "avg_logprob": -0.1598077384374475, "compression_ratio": 1.4883720930232558, "no_speech_prob": 3.966926215070998e-06}, {"id": 661, "seek": 296606, "start": 2976.14, "end": 2977.14, "text": " This is actually perfect.", "tokens": [639, 307, 767, 2176, 13], "temperature": 0.0, "avg_logprob": -0.1598077384374475, "compression_ratio": 1.4883720930232558, "no_speech_prob": 3.966926215070998e-06}, {"id": 662, "seek": 296606, "start": 2977.14, "end": 2979.62, "text": " This is exactly what we want.", "tokens": [639, 307, 2293, 437, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.1598077384374475, "compression_ratio": 1.4883720930232558, "no_speech_prob": 3.966926215070998e-06}, {"id": 663, "seek": 296606, "start": 2979.62, "end": 2985.1, "text": " So here is the equation one over one plus e to the negative minus x.", "tokens": [407, 510, 307, 264, 5367, 472, 670, 472, 1804, 308, 281, 264, 3671, 3175, 2031, 13], "temperature": 0.0, "avg_logprob": -0.1598077384374475, "compression_ratio": 1.4883720930232558, "no_speech_prob": 3.966926215070998e-06}, {"id": 664, "seek": 296606, "start": 2985.1, "end": 2989.86, "text": " And this is called the sigmoid function.", "tokens": [400, 341, 307, 1219, 264, 4556, 3280, 327, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1598077384374475, "compression_ratio": 1.4883720930232558, "no_speech_prob": 3.966926215070998e-06}, {"id": 665, "seek": 296606, "start": 2989.86, "end": 2995.1, "text": " By the way, if you haven't checked out simpi before, definitely do so.", "tokens": [3146, 264, 636, 11, 498, 291, 2378, 380, 10033, 484, 1034, 22630, 949, 11, 2138, 360, 370, 13], "temperature": 0.0, "avg_logprob": -0.1598077384374475, "compression_ratio": 1.4883720930232558, "no_speech_prob": 3.966926215070998e-06}, {"id": 666, "seek": 299510, "start": 2995.1, "end": 3001.94, "text": " This is the symbolic Python package, which can do it's kind of like Mathematica or Wolfram", "tokens": [639, 307, 264, 25755, 15329, 7372, 11, 597, 393, 360, 309, 311, 733, 295, 411, 15776, 8615, 2262, 420, 16634, 2356], "temperature": 0.0, "avg_logprob": -0.19156760754792587, "compression_ratio": 1.5044642857142858, "no_speech_prob": 8.013284968910739e-06}, {"id": 667, "seek": 299510, "start": 3001.94, "end": 3009.86, "text": " style symbolic calculations, including the ability to plot symbolic expressions, which", "tokens": [3758, 25755, 20448, 11, 3009, 264, 3485, 281, 7542, 25755, 15277, 11, 597], "temperature": 0.0, "avg_logprob": -0.19156760754792587, "compression_ratio": 1.5044642857142858, "no_speech_prob": 8.013284968910739e-06}, {"id": 668, "seek": 299510, "start": 3009.86, "end": 3013.2599999999998, "text": " is pretty nice.", "tokens": [307, 1238, 1481, 13], "temperature": 0.0, "avg_logprob": -0.19156760754792587, "compression_ratio": 1.5044642857142858, "no_speech_prob": 8.013284968910739e-06}, {"id": 669, "seek": 299510, "start": 3013.2599999999998, "end": 3015.22, "text": " PyTorch already has a sigmoid function.", "tokens": [9953, 51, 284, 339, 1217, 575, 257, 4556, 3280, 327, 2445, 13], "temperature": 0.0, "avg_logprob": -0.19156760754792587, "compression_ratio": 1.5044642857142858, "no_speech_prob": 8.013284968910739e-06}, {"id": 670, "seek": 299510, "start": 3015.22, "end": 3019.74, "text": " I mean, it just calculates this, but it does it in an optimized way.", "tokens": [286, 914, 11, 309, 445, 4322, 1024, 341, 11, 457, 309, 775, 309, 294, 364, 26941, 636, 13], "temperature": 0.0, "avg_logprob": -0.19156760754792587, "compression_ratio": 1.5044642857142858, "no_speech_prob": 8.013284968910739e-06}, {"id": 671, "seek": 299510, "start": 3019.74, "end": 3022.8399999999997, "text": " So what if we replaced calc preds?", "tokens": [407, 437, 498, 321, 10772, 2104, 66, 3852, 82, 30], "temperature": 0.0, "avg_logprob": -0.19156760754792587, "compression_ratio": 1.5044642857142858, "no_speech_prob": 8.013284968910739e-06}, {"id": 672, "seek": 302284, "start": 3022.84, "end": 3028.3, "text": " Remember before calc preds was just this.", "tokens": [5459, 949, 2104, 66, 3852, 82, 390, 445, 341, 13], "temperature": 0.0, "avg_logprob": -0.1294578150699013, "compression_ratio": 1.6945812807881773, "no_speech_prob": 6.747940005880082e-06}, {"id": 673, "seek": 302284, "start": 3028.3, "end": 3031.5, "text": " What if we took that and then put it through a sigmoid?", "tokens": [708, 498, 321, 1890, 300, 293, 550, 829, 309, 807, 257, 4556, 3280, 327, 30], "temperature": 0.0, "avg_logprob": -0.1294578150699013, "compression_ratio": 1.6945812807881773, "no_speech_prob": 6.747940005880082e-06}, {"id": 674, "seek": 302284, "start": 3031.5, "end": 3042.28, "text": " So calc preds will now basically the bigger this number is, the closer it's going to get", "tokens": [407, 2104, 66, 3852, 82, 486, 586, 1936, 264, 3801, 341, 1230, 307, 11, 264, 4966, 309, 311, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.1294578150699013, "compression_ratio": 1.6945812807881773, "no_speech_prob": 6.747940005880082e-06}, {"id": 675, "seek": 302284, "start": 3042.28, "end": 3045.7000000000003, "text": " to one, and the smaller it is, the closer it's going to get to zero.", "tokens": [281, 472, 11, 293, 264, 4356, 309, 307, 11, 264, 4966, 309, 311, 516, 281, 483, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1294578150699013, "compression_ratio": 1.6945812807881773, "no_speech_prob": 6.747940005880082e-06}, {"id": 676, "seek": 302284, "start": 3045.7000000000003, "end": 3050.1400000000003, "text": " This should be a much easier thing to optimize and ensures that all of our values are in", "tokens": [639, 820, 312, 257, 709, 3571, 551, 281, 19719, 293, 28111, 300, 439, 295, 527, 4190, 366, 294], "temperature": 0.0, "avg_logprob": -0.1294578150699013, "compression_ratio": 1.6945812807881773, "no_speech_prob": 6.747940005880082e-06}, {"id": 677, "seek": 305014, "start": 3050.14, "end": 3053.66, "text": " a sensible range.", "tokens": [257, 25380, 3613, 13], "temperature": 0.0, "avg_logprob": -0.18856939247676305, "compression_ratio": 1.408450704225352, "no_speech_prob": 3.237740884287632e-06}, {"id": 678, "seek": 305014, "start": 3053.66, "end": 3060.02, "text": " Now here's another cool thing about using Jupyter plus Python.", "tokens": [823, 510, 311, 1071, 1627, 551, 466, 1228, 22125, 88, 391, 1804, 15329, 13], "temperature": 0.0, "avg_logprob": -0.18856939247676305, "compression_ratio": 1.408450704225352, "no_speech_prob": 3.237740884287632e-06}, {"id": 679, "seek": 305014, "start": 3060.02, "end": 3063.54, "text": " Python is a dynamic language.", "tokens": [15329, 307, 257, 8546, 2856, 13], "temperature": 0.0, "avg_logprob": -0.18856939247676305, "compression_ratio": 1.408450704225352, "no_speech_prob": 3.237740884287632e-06}, {"id": 680, "seek": 305014, "start": 3063.54, "end": 3074.4, "text": " Even though I called calc preds train model calls one epoch, which calls calc loss, which", "tokens": [2754, 1673, 286, 1219, 2104, 66, 3852, 82, 3847, 2316, 5498, 472, 30992, 339, 11, 597, 5498, 2104, 66, 4470, 11, 597], "temperature": 0.0, "avg_logprob": -0.18856939247676305, "compression_ratio": 1.408450704225352, "no_speech_prob": 3.237740884287632e-06}, {"id": 681, "seek": 307440, "start": 3074.4, "end": 3081.38, "text": " calls calc preds, I can redefine calc preds now.", "tokens": [5498, 2104, 66, 3852, 82, 11, 286, 393, 38818, 533, 2104, 66, 3852, 82, 586, 13], "temperature": 0.0, "avg_logprob": -0.10245737698998782, "compression_ratio": 1.7019230769230769, "no_speech_prob": 5.173873887542868e-06}, {"id": 682, "seek": 307440, "start": 3081.38, "end": 3082.7400000000002, "text": " And I don't have to do anything.", "tokens": [400, 286, 500, 380, 362, 281, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.10245737698998782, "compression_ratio": 1.7019230769230769, "no_speech_prob": 5.173873887542868e-06}, {"id": 683, "seek": 307440, "start": 3082.7400000000002, "end": 3088.1, "text": " That's now inserted into Python symbol table, and that's the calc preds that train model", "tokens": [663, 311, 586, 27992, 666, 15329, 5986, 3199, 11, 293, 300, 311, 264, 2104, 66, 3852, 82, 300, 3847, 2316], "temperature": 0.0, "avg_logprob": -0.10245737698998782, "compression_ratio": 1.7019230769230769, "no_speech_prob": 5.173873887542868e-06}, {"id": 684, "seek": 307440, "start": 3088.1, "end": 3089.62, "text": " will eventually call.", "tokens": [486, 4728, 818, 13], "temperature": 0.0, "avg_logprob": -0.10245737698998782, "compression_ratio": 1.7019230769230769, "no_speech_prob": 5.173873887542868e-06}, {"id": 685, "seek": 307440, "start": 3089.62, "end": 3096.98, "text": " So if I now call train model, that's actually going to call my new version of calc preds.", "tokens": [407, 498, 286, 586, 818, 3847, 2316, 11, 300, 311, 767, 516, 281, 818, 452, 777, 3037, 295, 2104, 66, 3852, 82, 13], "temperature": 0.0, "avg_logprob": -0.10245737698998782, "compression_ratio": 1.7019230769230769, "no_speech_prob": 5.173873887542868e-06}, {"id": 686, "seek": 307440, "start": 3096.98, "end": 3102.98, "text": " So that's a really neat way of doing exploratory programming in Python.", "tokens": [407, 300, 311, 257, 534, 10654, 636, 295, 884, 24765, 4745, 9410, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.10245737698998782, "compression_ratio": 1.7019230769230769, "no_speech_prob": 5.173873887542868e-06}, {"id": 687, "seek": 310298, "start": 3102.98, "end": 3111.58, "text": " I wouldn't release a library that redefines calc preds multiple times.", "tokens": [286, 2759, 380, 4374, 257, 6405, 300, 38818, 1652, 2104, 66, 3852, 82, 3866, 1413, 13], "temperature": 0.0, "avg_logprob": -0.13768814007441202, "compression_ratio": 1.4681818181818183, "no_speech_prob": 1.6700674677849747e-05}, {"id": 688, "seek": 310298, "start": 3111.58, "end": 3114.02, "text": " When I'm done, I would just keep the final version, of course.", "tokens": [1133, 286, 478, 1096, 11, 286, 576, 445, 1066, 264, 2572, 3037, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.13768814007441202, "compression_ratio": 1.4681818181818183, "no_speech_prob": 1.6700674677849747e-05}, {"id": 689, "seek": 310298, "start": 3114.02, "end": 3118.3, "text": " But it's a great way to try things, as you'll see.", "tokens": [583, 309, 311, 257, 869, 636, 281, 853, 721, 11, 382, 291, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.13768814007441202, "compression_ratio": 1.4681818181818183, "no_speech_prob": 1.6700674677849747e-05}, {"id": 690, "seek": 310298, "start": 3118.3, "end": 3119.3, "text": " And so look what's happened.", "tokens": [400, 370, 574, 437, 311, 2011, 13], "temperature": 0.0, "avg_logprob": -0.13768814007441202, "compression_ratio": 1.4681818181818183, "no_speech_prob": 1.6700674677849747e-05}, {"id": 691, "seek": 310298, "start": 3119.3, "end": 3122.62, "text": " I found I was able to increase the learning rate from 0.1 to 2.", "tokens": [286, 1352, 286, 390, 1075, 281, 3488, 264, 2539, 3314, 490, 1958, 13, 16, 281, 568, 13], "temperature": 0.0, "avg_logprob": -0.13768814007441202, "compression_ratio": 1.4681818181818183, "no_speech_prob": 1.6700674677849747e-05}, {"id": 692, "seek": 310298, "start": 3122.62, "end": 3125.9, "text": " It was much easier to optimize, as I guessed.", "tokens": [467, 390, 709, 3571, 281, 19719, 11, 382, 286, 21852, 13], "temperature": 0.0, "avg_logprob": -0.13768814007441202, "compression_ratio": 1.4681818181818183, "no_speech_prob": 1.6700674677849747e-05}, {"id": 693, "seek": 312590, "start": 3125.9, "end": 3135.42, "text": " And the loss has improved from 0.295 to 0.197.", "tokens": [400, 264, 4470, 575, 9689, 490, 1958, 13, 11871, 20, 281, 1958, 13, 3405, 22, 13], "temperature": 0.0, "avg_logprob": -0.11594395015550696, "compression_ratio": 1.3836477987421383, "no_speech_prob": 2.0580337150022388e-06}, {"id": 694, "seek": 312590, "start": 3135.42, "end": 3143.1, "text": " The accuracy has improved from 0.79 to 0.82, nearly 0.83.", "tokens": [440, 14170, 575, 9689, 490, 1958, 13, 32042, 281, 1958, 13, 32848, 11, 6217, 1958, 13, 31849, 13], "temperature": 0.0, "avg_logprob": -0.11594395015550696, "compression_ratio": 1.3836477987421383, "no_speech_prob": 2.0580337150022388e-06}, {"id": 695, "seek": 312590, "start": 3143.1, "end": 3149.78, "text": " So as a rule, this is something that we're pretty much always going to do when we have", "tokens": [407, 382, 257, 4978, 11, 341, 307, 746, 300, 321, 434, 1238, 709, 1009, 516, 281, 360, 562, 321, 362], "temperature": 0.0, "avg_logprob": -0.11594395015550696, "compression_ratio": 1.3836477987421383, "no_speech_prob": 2.0580337150022388e-06}, {"id": 696, "seek": 312590, "start": 3149.78, "end": 3151.42, "text": " a binary dependent variable.", "tokens": [257, 17434, 12334, 7006, 13], "temperature": 0.0, "avg_logprob": -0.11594395015550696, "compression_ratio": 1.3836477987421383, "no_speech_prob": 2.0580337150022388e-06}, {"id": 697, "seek": 315142, "start": 3151.42, "end": 3158.62, "text": " So a dependent variable that's 1 or 0 is the very last step is chuck it through a sigmoid.", "tokens": [407, 257, 12334, 7006, 300, 311, 502, 420, 1958, 307, 264, 588, 1036, 1823, 307, 20870, 309, 807, 257, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.1309997455493824, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.805785359465517e-05}, {"id": 698, "seek": 315142, "start": 3158.62, "end": 3164.26, "text": " Generally speaking, if you're wondering why is my model with a binary dependent variable", "tokens": [21082, 4124, 11, 498, 291, 434, 6359, 983, 307, 452, 2316, 365, 257, 17434, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.1309997455493824, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.805785359465517e-05}, {"id": 699, "seek": 315142, "start": 3164.26, "end": 3168.46, "text": " not training very well, this is the thing you want to check.", "tokens": [406, 3097, 588, 731, 11, 341, 307, 264, 551, 291, 528, 281, 1520, 13], "temperature": 0.0, "avg_logprob": -0.1309997455493824, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.805785359465517e-05}, {"id": 700, "seek": 315142, "start": 3168.46, "end": 3170.82, "text": " Oh, are you chucking it through a sigmoid?", "tokens": [876, 11, 366, 291, 20870, 278, 309, 807, 257, 4556, 3280, 327, 30], "temperature": 0.0, "avg_logprob": -0.1309997455493824, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.805785359465517e-05}, {"id": 701, "seek": 315142, "start": 3170.82, "end": 3175.38, "text": " Or is the thing you're calling chucking it through a sigmoid or not?", "tokens": [1610, 307, 264, 551, 291, 434, 5141, 20870, 278, 309, 807, 257, 4556, 3280, 327, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.1309997455493824, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.805785359465517e-05}, {"id": 702, "seek": 315142, "start": 3175.38, "end": 3177.82, "text": " It can be surprisingly hard to find out if that's happening.", "tokens": [467, 393, 312, 17600, 1152, 281, 915, 484, 498, 300, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.1309997455493824, "compression_ratio": 1.7725321888412018, "no_speech_prob": 1.805785359465517e-05}, {"id": 703, "seek": 317782, "start": 3177.82, "end": 3181.6200000000003, "text": " So for example, with hugging face transformers, I actually found I had to look in their source", "tokens": [407, 337, 1365, 11, 365, 41706, 1851, 4088, 433, 11, 286, 767, 1352, 286, 632, 281, 574, 294, 641, 4009], "temperature": 0.0, "avg_logprob": -0.13048129215418736, "compression_ratio": 1.6477272727272727, "no_speech_prob": 7.183150046330411e-06}, {"id": 704, "seek": 317782, "start": 3181.6200000000003, "end": 3184.1000000000004, "text": " code to find out.", "tokens": [3089, 281, 915, 484, 13], "temperature": 0.0, "avg_logprob": -0.13048129215418736, "compression_ratio": 1.6477272727272727, "no_speech_prob": 7.183150046330411e-06}, {"id": 705, "seek": 317782, "start": 3184.1000000000004, "end": 3189.86, "text": " And I discovered that something I was doing wasn't and didn't seem to be documented anywhere.", "tokens": [400, 286, 6941, 300, 746, 286, 390, 884, 2067, 380, 293, 994, 380, 1643, 281, 312, 23007, 4992, 13], "temperature": 0.0, "avg_logprob": -0.13048129215418736, "compression_ratio": 1.6477272727272727, "no_speech_prob": 7.183150046330411e-06}, {"id": 706, "seek": 317782, "start": 3189.86, "end": 3194.34, "text": " But it is important to find these things out.", "tokens": [583, 309, 307, 1021, 281, 915, 613, 721, 484, 13], "temperature": 0.0, "avg_logprob": -0.13048129215418736, "compression_ratio": 1.6477272727272727, "no_speech_prob": 7.183150046330411e-06}, {"id": 707, "seek": 317782, "start": 3194.34, "end": 3203.1400000000003, "text": " As we'll discuss in the next lesson, we'll talk a lot about neural net architecture details.", "tokens": [1018, 321, 603, 2248, 294, 264, 958, 6898, 11, 321, 603, 751, 257, 688, 466, 18161, 2533, 9482, 4365, 13], "temperature": 0.0, "avg_logprob": -0.13048129215418736, "compression_ratio": 1.6477272727272727, "no_speech_prob": 7.183150046330411e-06}, {"id": 708, "seek": 317782, "start": 3203.1400000000003, "end": 3207.54, "text": " But the details we'll focus on are what happens to the inputs at the very first stage and", "tokens": [583, 264, 4365, 321, 603, 1879, 322, 366, 437, 2314, 281, 264, 15743, 412, 264, 588, 700, 3233, 293], "temperature": 0.0, "avg_logprob": -0.13048129215418736, "compression_ratio": 1.6477272727272727, "no_speech_prob": 7.183150046330411e-06}, {"id": 709, "seek": 320754, "start": 3207.54, "end": 3210.66, "text": " what happens to the outputs at the very last stage.", "tokens": [437, 2314, 281, 264, 23930, 412, 264, 588, 1036, 3233, 13], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 710, "seek": 320754, "start": 3210.66, "end": 3213.38, "text": " We'll talk a bit about what happens in the middle, but a lot less.", "tokens": [492, 603, 751, 257, 857, 466, 437, 2314, 294, 264, 2808, 11, 457, 257, 688, 1570, 13], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 711, "seek": 320754, "start": 3213.38, "end": 3218.18, "text": " And the reason why is it's the things that you put into the inputs that's going to change", "tokens": [400, 264, 1778, 983, 307, 309, 311, 264, 721, 300, 291, 829, 666, 264, 15743, 300, 311, 516, 281, 1319], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 712, "seek": 320754, "start": 3218.18, "end": 3220.86, "text": " for every single data set you do.", "tokens": [337, 633, 2167, 1412, 992, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 713, "seek": 320754, "start": 3220.86, "end": 3222.5, "text": " And what do you want to happen to the outputs?", "tokens": [400, 437, 360, 291, 528, 281, 1051, 281, 264, 23930, 30], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 714, "seek": 320754, "start": 3222.5, "end": 3227.6, "text": " It's just going to happen for every different target that you're trying to hit.", "tokens": [467, 311, 445, 516, 281, 1051, 337, 633, 819, 3779, 300, 291, 434, 1382, 281, 2045, 13], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 715, "seek": 320754, "start": 3227.6, "end": 3230.86, "text": " So those are the things that you actually need to know about.", "tokens": [407, 729, 366, 264, 721, 300, 291, 767, 643, 281, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 716, "seek": 320754, "start": 3230.86, "end": 3234.66, "text": " So for example, this thing of like, well, you need to know about the sigmoid function", "tokens": [407, 337, 1365, 11, 341, 551, 295, 411, 11, 731, 11, 291, 643, 281, 458, 466, 264, 4556, 3280, 327, 2445], "temperature": 0.0, "avg_logprob": -0.10967155063853544, "compression_ratio": 1.9077490774907748, "no_speech_prob": 2.586586015240755e-05}, {"id": 717, "seek": 323466, "start": 3234.66, "end": 3237.94, "text": " and you need to know that you need to use it.", "tokens": [293, 291, 643, 281, 458, 300, 291, 643, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 718, "seek": 323466, "start": 3237.94, "end": 3241.96, "text": " Fast AI is very good at handling this for you.", "tokens": [15968, 7318, 307, 588, 665, 412, 13175, 341, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 719, "seek": 323466, "start": 3241.96, "end": 3244.42, "text": " That's why we haven't had to talk about it much until now.", "tokens": [663, 311, 983, 321, 2378, 380, 632, 281, 751, 466, 309, 709, 1826, 586, 13], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 720, "seek": 323466, "start": 3244.42, "end": 3249.3799999999997, "text": " If you say, oh, it's a category block dependent variable, it's going to use the right kind", "tokens": [759, 291, 584, 11, 1954, 11, 309, 311, 257, 7719, 3461, 12334, 7006, 11, 309, 311, 516, 281, 764, 264, 558, 733], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 721, "seek": 323466, "start": 3249.3799999999997, "end": 3251.8199999999997, "text": " of thing for you.", "tokens": [295, 551, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 722, "seek": 323466, "start": 3251.8199999999997, "end": 3255.2999999999997, "text": " But most things are not so convenient.", "tokens": [583, 881, 721, 366, 406, 370, 10851, 13], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 723, "seek": 323466, "start": 3255.2999999999997, "end": 3258.5, "text": " John, is there a question?", "tokens": [2619, 11, 307, 456, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 724, "seek": 323466, "start": 3258.5, "end": 3259.7599999999998, "text": " Yes, there is.", "tokens": [1079, 11, 456, 307, 13], "temperature": 0.0, "avg_logprob": -0.17555346072298808, "compression_ratio": 1.5429864253393666, "no_speech_prob": 9.222797416441608e-06}, {"id": 725, "seek": 325976, "start": 3259.76, "end": 3266.34, "text": " It's back in the feature engineering topic, but a couple of people have liked it, so I", "tokens": [467, 311, 646, 294, 264, 4111, 7043, 4829, 11, 457, 257, 1916, 295, 561, 362, 4501, 309, 11, 370, 286], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 726, "seek": 325976, "start": 3266.34, "end": 3268.34, "text": " thought we'd put it out there.", "tokens": [1194, 321, 1116, 829, 309, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 727, "seek": 325976, "start": 3268.34, "end": 3274.7400000000002, "text": " So Shivam says, one concern I have while using get dummies, so it's in that get dummies phase,", "tokens": [407, 47839, 335, 1619, 11, 472, 3136, 286, 362, 1339, 1228, 483, 16784, 38374, 11, 370, 309, 311, 294, 300, 483, 16784, 38374, 5574, 11], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 728, "seek": 325976, "start": 3274.7400000000002, "end": 3277.94, "text": " is what happens while using test data?", "tokens": [307, 437, 2314, 1339, 1228, 1500, 1412, 30], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 729, "seek": 325976, "start": 3277.94, "end": 3283.2200000000003, "text": " I have a new category, let's say male, female, and other, and this will have an extra column", "tokens": [286, 362, 257, 777, 7719, 11, 718, 311, 584, 7133, 11, 6556, 11, 293, 661, 11, 293, 341, 486, 362, 364, 2857, 7738], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 730, "seek": 325976, "start": 3283.2200000000003, "end": 3286.3, "text": " missing from the training data.", "tokens": [5361, 490, 264, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 731, "seek": 325976, "start": 3286.3, "end": 3288.2000000000003, "text": " How do you take care of that?", "tokens": [1012, 360, 291, 747, 1127, 295, 300, 30], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 732, "seek": 325976, "start": 3288.2000000000003, "end": 3289.2000000000003, "text": " That's a great question.", "tokens": [663, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18039484493068006, "compression_ratio": 1.638783269961977, "no_speech_prob": 4.7528021241305396e-05}, {"id": 733, "seek": 328920, "start": 3289.2, "end": 3297.8199999999997, "text": " Yeah, so normally you've got to think about this pretty carefully and check pretty carefully,", "tokens": [865, 11, 370, 5646, 291, 600, 658, 281, 519, 466, 341, 1238, 7500, 293, 1520, 1238, 7500, 11], "temperature": 0.0, "avg_logprob": -0.1824253483822471, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.1478205124149099e-05}, {"id": 734, "seek": 328920, "start": 3297.8199999999997, "end": 3299.74, "text": " unless you use fast AI.", "tokens": [5969, 291, 764, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.1824253483822471, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.1478205124149099e-05}, {"id": 735, "seek": 328920, "start": 3299.74, "end": 3307.2599999999998, "text": " So fast AI always creates an extra category called other, and at test time, inference", "tokens": [407, 2370, 7318, 1009, 7829, 364, 2857, 7719, 1219, 661, 11, 293, 412, 1500, 565, 11, 38253], "temperature": 0.0, "avg_logprob": -0.1824253483822471, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.1478205124149099e-05}, {"id": 736, "seek": 328920, "start": 3307.2599999999998, "end": 3312.54, "text": " time, if you have some level that didn't exist before, we put it into the other category", "tokens": [565, 11, 498, 291, 362, 512, 1496, 300, 994, 380, 2514, 949, 11, 321, 829, 309, 666, 264, 661, 7719], "temperature": 0.0, "avg_logprob": -0.1824253483822471, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.1478205124149099e-05}, {"id": 737, "seek": 328920, "start": 3312.54, "end": 3314.1, "text": " for you.", "tokens": [337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1824253483822471, "compression_ratio": 1.5677083333333333, "no_speech_prob": 1.1478205124149099e-05}, {"id": 738, "seek": 331410, "start": 3314.1, "end": 3322.7799999999997, "text": " Otherwise, you basically have to do that yourself, or at least check.", "tokens": [10328, 11, 291, 1936, 362, 281, 360, 300, 1803, 11, 420, 412, 1935, 1520, 13], "temperature": 0.0, "avg_logprob": -0.1496172192730481, "compression_ratio": 1.5707070707070707, "no_speech_prob": 6.338885214063339e-06}, {"id": 739, "seek": 331410, "start": 3322.7799999999997, "end": 3331.18, "text": " Generally speaking, it's pretty likely that otherwise your extra level will be silently", "tokens": [21082, 4124, 11, 309, 311, 1238, 3700, 300, 5911, 428, 2857, 1496, 486, 312, 40087], "temperature": 0.0, "avg_logprob": -0.1496172192730481, "compression_ratio": 1.5707070707070707, "no_speech_prob": 6.338885214063339e-06}, {"id": 740, "seek": 331410, "start": 3331.18, "end": 3335.58, "text": " ignored because it's going to be in the data set, but it's not going to be matched to a", "tokens": [19735, 570, 309, 311, 516, 281, 312, 294, 264, 1412, 992, 11, 457, 309, 311, 406, 516, 281, 312, 21447, 281, 257], "temperature": 0.0, "avg_logprob": -0.1496172192730481, "compression_ratio": 1.5707070707070707, "no_speech_prob": 6.338885214063339e-06}, {"id": 741, "seek": 331410, "start": 3335.58, "end": 3336.58, "text": " column.", "tokens": [7738, 13], "temperature": 0.0, "avg_logprob": -0.1496172192730481, "compression_ratio": 1.5707070707070707, "no_speech_prob": 6.338885214063339e-06}, {"id": 742, "seek": 331410, "start": 3336.58, "end": 3340.8199999999997, "text": " So yeah, it's a good point and definitely worth checking.", "tokens": [407, 1338, 11, 309, 311, 257, 665, 935, 293, 2138, 3163, 8568, 13], "temperature": 0.0, "avg_logprob": -0.1496172192730481, "compression_ratio": 1.5707070707070707, "no_speech_prob": 6.338885214063339e-06}, {"id": 743, "seek": 334082, "start": 3340.82, "end": 3345.2200000000003, "text": " For categorical variables with lots of levels, I actually normally like to put the less common", "tokens": [1171, 19250, 804, 9102, 365, 3195, 295, 4358, 11, 286, 767, 5646, 411, 281, 829, 264, 1570, 2689], "temperature": 0.0, "avg_logprob": -0.2004554271697998, "compression_ratio": 1.5324675324675325, "no_speech_prob": 2.8855678465333767e-05}, {"id": 744, "seek": 334082, "start": 3345.2200000000003, "end": 3351.26, "text": " ones into an other category, and again, that's something that fast AI will do for you automatically.", "tokens": [2306, 666, 364, 661, 7719, 11, 293, 797, 11, 300, 311, 746, 300, 2370, 7318, 486, 360, 337, 291, 6772, 13], "temperature": 0.0, "avg_logprob": -0.2004554271697998, "compression_ratio": 1.5324675324675325, "no_speech_prob": 2.8855678465333767e-05}, {"id": 745, "seek": 334082, "start": 3351.26, "end": 3357.1800000000003, "text": " But yeah, definitely something to keep an eye out for.", "tokens": [583, 1338, 11, 2138, 746, 281, 1066, 364, 3313, 484, 337, 13], "temperature": 0.0, "avg_logprob": -0.2004554271697998, "compression_ratio": 1.5324675324675325, "no_speech_prob": 2.8855678465333767e-05}, {"id": 746, "seek": 334082, "start": 3357.1800000000003, "end": 3360.2200000000003, "text": " Good question.", "tokens": [2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2004554271697998, "compression_ratio": 1.5324675324675325, "no_speech_prob": 2.8855678465333767e-05}, {"id": 747, "seek": 334082, "start": 3360.2200000000003, "end": 3366.94, "text": " Okay, so before we take our break, we'll just do one last thing, which is we will submit", "tokens": [1033, 11, 370, 949, 321, 747, 527, 1821, 11, 321, 603, 445, 360, 472, 1036, 551, 11, 597, 307, 321, 486, 10315], "temperature": 0.0, "avg_logprob": -0.2004554271697998, "compression_ratio": 1.5324675324675325, "no_speech_prob": 2.8855678465333767e-05}, {"id": 748, "seek": 336694, "start": 3366.94, "end": 3371.7400000000002, "text": " this to Kaggle because I think it's quite cool that we have successfully built a model", "tokens": [341, 281, 48751, 22631, 570, 286, 519, 309, 311, 1596, 1627, 300, 321, 362, 10727, 3094, 257, 2316], "temperature": 0.0, "avg_logprob": -0.1555517851704299, "compression_ratio": 1.5296610169491525, "no_speech_prob": 2.2472613636637107e-05}, {"id": 749, "seek": 336694, "start": 3371.7400000000002, "end": 3373.54, "text": " from scratch.", "tokens": [490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.1555517851704299, "compression_ratio": 1.5296610169491525, "no_speech_prob": 2.2472613636637107e-05}, {"id": 750, "seek": 336694, "start": 3373.54, "end": 3378.46, "text": " So Kaggle provides us with a test.csv, which is exactly the same structure as the training", "tokens": [407, 48751, 22631, 6417, 505, 365, 257, 1500, 13, 14368, 85, 11, 597, 307, 2293, 264, 912, 3877, 382, 264, 3097], "temperature": 0.0, "avg_logprob": -0.1555517851704299, "compression_ratio": 1.5296610169491525, "no_speech_prob": 2.2472613636637107e-05}, {"id": 751, "seek": 336694, "start": 3378.46, "end": 3382.78, "text": " csv, except that it doesn't have a survived column.", "tokens": [28277, 85, 11, 3993, 300, 309, 1177, 380, 362, 257, 14433, 7738, 13], "temperature": 0.0, "avg_logprob": -0.1555517851704299, "compression_ratio": 1.5296610169491525, "no_speech_prob": 2.2472613636637107e-05}, {"id": 752, "seek": 336694, "start": 3382.78, "end": 3389.78, "text": " Now, interestingly, when I tried to submit to Kaggle, I got an error in my code saying", "tokens": [823, 11, 25873, 11, 562, 286, 3031, 281, 10315, 281, 48751, 22631, 11, 286, 658, 364, 6713, 294, 452, 3089, 1566], "temperature": 0.0, "avg_logprob": -0.1555517851704299, "compression_ratio": 1.5296610169491525, "no_speech_prob": 2.2472613636637107e-05}, {"id": 753, "seek": 336694, "start": 3389.78, "end": 3393.2200000000003, "text": " that one of my fairs is empty.", "tokens": [300, 472, 295, 452, 3143, 82, 307, 6707, 13], "temperature": 0.0, "avg_logprob": -0.1555517851704299, "compression_ratio": 1.5296610169491525, "no_speech_prob": 2.2472613636637107e-05}, {"id": 754, "seek": 339322, "start": 3393.22, "end": 3398.54, "text": " So that was interesting because the training set doesn't have any empty fairs.", "tokens": [407, 300, 390, 1880, 570, 264, 3097, 992, 1177, 380, 362, 604, 6707, 3143, 82, 13], "temperature": 0.0, "avg_logprob": -0.09984980689154731, "compression_ratio": 1.7445887445887447, "no_speech_prob": 1.922105366247706e-05}, {"id": 755, "seek": 339322, "start": 3398.54, "end": 3403.1, "text": " So sometimes this will happen that the training set and the test set have different things", "tokens": [407, 2171, 341, 486, 1051, 300, 264, 3097, 992, 293, 264, 1500, 992, 362, 819, 721], "temperature": 0.0, "avg_logprob": -0.09984980689154731, "compression_ratio": 1.7445887445887447, "no_speech_prob": 1.922105366247706e-05}, {"id": 756, "seek": 339322, "start": 3403.1, "end": 3404.18, "text": " to deal with.", "tokens": [281, 2028, 365, 13], "temperature": 0.0, "avg_logprob": -0.09984980689154731, "compression_ratio": 1.7445887445887447, "no_speech_prob": 1.922105366247706e-05}, {"id": 757, "seek": 339322, "start": 3404.18, "end": 3406.1, "text": " So in this case, I just said, oh, there's only one row.", "tokens": [407, 294, 341, 1389, 11, 286, 445, 848, 11, 1954, 11, 456, 311, 787, 472, 5386, 13], "temperature": 0.0, "avg_logprob": -0.09984980689154731, "compression_ratio": 1.7445887445887447, "no_speech_prob": 1.922105366247706e-05}, {"id": 758, "seek": 339322, "start": 3406.1, "end": 3407.1, "text": " I don't care.", "tokens": [286, 500, 380, 1127, 13], "temperature": 0.0, "avg_logprob": -0.09984980689154731, "compression_ratio": 1.7445887445887447, "no_speech_prob": 1.922105366247706e-05}, {"id": 759, "seek": 339322, "start": 3407.1, "end": 3413.02, "text": " So I just replaced the empty one with a zero for fair.", "tokens": [407, 286, 445, 10772, 264, 6707, 472, 365, 257, 4018, 337, 3143, 13], "temperature": 0.0, "avg_logprob": -0.09984980689154731, "compression_ratio": 1.7445887445887447, "no_speech_prob": 1.922105366247706e-05}, {"id": 760, "seek": 339322, "start": 3413.02, "end": 3421.7799999999997, "text": " So then I just copied and pasted the preprocessing steps from my training data frame and stuck", "tokens": [407, 550, 286, 445, 25365, 293, 1791, 292, 264, 2666, 340, 780, 278, 4439, 490, 452, 3097, 1412, 3920, 293, 5541], "temperature": 0.0, "avg_logprob": -0.09984980689154731, "compression_ratio": 1.7445887445887447, "no_speech_prob": 1.922105366247706e-05}, {"id": 761, "seek": 342178, "start": 3421.78, "end": 3426.5, "text": " them here for the test data frame and the normalization as well.", "tokens": [552, 510, 337, 264, 1500, 1412, 3920, 293, 264, 2710, 2144, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.20773379677220394, "compression_ratio": 1.51528384279476, "no_speech_prob": 9.223207598552108e-06}, {"id": 762, "seek": 342178, "start": 3426.5, "end": 3428.8, "text": " And so now I just call cat pretz.", "tokens": [400, 370, 586, 286, 445, 818, 3857, 1162, 89, 13], "temperature": 0.0, "avg_logprob": -0.20773379677220394, "compression_ratio": 1.51528384279476, "no_speech_prob": 9.223207598552108e-06}, {"id": 763, "seek": 342178, "start": 3428.8, "end": 3432.02, "text": " Is it greater than 0.5?", "tokens": [1119, 309, 5044, 813, 1958, 13, 20, 30], "temperature": 0.0, "avg_logprob": -0.20773379677220394, "compression_ratio": 1.51528384279476, "no_speech_prob": 9.223207598552108e-06}, {"id": 764, "seek": 342178, "start": 3432.02, "end": 3435.94, "text": " Turn it into a zero or one because that's what Kaggle expects and put that into the", "tokens": [7956, 309, 666, 257, 4018, 420, 472, 570, 300, 311, 437, 48751, 22631, 33280, 293, 829, 300, 666, 264], "temperature": 0.0, "avg_logprob": -0.20773379677220394, "compression_ratio": 1.51528384279476, "no_speech_prob": 9.223207598552108e-06}, {"id": 765, "seek": 342178, "start": 3435.94, "end": 3440.7400000000002, "text": " survived column, which previously, remember, didn't exist.", "tokens": [14433, 7738, 11, 597, 8046, 11, 1604, 11, 994, 380, 2514, 13], "temperature": 0.0, "avg_logprob": -0.20773379677220394, "compression_ratio": 1.51528384279476, "no_speech_prob": 9.223207598552108e-06}, {"id": 766, "seek": 342178, "start": 3440.7400000000002, "end": 3447.5400000000004, "text": " So then finally, I created data frame with just the two columns, ID and survived.", "tokens": [407, 550, 2721, 11, 286, 2942, 1412, 3920, 365, 445, 264, 732, 13766, 11, 7348, 293, 14433, 13], "temperature": 0.0, "avg_logprob": -0.20773379677220394, "compression_ratio": 1.51528384279476, "no_speech_prob": 9.223207598552108e-06}, {"id": 767, "seek": 344754, "start": 3447.54, "end": 3452.22, "text": " Take it in a CSV file and then I can call the Unix command head just to look at the", "tokens": [3664, 309, 294, 257, 48814, 3991, 293, 550, 286, 393, 818, 264, 1156, 970, 5622, 1378, 445, 281, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 768, "seek": 344754, "start": 3452.22, "end": 3454.1, "text": " first few rows.", "tokens": [700, 1326, 13241, 13], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 769, "seek": 344754, "start": 3454.1, "end": 3459.62, "text": " And if you look at the Kaggle competitions data page, you'll see this is what the submission", "tokens": [400, 498, 291, 574, 412, 264, 48751, 22631, 26185, 1412, 3028, 11, 291, 603, 536, 341, 307, 437, 264, 23689], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 770, "seek": 344754, "start": 3459.62, "end": 3461.14, "text": " file is expected to look like.", "tokens": [3991, 307, 5176, 281, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 771, "seek": 344754, "start": 3461.14, "end": 3462.5, "text": " So that made me feel good.", "tokens": [407, 300, 1027, 385, 841, 665, 13], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 772, "seek": 344754, "start": 3462.5, "end": 3465.2599999999998, "text": " So I went ahead and submitted it.", "tokens": [407, 286, 1437, 2286, 293, 14405, 309, 13], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 773, "seek": 344754, "start": 3465.2599999999998, "end": 3467.22, "text": " I didn't mention it.", "tokens": [286, 994, 380, 2152, 309, 13], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 774, "seek": 344754, "start": 3467.22, "end": 3468.22, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 775, "seek": 344754, "start": 3468.22, "end": 3472.7799999999997, "text": " So anyway, I submitted it and I remember I got like, I think I was basically right in", "tokens": [407, 4033, 11, 286, 14405, 309, 293, 286, 1604, 286, 658, 411, 11, 286, 519, 286, 390, 1936, 558, 294], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 776, "seek": 344754, "start": 3472.7799999999997, "end": 3477.2599999999998, "text": " the middle, about 50%, you know, better than half the people who have entered the competition,", "tokens": [264, 2808, 11, 466, 2625, 8923, 291, 458, 11, 1101, 813, 1922, 264, 561, 567, 362, 9065, 264, 6211, 11], "temperature": 0.0, "avg_logprob": -0.1710638338631957, "compression_ratio": 1.6565656565656566, "no_speech_prob": 1.9524301023920998e-05}, {"id": 777, "seek": 347726, "start": 3477.26, "end": 3478.78, "text": " worse than half the people.", "tokens": [5324, 813, 1922, 264, 561, 13], "temperature": 0.0, "avg_logprob": -0.21545623184798599, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.9618058660125826e-06}, {"id": 778, "seek": 347726, "start": 3478.78, "end": 3484.42, "text": " So you know, solid middle of the pack result for a linear model from scratch, I think is", "tokens": [407, 291, 458, 11, 5100, 2808, 295, 264, 2844, 1874, 337, 257, 8213, 2316, 490, 8459, 11, 286, 519, 307], "temperature": 0.0, "avg_logprob": -0.21545623184798599, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.9618058660125826e-06}, {"id": 779, "seek": 347726, "start": 3484.42, "end": 3485.42, "text": " a pretty good result.", "tokens": [257, 1238, 665, 1874, 13], "temperature": 0.0, "avg_logprob": -0.21545623184798599, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.9618058660125826e-06}, {"id": 780, "seek": 347726, "start": 3485.42, "end": 3487.76, "text": " So that's a great place to start.", "tokens": [407, 300, 311, 257, 869, 1081, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.21545623184798599, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.9618058660125826e-06}, {"id": 781, "seek": 347726, "start": 3487.76, "end": 3489.6200000000003, "text": " So let's take a 10 minute break.", "tokens": [407, 718, 311, 747, 257, 1266, 3456, 1821, 13], "temperature": 0.0, "avg_logprob": -0.21545623184798599, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.9618058660125826e-06}, {"id": 782, "seek": 347726, "start": 3489.6200000000003, "end": 3501.2200000000003, "text": " We'll come back at 7.17 and continue on our journey.", "tokens": [492, 603, 808, 646, 412, 1614, 13, 7773, 293, 2354, 322, 527, 4671, 13], "temperature": 0.0, "avg_logprob": -0.21545623184798599, "compression_ratio": 1.4333333333333333, "no_speech_prob": 6.9618058660125826e-06}, {"id": 783, "seek": 350122, "start": 3501.22, "end": 3509.4199999999996, "text": " All right, welcome back.", "tokens": [1057, 558, 11, 2928, 646, 13], "temperature": 0.0, "avg_logprob": -0.41025764063784953, "compression_ratio": 1.3076923076923077, "no_speech_prob": 6.400876736734062e-05}, {"id": 784, "seek": 350122, "start": 3509.4199999999996, "end": 3522.18, "text": " You might remember from Excel that after we did the sum product version, we then replaced", "tokens": [509, 1062, 1604, 490, 19060, 300, 934, 321, 630, 264, 2408, 1674, 3037, 11, 321, 550, 10772], "temperature": 0.0, "avg_logprob": -0.41025764063784953, "compression_ratio": 1.3076923076923077, "no_speech_prob": 6.400876736734062e-05}, {"id": 785, "seek": 350122, "start": 3522.18, "end": 3524.18, "text": " it with a matrix model play.", "tokens": [309, 365, 257, 8141, 2316, 862, 13], "temperature": 0.0, "avg_logprob": -0.41025764063784953, "compression_ratio": 1.3076923076923077, "no_speech_prob": 6.400876736734062e-05}, {"id": 786, "seek": 350122, "start": 3524.18, "end": 3526.2999999999997, "text": " Wait, not there.", "tokens": [3802, 11, 406, 456, 13], "temperature": 0.0, "avg_logprob": -0.41025764063784953, "compression_ratio": 1.3076923076923077, "no_speech_prob": 6.400876736734062e-05}, {"id": 787, "seek": 350122, "start": 3526.2999999999997, "end": 3528.3399999999997, "text": " Must be here.", "tokens": [13252, 312, 510, 13], "temperature": 0.0, "avg_logprob": -0.41025764063784953, "compression_ratio": 1.3076923076923077, "no_speech_prob": 6.400876736734062e-05}, {"id": 788, "seek": 350122, "start": 3528.3399999999997, "end": 3530.3399999999997, "text": " Here we are.", "tokens": [1692, 321, 366, 13], "temperature": 0.0, "avg_logprob": -0.41025764063784953, "compression_ratio": 1.3076923076923077, "no_speech_prob": 6.400876736734062e-05}, {"id": 789, "seek": 353034, "start": 3530.34, "end": 3533.1000000000004, "text": " We're the matrix model play.", "tokens": [492, 434, 264, 8141, 2316, 862, 13], "temperature": 0.0, "avg_logprob": -0.14083341650060704, "compression_ratio": 1.7195121951219512, "no_speech_prob": 1.670070741965901e-05}, {"id": 790, "seek": 353034, "start": 3533.1000000000004, "end": 3535.02, "text": " So let's do that step now.", "tokens": [407, 718, 311, 360, 300, 1823, 586, 13], "temperature": 0.0, "avg_logprob": -0.14083341650060704, "compression_ratio": 1.7195121951219512, "no_speech_prob": 1.670070741965901e-05}, {"id": 791, "seek": 353034, "start": 3535.02, "end": 3546.1000000000004, "text": " So matrix times vector dot sum over axis equals one is the same thing as matrix model play.", "tokens": [407, 8141, 1413, 8062, 5893, 2408, 670, 10298, 6915, 472, 307, 264, 912, 551, 382, 8141, 2316, 862, 13], "temperature": 0.0, "avg_logprob": -0.14083341650060704, "compression_ratio": 1.7195121951219512, "no_speech_prob": 1.670070741965901e-05}, {"id": 792, "seek": 353034, "start": 3546.1000000000004, "end": 3550.2200000000003, "text": " So here is the times dot sum version.", "tokens": [407, 510, 307, 264, 1413, 5893, 2408, 3037, 13], "temperature": 0.0, "avg_logprob": -0.14083341650060704, "compression_ratio": 1.7195121951219512, "no_speech_prob": 1.670070741965901e-05}, {"id": 793, "seek": 353034, "start": 3550.2200000000003, "end": 3556.4, "text": " Now we can't use this character for a matrix model play because it means element wise operation.", "tokens": [823, 321, 393, 380, 764, 341, 2517, 337, 257, 8141, 2316, 862, 570, 309, 1355, 4478, 10829, 6916, 13], "temperature": 0.0, "avg_logprob": -0.14083341650060704, "compression_ratio": 1.7195121951219512, "no_speech_prob": 1.670070741965901e-05}, {"id": 794, "seek": 355640, "start": 3556.4, "end": 3562.6600000000003, "text": " All of the times plus minus divide in pie torch numpy mean element wise.", "tokens": [1057, 295, 264, 1413, 1804, 3175, 9845, 294, 1730, 27822, 1031, 8200, 914, 4478, 10829, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 795, "seek": 355640, "start": 3562.6600000000003, "end": 3566.54, "text": " So corresponding elements.", "tokens": [407, 11760, 4959, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 796, "seek": 355640, "start": 3566.54, "end": 3568.6600000000003, "text": " So in Python instead we use this character.", "tokens": [407, 294, 15329, 2602, 321, 764, 341, 2517, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 797, "seek": 355640, "start": 3568.6600000000003, "end": 3570.7000000000003, "text": " As far as I know, it's pretty arbitrary.", "tokens": [1018, 1400, 382, 286, 458, 11, 309, 311, 1238, 23211, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 798, "seek": 355640, "start": 3570.7000000000003, "end": 3574.38, "text": " It's one of the ones that wasn't used.", "tokens": [467, 311, 472, 295, 264, 2306, 300, 2067, 380, 1143, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 799, "seek": 355640, "start": 3574.38, "end": 3575.7400000000002, "text": " So that is an official Python.", "tokens": [407, 300, 307, 364, 4783, 15329, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 800, "seek": 355640, "start": 3575.7400000000002, "end": 3577.6600000000003, "text": " It's a bit unusual.", "tokens": [467, 311, 257, 857, 10901, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 801, "seek": 355640, "start": 3577.6600000000003, "end": 3578.98, "text": " It's an official Python operator.", "tokens": [467, 311, 364, 4783, 15329, 12973, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 802, "seek": 355640, "start": 3578.98, "end": 3580.78, "text": " It means matrix model play.", "tokens": [467, 1355, 8141, 2316, 862, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 803, "seek": 355640, "start": 3580.78, "end": 3583.48, "text": " But Python doesn't come with an implementation of it.", "tokens": [583, 15329, 1177, 380, 808, 365, 364, 11420, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.26361647519198333, "compression_ratio": 1.652542372881356, "no_speech_prob": 9.080268682737369e-06}, {"id": 804, "seek": 358348, "start": 3583.48, "end": 3587.78, "text": " So because we've imported, because these are tensors, and in pie torch it will use pie", "tokens": [407, 570, 321, 600, 25524, 11, 570, 613, 366, 10688, 830, 11, 293, 294, 1730, 27822, 309, 486, 764, 1730], "temperature": 0.0, "avg_logprob": -0.1908017938787287, "compression_ratio": 1.5857740585774058, "no_speech_prob": 1.7231011952389963e-05}, {"id": 805, "seek": 358348, "start": 3587.78, "end": 3589.38, "text": " torches.", "tokens": [3930, 3781, 13], "temperature": 0.0, "avg_logprob": -0.1908017938787287, "compression_ratio": 1.5857740585774058, "no_speech_prob": 1.7231011952389963e-05}, {"id": 806, "seek": 358348, "start": 3589.38, "end": 3592.14, "text": " And as you can see, they're exactly the same.", "tokens": [400, 382, 291, 393, 536, 11, 436, 434, 2293, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1908017938787287, "compression_ratio": 1.5857740585774058, "no_speech_prob": 1.7231011952389963e-05}, {"id": 807, "seek": 358348, "start": 3592.14, "end": 3595.58, "text": " So we can now just simplify a little bit what we had before.", "tokens": [407, 321, 393, 586, 445, 20460, 257, 707, 857, 437, 321, 632, 949, 13], "temperature": 0.0, "avg_logprob": -0.1908017938787287, "compression_ratio": 1.5857740585774058, "no_speech_prob": 1.7231011952389963e-05}, {"id": 808, "seek": 358348, "start": 3595.58, "end": 3601.42, "text": " Calc preds is now torch dot sigmoid of the matrix model play.", "tokens": [3511, 66, 3852, 82, 307, 586, 27822, 5893, 4556, 3280, 327, 295, 264, 8141, 2316, 862, 13], "temperature": 0.0, "avg_logprob": -0.1908017938787287, "compression_ratio": 1.5857740585774058, "no_speech_prob": 1.7231011952389963e-05}, {"id": 809, "seek": 358348, "start": 3601.42, "end": 3606.66, "text": " Now there is one thing I'd like to move towards now is that we're going to try to create a", "tokens": [823, 456, 307, 472, 551, 286, 1116, 411, 281, 1286, 3030, 586, 307, 300, 321, 434, 516, 281, 853, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.1908017938787287, "compression_ratio": 1.5857740585774058, "no_speech_prob": 1.7231011952389963e-05}, {"id": 810, "seek": 358348, "start": 3606.66, "end": 3609.18, "text": " neural net in a moment.", "tokens": [18161, 2533, 294, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.1908017938787287, "compression_ratio": 1.5857740585774058, "no_speech_prob": 1.7231011952389963e-05}, {"id": 811, "seek": 360918, "start": 3609.18, "end": 3615.4199999999996, "text": " And so that means rather than treat this as a matrix times a vector, I want to treat this", "tokens": [400, 370, 300, 1355, 2831, 813, 2387, 341, 382, 257, 8141, 1413, 257, 8062, 11, 286, 528, 281, 2387, 341], "temperature": 0.0, "avg_logprob": -0.13626536657643873, "compression_ratio": 1.8614457831325302, "no_speech_prob": 2.546383439039346e-05}, {"id": 812, "seek": 360918, "start": 3615.4199999999996, "end": 3618.46, "text": " as a matrix times a matrix.", "tokens": [382, 257, 8141, 1413, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13626536657643873, "compression_ratio": 1.8614457831325302, "no_speech_prob": 2.546383439039346e-05}, {"id": 813, "seek": 360918, "start": 3618.46, "end": 3623.62, "text": " Because we're about to add some more columns of coefficients.", "tokens": [1436, 321, 434, 466, 281, 909, 512, 544, 13766, 295, 31994, 13], "temperature": 0.0, "avg_logprob": -0.13626536657643873, "compression_ratio": 1.8614457831325302, "no_speech_prob": 2.546383439039346e-05}, {"id": 814, "seek": 360918, "start": 3623.62, "end": 3631.7, "text": " So we're going to change in a coefs so that rather than creating an n coef vector, we're", "tokens": [407, 321, 434, 516, 281, 1319, 294, 257, 598, 5666, 82, 370, 300, 2831, 813, 4084, 364, 297, 598, 5666, 8062, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.13626536657643873, "compression_ratio": 1.8614457831325302, "no_speech_prob": 2.546383439039346e-05}, {"id": 815, "seek": 360918, "start": 3631.7, "end": 3637.62, "text": " going to create an n coef by one matrix.", "tokens": [516, 281, 1884, 364, 297, 598, 5666, 538, 472, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13626536657643873, "compression_ratio": 1.8614457831325302, "no_speech_prob": 2.546383439039346e-05}, {"id": 816, "seek": 363762, "start": 3637.62, "end": 3640.58, "text": " So in math we would probably call that a column vector.", "tokens": [407, 294, 5221, 321, 576, 1391, 818, 300, 257, 7738, 8062, 13], "temperature": 0.0, "avg_logprob": -0.1866223787006579, "compression_ratio": 1.6305418719211822, "no_speech_prob": 1.28046958707273e-05}, {"id": 817, "seek": 363762, "start": 3640.58, "end": 3643.54, "text": " But I think that's kind of a dumb name in some ways.", "tokens": [583, 286, 519, 300, 311, 733, 295, 257, 10316, 1315, 294, 512, 2098, 13], "temperature": 0.0, "avg_logprob": -0.1866223787006579, "compression_ratio": 1.6305418719211822, "no_speech_prob": 1.28046958707273e-05}, {"id": 818, "seek": 363762, "start": 3643.54, "end": 3645.16, "text": " Because it's a matrix, right?", "tokens": [1436, 309, 311, 257, 8141, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1866223787006579, "compression_ratio": 1.6305418719211822, "no_speech_prob": 1.28046958707273e-05}, {"id": 819, "seek": 363762, "start": 3645.16, "end": 3651.9, "text": " It's a rank two tensor.", "tokens": [467, 311, 257, 6181, 732, 40863, 13], "temperature": 0.0, "avg_logprob": -0.1866223787006579, "compression_ratio": 1.6305418719211822, "no_speech_prob": 1.28046958707273e-05}, {"id": 820, "seek": 363762, "start": 3651.9, "end": 3656.94, "text": " So the matrix model play will work fine either way.", "tokens": [407, 264, 8141, 2316, 862, 486, 589, 2489, 2139, 636, 13], "temperature": 0.0, "avg_logprob": -0.1866223787006579, "compression_ratio": 1.6305418719211822, "no_speech_prob": 1.28046958707273e-05}, {"id": 821, "seek": 363762, "start": 3656.94, "end": 3662.8599999999997, "text": " But the key difference is that if we do it this way, then the result of the matrix model", "tokens": [583, 264, 2141, 2649, 307, 300, 498, 321, 360, 309, 341, 636, 11, 550, 264, 1874, 295, 264, 8141, 2316], "temperature": 0.0, "avg_logprob": -0.1866223787006579, "compression_ratio": 1.6305418719211822, "no_speech_prob": 1.28046958707273e-05}, {"id": 822, "seek": 363762, "start": 3662.8599999999997, "end": 3665.22, "text": " play will also be a matrix.", "tokens": [862, 486, 611, 312, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1866223787006579, "compression_ratio": 1.6305418719211822, "no_speech_prob": 1.28046958707273e-05}, {"id": 823, "seek": 366522, "start": 3665.22, "end": 3671.06, "text": " It will be again a n rows by one matrix.", "tokens": [467, 486, 312, 797, 257, 297, 13241, 538, 472, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1164697806040446, "compression_ratio": 1.781725888324873, "no_speech_prob": 3.9441008993890136e-05}, {"id": 824, "seek": 366522, "start": 3671.06, "end": 3674.5, "text": " That means when we compare it to the dependent variable, we need the dependent variable to", "tokens": [663, 1355, 562, 321, 6794, 309, 281, 264, 12334, 7006, 11, 321, 643, 264, 12334, 7006, 281], "temperature": 0.0, "avg_logprob": -0.1164697806040446, "compression_ratio": 1.781725888324873, "no_speech_prob": 3.9441008993890136e-05}, {"id": 825, "seek": 366522, "start": 3674.5, "end": 3677.4199999999996, "text": " be an n rows by one matrix as well.", "tokens": [312, 364, 297, 13241, 538, 472, 8141, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1164697806040446, "compression_ratio": 1.781725888324873, "no_speech_prob": 3.9441008993890136e-05}, {"id": 826, "seek": 366522, "start": 3677.4199999999996, "end": 3682.98, "text": " So effectively we need to take the n rows long vector and turn it into an n rows by", "tokens": [407, 8659, 321, 643, 281, 747, 264, 297, 13241, 938, 8062, 293, 1261, 309, 666, 364, 297, 13241, 538], "temperature": 0.0, "avg_logprob": -0.1164697806040446, "compression_ratio": 1.781725888324873, "no_speech_prob": 3.9441008993890136e-05}, {"id": 827, "seek": 366522, "start": 3682.98, "end": 3686.3799999999997, "text": " one matrix.", "tokens": [472, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1164697806040446, "compression_ratio": 1.781725888324873, "no_speech_prob": 3.9441008993890136e-05}, {"id": 828, "seek": 366522, "start": 3686.3799999999997, "end": 3694.22, "text": " So there's some useful, very useful, and at first maybe a bit weird notation in PyTorch", "tokens": [407, 456, 311, 512, 4420, 11, 588, 4420, 11, 293, 412, 700, 1310, 257, 857, 3657, 24657, 294, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.1164697806040446, "compression_ratio": 1.781725888324873, "no_speech_prob": 3.9441008993890136e-05}, {"id": 829, "seek": 369422, "start": 3694.22, "end": 3703.5, "text": " NumPy for this, which is if I take my training dependent variables vector, I index into it", "tokens": [22592, 47, 88, 337, 341, 11, 597, 307, 498, 286, 747, 452, 3097, 12334, 9102, 8062, 11, 286, 8186, 666, 309], "temperature": 0.0, "avg_logprob": -0.16285011224579393, "compression_ratio": 1.3533333333333333, "no_speech_prob": 2.4682383809704334e-05}, {"id": 830, "seek": 369422, "start": 3703.5, "end": 3706.7, "text": " and colon means every row.", "tokens": [293, 8255, 1355, 633, 5386, 13], "temperature": 0.0, "avg_logprob": -0.16285011224579393, "compression_ratio": 1.3533333333333333, "no_speech_prob": 2.4682383809704334e-05}, {"id": 831, "seek": 369422, "start": 3706.7, "end": 3715.14, "text": " So in other words, that just means the whole vector.", "tokens": [407, 294, 661, 2283, 11, 300, 445, 1355, 264, 1379, 8062, 13], "temperature": 0.0, "avg_logprob": -0.16285011224579393, "compression_ratio": 1.3533333333333333, "no_speech_prob": 2.4682383809704334e-05}, {"id": 832, "seek": 369422, "start": 3715.14, "end": 3721.8999999999996, "text": " It's the same basically as that.", "tokens": [467, 311, 264, 912, 1936, 382, 300, 13], "temperature": 0.0, "avg_logprob": -0.16285011224579393, "compression_ratio": 1.3533333333333333, "no_speech_prob": 2.4682383809704334e-05}, {"id": 833, "seek": 372190, "start": 3721.9, "end": 3725.3, "text": " And then I index into a second dimension.", "tokens": [400, 550, 286, 8186, 666, 257, 1150, 10139, 13], "temperature": 0.0, "avg_logprob": -0.09897695717058684, "compression_ratio": 1.7988165680473374, "no_speech_prob": 2.046188456006348e-05}, {"id": 834, "seek": 372190, "start": 3725.3, "end": 3729.06, "text": " Now this doesn't have a second dimension.", "tokens": [823, 341, 1177, 380, 362, 257, 1150, 10139, 13], "temperature": 0.0, "avg_logprob": -0.09897695717058684, "compression_ratio": 1.7988165680473374, "no_speech_prob": 2.046188456006348e-05}, {"id": 835, "seek": 372190, "start": 3729.06, "end": 3732.9, "text": " So there's a special thing you can do, which is if you index into a second dimension with", "tokens": [407, 456, 311, 257, 2121, 551, 291, 393, 360, 11, 597, 307, 498, 291, 8186, 666, 257, 1150, 10139, 365], "temperature": 0.0, "avg_logprob": -0.09897695717058684, "compression_ratio": 1.7988165680473374, "no_speech_prob": 2.046188456006348e-05}, {"id": 836, "seek": 372190, "start": 3732.9, "end": 3739.0, "text": " a special value none, it creates that dimension.", "tokens": [257, 2121, 2158, 6022, 11, 309, 7829, 300, 10139, 13], "temperature": 0.0, "avg_logprob": -0.09897695717058684, "compression_ratio": 1.7988165680473374, "no_speech_prob": 2.046188456006348e-05}, {"id": 837, "seek": 372190, "start": 3739.0, "end": 3748.26, "text": " So this has the effect of adding an extra trailing dimension to train dependence.", "tokens": [407, 341, 575, 264, 1802, 295, 5127, 364, 2857, 944, 4883, 10139, 281, 3847, 31704, 13], "temperature": 0.0, "avg_logprob": -0.09897695717058684, "compression_ratio": 1.7988165680473374, "no_speech_prob": 2.046188456006348e-05}, {"id": 838, "seek": 374826, "start": 3748.26, "end": 3756.0600000000004, "text": " So it turns it from a vector to a matrix with one column.", "tokens": [407, 309, 4523, 309, 490, 257, 8062, 281, 257, 8141, 365, 472, 7738, 13], "temperature": 0.0, "avg_logprob": -0.08362373438748447, "compression_ratio": 1.3904109589041096, "no_speech_prob": 5.422092272056034e-06}, {"id": 839, "seek": 374826, "start": 3756.0600000000004, "end": 3765.86, "text": " So if we look at the shape after that, as you see, it's now got, we call this a unit", "tokens": [407, 498, 321, 574, 412, 264, 3909, 934, 300, 11, 382, 291, 536, 11, 309, 311, 586, 658, 11, 321, 818, 341, 257, 4985], "temperature": 0.0, "avg_logprob": -0.08362373438748447, "compression_ratio": 1.3904109589041096, "no_speech_prob": 5.422092272056034e-06}, {"id": 840, "seek": 374826, "start": 3765.86, "end": 3766.86, "text": " axis.", "tokens": [10298, 13], "temperature": 0.0, "avg_logprob": -0.08362373438748447, "compression_ratio": 1.3904109589041096, "no_speech_prob": 5.422092272056034e-06}, {"id": 841, "seek": 374826, "start": 3766.86, "end": 3772.42, "text": " It's got a trailing unit axis, 713 rows in one column.", "tokens": [467, 311, 658, 257, 944, 4883, 4985, 10298, 11, 1614, 7668, 13241, 294, 472, 7738, 13], "temperature": 0.0, "avg_logprob": -0.08362373438748447, "compression_ratio": 1.3904109589041096, "no_speech_prob": 5.422092272056034e-06}, {"id": 842, "seek": 377242, "start": 3772.42, "end": 3788.5, "text": " So now if we train our model, we'll get coefficients just like before, except that it's now a column", "tokens": [407, 586, 498, 321, 3847, 527, 2316, 11, 321, 603, 483, 31994, 445, 411, 949, 11, 3993, 300, 309, 311, 586, 257, 7738], "temperature": 0.0, "avg_logprob": -0.17858282157352992, "compression_ratio": 1.3443708609271523, "no_speech_prob": 3.089458914473653e-06}, {"id": 843, "seek": 377242, "start": 3788.5, "end": 3795.1800000000003, "text": " vector also known as a rank-2 matrix with a trailing unit axis.", "tokens": [8062, 611, 2570, 382, 257, 6181, 12, 17, 8141, 365, 257, 944, 4883, 4985, 10298, 13], "temperature": 0.0, "avg_logprob": -0.17858282157352992, "compression_ratio": 1.3443708609271523, "no_speech_prob": 3.089458914473653e-06}, {"id": 844, "seek": 377242, "start": 3795.1800000000003, "end": 3798.7400000000002, "text": " Okay, so that hasn't changed anything.", "tokens": [1033, 11, 370, 300, 6132, 380, 3105, 1340, 13], "temperature": 0.0, "avg_logprob": -0.17858282157352992, "compression_ratio": 1.3443708609271523, "no_speech_prob": 3.089458914473653e-06}, {"id": 845, "seek": 379874, "start": 3798.74, "end": 3803.8999999999996, "text": " It's just repeated what we did in the previous section, but it's kind of set us up to expand.", "tokens": [467, 311, 445, 10477, 437, 321, 630, 294, 264, 3894, 3541, 11, 457, 309, 311, 733, 295, 992, 505, 493, 281, 5268, 13], "temperature": 0.0, "avg_logprob": -0.13970640593884037, "compression_ratio": 1.6092436974789917, "no_speech_prob": 2.7107838832307607e-05}, {"id": 846, "seek": 379874, "start": 3803.8999999999996, "end": 3808.3599999999997, "text": " Because now that we've done this using matrix model play, we can go crazy and we can go", "tokens": [1436, 586, 300, 321, 600, 1096, 341, 1228, 8141, 2316, 862, 11, 321, 393, 352, 3219, 293, 321, 393, 352], "temperature": 0.0, "avg_logprob": -0.13970640593884037, "compression_ratio": 1.6092436974789917, "no_speech_prob": 2.7107838832307607e-05}, {"id": 847, "seek": 379874, "start": 3808.3599999999997, "end": 3812.1, "text": " ahead and create a neural network.", "tokens": [2286, 293, 1884, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.13970640593884037, "compression_ratio": 1.6092436974789917, "no_speech_prob": 2.7107838832307607e-05}, {"id": 848, "seek": 379874, "start": 3812.1, "end": 3820.22, "text": " So with our neural network, remember back to the Excel days, notice here it's the same", "tokens": [407, 365, 527, 18161, 3209, 11, 1604, 646, 281, 264, 19060, 1708, 11, 3449, 510, 309, 311, 264, 912], "temperature": 0.0, "avg_logprob": -0.13970640593884037, "compression_ratio": 1.6092436974789917, "no_speech_prob": 2.7107838832307607e-05}, {"id": 849, "seek": 379874, "start": 3820.22, "end": 3821.22, "text": " thing, right?", "tokens": [551, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13970640593884037, "compression_ratio": 1.6092436974789917, "no_speech_prob": 2.7107838832307607e-05}, {"id": 850, "seek": 379874, "start": 3821.22, "end": 3826.66, "text": " We created a column vector, but we didn't create a column vector.", "tokens": [492, 2942, 257, 7738, 8062, 11, 457, 321, 994, 380, 1884, 257, 7738, 8062, 13], "temperature": 0.0, "avg_logprob": -0.13970640593884037, "compression_ratio": 1.6092436974789917, "no_speech_prob": 2.7107838832307607e-05}, {"id": 851, "seek": 382666, "start": 3826.66, "end": 3832.3399999999997, "text": " We actually created a matrix with kind of two sets of coefficients.", "tokens": [492, 767, 2942, 257, 8141, 365, 733, 295, 732, 6352, 295, 31994, 13], "temperature": 0.0, "avg_logprob": -0.17568339407444, "compression_ratio": 1.4659090909090908, "no_speech_prob": 1.9524853996699676e-05}, {"id": 852, "seek": 382666, "start": 3832.3399999999997, "end": 3839.74, "text": " So when we did our matrix multiply, every row gave us two sets of outputs, which we", "tokens": [407, 562, 321, 630, 527, 8141, 12972, 11, 633, 5386, 2729, 505, 732, 6352, 295, 23930, 11, 597, 321], "temperature": 0.0, "avg_logprob": -0.17568339407444, "compression_ratio": 1.4659090909090908, "no_speech_prob": 1.9524853996699676e-05}, {"id": 853, "seek": 382666, "start": 3839.74, "end": 3842.8599999999997, "text": " then chucked through value, right?", "tokens": [550, 20870, 292, 807, 2158, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17568339407444, "compression_ratio": 1.4659090909090908, "no_speech_prob": 1.9524853996699676e-05}, {"id": 854, "seek": 382666, "start": 3842.8599999999997, "end": 3848.46, "text": " Which remember we just used an if statement and we added them together.", "tokens": [3013, 1604, 321, 445, 1143, 364, 498, 5629, 293, 321, 3869, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.17568339407444, "compression_ratio": 1.4659090909090908, "no_speech_prob": 1.9524853996699676e-05}, {"id": 855, "seek": 384846, "start": 3848.46, "end": 3860.94, "text": " So our coefs now, to make a proper neural net, we need one set of coefs here.", "tokens": [407, 527, 598, 5666, 82, 586, 11, 281, 652, 257, 2296, 18161, 2533, 11, 321, 643, 472, 992, 295, 598, 5666, 82, 510, 13], "temperature": 0.0, "avg_logprob": -0.1681436376368746, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.2218602932989597e-05}, {"id": 856, "seek": 384846, "start": 3860.94, "end": 3865.66, "text": " And so here they are, torch.rand and coef by what?", "tokens": [400, 370, 510, 436, 366, 11, 27822, 13, 3699, 293, 598, 5666, 538, 437, 30], "temperature": 0.0, "avg_logprob": -0.1681436376368746, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.2218602932989597e-05}, {"id": 857, "seek": 384846, "start": 3865.66, "end": 3871.26, "text": " Well, in Excel we just did two, because I kind of got bored of getting everything working", "tokens": [1042, 11, 294, 19060, 321, 445, 630, 732, 11, 570, 286, 733, 295, 658, 13521, 295, 1242, 1203, 1364], "temperature": 0.0, "avg_logprob": -0.1681436376368746, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.2218602932989597e-05}, {"id": 858, "seek": 384846, "start": 3871.26, "end": 3872.26, "text": " properly.", "tokens": [6108, 13], "temperature": 0.0, "avg_logprob": -0.1681436376368746, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.2218602932989597e-05}, {"id": 859, "seek": 384846, "start": 3872.26, "end": 3876.78, "text": " But you don't have to worry about filling right and creating columns and blah, blah,", "tokens": [583, 291, 500, 380, 362, 281, 3292, 466, 10623, 558, 293, 4084, 13766, 293, 12288, 11, 12288, 11], "temperature": 0.0, "avg_logprob": -0.1681436376368746, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.2218602932989597e-05}, {"id": 860, "seek": 384846, "start": 3876.78, "end": 3877.78, "text": " blah.", "tokens": [12288, 13], "temperature": 0.0, "avg_logprob": -0.1681436376368746, "compression_ratio": 1.5336538461538463, "no_speech_prob": 1.2218602932989597e-05}, {"id": 861, "seek": 387778, "start": 3877.78, "end": 3880.1000000000004, "text": " You can create as many as you like.", "tokens": [509, 393, 1884, 382, 867, 382, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.20213282735724197, "compression_ratio": 1.5100401606425702, "no_speech_prob": 1.7231070160050876e-05}, {"id": 862, "seek": 387778, "start": 3880.1000000000004, "end": 3884.82, "text": " So I made something you can change, I called it n hidden, number of hidden activations.", "tokens": [407, 286, 1027, 746, 291, 393, 1319, 11, 286, 1219, 309, 297, 7633, 11, 1230, 295, 7633, 2430, 763, 13], "temperature": 0.0, "avg_logprob": -0.20213282735724197, "compression_ratio": 1.5100401606425702, "no_speech_prob": 1.7231070160050876e-05}, {"id": 863, "seek": 387778, "start": 3884.82, "end": 3887.5800000000004, "text": " I just set it to 20.", "tokens": [286, 445, 992, 309, 281, 945, 13], "temperature": 0.0, "avg_logprob": -0.20213282735724197, "compression_ratio": 1.5100401606425702, "no_speech_prob": 1.7231070160050876e-05}, {"id": 864, "seek": 387778, "start": 3887.5800000000004, "end": 3894.5800000000004, "text": " And as before, we centralize them by making them go from minus 0.5 to 0.5.", "tokens": [400, 382, 949, 11, 321, 5777, 1125, 552, 538, 1455, 552, 352, 490, 3175, 1958, 13, 20, 281, 1958, 13, 20, 13], "temperature": 0.0, "avg_logprob": -0.20213282735724197, "compression_ratio": 1.5100401606425702, "no_speech_prob": 1.7231070160050876e-05}, {"id": 865, "seek": 387778, "start": 3894.5800000000004, "end": 3899.38, "text": " Now when you do stuff by hand, everything does get more fiddly.", "tokens": [823, 562, 291, 360, 1507, 538, 1011, 11, 1203, 775, 483, 544, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.20213282735724197, "compression_ratio": 1.5100401606425702, "no_speech_prob": 1.7231070160050876e-05}, {"id": 866, "seek": 387778, "start": 3899.38, "end": 3905.94, "text": " If our coefficients aren't, if they're too big or too small, it's not going to train", "tokens": [759, 527, 31994, 3212, 380, 11, 498, 436, 434, 886, 955, 420, 886, 1359, 11, 309, 311, 406, 516, 281, 3847], "temperature": 0.0, "avg_logprob": -0.20213282735724197, "compression_ratio": 1.5100401606425702, "no_speech_prob": 1.7231070160050876e-05}, {"id": 867, "seek": 387778, "start": 3905.94, "end": 3906.94, "text": " at all.", "tokens": [412, 439, 13], "temperature": 0.0, "avg_logprob": -0.20213282735724197, "compression_ratio": 1.5100401606425702, "no_speech_prob": 1.7231070160050876e-05}, {"id": 868, "seek": 390694, "start": 3906.94, "end": 3910.42, "text": " Obviously the gradients will kind of vaguely point in the right direction, but you'll jump", "tokens": [7580, 264, 2771, 2448, 486, 733, 295, 13501, 48863, 935, 294, 264, 558, 3513, 11, 457, 291, 603, 3012], "temperature": 0.0, "avg_logprob": -0.12745677947998046, "compression_ratio": 1.6096491228070176, "no_speech_prob": 4.9082980694947764e-05}, {"id": 869, "seek": 390694, "start": 3910.42, "end": 3913.02, "text": " too far or not far enough or whatever.", "tokens": [886, 1400, 420, 406, 1400, 1547, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.12745677947998046, "compression_ratio": 1.6096491228070176, "no_speech_prob": 4.9082980694947764e-05}, {"id": 870, "seek": 390694, "start": 3913.02, "end": 3917.32, "text": " So I want my gradients to be about the same as they were before.", "tokens": [407, 286, 528, 452, 2771, 2448, 281, 312, 466, 264, 912, 382, 436, 645, 949, 13], "temperature": 0.0, "avg_logprob": -0.12745677947998046, "compression_ratio": 1.6096491228070176, "no_speech_prob": 4.9082980694947764e-05}, {"id": 871, "seek": 390694, "start": 3917.32, "end": 3924.3, "text": " So I divide by n hidden, because otherwise at the next step when I add up the next matrix", "tokens": [407, 286, 9845, 538, 297, 7633, 11, 570, 5911, 412, 264, 958, 1823, 562, 286, 909, 493, 264, 958, 8141], "temperature": 0.0, "avg_logprob": -0.12745677947998046, "compression_ratio": 1.6096491228070176, "no_speech_prob": 4.9082980694947764e-05}, {"id": 872, "seek": 390694, "start": 3924.3, "end": 3928.7000000000003, "text": " multiply it's going to be much bigger than it was before.", "tokens": [12972, 309, 311, 516, 281, 312, 709, 3801, 813, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.12745677947998046, "compression_ratio": 1.6096491228070176, "no_speech_prob": 4.9082980694947764e-05}, {"id": 873, "seek": 390694, "start": 3928.7000000000003, "end": 3931.7400000000002, "text": " So it's all very fiddly.", "tokens": [407, 309, 311, 439, 588, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.12745677947998046, "compression_ratio": 1.6096491228070176, "no_speech_prob": 4.9082980694947764e-05}, {"id": 874, "seek": 393174, "start": 3931.74, "end": 3937.74, "text": " So then I want to take, so that's going to give me for every row, it's going to give", "tokens": [407, 550, 286, 528, 281, 747, 11, 370, 300, 311, 516, 281, 976, 385, 337, 633, 5386, 11, 309, 311, 516, 281, 976], "temperature": 0.0, "avg_logprob": -0.13911066803277708, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.13959377410356e-06}, {"id": 875, "seek": 393174, "start": 3937.74, "end": 3940.7799999999997, "text": " me 20 activations, 20 values.", "tokens": [385, 945, 2430, 763, 11, 945, 4190, 13], "temperature": 0.0, "avg_logprob": -0.13911066803277708, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.13959377410356e-06}, {"id": 876, "seek": 393174, "start": 3940.7799999999997, "end": 3947.5, "text": " Just like in Excel we had two values because we had two sets of coefficients.", "tokens": [1449, 411, 294, 19060, 321, 632, 732, 4190, 570, 321, 632, 732, 6352, 295, 31994, 13], "temperature": 0.0, "avg_logprob": -0.13911066803277708, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.13959377410356e-06}, {"id": 877, "seek": 393174, "start": 3947.5, "end": 3957.3399999999997, "text": " And so to create a neural net, I now need to multiply each of those 20 things by a coefficient.", "tokens": [400, 370, 281, 1884, 257, 18161, 2533, 11, 286, 586, 643, 281, 12972, 1184, 295, 729, 945, 721, 538, 257, 17619, 13], "temperature": 0.0, "avg_logprob": -0.13911066803277708, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.13959377410356e-06}, {"id": 878, "seek": 393174, "start": 3957.3399999999997, "end": 3961.1, "text": " And this time it's going to be a column vector because I want to create one output, predictor", "tokens": [400, 341, 565, 309, 311, 516, 281, 312, 257, 7738, 8062, 570, 286, 528, 281, 1884, 472, 5598, 11, 6069, 284], "temperature": 0.0, "avg_logprob": -0.13911066803277708, "compression_ratio": 1.7053571428571428, "no_speech_prob": 8.13959377410356e-06}, {"id": 879, "seek": 396110, "start": 3961.1, "end": 3962.1, "text": " of survival.", "tokens": [295, 12559, 13], "temperature": 0.0, "avg_logprob": -0.17082586566221367, "compression_ratio": 1.5748987854251013, "no_speech_prob": 2.1444580852403305e-05}, {"id": 880, "seek": 396110, "start": 3962.1, "end": 3969.14, "text": " So again, torch.rand, and this time the n hidden will be the number of coefficients by", "tokens": [407, 797, 11, 27822, 13, 3699, 11, 293, 341, 565, 264, 297, 7633, 486, 312, 264, 1230, 295, 31994, 538], "temperature": 0.0, "avg_logprob": -0.17082586566221367, "compression_ratio": 1.5748987854251013, "no_speech_prob": 2.1444580852403305e-05}, {"id": 881, "seek": 396110, "start": 3969.14, "end": 3971.06, "text": " 1.", "tokens": [502, 13], "temperature": 0.0, "avg_logprob": -0.17082586566221367, "compression_ratio": 1.5748987854251013, "no_speech_prob": 2.1444580852403305e-05}, {"id": 882, "seek": 396110, "start": 3971.06, "end": 3975.7799999999997, "text": " And again, like trying to find something that actually trains properly required me some", "tokens": [400, 797, 11, 411, 1382, 281, 915, 746, 300, 767, 16329, 6108, 4739, 385, 512], "temperature": 0.0, "avg_logprob": -0.17082586566221367, "compression_ratio": 1.5748987854251013, "no_speech_prob": 2.1444580852403305e-05}, {"id": 883, "seek": 396110, "start": 3975.7799999999997, "end": 3978.2999999999997, "text": " fiddling around to figure out how much to subtract.", "tokens": [283, 14273, 1688, 926, 281, 2573, 484, 577, 709, 281, 16390, 13], "temperature": 0.0, "avg_logprob": -0.17082586566221367, "compression_ratio": 1.5748987854251013, "no_speech_prob": 2.1444580852403305e-05}, {"id": 884, "seek": 396110, "start": 3978.2999999999997, "end": 3984.8399999999997, "text": " And I found if I subtract 0.3 I could get it to train.", "tokens": [400, 286, 1352, 498, 286, 16390, 1958, 13, 18, 286, 727, 483, 309, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.17082586566221367, "compression_ratio": 1.5748987854251013, "no_speech_prob": 2.1444580852403305e-05}, {"id": 885, "seek": 396110, "start": 3984.8399999999997, "end": 3989.8199999999997, "text": " And then finally, I didn't need a constant term for the first layer as we discussed because", "tokens": [400, 550, 2721, 11, 286, 994, 380, 643, 257, 5754, 1433, 337, 264, 700, 4583, 382, 321, 7152, 570], "temperature": 0.0, "avg_logprob": -0.17082586566221367, "compression_ratio": 1.5748987854251013, "no_speech_prob": 2.1444580852403305e-05}, {"id": 886, "seek": 398982, "start": 3989.82, "end": 3997.3, "text": " our dummy variables have n columns rather than n minus 1 columns.", "tokens": [527, 35064, 9102, 362, 297, 13766, 2831, 813, 297, 3175, 502, 13766, 13], "temperature": 0.0, "avg_logprob": -0.17118127318634385, "compression_ratio": 1.5829596412556053, "no_speech_prob": 2.6273655748809688e-05}, {"id": 887, "seek": 398982, "start": 3997.3, "end": 4001.54, "text": " But layer 2 absolutely needs a constant term.", "tokens": [583, 4583, 568, 3122, 2203, 257, 5754, 1433, 13], "temperature": 0.0, "avg_logprob": -0.17118127318634385, "compression_ratio": 1.5829596412556053, "no_speech_prob": 2.6273655748809688e-05}, {"id": 888, "seek": 398982, "start": 4001.54, "end": 4005.6600000000003, "text": " And we could do that as we discussed last time by having a column of ones.", "tokens": [400, 321, 727, 360, 300, 382, 321, 7152, 1036, 565, 538, 1419, 257, 7738, 295, 2306, 13], "temperature": 0.0, "avg_logprob": -0.17118127318634385, "compression_ratio": 1.5829596412556053, "no_speech_prob": 2.6273655748809688e-05}, {"id": 889, "seek": 398982, "start": 4005.6600000000003, "end": 4009.9, "text": " Although in practice I actually find it's just easier just to create a constant term.", "tokens": [5780, 294, 3124, 286, 767, 915, 309, 311, 445, 3571, 445, 281, 1884, 257, 5754, 1433, 13], "temperature": 0.0, "avg_logprob": -0.17118127318634385, "compression_ratio": 1.5829596412556053, "no_speech_prob": 2.6273655748809688e-05}, {"id": 890, "seek": 398982, "start": 4009.9, "end": 4015.6200000000003, "text": " So here is a single scalar random number.", "tokens": [407, 510, 307, 257, 2167, 39684, 4974, 1230, 13], "temperature": 0.0, "avg_logprob": -0.17118127318634385, "compression_ratio": 1.5829596412556053, "no_speech_prob": 2.6273655748809688e-05}, {"id": 891, "seek": 398982, "start": 4015.6200000000003, "end": 4017.46, "text": " So those are the coefficients we need.", "tokens": [407, 729, 366, 264, 31994, 321, 643, 13], "temperature": 0.0, "avg_logprob": -0.17118127318634385, "compression_ratio": 1.5829596412556053, "no_speech_prob": 2.6273655748809688e-05}, {"id": 892, "seek": 401746, "start": 4017.46, "end": 4022.82, "text": " One set of coefficients to go from input to hidden, one goes from hidden to a single output", "tokens": [1485, 992, 295, 31994, 281, 352, 490, 4846, 281, 7633, 11, 472, 1709, 490, 7633, 281, 257, 2167, 5598], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 893, "seek": 401746, "start": 4022.82, "end": 4024.06, "text": " and a constant.", "tokens": [293, 257, 5754, 13], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 894, "seek": 401746, "start": 4024.06, "end": 4027.3, "text": " So they are all going to need grad.", "tokens": [407, 436, 366, 439, 516, 281, 643, 2771, 13], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 895, "seek": 401746, "start": 4027.3, "end": 4031.98, "text": " And so now we can change how we calculate predictions.", "tokens": [400, 370, 586, 321, 393, 1319, 577, 321, 8873, 21264, 13], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 896, "seek": 401746, "start": 4031.98, "end": 4034.42, "text": " So we're going to pass in all of our coefficients.", "tokens": [407, 321, 434, 516, 281, 1320, 294, 439, 295, 527, 31994, 13], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 897, "seek": 401746, "start": 4034.42, "end": 4040.7400000000002, "text": " So a nice thing in Python is if you've got a list or a tuple of values, on the left-hand", "tokens": [407, 257, 1481, 551, 294, 15329, 307, 498, 291, 600, 658, 257, 1329, 420, 257, 2604, 781, 295, 4190, 11, 322, 264, 1411, 12, 5543], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 898, "seek": 401746, "start": 4040.7400000000002, "end": 4043.94, "text": " side you can expand them out into variables.", "tokens": [1252, 291, 393, 5268, 552, 484, 666, 9102, 13], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 899, "seek": 401746, "start": 4043.94, "end": 4046.76, "text": " So this is going to be a list of three things.", "tokens": [407, 341, 307, 516, 281, 312, 257, 1329, 295, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.09959379564814207, "compression_ratio": 1.72, "no_speech_prob": 1.8342365365242586e-05}, {"id": 900, "seek": 404676, "start": 4046.76, "end": 4051.9, "text": " So we'll call them layer 1, layer 2, and the constant term.", "tokens": [407, 321, 603, 818, 552, 4583, 502, 11, 4583, 568, 11, 293, 264, 5754, 1433, 13], "temperature": 0.0, "avg_logprob": -0.12616844177246095, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9525428797351196e-05}, {"id": 901, "seek": 404676, "start": 4051.9, "end": 4055.6600000000003, "text": " Because those are the list of three things we returned.", "tokens": [1436, 729, 366, 264, 1329, 295, 1045, 721, 321, 8752, 13], "temperature": 0.0, "avg_logprob": -0.12616844177246095, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9525428797351196e-05}, {"id": 902, "seek": 404676, "start": 4055.6600000000003, "end": 4059.98, "text": " So in Python if you just chuck things with commas between them like this, it creates", "tokens": [407, 294, 15329, 498, 291, 445, 20870, 721, 365, 800, 296, 1296, 552, 411, 341, 11, 309, 7829], "temperature": 0.0, "avg_logprob": -0.12616844177246095, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9525428797351196e-05}, {"id": 903, "seek": 404676, "start": 4059.98, "end": 4060.98, "text": " a tuple.", "tokens": [257, 2604, 781, 13], "temperature": 0.0, "avg_logprob": -0.12616844177246095, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9525428797351196e-05}, {"id": 904, "seek": 404676, "start": 4060.98, "end": 4065.0200000000004, "text": " A tuple is a list, it's an immutable list.", "tokens": [316, 2604, 781, 307, 257, 1329, 11, 309, 311, 364, 3397, 32148, 1329, 13], "temperature": 0.0, "avg_logprob": -0.12616844177246095, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9525428797351196e-05}, {"id": 905, "seek": 404676, "start": 4065.0200000000004, "end": 4067.7000000000003, "text": " So now we're going to grab those three things.", "tokens": [407, 586, 321, 434, 516, 281, 4444, 729, 1045, 721, 13], "temperature": 0.0, "avg_logprob": -0.12616844177246095, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9525428797351196e-05}, {"id": 906, "seek": 404676, "start": 4067.7000000000003, "end": 4070.7400000000002, "text": " So step one is to do our matrix multiply.", "tokens": [407, 1823, 472, 307, 281, 360, 527, 8141, 12972, 13], "temperature": 0.0, "avg_logprob": -0.12616844177246095, "compression_ratio": 1.5934579439252337, "no_speech_prob": 1.9525428797351196e-05}, {"id": 907, "seek": 407074, "start": 4070.74, "end": 4076.74, "text": " And as we discussed, we then have to replace the negatives with zeros.", "tokens": [400, 382, 321, 7152, 11, 321, 550, 362, 281, 7406, 264, 40019, 365, 35193, 13], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 908, "seek": 407074, "start": 4076.74, "end": 4080.3799999999997, "text": " And then we put that through our second matrix multiply, so our second layer, and add the", "tokens": [400, 550, 321, 829, 300, 807, 527, 1150, 8141, 12972, 11, 370, 527, 1150, 4583, 11, 293, 909, 264], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 909, "seek": 407074, "start": 4080.3799999999997, "end": 4081.9199999999996, "text": " constant term.", "tokens": [5754, 1433, 13], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 910, "seek": 407074, "start": 4081.9199999999996, "end": 4086.56, "text": " And remember, of course, at the end, chuck it through a sigmoid.", "tokens": [400, 1604, 11, 295, 1164, 11, 412, 264, 917, 11, 20870, 309, 807, 257, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 911, "seek": 407074, "start": 4086.56, "end": 4090.66, "text": " So here is a neural network.", "tokens": [407, 510, 307, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 912, "seek": 407074, "start": 4090.66, "end": 4096.179999999999, "text": " Now update coefs previously subtracted the coefficients, the gradients, times the learning", "tokens": [823, 5623, 598, 5666, 82, 8046, 16390, 292, 264, 31994, 11, 264, 2771, 2448, 11, 1413, 264, 2539], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 913, "seek": 407074, "start": 4096.179999999999, "end": 4097.179999999999, "text": " rate from the coefficients.", "tokens": [3314, 490, 264, 31994, 13], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 914, "seek": 407074, "start": 4097.179999999999, "end": 4100.219999999999, "text": " But now we've got three sets of those.", "tokens": [583, 586, 321, 600, 658, 1045, 6352, 295, 729, 13], "temperature": 0.0, "avg_logprob": -0.1418864326139467, "compression_ratio": 1.66147859922179, "no_speech_prob": 9.516135833109729e-06}, {"id": 915, "seek": 410022, "start": 4100.22, "end": 4104.04, "text": " So we have to just chuck that in a for loop.", "tokens": [407, 321, 362, 281, 445, 20870, 300, 294, 257, 337, 6367, 13], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 916, "seek": 410022, "start": 4104.04, "end": 4105.68, "text": " So change that as well.", "tokens": [407, 1319, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 917, "seek": 410022, "start": 4105.68, "end": 4108.14, "text": " And now we can go ahead and train our model.", "tokens": [400, 586, 321, 393, 352, 2286, 293, 3847, 527, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 918, "seek": 410022, "start": 4108.14, "end": 4109.58, "text": " Ta-da!", "tokens": [6551, 12, 2675, 0], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 919, "seek": 410022, "start": 4109.58, "end": 4111.66, "text": " We just trained a model.", "tokens": [492, 445, 8895, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 920, "seek": 410022, "start": 4111.66, "end": 4114.62, "text": " And how does that compare?", "tokens": [400, 577, 775, 300, 6794, 30], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 921, "seek": 410022, "start": 4114.62, "end": 4119.18, "text": " So the loss function is a little better than before.", "tokens": [407, 264, 4470, 2445, 307, 257, 707, 1101, 813, 949, 13], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 922, "seek": 410022, "start": 4119.18, "end": 4126.38, "text": " Accuracy exactly the same as before.", "tokens": [5725, 374, 2551, 2293, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.21974547512560005, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.540180947922636e-06}, {"id": 923, "seek": 412638, "start": 4126.38, "end": 4134.46, "text": " And I will say it was very annoying to get to this point, trying to get these constants", "tokens": [400, 286, 486, 584, 309, 390, 588, 11304, 281, 483, 281, 341, 935, 11, 1382, 281, 483, 613, 35870], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 924, "seek": 412638, "start": 4134.46, "end": 4136.78, "text": " right and find a learning rate that worked.", "tokens": [558, 293, 915, 257, 2539, 3314, 300, 2732, 13], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 925, "seek": 412638, "start": 4136.78, "end": 4140.54, "text": " Like it was super fiddly.", "tokens": [1743, 309, 390, 1687, 283, 14273, 356, 13], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 926, "seek": 412638, "start": 4140.54, "end": 4141.9800000000005, "text": " But we got there.", "tokens": [583, 321, 658, 456, 13], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 927, "seek": 412638, "start": 4141.9800000000005, "end": 4142.9800000000005, "text": " We got there.", "tokens": [492, 658, 456, 13], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 928, "seek": 412638, "start": 4142.9800000000005, "end": 4144.78, "text": " It's a very small test set.", "tokens": [467, 311, 257, 588, 1359, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 929, "seek": 412638, "start": 4144.78, "end": 4147.9800000000005, "text": " I don't know if this is necessarily better or worse than the linear model, but it's certainly", "tokens": [286, 500, 380, 458, 498, 341, 307, 4725, 1101, 420, 5324, 813, 264, 8213, 2316, 11, 457, 309, 311, 3297], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 930, "seek": 412638, "start": 4147.9800000000005, "end": 4150.62, "text": " fine.", "tokens": [2489, 13], "temperature": 0.0, "avg_logprob": -0.14968516236992294, "compression_ratio": 1.561576354679803, "no_speech_prob": 2.546480936871376e-05}, {"id": 931, "seek": 415062, "start": 4150.62, "end": 4157.0199999999995, "text": " And I think that's pretty cool that we were able to build a neural net from scratch.", "tokens": [400, 286, 519, 300, 311, 1238, 1627, 300, 321, 645, 1075, 281, 1322, 257, 18161, 2533, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.09315051672593602, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4284626558946911e-05}, {"id": 932, "seek": 415062, "start": 4157.0199999999995, "end": 4158.58, "text": " That's doing pretty well.", "tokens": [663, 311, 884, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.09315051672593602, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4284626558946911e-05}, {"id": 933, "seek": 415062, "start": 4158.58, "end": 4163.18, "text": " But I hear that all the cool kids nowadays are doing deep learning, not just neural nets.", "tokens": [583, 286, 1568, 300, 439, 264, 1627, 2301, 13434, 366, 884, 2452, 2539, 11, 406, 445, 18161, 36170, 13], "temperature": 0.0, "avg_logprob": -0.09315051672593602, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4284626558946911e-05}, {"id": 934, "seek": 415062, "start": 4163.18, "end": 4164.62, "text": " So we better make this deep learning.", "tokens": [407, 321, 1101, 652, 341, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.09315051672593602, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4284626558946911e-05}, {"id": 935, "seek": 415062, "start": 4164.62, "end": 4168.9, "text": " So this one only has one hidden layer.", "tokens": [407, 341, 472, 787, 575, 472, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.09315051672593602, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4284626558946911e-05}, {"id": 936, "seek": 415062, "start": 4168.9, "end": 4172.78, "text": " So let's create one with n hidden layers.", "tokens": [407, 718, 311, 1884, 472, 365, 297, 7633, 7914, 13], "temperature": 0.0, "avg_logprob": -0.09315051672593602, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4284626558946911e-05}, {"id": 937, "seek": 415062, "start": 4172.78, "end": 4178.4, "text": " So for example, let's say we want two hidden layers, 10 activations in each.", "tokens": [407, 337, 1365, 11, 718, 311, 584, 321, 528, 732, 7633, 7914, 11, 1266, 2430, 763, 294, 1184, 13], "temperature": 0.0, "avg_logprob": -0.09315051672593602, "compression_ratio": 1.736842105263158, "no_speech_prob": 1.4284626558946911e-05}, {"id": 938, "seek": 417840, "start": 4178.4, "end": 4180.62, "text": " You can put as many as you like here.", "tokens": [509, 393, 829, 382, 867, 382, 291, 411, 510, 13], "temperature": 0.0, "avg_logprob": -0.1749705237311286, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.76684009906603e-06}, {"id": 939, "seek": 417840, "start": 4180.62, "end": 4190.7, "text": " So init coefs now is going to have to create a torch.rand for every one of those hidden", "tokens": [407, 3157, 598, 5666, 82, 586, 307, 516, 281, 362, 281, 1884, 257, 27822, 13, 3699, 337, 633, 472, 295, 729, 7633], "temperature": 0.0, "avg_logprob": -0.1749705237311286, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.76684009906603e-06}, {"id": 940, "seek": 417840, "start": 4190.7, "end": 4192.4, "text": " layers.", "tokens": [7914, 13], "temperature": 0.0, "avg_logprob": -0.1749705237311286, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.76684009906603e-06}, {"id": 941, "seek": 417840, "start": 4192.4, "end": 4198.74, "text": " And then another torch.rand for your constant terms.", "tokens": [400, 550, 1071, 27822, 13, 3699, 337, 428, 5754, 2115, 13], "temperature": 0.0, "avg_logprob": -0.1749705237311286, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.76684009906603e-06}, {"id": 942, "seek": 417840, "start": 4198.74, "end": 4201.98, "text": " Stick requires grad and all of them.", "tokens": [22744, 7029, 2771, 293, 439, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.1749705237311286, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.76684009906603e-06}, {"id": 943, "seek": 417840, "start": 4201.98, "end": 4203.42, "text": " And then we can return that.", "tokens": [400, 550, 321, 393, 2736, 300, 13], "temperature": 0.0, "avg_logprob": -0.1749705237311286, "compression_ratio": 1.5555555555555556, "no_speech_prob": 7.76684009906603e-06}, {"id": 944, "seek": 420342, "start": 4203.42, "end": 4209.7, "text": " So that's how we can just initialize as many layers as we want of coefficients.", "tokens": [407, 300, 311, 577, 321, 393, 445, 5883, 1125, 382, 867, 7914, 382, 321, 528, 295, 31994, 13], "temperature": 0.0, "avg_logprob": -0.11995349496097887, "compression_ratio": 1.7751004016064258, "no_speech_prob": 1.5688759958720766e-05}, {"id": 945, "seek": 420342, "start": 4209.7, "end": 4214.58, "text": " So the first one, the first layer, so the sizes of each one, the first layer will go", "tokens": [407, 264, 700, 472, 11, 264, 700, 4583, 11, 370, 264, 11602, 295, 1184, 472, 11, 264, 700, 4583, 486, 352], "temperature": 0.0, "avg_logprob": -0.11995349496097887, "compression_ratio": 1.7751004016064258, "no_speech_prob": 1.5688759958720766e-05}, {"id": 946, "seek": 420342, "start": 4214.58, "end": 4217.18, "text": " from n coef to 10.", "tokens": [490, 297, 598, 5666, 281, 1266, 13], "temperature": 0.0, "avg_logprob": -0.11995349496097887, "compression_ratio": 1.7751004016064258, "no_speech_prob": 1.5688759958720766e-05}, {"id": 947, "seek": 420342, "start": 4217.18, "end": 4219.78, "text": " The second matrix will go from 10 to 10.", "tokens": [440, 1150, 8141, 486, 352, 490, 1266, 281, 1266, 13], "temperature": 0.0, "avg_logprob": -0.11995349496097887, "compression_ratio": 1.7751004016064258, "no_speech_prob": 1.5688759958720766e-05}, {"id": 948, "seek": 420342, "start": 4219.78, "end": 4223.38, "text": " And the third matrix will go from 10 to 1.", "tokens": [400, 264, 2636, 8141, 486, 352, 490, 1266, 281, 502, 13], "temperature": 0.0, "avg_logprob": -0.11995349496097887, "compression_ratio": 1.7751004016064258, "no_speech_prob": 1.5688759958720766e-05}, {"id": 949, "seek": 420342, "start": 4223.38, "end": 4227.7, "text": " So it's worth working through these matrix modellpliers on a spreadsheet or a piece of", "tokens": [407, 309, 311, 3163, 1364, 807, 613, 8141, 1072, 898, 564, 4890, 322, 257, 27733, 420, 257, 2522, 295], "temperature": 0.0, "avg_logprob": -0.11995349496097887, "compression_ratio": 1.7751004016064258, "no_speech_prob": 1.5688759958720766e-05}, {"id": 950, "seek": 420342, "start": 4227.7, "end": 4231.3, "text": " paper or something to convince yourself that there's the right number of activations at", "tokens": [3035, 420, 746, 281, 13447, 1803, 300, 456, 311, 264, 558, 1230, 295, 2430, 763, 412], "temperature": 0.0, "avg_logprob": -0.11995349496097887, "compression_ratio": 1.7751004016064258, "no_speech_prob": 1.5688759958720766e-05}, {"id": 951, "seek": 423130, "start": 4231.3, "end": 4235.84, "text": " each point.", "tokens": [1184, 935, 13], "temperature": 0.0, "avg_logprob": -0.15380416745724884, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.17380294695613e-06}, {"id": 952, "seek": 423130, "start": 4235.84, "end": 4241.66, "text": " And so then we need to update calc preds so that rather than doing each of these steps", "tokens": [400, 370, 550, 321, 643, 281, 5623, 2104, 66, 3852, 82, 370, 300, 2831, 813, 884, 1184, 295, 613, 4439], "temperature": 0.0, "avg_logprob": -0.15380416745724884, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.17380294695613e-06}, {"id": 953, "seek": 423130, "start": 4241.66, "end": 4252.18, "text": " manually we now need to loop through all the layers, do the matrix modellply, add the constant,", "tokens": [16945, 321, 586, 643, 281, 6367, 807, 439, 264, 7914, 11, 360, 264, 8141, 1072, 898, 2724, 11, 909, 264, 5754, 11], "temperature": 0.0, "avg_logprob": -0.15380416745724884, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.17380294695613e-06}, {"id": 954, "seek": 423130, "start": 4252.18, "end": 4256.14, "text": " and as long as it's not the last layer, do the ReLU.", "tokens": [293, 382, 938, 382, 309, 311, 406, 264, 1036, 4583, 11, 360, 264, 1300, 43, 52, 13], "temperature": 0.0, "avg_logprob": -0.15380416745724884, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.17380294695613e-06}, {"id": 955, "seek": 423130, "start": 4256.14, "end": 4257.400000000001, "text": " Why not the last layer?", "tokens": [1545, 406, 264, 1036, 4583, 30], "temperature": 0.0, "avg_logprob": -0.15380416745724884, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.17380294695613e-06}, {"id": 956, "seek": 423130, "start": 4257.400000000001, "end": 4260.26, "text": " Because remember the last layer has sigmoid.", "tokens": [1436, 1604, 264, 1036, 4583, 575, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.15380416745724884, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.17380294695613e-06}, {"id": 957, "seek": 426026, "start": 4260.26, "end": 4265.320000000001, "text": " So these things about like, remember what happens on the last layer?", "tokens": [407, 613, 721, 466, 411, 11, 1604, 437, 2314, 322, 264, 1036, 4583, 30], "temperature": 0.0, "avg_logprob": -0.16129338984586755, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.8630948034115136e-05}, {"id": 958, "seek": 426026, "start": 4265.320000000001, "end": 4267.5, "text": " This is an important thing you need to know about.", "tokens": [639, 307, 364, 1021, 551, 291, 643, 281, 458, 466, 13], "temperature": 0.0, "avg_logprob": -0.16129338984586755, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.8630948034115136e-05}, {"id": 959, "seek": 426026, "start": 4267.5, "end": 4271.9800000000005, "text": " You need to kind of check if things aren't working.", "tokens": [509, 643, 281, 733, 295, 1520, 498, 721, 3212, 380, 1364, 13], "temperature": 0.0, "avg_logprob": -0.16129338984586755, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.8630948034115136e-05}, {"id": 960, "seek": 426026, "start": 4271.9800000000005, "end": 4277.46, "text": " This thing here is called the activation function, torch.sigmoid and f.relu.", "tokens": [639, 551, 510, 307, 1219, 264, 24433, 2445, 11, 27822, 13, 82, 328, 3280, 327, 293, 283, 13, 265, 2781, 13], "temperature": 0.0, "avg_logprob": -0.16129338984586755, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.8630948034115136e-05}, {"id": 961, "seek": 426026, "start": 4277.46, "end": 4282.320000000001, "text": " They're the activation functions for these layers.", "tokens": [814, 434, 264, 24433, 6828, 337, 613, 7914, 13], "temperature": 0.0, "avg_logprob": -0.16129338984586755, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.8630948034115136e-05}, {"id": 962, "seek": 426026, "start": 4282.320000000001, "end": 4289.3, "text": " One of the most common mistakes amongst people trying to kind of create their own architectures", "tokens": [1485, 295, 264, 881, 2689, 8038, 12918, 561, 1382, 281, 733, 295, 1884, 641, 1065, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.16129338984586755, "compression_ratio": 1.673728813559322, "no_speech_prob": 1.8630948034115136e-05}, {"id": 963, "seek": 428930, "start": 4289.3, "end": 4295.46, "text": " or kind of variants of architectures is to mess up their final activation function and", "tokens": [420, 733, 295, 21669, 295, 6331, 1303, 307, 281, 2082, 493, 641, 2572, 24433, 2445, 293], "temperature": 0.0, "avg_logprob": -0.1013195952590631, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.5689027350163087e-05}, {"id": 964, "seek": 428930, "start": 4295.46, "end": 4297.5, "text": " that makes things very hard to train.", "tokens": [300, 1669, 721, 588, 1152, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.1013195952590631, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.5689027350163087e-05}, {"id": 965, "seek": 428930, "start": 4297.5, "end": 4302.900000000001, "text": " So make sure we've got a torch.sigmoid at the end and no ReLU at the end.", "tokens": [407, 652, 988, 321, 600, 658, 257, 27822, 13, 82, 328, 3280, 327, 412, 264, 917, 293, 572, 1300, 43, 52, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1013195952590631, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.5689027350163087e-05}, {"id": 966, "seek": 428930, "start": 4302.900000000001, "end": 4306.78, "text": " So there's our deep learning calc preds.", "tokens": [407, 456, 311, 527, 2452, 2539, 2104, 66, 3852, 82, 13], "temperature": 0.0, "avg_logprob": -0.1013195952590631, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.5689027350163087e-05}, {"id": 967, "seek": 428930, "start": 4306.78, "end": 4311.06, "text": " And then just one last change is now when we update our coefficients we go through all", "tokens": [400, 550, 445, 472, 1036, 1319, 307, 586, 562, 321, 5623, 527, 31994, 321, 352, 807, 439], "temperature": 0.0, "avg_logprob": -0.1013195952590631, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.5689027350163087e-05}, {"id": 968, "seek": 428930, "start": 4311.06, "end": 4315.400000000001, "text": " the layers and all the constants.", "tokens": [264, 7914, 293, 439, 264, 35870, 13], "temperature": 0.0, "avg_logprob": -0.1013195952590631, "compression_ratio": 1.5789473684210527, "no_speech_prob": 1.5689027350163087e-05}, {"id": 969, "seek": 431540, "start": 4315.4, "end": 4319.7, "text": " And again, there was so much messing around here with trying to find like exact ranges", "tokens": [400, 797, 11, 456, 390, 370, 709, 23258, 926, 510, 365, 1382, 281, 915, 411, 1900, 22526], "temperature": 0.0, "avg_logprob": -0.11586238037456166, "compression_ratio": 1.6061946902654867, "no_speech_prob": 9.223203051078599e-06}, {"id": 970, "seek": 431540, "start": 4319.7, "end": 4323.86, "text": " of random numbers that end up training okay.", "tokens": [295, 4974, 3547, 300, 917, 493, 3097, 1392, 13], "temperature": 0.0, "avg_logprob": -0.11586238037456166, "compression_ratio": 1.6061946902654867, "no_speech_prob": 9.223203051078599e-06}, {"id": 971, "seek": 431540, "start": 4323.86, "end": 4330.74, "text": " But eventually I found some and as you can see it gets to about the same loss and about", "tokens": [583, 4728, 286, 1352, 512, 293, 382, 291, 393, 536, 309, 2170, 281, 466, 264, 912, 4470, 293, 466], "temperature": 0.0, "avg_logprob": -0.11586238037456166, "compression_ratio": 1.6061946902654867, "no_speech_prob": 9.223203051078599e-06}, {"id": 972, "seek": 431540, "start": 4330.74, "end": 4336.299999999999, "text": " the same accuracy.", "tokens": [264, 912, 14170, 13], "temperature": 0.0, "avg_logprob": -0.11586238037456166, "compression_ratio": 1.6061946902654867, "no_speech_prob": 9.223203051078599e-06}, {"id": 973, "seek": 431540, "start": 4336.299999999999, "end": 4343.42, "text": " This code is worth spending time with and when the code's inside a function it can be", "tokens": [639, 3089, 307, 3163, 6434, 565, 365, 293, 562, 264, 3089, 311, 1854, 257, 2445, 309, 393, 312], "temperature": 0.0, "avg_logprob": -0.11586238037456166, "compression_ratio": 1.6061946902654867, "no_speech_prob": 9.223203051078599e-06}, {"id": 974, "seek": 431540, "start": 4343.42, "end": 4345.219999999999, "text": " a little difficult to experiment with.", "tokens": [257, 707, 2252, 281, 5120, 365, 13], "temperature": 0.0, "avg_logprob": -0.11586238037456166, "compression_ratio": 1.6061946902654867, "no_speech_prob": 9.223203051078599e-06}, {"id": 975, "seek": 434522, "start": 4345.22, "end": 4349.06, "text": " So you know what I would be inclined to do to understand this code is to kind of copy", "tokens": [407, 291, 458, 437, 286, 576, 312, 28173, 281, 360, 281, 1223, 341, 3089, 307, 281, 733, 295, 5055], "temperature": 0.0, "avg_logprob": -0.1315846183083274, "compression_ratio": 1.726530612244898, "no_speech_prob": 2.111170942953322e-05}, {"id": 976, "seek": 434522, "start": 4349.06, "end": 4355.14, "text": " and paste this cell, make it so it's not in a function anymore and then use control shift", "tokens": [293, 9163, 341, 2815, 11, 652, 309, 370, 309, 311, 406, 294, 257, 2445, 3602, 293, 550, 764, 1969, 5513], "temperature": 0.0, "avg_logprob": -0.1315846183083274, "compression_ratio": 1.726530612244898, "no_speech_prob": 2.111170942953322e-05}, {"id": 977, "seek": 434522, "start": 4355.14, "end": 4358.820000000001, "text": " dash to separate these out into separate cells.", "tokens": [8240, 281, 4994, 613, 484, 666, 4994, 5438, 13], "temperature": 0.0, "avg_logprob": -0.1315846183083274, "compression_ratio": 1.726530612244898, "no_speech_prob": 2.111170942953322e-05}, {"id": 978, "seek": 434522, "start": 4358.820000000001, "end": 4363.54, "text": " And then try to kind of set it up so you can run a single layer at a time or a single coefficient.", "tokens": [400, 550, 853, 281, 733, 295, 992, 309, 493, 370, 291, 393, 1190, 257, 2167, 4583, 412, 257, 565, 420, 257, 2167, 17619, 13], "temperature": 0.0, "avg_logprob": -0.1315846183083274, "compression_ratio": 1.726530612244898, "no_speech_prob": 2.111170942953322e-05}, {"id": 979, "seek": 434522, "start": 4363.54, "end": 4368.9400000000005, "text": " Make sure you can see what's going on.", "tokens": [4387, 988, 291, 393, 536, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1315846183083274, "compression_ratio": 1.726530612244898, "no_speech_prob": 2.111170942953322e-05}, {"id": 980, "seek": 434522, "start": 4368.9400000000005, "end": 4373.900000000001, "text": " And that's why we use notebooks is so that we can experiment.", "tokens": [400, 300, 311, 983, 321, 764, 43782, 307, 370, 300, 321, 393, 5120, 13], "temperature": 0.0, "avg_logprob": -0.1315846183083274, "compression_ratio": 1.726530612244898, "no_speech_prob": 2.111170942953322e-05}, {"id": 981, "seek": 437390, "start": 4373.9, "end": 4378.94, "text": " And it's only through experimenting like that that at least for me I find that I can really", "tokens": [400, 309, 311, 787, 807, 29070, 411, 300, 300, 412, 1935, 337, 385, 286, 915, 300, 286, 393, 534], "temperature": 0.0, "avg_logprob": -0.170347575483651, "compression_ratio": 1.5625, "no_speech_prob": 2.468080128892325e-05}, {"id": 982, "seek": 437390, "start": 4378.94, "end": 4380.219999999999, "text": " understand what's going on.", "tokens": [1223, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.170347575483651, "compression_ratio": 1.5625, "no_speech_prob": 2.468080128892325e-05}, {"id": 983, "seek": 437390, "start": 4380.219999999999, "end": 4384.94, "text": " Nobody can look at this code and immediately say I don't think anybody can.", "tokens": [9297, 393, 574, 412, 341, 3089, 293, 4258, 584, 286, 500, 380, 519, 4472, 393, 13], "temperature": 0.0, "avg_logprob": -0.170347575483651, "compression_ratio": 1.5625, "no_speech_prob": 2.468080128892325e-05}, {"id": 984, "seek": 437390, "start": 4384.94, "end": 4386.099999999999, "text": " I get it.", "tokens": [286, 483, 309, 13], "temperature": 0.0, "avg_logprob": -0.170347575483651, "compression_ratio": 1.5625, "no_speech_prob": 2.468080128892325e-05}, {"id": 985, "seek": 437390, "start": 4386.099999999999, "end": 4387.54, "text": " That all makes perfect sense.", "tokens": [663, 439, 1669, 2176, 2020, 13], "temperature": 0.0, "avg_logprob": -0.170347575483651, "compression_ratio": 1.5625, "no_speech_prob": 2.468080128892325e-05}, {"id": 986, "seek": 437390, "start": 4387.54, "end": 4396.099999999999, "text": " But once you try running through it yourself you'll be like oh I see why that's as it is.", "tokens": [583, 1564, 291, 853, 2614, 807, 309, 1803, 291, 603, 312, 411, 1954, 286, 536, 983, 300, 311, 382, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.170347575483651, "compression_ratio": 1.5625, "no_speech_prob": 2.468080128892325e-05}, {"id": 987, "seek": 439610, "start": 4396.1, "end": 4406.58, "text": " So you know one thing to point out here is that our neural nets and deep learning models", "tokens": [407, 291, 458, 472, 551, 281, 935, 484, 510, 307, 300, 527, 18161, 36170, 293, 2452, 2539, 5245], "temperature": 0.0, "avg_logprob": -0.15858956654866538, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.0450562513142359e-05}, {"id": 988, "seek": 439610, "start": 4406.58, "end": 4411.34, "text": " didn't particularly seem to help.", "tokens": [994, 380, 4098, 1643, 281, 854, 13], "temperature": 0.0, "avg_logprob": -0.15858956654866538, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.0450562513142359e-05}, {"id": 989, "seek": 439610, "start": 4411.34, "end": 4416.900000000001, "text": " So does that mean that deep learning is a waste of time and you just did five lessons", "tokens": [407, 775, 300, 914, 300, 2452, 2539, 307, 257, 5964, 295, 565, 293, 291, 445, 630, 1732, 8820], "temperature": 0.0, "avg_logprob": -0.15858956654866538, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.0450562513142359e-05}, {"id": 990, "seek": 439610, "start": 4416.900000000001, "end": 4418.900000000001, "text": " that you shouldn't have done?", "tokens": [300, 291, 4659, 380, 362, 1096, 30], "temperature": 0.0, "avg_logprob": -0.15858956654866538, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.0450562513142359e-05}, {"id": 991, "seek": 439610, "start": 4418.900000000001, "end": 4421.34, "text": " No, not necessarily.", "tokens": [883, 11, 406, 4725, 13], "temperature": 0.0, "avg_logprob": -0.15858956654866538, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.0450562513142359e-05}, {"id": 992, "seek": 439610, "start": 4421.34, "end": 4422.900000000001, "text": " This is a playground competition.", "tokens": [639, 307, 257, 24646, 6211, 13], "temperature": 0.0, "avg_logprob": -0.15858956654866538, "compression_ratio": 1.5340314136125655, "no_speech_prob": 1.0450562513142359e-05}, {"id": 993, "seek": 442290, "start": 4422.9, "end": 4426.42, "text": " You're doing it because it's easy to get your head around.", "tokens": [509, 434, 884, 309, 570, 309, 311, 1858, 281, 483, 428, 1378, 926, 13], "temperature": 0.0, "avg_logprob": -0.12185980081558227, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.1477763109724037e-05}, {"id": 994, "seek": 442290, "start": 4426.42, "end": 4430.099999999999, "text": " But for very small data sets like this with very, very few columns and the columns are", "tokens": [583, 337, 588, 1359, 1412, 6352, 411, 341, 365, 588, 11, 588, 1326, 13766, 293, 264, 13766, 366], "temperature": 0.0, "avg_logprob": -0.12185980081558227, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.1477763109724037e-05}, {"id": 995, "seek": 442290, "start": 4430.099999999999, "end": 4437.54, "text": " really simple, you know deep learning is not necessarily going to give you the best result.", "tokens": [534, 2199, 11, 291, 458, 2452, 2539, 307, 406, 4725, 516, 281, 976, 291, 264, 1151, 1874, 13], "temperature": 0.0, "avg_logprob": -0.12185980081558227, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.1477763109724037e-05}, {"id": 996, "seek": 442290, "start": 4437.54, "end": 4451.259999999999, "text": " In fact, as I mentioned, nothing we do is going to be as good as a carefully designed", "tokens": [682, 1186, 11, 382, 286, 2835, 11, 1825, 321, 360, 307, 516, 281, 312, 382, 665, 382, 257, 7500, 4761], "temperature": 0.0, "avg_logprob": -0.12185980081558227, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.1477763109724037e-05}, {"id": 997, "seek": 445126, "start": 4451.26, "end": 4456.18, "text": " model that uses just the name column.", "tokens": [2316, 300, 4960, 445, 264, 1315, 7738, 13], "temperature": 0.0, "avg_logprob": -0.17486606944691052, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.023932185897138e-05}, {"id": 998, "seek": 445126, "start": 4456.18, "end": 4458.62, "text": " So you know I think that's an interesting insight, right?", "tokens": [407, 291, 458, 286, 519, 300, 311, 364, 1880, 11269, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17486606944691052, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.023932185897138e-05}, {"id": 999, "seek": 445126, "start": 4458.62, "end": 4466.3, "text": " Is that the kind of data types which have a very consistent structure like for example", "tokens": [1119, 300, 264, 733, 295, 1412, 3467, 597, 362, 257, 588, 8398, 3877, 411, 337, 1365], "temperature": 0.0, "avg_logprob": -0.17486606944691052, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.023932185897138e-05}, {"id": 1000, "seek": 445126, "start": 4466.3, "end": 4475.3, "text": " images or natural language text documents, quite often you can somewhat brainlessly chuck", "tokens": [5267, 420, 3303, 2856, 2487, 8512, 11, 1596, 2049, 291, 393, 8344, 3567, 12048, 20870], "temperature": 0.0, "avg_logprob": -0.17486606944691052, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.023932185897138e-05}, {"id": 1001, "seek": 445126, "start": 4475.3, "end": 4481.14, "text": " a deep learning neural net at it and get a great result.", "tokens": [257, 2452, 2539, 18161, 2533, 412, 309, 293, 483, 257, 869, 1874, 13], "temperature": 0.0, "avg_logprob": -0.17486606944691052, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.023932185897138e-05}, {"id": 1002, "seek": 448114, "start": 4481.14, "end": 4484.660000000001, "text": " Generally for tabular data, I find that's not the case.", "tokens": [21082, 337, 4421, 1040, 1412, 11, 286, 915, 300, 311, 406, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.14061793502496214, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.6685071134124883e-05}, {"id": 1003, "seek": 448114, "start": 4484.660000000001, "end": 4490.18, "text": " I find I normally have to think pretty long and hard about the feature engineering in", "tokens": [286, 915, 286, 5646, 362, 281, 519, 1238, 938, 293, 1152, 466, 264, 4111, 7043, 294], "temperature": 0.0, "avg_logprob": -0.14061793502496214, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.6685071134124883e-05}, {"id": 1004, "seek": 448114, "start": 4490.18, "end": 4492.26, "text": " order to get good results.", "tokens": [1668, 281, 483, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.14061793502496214, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.6685071134124883e-05}, {"id": 1005, "seek": 448114, "start": 4492.26, "end": 4498.820000000001, "text": " But once you've got good features, you then want a good model.", "tokens": [583, 1564, 291, 600, 658, 665, 4122, 11, 291, 550, 528, 257, 665, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14061793502496214, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.6685071134124883e-05}, {"id": 1006, "seek": 448114, "start": 4498.820000000001, "end": 4502.700000000001, "text": " And generally like the more features you have and the more levels in your categorical features", "tokens": [400, 5101, 411, 264, 544, 4122, 291, 362, 293, 264, 544, 4358, 294, 428, 19250, 804, 4122], "temperature": 0.0, "avg_logprob": -0.14061793502496214, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.6685071134124883e-05}, {"id": 1007, "seek": 448114, "start": 4502.700000000001, "end": 4509.26, "text": " and stuff like that, the more value you'll get from more sophisticated models.", "tokens": [293, 1507, 411, 300, 11, 264, 544, 2158, 291, 603, 483, 490, 544, 16950, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14061793502496214, "compression_ratio": 1.7685589519650655, "no_speech_prob": 2.6685071134124883e-05}, {"id": 1008, "seek": 450926, "start": 4509.26, "end": 4516.3, "text": " But yeah, I definitely would say an insight here is that you want to include simple baselines", "tokens": [583, 1338, 11, 286, 2138, 576, 584, 364, 11269, 510, 307, 300, 291, 528, 281, 4090, 2199, 987, 9173], "temperature": 0.0, "avg_logprob": -0.15710151763189406, "compression_ratio": 1.505050505050505, "no_speech_prob": 2.0782783394679427e-05}, {"id": 1009, "seek": 450926, "start": 4516.3, "end": 4517.3, "text": " as well.", "tokens": [382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15710151763189406, "compression_ratio": 1.505050505050505, "no_speech_prob": 2.0782783394679427e-05}, {"id": 1010, "seek": 450926, "start": 4517.3, "end": 4530.3, "text": " And we're going to be seeing even more of that in a couple of notebooks time.", "tokens": [400, 321, 434, 516, 281, 312, 2577, 754, 544, 295, 300, 294, 257, 1916, 295, 43782, 565, 13], "temperature": 0.0, "avg_logprob": -0.15710151763189406, "compression_ratio": 1.505050505050505, "no_speech_prob": 2.0782783394679427e-05}, {"id": 1011, "seek": 450926, "start": 4530.3, "end": 4533.860000000001, "text": " So we've just seen how you can build stuff from scratch.", "tokens": [407, 321, 600, 445, 1612, 577, 291, 393, 1322, 1507, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.15710151763189406, "compression_ratio": 1.505050505050505, "no_speech_prob": 2.0782783394679427e-05}, {"id": 1012, "seek": 450926, "start": 4533.860000000001, "end": 4536.58, "text": " We'll now see why you shouldn't.", "tokens": [492, 603, 586, 536, 983, 291, 4659, 380, 13], "temperature": 0.0, "avg_logprob": -0.15710151763189406, "compression_ratio": 1.505050505050505, "no_speech_prob": 2.0782783394679427e-05}, {"id": 1013, "seek": 450926, "start": 4536.58, "end": 4538.26, "text": " I mean I say you shouldn't.", "tokens": [286, 914, 286, 584, 291, 4659, 380, 13], "temperature": 0.0, "avg_logprob": -0.15710151763189406, "compression_ratio": 1.505050505050505, "no_speech_prob": 2.0782783394679427e-05}, {"id": 1014, "seek": 453826, "start": 4538.26, "end": 4542.54, "text": " You should to learn that why you probably won't want to in real life.", "tokens": [509, 820, 281, 1466, 300, 983, 291, 1391, 1582, 380, 528, 281, 294, 957, 993, 13], "temperature": 0.0, "avg_logprob": -0.10761776167092864, "compression_ratio": 1.757847533632287, "no_speech_prob": 1.1299928701191675e-05}, {"id": 1015, "seek": 453826, "start": 4542.54, "end": 4546.5, "text": " When you're doing stuff in real life, you don't want to be fiddling around with all", "tokens": [1133, 291, 434, 884, 1507, 294, 957, 993, 11, 291, 500, 380, 528, 281, 312, 283, 14273, 1688, 926, 365, 439], "temperature": 0.0, "avg_logprob": -0.10761776167092864, "compression_ratio": 1.757847533632287, "no_speech_prob": 1.1299928701191675e-05}, {"id": 1016, "seek": 453826, "start": 4546.5, "end": 4554.5, "text": " this annoying initialization stuff and learning rate stuff and dummy variable stuff and normalization", "tokens": [341, 11304, 5883, 2144, 1507, 293, 2539, 3314, 1507, 293, 35064, 7006, 1507, 293, 2710, 2144], "temperature": 0.0, "avg_logprob": -0.10761776167092864, "compression_ratio": 1.757847533632287, "no_speech_prob": 1.1299928701191675e-05}, {"id": 1017, "seek": 453826, "start": 4554.5, "end": 4560.1, "text": " stuff and so forth because we can do it for you.", "tokens": [1507, 293, 370, 5220, 570, 321, 393, 360, 309, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.10761776167092864, "compression_ratio": 1.757847533632287, "no_speech_prob": 1.1299928701191675e-05}, {"id": 1018, "seek": 453826, "start": 4560.1, "end": 4563.74, "text": " And it's not like everything's so automated that you don't get to make choices, but you", "tokens": [400, 309, 311, 406, 411, 1203, 311, 370, 18473, 300, 291, 500, 380, 483, 281, 652, 7994, 11, 457, 291], "temperature": 0.0, "avg_logprob": -0.10761776167092864, "compression_ratio": 1.757847533632287, "no_speech_prob": 1.1299928701191675e-05}, {"id": 1019, "seek": 456374, "start": 4563.74, "end": 4570.0599999999995, "text": " want to make the choice not to do things the obvious way and have everything else done", "tokens": [528, 281, 652, 264, 3922, 406, 281, 360, 721, 264, 6322, 636, 293, 362, 1203, 1646, 1096], "temperature": 0.0, "avg_logprob": -0.12575960159301758, "compression_ratio": 1.8197424892703862, "no_speech_prob": 8.93939704837976e-06}, {"id": 1020, "seek": 456374, "start": 4570.0599999999995, "end": 4572.26, "text": " the obvious way for you.", "tokens": [264, 6322, 636, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.12575960159301758, "compression_ratio": 1.8197424892703862, "no_speech_prob": 8.93939704837976e-06}, {"id": 1021, "seek": 456374, "start": 4572.26, "end": 4577.0199999999995, "text": " So that's why we're going to look at this why you should use a framework notebook.", "tokens": [407, 300, 311, 983, 321, 434, 516, 281, 574, 412, 341, 983, 291, 820, 764, 257, 8388, 21060, 13], "temperature": 0.0, "avg_logprob": -0.12575960159301758, "compression_ratio": 1.8197424892703862, "no_speech_prob": 8.93939704837976e-06}, {"id": 1022, "seek": 456374, "start": 4577.0199999999995, "end": 4579.66, "text": " And again I'm going to look at the clean version of it.", "tokens": [400, 797, 286, 478, 516, 281, 574, 412, 264, 2541, 3037, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.12575960159301758, "compression_ratio": 1.8197424892703862, "no_speech_prob": 8.93939704837976e-06}, {"id": 1023, "seek": 456374, "start": 4579.66, "end": 4585.0199999999995, "text": " And again in the clean version of it, step one is to download the data as appropriate", "tokens": [400, 797, 294, 264, 2541, 3037, 295, 309, 11, 1823, 472, 307, 281, 5484, 264, 1412, 382, 6854], "temperature": 0.0, "avg_logprob": -0.12575960159301758, "compression_ratio": 1.8197424892703862, "no_speech_prob": 8.93939704837976e-06}, {"id": 1024, "seek": 456374, "start": 4585.0199999999995, "end": 4590.94, "text": " for the Kaggle or non Kaggle environment and set the display options and set the random", "tokens": [337, 264, 48751, 22631, 420, 2107, 48751, 22631, 2823, 293, 992, 264, 4674, 3956, 293, 992, 264, 4974], "temperature": 0.0, "avg_logprob": -0.12575960159301758, "compression_ratio": 1.8197424892703862, "no_speech_prob": 8.93939704837976e-06}, {"id": 1025, "seek": 459094, "start": 4590.94, "end": 4594.86, "text": " seed and read the data frame.", "tokens": [8871, 293, 1401, 264, 1412, 3920, 13], "temperature": 0.0, "avg_logprob": -0.16786385907067192, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8058100977214053e-05}, {"id": 1026, "seek": 459094, "start": 4594.86, "end": 4602.78, "text": " Now there was so much fussing around with the doing it from scratch version that I did", "tokens": [823, 456, 390, 370, 709, 34792, 278, 926, 365, 264, 884, 309, 490, 8459, 3037, 300, 286, 630], "temperature": 0.0, "avg_logprob": -0.16786385907067192, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8058100977214053e-05}, {"id": 1027, "seek": 459094, "start": 4602.78, "end": 4606.259999999999, "text": " not want to do any feature engineering because every column I added was another thing I had", "tokens": [406, 528, 281, 360, 604, 4111, 7043, 570, 633, 7738, 286, 3869, 390, 1071, 551, 286, 632], "temperature": 0.0, "avg_logprob": -0.16786385907067192, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8058100977214053e-05}, {"id": 1028, "seek": 459094, "start": 4606.259999999999, "end": 4612.66, "text": " to think about dummy variables and normalization and random coefficient initialization and", "tokens": [281, 519, 466, 35064, 9102, 293, 2710, 2144, 293, 4974, 17619, 5883, 2144, 293], "temperature": 0.0, "avg_logprob": -0.16786385907067192, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8058100977214053e-05}, {"id": 1029, "seek": 459094, "start": 4612.66, "end": 4614.0599999999995, "text": " blah blah blah.", "tokens": [12288, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.16786385907067192, "compression_ratio": 1.5909090909090908, "no_speech_prob": 1.8058100977214053e-05}, {"id": 1030, "seek": 461406, "start": 4614.06, "end": 4621.22, "text": " But with a framework everything's so easy you can do all the feature engineering you", "tokens": [583, 365, 257, 8388, 1203, 311, 370, 1858, 291, 393, 360, 439, 264, 4111, 7043, 291], "temperature": 0.0, "avg_logprob": -0.12367796897888184, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.7777541870600544e-05}, {"id": 1031, "seek": 461406, "start": 4621.22, "end": 4623.22, "text": " want.", "tokens": [528, 13], "temperature": 0.0, "avg_logprob": -0.12367796897888184, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.7777541870600544e-05}, {"id": 1032, "seek": 461406, "start": 4623.22, "end": 4628.1, "text": " Because this isn't a lesson about feature engineering instead I plagiarized entirely", "tokens": [1436, 341, 1943, 380, 257, 6898, 466, 4111, 7043, 2602, 286, 33756, 9448, 1602, 7696], "temperature": 0.0, "avg_logprob": -0.12367796897888184, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.7777541870600544e-05}, {"id": 1033, "seek": 461406, "start": 4628.1, "end": 4637.9400000000005, "text": " from this fantastic advanced feature engineering tutorial on Kaggle.", "tokens": [490, 341, 5456, 7339, 4111, 7043, 7073, 322, 48751, 22631, 13], "temperature": 0.0, "avg_logprob": -0.12367796897888184, "compression_ratio": 1.5844155844155845, "no_speech_prob": 1.7777541870600544e-05}, {"id": 1034, "seek": 463794, "start": 4637.94, "end": 4644.54, "text": " And what this tutorial found was that in addition to the log fair we've already done that you", "tokens": [400, 437, 341, 7073, 1352, 390, 300, 294, 4500, 281, 264, 3565, 3143, 321, 600, 1217, 1096, 300, 291], "temperature": 0.0, "avg_logprob": -0.18905174847945427, "compression_ratio": 1.7804878048780488, "no_speech_prob": 8.938729479268659e-06}, {"id": 1035, "seek": 463794, "start": 4644.54, "end": 4648.94, "text": " can do cool stuff with the deck with adding up the number of family members with the people", "tokens": [393, 360, 1627, 1507, 365, 264, 9341, 365, 5127, 493, 264, 1230, 295, 1605, 2679, 365, 264, 561], "temperature": 0.0, "avg_logprob": -0.18905174847945427, "compression_ratio": 1.7804878048780488, "no_speech_prob": 8.938729479268659e-06}, {"id": 1036, "seek": 463794, "start": 4648.94, "end": 4652.98, "text": " traveling alone how many people are on each ticket and finally we're going to do stuff", "tokens": [9712, 3312, 577, 867, 561, 366, 322, 1184, 10550, 293, 2721, 321, 434, 516, 281, 360, 1507], "temperature": 0.0, "avg_logprob": -0.18905174847945427, "compression_ratio": 1.7804878048780488, "no_speech_prob": 8.938729479268659e-06}, {"id": 1037, "seek": 463794, "start": 4652.98, "end": 4660.5, "text": " with the name which is we're going to grab the Mr. Miss Mrs. Master whatever.", "tokens": [365, 264, 1315, 597, 307, 321, 434, 516, 281, 4444, 264, 2221, 13, 5275, 9814, 13, 6140, 2035, 13], "temperature": 0.0, "avg_logprob": -0.18905174847945427, "compression_ratio": 1.7804878048780488, "no_speech_prob": 8.938729479268659e-06}, {"id": 1038, "seek": 463794, "start": 4660.5, "end": 4664.299999999999, "text": " So we're going to create a function to like do some feature engineering and if you want", "tokens": [407, 321, 434, 516, 281, 1884, 257, 2445, 281, 411, 360, 512, 4111, 7043, 293, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.18905174847945427, "compression_ratio": 1.7804878048780488, "no_speech_prob": 8.938729479268659e-06}, {"id": 1039, "seek": 466430, "start": 4664.3, "end": 4672.3, "text": " to learn a bit of Python pandas here's some great lines of code to step through one by", "tokens": [281, 1466, 257, 857, 295, 15329, 4565, 296, 510, 311, 512, 869, 3876, 295, 3089, 281, 1823, 807, 472, 538], "temperature": 0.0, "avg_logprob": -0.17806298233741938, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.9309818273759447e-05}, {"id": 1040, "seek": 466430, "start": 4672.3, "end": 4677.58, "text": " one and again like take this out of a function put them into individual cells run each one", "tokens": [472, 293, 797, 411, 747, 341, 484, 295, 257, 2445, 829, 552, 666, 2609, 5438, 1190, 1184, 472], "temperature": 0.0, "avg_logprob": -0.17806298233741938, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.9309818273759447e-05}, {"id": 1041, "seek": 466430, "start": 4677.58, "end": 4684.02, "text": " look up the tutorials what does juror do what does map do what does group by and transform", "tokens": [574, 493, 264, 17616, 437, 775, 12721, 284, 360, 437, 775, 4471, 360, 437, 775, 1594, 538, 293, 4088], "temperature": 0.0, "avg_logprob": -0.17806298233741938, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.9309818273759447e-05}, {"id": 1042, "seek": 466430, "start": 4684.02, "end": 4690.58, "text": " do what does value counts do like these are all like part of the reason I put this here", "tokens": [360, 437, 775, 2158, 14893, 360, 411, 613, 366, 439, 411, 644, 295, 264, 1778, 286, 829, 341, 510], "temperature": 0.0, "avg_logprob": -0.17806298233741938, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.9309818273759447e-05}, {"id": 1043, "seek": 469058, "start": 4690.58, "end": 4695.82, "text": " was the folks that haven't done much if any pandas to have some you know examples of functions", "tokens": [390, 264, 4024, 300, 2378, 380, 1096, 709, 498, 604, 4565, 296, 281, 362, 512, 291, 458, 5110, 295, 6828], "temperature": 0.0, "avg_logprob": -0.10788311004638672, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.169016054016538e-05}, {"id": 1044, "seek": 469058, "start": 4695.82, "end": 4701.66, "text": " that I think are useful and actually refactored this code quite a bit to try to show off some", "tokens": [300, 286, 519, 366, 4420, 293, 767, 1895, 578, 2769, 341, 3089, 1596, 257, 857, 281, 853, 281, 855, 766, 512], "temperature": 0.0, "avg_logprob": -0.10788311004638672, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.169016054016538e-05}, {"id": 1045, "seek": 469058, "start": 4701.66, "end": 4705.38, "text": " features of pandas I think are really nice.", "tokens": [4122, 295, 4565, 296, 286, 519, 366, 534, 1481, 13], "temperature": 0.0, "avg_logprob": -0.10788311004638672, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.169016054016538e-05}, {"id": 1046, "seek": 469058, "start": 4705.38, "end": 4712.38, "text": " So we'll do the same random split as before so passing in the same seed and so now we're", "tokens": [407, 321, 603, 360, 264, 912, 4974, 7472, 382, 949, 370, 8437, 294, 264, 912, 8871, 293, 370, 586, 321, 434], "temperature": 0.0, "avg_logprob": -0.10788311004638672, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.169016054016538e-05}, {"id": 1047, "seek": 469058, "start": 4712.38, "end": 4717.5599999999995, "text": " going to do the same set of steps that we did manually with fast AI.", "tokens": [516, 281, 360, 264, 912, 992, 295, 4439, 300, 321, 630, 16945, 365, 2370, 7318, 13], "temperature": 0.0, "avg_logprob": -0.10788311004638672, "compression_ratio": 1.6883116883116882, "no_speech_prob": 3.169016054016538e-05}, {"id": 1048, "seek": 471756, "start": 4717.56, "end": 4724.9800000000005, "text": " So we want to create a tabular model data set based on a pandas data frame and here", "tokens": [407, 321, 528, 281, 1884, 257, 4421, 1040, 2316, 1412, 992, 2361, 322, 257, 4565, 296, 1412, 3920, 293, 510], "temperature": 0.0, "avg_logprob": -0.1519993501551011, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.0952556294796523e-05}, {"id": 1049, "seek": 471756, "start": 4724.9800000000005, "end": 4733.14, "text": " is the data frame these are the train versus validation splits I want to use here's a list", "tokens": [307, 264, 1412, 3920, 613, 366, 264, 3847, 5717, 24071, 37741, 286, 528, 281, 764, 510, 311, 257, 1329], "temperature": 0.0, "avg_logprob": -0.1519993501551011, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.0952556294796523e-05}, {"id": 1050, "seek": 471756, "start": 4733.14, "end": 4739.9400000000005, "text": " of all the stuff I want done please deal with dummy variables for me deal with missing values", "tokens": [295, 439, 264, 1507, 286, 528, 1096, 1767, 2028, 365, 35064, 9102, 337, 385, 2028, 365, 5361, 4190], "temperature": 0.0, "avg_logprob": -0.1519993501551011, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.0952556294796523e-05}, {"id": 1051, "seek": 471756, "start": 4739.9400000000005, "end": 4746.3, "text": " for me normalize continuous variables for me I'm going to tell you which ones of the", "tokens": [337, 385, 2710, 1125, 10957, 9102, 337, 385, 286, 478, 516, 281, 980, 291, 597, 2306, 295, 264], "temperature": 0.0, "avg_logprob": -0.1519993501551011, "compression_ratio": 1.7135922330097086, "no_speech_prob": 1.0952556294796523e-05}, {"id": 1052, "seek": 474630, "start": 4746.3, "end": 4751.54, "text": " categorical variables so here's for example P class is a number but I'm telling fast AI", "tokens": [19250, 804, 9102, 370, 510, 311, 337, 1365, 430, 1508, 307, 257, 1230, 457, 286, 478, 3585, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.18942585797377035, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.169013871229254e-05}, {"id": 1053, "seek": 474630, "start": 4751.54, "end": 4759.02, "text": " to treat it as categorical here's all the continuous variables here's my dependent variable", "tokens": [281, 2387, 309, 382, 19250, 804, 510, 311, 439, 264, 10957, 9102, 510, 311, 452, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.18942585797377035, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.169013871229254e-05}, {"id": 1054, "seek": 474630, "start": 4759.02, "end": 4763.860000000001, "text": " and the dependent variable is a category.", "tokens": [293, 264, 12334, 7006, 307, 257, 7719, 13], "temperature": 0.0, "avg_logprob": -0.18942585797377035, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.169013871229254e-05}, {"id": 1055, "seek": 474630, "start": 4763.860000000001, "end": 4772.1, "text": " So create data loaders from that place and save models right here in this directory.", "tokens": [407, 1884, 1412, 3677, 433, 490, 300, 1081, 293, 3155, 5245, 558, 510, 294, 341, 21120, 13], "temperature": 0.0, "avg_logprob": -0.18942585797377035, "compression_ratio": 1.6721311475409837, "no_speech_prob": 3.169013871229254e-05}, {"id": 1056, "seek": 477210, "start": 4772.1, "end": 4777.26, "text": " That's it that's all the pre-processing I need to do even with all those extra engineered", "tokens": [663, 311, 309, 300, 311, 439, 264, 659, 12, 41075, 278, 286, 643, 281, 360, 754, 365, 439, 729, 2857, 38648], "temperature": 0.0, "avg_logprob": -0.12313768598768446, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.892405816761311e-05}, {"id": 1057, "seek": 477210, "start": 4777.26, "end": 4788.22, "text": " features create a learner okay so this remember is something that contains a model and data", "tokens": [4122, 1884, 257, 33347, 1392, 370, 341, 1604, 307, 746, 300, 8306, 257, 2316, 293, 1412], "temperature": 0.0, "avg_logprob": -0.12313768598768446, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.892405816761311e-05}, {"id": 1058, "seek": 477210, "start": 4788.22, "end": 4794.1, "text": " and I want you to put in two hidden layers with 10 units and 10 units just like we did", "tokens": [293, 286, 528, 291, 281, 829, 294, 732, 7633, 7914, 365, 1266, 6815, 293, 1266, 6815, 445, 411, 321, 630], "temperature": 0.0, "avg_logprob": -0.12313768598768446, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.892405816761311e-05}, {"id": 1059, "seek": 477210, "start": 4794.1, "end": 4797.22, "text": " in our final example.", "tokens": [294, 527, 2572, 1365, 13], "temperature": 0.0, "avg_logprob": -0.12313768598768446, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.892405816761311e-05}, {"id": 1060, "seek": 479722, "start": 4797.22, "end": 4803.66, "text": " What learning rate should I use? Make a suggestion for me please so call LR find you can use", "tokens": [708, 2539, 3314, 820, 286, 764, 30, 4387, 257, 16541, 337, 385, 1767, 370, 818, 441, 49, 915, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.11408169452960674, "compression_ratio": 1.8493723849372385, "no_speech_prob": 3.119971734122373e-05}, {"id": 1061, "seek": 479722, "start": 4803.66, "end": 4810.58, "text": " this for any fast AI model now what this does is it starts at a learning rate that's very", "tokens": [341, 337, 604, 2370, 7318, 2316, 586, 437, 341, 775, 307, 309, 3719, 412, 257, 2539, 3314, 300, 311, 588], "temperature": 0.0, "avg_logprob": -0.11408169452960674, "compression_ratio": 1.8493723849372385, "no_speech_prob": 3.119971734122373e-05}, {"id": 1062, "seek": 479722, "start": 4810.58, "end": 4816.34, "text": " very small 10 to the negative 7 and it puts in one batch of data and it calculates the", "tokens": [588, 1359, 1266, 281, 264, 3671, 1614, 293, 309, 8137, 294, 472, 15245, 295, 1412, 293, 309, 4322, 1024, 264], "temperature": 0.0, "avg_logprob": -0.11408169452960674, "compression_ratio": 1.8493723849372385, "no_speech_prob": 3.119971734122373e-05}, {"id": 1063, "seek": 479722, "start": 4816.34, "end": 4821.58, "text": " loss and then it puts through and then it increases the learning rate slightly and puts", "tokens": [4470, 293, 550, 309, 8137, 807, 293, 550, 309, 8637, 264, 2539, 3314, 4748, 293, 8137], "temperature": 0.0, "avg_logprob": -0.11408169452960674, "compression_ratio": 1.8493723849372385, "no_speech_prob": 3.119971734122373e-05}, {"id": 1064, "seek": 479722, "start": 4821.58, "end": 4824.9800000000005, "text": " through another batch of data and it keeps doing that for higher and higher learning", "tokens": [807, 1071, 15245, 295, 1412, 293, 309, 5965, 884, 300, 337, 2946, 293, 2946, 2539], "temperature": 0.0, "avg_logprob": -0.11408169452960674, "compression_ratio": 1.8493723849372385, "no_speech_prob": 3.119971734122373e-05}, {"id": 1065, "seek": 482498, "start": 4824.98, "end": 4829.7, "text": " rates and it keeps track of the loss as it increases the learning rate just one batch", "tokens": [6846, 293, 309, 5965, 2837, 295, 264, 4470, 382, 309, 8637, 264, 2539, 3314, 445, 472, 15245], "temperature": 0.0, "avg_logprob": -0.08024526615532077, "compression_ratio": 1.8874458874458875, "no_speech_prob": 3.82250000257045e-05}, {"id": 1066, "seek": 482498, "start": 4829.7, "end": 4837.219999999999, "text": " of data at a time and what happens is for the very small learning rates nothing happens", "tokens": [295, 1412, 412, 257, 565, 293, 437, 2314, 307, 337, 264, 588, 1359, 2539, 6846, 1825, 2314], "temperature": 0.0, "avg_logprob": -0.08024526615532077, "compression_ratio": 1.8874458874458875, "no_speech_prob": 3.82250000257045e-05}, {"id": 1067, "seek": 482498, "start": 4837.219999999999, "end": 4841.78, "text": " but then once you get high enough the loss starts improving and then as it gets higher", "tokens": [457, 550, 1564, 291, 483, 1090, 1547, 264, 4470, 3719, 11470, 293, 550, 382, 309, 2170, 2946], "temperature": 0.0, "avg_logprob": -0.08024526615532077, "compression_ratio": 1.8874458874458875, "no_speech_prob": 3.82250000257045e-05}, {"id": 1068, "seek": 482498, "start": 4841.78, "end": 4846.419999999999, "text": " it improves faster until you make the learning rate so big that it overshoots and then it", "tokens": [309, 24771, 4663, 1826, 291, 652, 264, 2539, 3314, 370, 955, 300, 309, 15488, 1289, 1971, 293, 550, 309], "temperature": 0.0, "avg_logprob": -0.08024526615532077, "compression_ratio": 1.8874458874458875, "no_speech_prob": 3.82250000257045e-05}, {"id": 1069, "seek": 482498, "start": 4846.419999999999, "end": 4853.379999999999, "text": " kills it and so generally somewhere around here is the learning rate you want fast AI", "tokens": [14563, 309, 293, 370, 5101, 4079, 926, 510, 307, 264, 2539, 3314, 291, 528, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.08024526615532077, "compression_ratio": 1.8874458874458875, "no_speech_prob": 3.82250000257045e-05}, {"id": 1070, "seek": 485338, "start": 4853.38, "end": 4856.82, "text": " has a few different ways of recommending a learning rate you can look up the docs to", "tokens": [575, 257, 1326, 819, 2098, 295, 30559, 257, 2539, 3314, 291, 393, 574, 493, 264, 45623, 281], "temperature": 0.0, "avg_logprob": -0.08562026998048188, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3845515240973327e-05}, {"id": 1071, "seek": 485338, "start": 4856.82, "end": 4862.42, "text": " see what they mean I generally find if you choose slide and valley and pick one between", "tokens": [536, 437, 436, 914, 286, 5101, 915, 498, 291, 2826, 4137, 293, 17636, 293, 1888, 472, 1296], "temperature": 0.0, "avg_logprob": -0.08562026998048188, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3845515240973327e-05}, {"id": 1072, "seek": 485338, "start": 4862.42, "end": 4870.46, "text": " the two you get a pretty good learning rate so here we've got about 0.01 and about 0.08", "tokens": [264, 732, 291, 483, 257, 1238, 665, 2539, 3314, 370, 510, 321, 600, 658, 466, 1958, 13, 10607, 293, 466, 1958, 13, 16133], "temperature": 0.0, "avg_logprob": -0.08562026998048188, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3845515240973327e-05}, {"id": 1073, "seek": 485338, "start": 4870.46, "end": 4881.7, "text": " so I picked 0.03 so just run a bunch of epochs away it goes ta-da this is a bit crazy after", "tokens": [370, 286, 6183, 1958, 13, 11592, 370, 445, 1190, 257, 3840, 295, 30992, 28346, 1314, 309, 1709, 1846, 12, 2675, 341, 307, 257, 857, 3219, 934], "temperature": 0.0, "avg_logprob": -0.08562026998048188, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.3845515240973327e-05}, {"id": 1074, "seek": 488170, "start": 4881.7, "end": 4886.66, "text": " all that we've ended up exactly the same accuracy as the last two models that's just a coincidence", "tokens": [439, 300, 321, 600, 4590, 493, 2293, 264, 912, 14170, 382, 264, 1036, 732, 5245, 300, 311, 445, 257, 22137], "temperature": 0.0, "avg_logprob": -0.10608861156713183, "compression_ratio": 1.7738095238095237, "no_speech_prob": 4.3992589780827984e-05}, {"id": 1075, "seek": 488170, "start": 4886.66, "end": 4893.099999999999, "text": " right I mean there's nothing particularly about that accuracy and so at this point we", "tokens": [558, 286, 914, 456, 311, 1825, 4098, 466, 300, 14170, 293, 370, 412, 341, 935, 321], "temperature": 0.0, "avg_logprob": -0.10608861156713183, "compression_ratio": 1.7738095238095237, "no_speech_prob": 4.3992589780827984e-05}, {"id": 1076, "seek": 488170, "start": 4893.099999999999, "end": 4899.78, "text": " can now submit that to Kaggle now remember with the linear model we had to repeat all", "tokens": [393, 586, 10315, 300, 281, 48751, 22631, 586, 1604, 365, 264, 8213, 2316, 321, 632, 281, 7149, 439], "temperature": 0.0, "avg_logprob": -0.10608861156713183, "compression_ratio": 1.7738095238095237, "no_speech_prob": 4.3992589780827984e-05}, {"id": 1077, "seek": 488170, "start": 4899.78, "end": 4905.38, "text": " of the pre-processing steps on the test set in exactly the same way don't have to worry", "tokens": [295, 264, 659, 12, 41075, 278, 4439, 322, 264, 1500, 992, 294, 2293, 264, 912, 636, 500, 380, 362, 281, 3292], "temperature": 0.0, "avg_logprob": -0.10608861156713183, "compression_ratio": 1.7738095238095237, "no_speech_prob": 4.3992589780827984e-05}, {"id": 1078, "seek": 488170, "start": 4905.38, "end": 4910.66, "text": " about it with fast AI and fast AI I mean we still have to deal with the fill missing for", "tokens": [466, 309, 365, 2370, 7318, 293, 2370, 7318, 286, 914, 321, 920, 362, 281, 2028, 365, 264, 2836, 5361, 337], "temperature": 0.0, "avg_logprob": -0.10608861156713183, "compression_ratio": 1.7738095238095237, "no_speech_prob": 4.3992589780827984e-05}, {"id": 1079, "seek": 491066, "start": 4910.66, "end": 4916.5, "text": " fair because that's that's that we have to add our feature engineering features but all", "tokens": [3143, 570, 300, 311, 300, 311, 300, 321, 362, 281, 909, 527, 4111, 7043, 4122, 457, 439], "temperature": 0.0, "avg_logprob": -0.09926358177548363, "compression_ratio": 1.8940677966101696, "no_speech_prob": 1.777743273123633e-05}, {"id": 1080, "seek": 491066, "start": 4916.5, "end": 4921.5, "text": " the pre-processing we just have to use this one function called test DL that says create", "tokens": [264, 659, 12, 41075, 278, 321, 445, 362, 281, 764, 341, 472, 2445, 1219, 1500, 413, 43, 300, 1619, 1884], "temperature": 0.0, "avg_logprob": -0.09926358177548363, "compression_ratio": 1.8940677966101696, "no_speech_prob": 1.777743273123633e-05}, {"id": 1081, "seek": 491066, "start": 4921.5, "end": 4927.98, "text": " a data loader that contains exactly the same pre-processing steps that our learner used", "tokens": [257, 1412, 3677, 260, 300, 8306, 2293, 264, 912, 659, 12, 41075, 278, 4439, 300, 527, 33347, 1143], "temperature": 0.0, "avg_logprob": -0.09926358177548363, "compression_ratio": 1.8940677966101696, "no_speech_prob": 1.777743273123633e-05}, {"id": 1082, "seek": 491066, "start": 4927.98, "end": 4932.639999999999, "text": " and that's it that's all you need so just because you want to make sure that your inference", "tokens": [293, 300, 311, 309, 300, 311, 439, 291, 643, 370, 445, 570, 291, 528, 281, 652, 988, 300, 428, 38253], "temperature": 0.0, "avg_logprob": -0.09926358177548363, "compression_ratio": 1.8940677966101696, "no_speech_prob": 1.777743273123633e-05}, {"id": 1083, "seek": 491066, "start": 4932.639999999999, "end": 4939.3, "text": " time transformations pre-processing are exactly the same as a training time so this is the", "tokens": [565, 34852, 659, 12, 41075, 278, 366, 2293, 264, 912, 382, 257, 3097, 565, 370, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.09926358177548363, "compression_ratio": 1.8940677966101696, "no_speech_prob": 1.777743273123633e-05}, {"id": 1084, "seek": 493930, "start": 4939.3, "end": 4945.9400000000005, "text": " magic method which does that just one line of code and then to get your predictions you", "tokens": [5585, 3170, 597, 775, 300, 445, 472, 1622, 295, 3089, 293, 550, 281, 483, 428, 21264, 291], "temperature": 0.0, "avg_logprob": -0.07567883982802882, "compression_ratio": 1.6835443037974684, "no_speech_prob": 1.4509113498206716e-05}, {"id": 1085, "seek": 493930, "start": 4945.9400000000005, "end": 4953.38, "text": " just say get preds and pass in that data loader I just built and so then these three lines", "tokens": [445, 584, 483, 3852, 82, 293, 1320, 294, 300, 1412, 3677, 260, 286, 445, 3094, 293, 370, 550, 613, 1045, 3876], "temperature": 0.0, "avg_logprob": -0.07567883982802882, "compression_ratio": 1.6835443037974684, "no_speech_prob": 1.4509113498206716e-05}, {"id": 1086, "seek": 493930, "start": 4953.38, "end": 4959.78, "text": " of code are the same as the previous notebook and we can take a look at the top and you", "tokens": [295, 3089, 366, 264, 912, 382, 264, 3894, 21060, 293, 321, 393, 747, 257, 574, 412, 264, 1192, 293, 291], "temperature": 0.0, "avg_logprob": -0.07567883982802882, "compression_ratio": 1.6835443037974684, "no_speech_prob": 1.4509113498206716e-05}, {"id": 1087, "seek": 495978, "start": 4959.78, "end": 4978.38, "text": " can see there it is so how did that go I don't remember no I didn't say I think it was again", "tokens": [393, 536, 456, 309, 307, 370, 577, 630, 300, 352, 286, 500, 380, 1604, 572, 286, 994, 380, 584, 286, 519, 309, 390, 797], "temperature": 0.0, "avg_logprob": -0.1258585971334706, "compression_ratio": 1.4508196721311475, "no_speech_prob": 1.6441135812783614e-05}, {"id": 1088, "seek": 495978, "start": 4978.38, "end": 4989.34, "text": " basically middle of the pack if I remember correctly so one of the nice things about", "tokens": [1936, 2808, 295, 264, 2844, 498, 286, 1604, 8944, 370, 472, 295, 264, 1481, 721, 466], "temperature": 0.0, "avg_logprob": -0.1258585971334706, "compression_ratio": 1.4508196721311475, "no_speech_prob": 1.6441135812783614e-05}, {"id": 1089, "seek": 498934, "start": 4989.34, "end": 4996.5, "text": " now that it's so easy so like add features and build models is we can experiment with", "tokens": [586, 300, 309, 311, 370, 1858, 370, 411, 909, 4122, 293, 1322, 5245, 307, 321, 393, 5120, 365], "temperature": 0.0, "avg_logprob": -0.09371182976699458, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.005730079370551e-05}, {"id": 1090, "seek": 498934, "start": 4996.5, "end": 5001.74, "text": " things much more quickly so I'm going to show you how easy it is to experiment with you", "tokens": [721, 709, 544, 2661, 370, 286, 478, 516, 281, 855, 291, 577, 1858, 309, 307, 281, 5120, 365, 291], "temperature": 0.0, "avg_logprob": -0.09371182976699458, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.005730079370551e-05}, {"id": 1091, "seek": 498934, "start": 5001.74, "end": 5007.14, "text": " know what's often considered a fairly advanced idea which is called ensembling there's lots", "tokens": [458, 437, 311, 2049, 4888, 257, 6457, 7339, 1558, 597, 307, 1219, 12567, 2504, 1688, 456, 311, 3195], "temperature": 0.0, "avg_logprob": -0.09371182976699458, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.005730079370551e-05}, {"id": 1092, "seek": 498934, "start": 5007.14, "end": 5014.38, "text": " of ways of doing ensembling but basically ensembling is about creating multiple models", "tokens": [295, 2098, 295, 884, 12567, 2504, 1688, 457, 1936, 12567, 2504, 1688, 307, 466, 4084, 3866, 5245], "temperature": 0.0, "avg_logprob": -0.09371182976699458, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.005730079370551e-05}, {"id": 1093, "seek": 501438, "start": 5014.38, "end": 5022.9800000000005, "text": " and combining their predictions and the easiest kind of ensemble to do is just to literally", "tokens": [293, 21928, 641, 21264, 293, 264, 12889, 733, 295, 19492, 281, 360, 307, 445, 281, 3736], "temperature": 0.0, "avg_logprob": -0.08216296015559016, "compression_ratio": 1.8080808080808082, "no_speech_prob": 1.078287823474966e-05}, {"id": 1094, "seek": 501438, "start": 5022.9800000000005, "end": 5027.92, "text": " just build multiple models and so each one is going to have a different set of randomly", "tokens": [445, 1322, 3866, 5245, 293, 370, 1184, 472, 307, 516, 281, 362, 257, 819, 992, 295, 16979], "temperature": 0.0, "avg_logprob": -0.08216296015559016, "compression_ratio": 1.8080808080808082, "no_speech_prob": 1.078287823474966e-05}, {"id": 1095, "seek": 501438, "start": 5027.92, "end": 5032.58, "text": " initialized coefficients and therefore each one is going to end up with a different set", "tokens": [5883, 1602, 31994, 293, 4412, 1184, 472, 307, 516, 281, 917, 493, 365, 257, 819, 992], "temperature": 0.0, "avg_logprob": -0.08216296015559016, "compression_ratio": 1.8080808080808082, "no_speech_prob": 1.078287823474966e-05}, {"id": 1096, "seek": 501438, "start": 5032.58, "end": 5038.38, "text": " of predictions so I just create a function called ensemble which creates a learner exactly", "tokens": [295, 21264, 370, 286, 445, 1884, 257, 2445, 1219, 19492, 597, 7829, 257, 33347, 2293], "temperature": 0.0, "avg_logprob": -0.08216296015559016, "compression_ratio": 1.8080808080808082, "no_speech_prob": 1.078287823474966e-05}, {"id": 1097, "seek": 503838, "start": 5038.38, "end": 5045.06, "text": " the same as before fits exactly the same as before and returns the predictions and so", "tokens": [264, 912, 382, 949, 9001, 2293, 264, 912, 382, 949, 293, 11247, 264, 21264, 293, 370], "temperature": 0.0, "avg_logprob": -0.09098147537748692, "compression_ratio": 1.7181208053691275, "no_speech_prob": 3.611941565395682e-06}, {"id": 1098, "seek": 503838, "start": 5045.06, "end": 5050.82, "text": " we'll just use a list comprehension to do that five times so that's going to create", "tokens": [321, 603, 445, 764, 257, 1329, 44991, 281, 360, 300, 1732, 1413, 370, 300, 311, 516, 281, 1884], "temperature": 0.0, "avg_logprob": -0.09098147537748692, "compression_ratio": 1.7181208053691275, "no_speech_prob": 3.611941565395682e-06}, {"id": 1099, "seek": 503838, "start": 5050.82, "end": 5063.36, "text": " a set of five predictions done so now we can take all those predictions and stack them", "tokens": [257, 992, 295, 1732, 21264, 1096, 370, 586, 321, 393, 747, 439, 729, 21264, 293, 8630, 552], "temperature": 0.0, "avg_logprob": -0.09098147537748692, "compression_ratio": 1.7181208053691275, "no_speech_prob": 3.611941565395682e-06}, {"id": 1100, "seek": 506336, "start": 5063.36, "end": 5069.82, "text": " together and take the mean over the rows so that's going to give us the was actually", "tokens": [1214, 293, 747, 264, 914, 670, 264, 13241, 370, 300, 311, 516, 281, 976, 505, 264, 390, 767], "temperature": 0.0, "avg_logprob": -0.1170795117655108, "compression_ratio": 1.8402777777777777, "no_speech_prob": 7.646202902833465e-06}, {"id": 1101, "seek": 506336, "start": 5069.82, "end": 5076.78, "text": " so the mean over the over the first dimension so the mean over the sets of predictions and", "tokens": [370, 264, 914, 670, 264, 670, 264, 700, 10139, 370, 264, 914, 670, 264, 6352, 295, 21264, 293], "temperature": 0.0, "avg_logprob": -0.1170795117655108, "compression_ratio": 1.8402777777777777, "no_speech_prob": 7.646202902833465e-06}, {"id": 1102, "seek": 506336, "start": 5076.78, "end": 5082.46, "text": " so that will give us the average prediction of our five models but again we can turn that", "tokens": [370, 300, 486, 976, 505, 264, 4274, 17630, 295, 527, 1732, 5245, 457, 797, 321, 393, 1261, 300], "temperature": 0.0, "avg_logprob": -0.1170795117655108, "compression_ratio": 1.8402777777777777, "no_speech_prob": 7.646202902833465e-06}, {"id": 1103, "seek": 508246, "start": 5082.46, "end": 5096.3, "text": " into a CSV and submit it to cattle and that one I think that went a bit better let's check", "tokens": [666, 257, 48814, 293, 10315, 309, 281, 19992, 293, 300, 472, 286, 519, 300, 1437, 257, 857, 1101, 718, 311, 1520], "temperature": 0.0, "avg_logprob": -0.10550159063094701, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.2028938726871274e-05}, {"id": 1104, "seek": 508246, "start": 5096.3, "end": 5100.58, "text": " yeah okay so that one actually finally gets into the top 20 percent 25 percent in the", "tokens": [1338, 1392, 370, 300, 472, 767, 2721, 2170, 666, 264, 1192, 945, 3043, 3552, 3043, 294, 264], "temperature": 0.0, "avg_logprob": -0.10550159063094701, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.2028938726871274e-05}, {"id": 1105, "seek": 508246, "start": 5100.58, "end": 5105.46, "text": " competition so I mean not amazing by any means but you can see that you know this simple", "tokens": [6211, 370, 286, 914, 406, 2243, 538, 604, 1355, 457, 291, 393, 536, 300, 291, 458, 341, 2199], "temperature": 0.0, "avg_logprob": -0.10550159063094701, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.2028938726871274e-05}, {"id": 1106, "seek": 508246, "start": 5105.46, "end": 5111.5, "text": " step of creating five independently trained models just starting from different starting", "tokens": [1823, 295, 4084, 1732, 21761, 8895, 5245, 445, 2891, 490, 819, 2891], "temperature": 0.0, "avg_logprob": -0.10550159063094701, "compression_ratio": 1.5803571428571428, "no_speech_prob": 1.2028938726871274e-05}, {"id": 1107, "seek": 511150, "start": 5111.5, "end": 5119.02, "text": " points in terms of random coefficients actually improved us from top 50 percent to top 25", "tokens": [2793, 294, 2115, 295, 4974, 31994, 767, 9689, 505, 490, 1192, 2625, 3043, 281, 1192, 3552], "temperature": 0.0, "avg_logprob": -0.163627104325728, "compression_ratio": 1.7184466019417475, "no_speech_prob": 6.921311432961375e-05}, {"id": 1108, "seek": 511150, "start": 5119.02, "end": 5125.9, "text": " percent John is there an argument because you've got a categorical result you're zero", "tokens": [3043, 2619, 307, 456, 364, 6770, 570, 291, 600, 658, 257, 19250, 804, 1874, 291, 434, 4018], "temperature": 0.0, "avg_logprob": -0.163627104325728, "compression_ratio": 1.7184466019417475, "no_speech_prob": 6.921311432961375e-05}, {"id": 1109, "seek": 511150, "start": 5125.9, "end": 5129.34, "text": " one effectively is there an argument that you might use the mode of the ensemble rather", "tokens": [472, 8659, 307, 456, 364, 6770, 300, 291, 1062, 764, 264, 4391, 295, 264, 19492, 2831], "temperature": 0.0, "avg_logprob": -0.163627104325728, "compression_ratio": 1.7184466019417475, "no_speech_prob": 6.921311432961375e-05}, {"id": 1110, "seek": 511150, "start": 5129.34, "end": 5141.42, "text": " than the numerical mean I mean yes there's an argument that's been made and yeah something", "tokens": [813, 264, 29054, 914, 286, 914, 2086, 456, 311, 364, 6770, 300, 311, 668, 1027, 293, 1338, 746], "temperature": 0.0, "avg_logprob": -0.163627104325728, "compression_ratio": 1.7184466019417475, "no_speech_prob": 6.921311432961375e-05}, {"id": 1111, "seek": 514142, "start": 5141.42, "end": 5150.14, "text": " I would just try I generally find it's less good but not always and I don't feel like", "tokens": [286, 576, 445, 853, 286, 5101, 915, 309, 311, 1570, 665, 457, 406, 1009, 293, 286, 500, 380, 841, 411], "temperature": 0.0, "avg_logprob": -0.1037094489387844, "compression_ratio": 1.7868020304568528, "no_speech_prob": 2.429884443699848e-05}, {"id": 1112, "seek": 514142, "start": 5150.14, "end": 5155.66, "text": " I've got a great intuition as to why and I don't feel like I've seen any studies as to", "tokens": [286, 600, 658, 257, 869, 24002, 382, 281, 983, 293, 286, 500, 380, 841, 411, 286, 600, 1612, 604, 5313, 382, 281], "temperature": 0.0, "avg_logprob": -0.1037094489387844, "compression_ratio": 1.7868020304568528, "no_speech_prob": 2.429884443699848e-05}, {"id": 1113, "seek": 514142, "start": 5155.66, "end": 5160.22, "text": " why you could predict like there's a few there's there's at least three things you could do", "tokens": [983, 291, 727, 6069, 411, 456, 311, 257, 1326, 456, 311, 456, 311, 412, 1935, 1045, 721, 291, 727, 360], "temperature": 0.0, "avg_logprob": -0.1037094489387844, "compression_ratio": 1.7868020304568528, "no_speech_prob": 2.429884443699848e-05}, {"id": 1114, "seek": 514142, "start": 5160.22, "end": 5168.14, "text": " right you could take the is it greater or less than 0.5 ones and zeros and average them", "tokens": [558, 291, 727, 747, 264, 307, 309, 5044, 420, 1570, 813, 1958, 13, 20, 2306, 293, 35193, 293, 4274, 552], "temperature": 0.0, "avg_logprob": -0.1037094489387844, "compression_ratio": 1.7868020304568528, "no_speech_prob": 2.429884443699848e-05}, {"id": 1115, "seek": 516814, "start": 5168.14, "end": 5172.14, "text": " or you could take the mode of them or you could take the actual probable probability", "tokens": [420, 291, 727, 747, 264, 4391, 295, 552, 420, 291, 727, 747, 264, 3539, 21759, 8482], "temperature": 0.0, "avg_logprob": -0.09771212553366637, "compression_ratio": 1.7959183673469388, "no_speech_prob": 2.586571099527646e-05}, {"id": 1116, "seek": 516814, "start": 5172.14, "end": 5176.660000000001, "text": " predictions and take the average of those and then threshold that and I've seen examples", "tokens": [21264, 293, 747, 264, 4274, 295, 729, 293, 550, 14678, 300, 293, 286, 600, 1612, 5110], "temperature": 0.0, "avg_logprob": -0.09771212553366637, "compression_ratio": 1.7959183673469388, "no_speech_prob": 2.586571099527646e-05}, {"id": 1117, "seek": 516814, "start": 5176.660000000001, "end": 5182.26, "text": " where certainly both of the different averaging versions each of them has been better I don't", "tokens": [689, 3297, 1293, 295, 264, 819, 47308, 9606, 1184, 295, 552, 575, 668, 1101, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.09771212553366637, "compression_ratio": 1.7959183673469388, "no_speech_prob": 2.586571099527646e-05}, {"id": 1118, "seek": 516814, "start": 5182.26, "end": 5189.660000000001, "text": " think I've seen one where the modes better but that was very popular back in the 90s", "tokens": [519, 286, 600, 1612, 472, 689, 264, 14068, 1101, 457, 300, 390, 588, 3743, 646, 294, 264, 4289, 82], "temperature": 0.0, "avg_logprob": -0.09771212553366637, "compression_ratio": 1.7959183673469388, "no_speech_prob": 2.586571099527646e-05}, {"id": 1119, "seek": 518966, "start": 5189.66, "end": 5203.54, "text": " so yeah so it's so easy to try you might as well give it a go okay we don't have time", "tokens": [370, 1338, 370, 309, 311, 370, 1858, 281, 853, 291, 1062, 382, 731, 976, 309, 257, 352, 1392, 321, 500, 380, 362, 565], "temperature": 0.0, "avg_logprob": -0.127602090438207, "compression_ratio": 1.4786324786324787, "no_speech_prob": 3.761963307624683e-05}, {"id": 1120, "seek": 518966, "start": 5203.54, "end": 5214.74, "text": " to finish the next notebook but let's make a start on it so the next notebook is random", "tokens": [281, 2413, 264, 958, 21060, 457, 718, 311, 652, 257, 722, 322, 309, 370, 264, 958, 21060, 307, 4974], "temperature": 0.0, "avg_logprob": -0.127602090438207, "compression_ratio": 1.4786324786324787, "no_speech_prob": 3.761963307624683e-05}, {"id": 1121, "seek": 521474, "start": 5214.74, "end": 5223.62, "text": " forests how random forests really work who here has heard of random forests before nearly", "tokens": [21700, 577, 4974, 21700, 534, 589, 567, 510, 575, 2198, 295, 4974, 21700, 949, 6217], "temperature": 0.0, "avg_logprob": -0.11896663052695138, "compression_ratio": 1.5470588235294118, "no_speech_prob": 3.881194788846187e-05}, {"id": 1122, "seek": 521474, "start": 5223.62, "end": 5232.34, "text": " everybody okay so very popular developed I think initially in 1999 but you know gradually", "tokens": [2201, 1392, 370, 588, 3743, 4743, 286, 519, 9105, 294, 19952, 457, 291, 458, 13145], "temperature": 0.0, "avg_logprob": -0.11896663052695138, "compression_ratio": 1.5470588235294118, "no_speech_prob": 3.881194788846187e-05}, {"id": 1123, "seek": 521474, "start": 5232.34, "end": 5238.36, "text": " improved in popularity during the 2000s I was like everybody kind of knew me as Mr.", "tokens": [9689, 294, 19301, 1830, 264, 8132, 82, 286, 390, 411, 2201, 733, 295, 2586, 385, 382, 2221, 13], "temperature": 0.0, "avg_logprob": -0.11896663052695138, "compression_ratio": 1.5470588235294118, "no_speech_prob": 3.881194788846187e-05}, {"id": 1124, "seek": 523836, "start": 5238.36, "end": 5247.46, "text": " Random Forests for years I implemented them like a couple of days after the original technical", "tokens": [37603, 18124, 82, 337, 924, 286, 12270, 552, 411, 257, 1916, 295, 1708, 934, 264, 3380, 6191], "temperature": 0.0, "avg_logprob": -0.10191436111927032, "compression_ratio": 1.4915254237288136, "no_speech_prob": 1.4282833944889717e-05}, {"id": 1125, "seek": 523836, "start": 5247.46, "end": 5254.099999999999, "text": " report came out I was such a fan all of my early Kaggle results random forests I love", "tokens": [2275, 1361, 484, 286, 390, 1270, 257, 3429, 439, 295, 452, 2440, 48751, 22631, 3542, 4974, 21700, 286, 959], "temperature": 0.0, "avg_logprob": -0.10191436111927032, "compression_ratio": 1.4915254237288136, "no_speech_prob": 1.4282833944889717e-05}, {"id": 1126, "seek": 523836, "start": 5254.099999999999, "end": 5261.179999999999, "text": " them and I think hopefully you'll see why I'm such a fan of them because they're so", "tokens": [552, 293, 286, 519, 4696, 291, 603, 536, 983, 286, 478, 1270, 257, 3429, 295, 552, 570, 436, 434, 370], "temperature": 0.0, "avg_logprob": -0.10191436111927032, "compression_ratio": 1.4915254237288136, "no_speech_prob": 1.4282833944889717e-05}, {"id": 1127, "seek": 526118, "start": 5261.18, "end": 5269.740000000001, "text": " elegant and they're almost impossible to mess up a lot of people will say like oh why are", "tokens": [21117, 293, 436, 434, 1920, 6243, 281, 2082, 493, 257, 688, 295, 561, 486, 584, 411, 1954, 983, 366], "temperature": 0.0, "avg_logprob": -0.12007031148793745, "compression_ratio": 1.8493723849372385, "no_speech_prob": 2.2122723748907447e-05}, {"id": 1128, "seek": 526118, "start": 5269.740000000001, "end": 5275.9800000000005, "text": " you using machine learning why don't you use something simple like logistic regression", "tokens": [291, 1228, 3479, 2539, 983, 500, 380, 291, 764, 746, 2199, 411, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.12007031148793745, "compression_ratio": 1.8493723849372385, "no_speech_prob": 2.2122723748907447e-05}, {"id": 1129, "seek": 526118, "start": 5275.9800000000005, "end": 5281.3, "text": " and I think like oh gosh in industry I've seen far more examples of people screwing", "tokens": [293, 286, 519, 411, 1954, 6502, 294, 3518, 286, 600, 1612, 1400, 544, 5110, 295, 561, 5630, 278], "temperature": 0.0, "avg_logprob": -0.12007031148793745, "compression_ratio": 1.8493723849372385, "no_speech_prob": 2.2122723748907447e-05}, {"id": 1130, "seek": 526118, "start": 5281.3, "end": 5285.06, "text": " up logistic regression than successfully using logistic regression because it's very very", "tokens": [493, 3565, 3142, 24590, 813, 10727, 1228, 3565, 3142, 24590, 570, 309, 311, 588, 588], "temperature": 0.0, "avg_logprob": -0.12007031148793745, "compression_ratio": 1.8493723849372385, "no_speech_prob": 2.2122723748907447e-05}, {"id": 1131, "seek": 526118, "start": 5285.06, "end": 5290.06, "text": " very very difficult to do correctly you know you've got to make sure you've got the correct", "tokens": [588, 588, 2252, 281, 360, 8944, 291, 458, 291, 600, 658, 281, 652, 988, 291, 600, 658, 264, 3006], "temperature": 0.0, "avg_logprob": -0.12007031148793745, "compression_ratio": 1.8493723849372385, "no_speech_prob": 2.2122723748907447e-05}, {"id": 1132, "seek": 529006, "start": 5290.06, "end": 5294.820000000001, "text": " transformations and the correct interactions and the correct outlier handling and blah", "tokens": [34852, 293, 264, 3006, 13280, 293, 264, 3006, 484, 2753, 13175, 293, 12288], "temperature": 0.0, "avg_logprob": -0.1406343556657622, "compression_ratio": 1.7772277227722773, "no_speech_prob": 1.952359343704302e-05}, {"id": 1133, "seek": 529006, "start": 5294.820000000001, "end": 5303.780000000001, "text": " blah blah and anything you get wrong the entire thing falls apart random forests I it's very", "tokens": [12288, 12288, 293, 1340, 291, 483, 2085, 264, 2302, 551, 8804, 4936, 4974, 21700, 286, 309, 311, 588], "temperature": 0.0, "avg_logprob": -0.1406343556657622, "compression_ratio": 1.7772277227722773, "no_speech_prob": 1.952359343704302e-05}, {"id": 1134, "seek": 529006, "start": 5303.780000000001, "end": 5308.14, "text": " rare to that I've seen somebody screw up a random forest in industry they're very hard", "tokens": [5892, 281, 300, 286, 600, 1612, 2618, 5630, 493, 257, 4974, 6719, 294, 3518, 436, 434, 588, 1152], "temperature": 0.0, "avg_logprob": -0.1406343556657622, "compression_ratio": 1.7772277227722773, "no_speech_prob": 1.952359343704302e-05}, {"id": 1135, "seek": 529006, "start": 5308.14, "end": 5319.9800000000005, "text": " to screw up because they're they're so resilient and you'll see why so in this notebook just", "tokens": [281, 5630, 493, 570, 436, 434, 436, 434, 370, 23699, 293, 291, 603, 536, 983, 370, 294, 341, 21060, 445], "temperature": 0.0, "avg_logprob": -0.1406343556657622, "compression_ratio": 1.7772277227722773, "no_speech_prob": 1.952359343704302e-05}, {"id": 1136, "seek": 531998, "start": 5319.98, "end": 5324.0599999999995, "text": " by the way rather than importing lump I and pandas and that plot lid and blah blah blah", "tokens": [538, 264, 636, 2831, 813, 43866, 25551, 286, 293, 4565, 296, 293, 300, 7542, 10252, 293, 12288, 12288, 12288], "temperature": 0.0, "avg_logprob": -0.20970492135910762, "compression_ratio": 1.6854460093896713, "no_speech_prob": 5.64864712941926e-05}, {"id": 1137, "seek": 531998, "start": 5324.0599999999995, "end": 5329.099999999999, "text": " there's a little handy shortcut which is if you just import everything from fast AI imports", "tokens": [456, 311, 257, 707, 13239, 24822, 597, 307, 498, 291, 445, 974, 1203, 490, 2370, 7318, 41596], "temperature": 0.0, "avg_logprob": -0.20970492135910762, "compression_ratio": 1.6854460093896713, "no_speech_prob": 5.64864712941926e-05}, {"id": 1138, "seek": 531998, "start": 5329.099999999999, "end": 5334.5, "text": " that imports all the things that you normally want so I mean doesn't do anything special", "tokens": [300, 41596, 439, 264, 721, 300, 291, 5646, 528, 370, 286, 914, 1177, 380, 360, 1340, 2121], "temperature": 0.0, "avg_logprob": -0.20970492135910762, "compression_ratio": 1.6854460093896713, "no_speech_prob": 5.64864712941926e-05}, {"id": 1139, "seek": 531998, "start": 5334.5, "end": 5342.219999999999, "text": " but it's just save so messing around so again we've got our cell here to grab the data and", "tokens": [457, 309, 311, 445, 3155, 370, 23258, 926, 370, 797, 321, 600, 658, 527, 2815, 510, 281, 4444, 264, 1412, 293], "temperature": 0.0, "avg_logprob": -0.20970492135910762, "compression_ratio": 1.6854460093896713, "no_speech_prob": 5.64864712941926e-05}, {"id": 1140, "seek": 534222, "start": 5342.22, "end": 5352.34, "text": " I'm just going to do some basic pre-processing here with my fill in a for the fair only needed", "tokens": [286, 478, 445, 516, 281, 360, 512, 3875, 659, 12, 41075, 278, 510, 365, 452, 2836, 294, 257, 337, 264, 3143, 787, 2978], "temperature": 0.0, "avg_logprob": -0.1426374677201392, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.720087887719274e-05}, {"id": 1141, "seek": 534222, "start": 5352.34, "end": 5360.66, "text": " for the test set of course grab the modes and do the fill in a on the in the modes take", "tokens": [337, 264, 1500, 992, 295, 1164, 4444, 264, 14068, 293, 360, 264, 2836, 294, 257, 322, 264, 294, 264, 14068, 747], "temperature": 0.0, "avg_logprob": -0.1426374677201392, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.720087887719274e-05}, {"id": 1142, "seek": 534222, "start": 5360.66, "end": 5368.7, "text": " the log fair and then I got a couple of new steps here which is converting embarked and", "tokens": [264, 3565, 3143, 293, 550, 286, 658, 257, 1916, 295, 777, 4439, 510, 597, 307, 29942, 29832, 292, 293], "temperature": 0.0, "avg_logprob": -0.1426374677201392, "compression_ratio": 1.588235294117647, "no_speech_prob": 7.720087887719274e-05}, {"id": 1143, "seek": 536870, "start": 5368.7, "end": 5375.78, "text": " sex into categorical variables what does that mean well let's just run this on both the", "tokens": [3260, 666, 19250, 804, 9102, 437, 775, 300, 914, 731, 718, 311, 445, 1190, 341, 322, 1293, 264], "temperature": 0.0, "avg_logprob": -0.07352487150445042, "compression_ratio": 1.8983957219251337, "no_speech_prob": 2.355134711251594e-05}, {"id": 1144, "seek": 536870, "start": 5375.78, "end": 5381.54, "text": " data frame a data frame and the test data frame split things into set into categories", "tokens": [1412, 3920, 257, 1412, 3920, 293, 264, 1500, 1412, 3920, 7472, 721, 666, 992, 666, 10479], "temperature": 0.0, "avg_logprob": -0.07352487150445042, "compression_ratio": 1.8983957219251337, "no_speech_prob": 2.355134711251594e-05}, {"id": 1145, "seek": 536870, "start": 5381.54, "end": 5391.46, "text": " and continuous and sex is a categorical variable so let's look at it well that's interesting", "tokens": [293, 10957, 293, 3260, 307, 257, 19250, 804, 7006, 370, 718, 311, 574, 412, 309, 731, 300, 311, 1880], "temperature": 0.0, "avg_logprob": -0.07352487150445042, "compression_ratio": 1.8983957219251337, "no_speech_prob": 2.355134711251594e-05}, {"id": 1146, "seek": 536870, "start": 5391.46, "end": 5397.099999999999, "text": " it looks exactly the same as before male and female but now it's got a category and it's", "tokens": [309, 1542, 2293, 264, 912, 382, 949, 7133, 293, 6556, 457, 586, 309, 311, 658, 257, 7719, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.07352487150445042, "compression_ratio": 1.8983957219251337, "no_speech_prob": 2.355134711251594e-05}, {"id": 1147, "seek": 539710, "start": 5397.1, "end": 5403.620000000001, "text": " got a list of categories what's happened here well what's happened is pandas has made a", "tokens": [658, 257, 1329, 295, 10479, 437, 311, 2011, 510, 731, 437, 311, 2011, 307, 4565, 296, 575, 1027, 257], "temperature": 0.0, "avg_logprob": -0.07914731668871502, "compression_ratio": 1.8941798941798942, "no_speech_prob": 1.7502517948742025e-05}, {"id": 1148, "seek": 539710, "start": 5403.620000000001, "end": 5410.42, "text": " list of all of the unique values of this field and behind the scenes if you look at the cat", "tokens": [1329, 295, 439, 295, 264, 3845, 4190, 295, 341, 2519, 293, 2261, 264, 8026, 498, 291, 574, 412, 264, 3857], "temperature": 0.0, "avg_logprob": -0.07914731668871502, "compression_ratio": 1.8941798941798942, "no_speech_prob": 1.7502517948742025e-05}, {"id": 1149, "seek": 539710, "start": 5410.42, "end": 5417.02, "text": " codes you can see behind the scenes it's actually turned them into numbers it looks up this", "tokens": [14211, 291, 393, 536, 2261, 264, 8026, 309, 311, 767, 3574, 552, 666, 3547, 309, 1542, 493, 341], "temperature": 0.0, "avg_logprob": -0.07914731668871502, "compression_ratio": 1.8941798941798942, "no_speech_prob": 1.7502517948742025e-05}, {"id": 1150, "seek": 539710, "start": 5417.02, "end": 5423.58, "text": " one into this list to get male looks at this zero into this list to get female so when", "tokens": [472, 666, 341, 1329, 281, 483, 7133, 1542, 412, 341, 4018, 666, 341, 1329, 281, 483, 6556, 370, 562], "temperature": 0.0, "avg_logprob": -0.07914731668871502, "compression_ratio": 1.8941798941798942, "no_speech_prob": 1.7502517948742025e-05}, {"id": 1151, "seek": 542358, "start": 5423.58, "end": 5431.18, "text": " it printed out it prints out the friendly version but it stores it as numbers now you'll", "tokens": [309, 13567, 484, 309, 22305, 484, 264, 9208, 3037, 457, 309, 9512, 309, 382, 3547, 586, 291, 603], "temperature": 0.0, "avg_logprob": -0.12931825244237508, "compression_ratio": 1.6024844720496894, "no_speech_prob": 2.1780928364023566e-05}, {"id": 1152, "seek": 542358, "start": 5431.18, "end": 5438.42, "text": " see in a moment why this is helpful but a key thing to point out is we're not going", "tokens": [536, 294, 257, 1623, 983, 341, 307, 4961, 457, 257, 2141, 551, 281, 935, 484, 307, 321, 434, 406, 516], "temperature": 0.0, "avg_logprob": -0.12931825244237508, "compression_ratio": 1.6024844720496894, "no_speech_prob": 2.1780928364023566e-05}, {"id": 1153, "seek": 542358, "start": 5438.42, "end": 5447.0599999999995, "text": " to have to create any dummy variables and even that first second or third class we're", "tokens": [281, 362, 281, 1884, 604, 35064, 9102, 293, 754, 300, 700, 1150, 420, 2636, 1508, 321, 434], "temperature": 0.0, "avg_logprob": -0.12931825244237508, "compression_ratio": 1.6024844720496894, "no_speech_prob": 2.1780928364023566e-05}, {"id": 1154, "seek": 544706, "start": 5447.06, "end": 5455.580000000001, "text": " not going to consider that categorical at all and just see why in a moment a random", "tokens": [406, 516, 281, 1949, 300, 19250, 804, 412, 439, 293, 445, 536, 983, 294, 257, 1623, 257, 4974], "temperature": 0.0, "avg_logprob": -0.07902169227600098, "compression_ratio": 1.7916666666666667, "no_speech_prob": 6.643213964707684e-06}, {"id": 1155, "seek": 544706, "start": 5455.580000000001, "end": 5464.06, "text": " forest is an ensemble of trees a tree is an ensemble of binary splits and so we're going", "tokens": [6719, 307, 364, 19492, 295, 5852, 257, 4230, 307, 364, 19492, 295, 17434, 37741, 293, 370, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.07902169227600098, "compression_ratio": 1.7916666666666667, "no_speech_prob": 6.643213964707684e-06}, {"id": 1156, "seek": 544706, "start": 5464.06, "end": 5468.22, "text": " to work from the bottom up we're going to first work we're going to first learn about", "tokens": [281, 589, 490, 264, 2767, 493, 321, 434, 516, 281, 700, 589, 321, 434, 516, 281, 700, 1466, 466], "temperature": 0.0, "avg_logprob": -0.07902169227600098, "compression_ratio": 1.7916666666666667, "no_speech_prob": 6.643213964707684e-06}, {"id": 1157, "seek": 546822, "start": 5468.22, "end": 5478.900000000001, "text": " what is a binary split and we're going to do it by looking at example let's consider", "tokens": [437, 307, 257, 17434, 7472, 293, 321, 434, 516, 281, 360, 309, 538, 1237, 412, 1365, 718, 311, 1949], "temperature": 0.0, "avg_logprob": -0.10293497690340368, "compression_ratio": 1.7587939698492463, "no_speech_prob": 1.4970661140978336e-05}, {"id": 1158, "seek": 546822, "start": 5478.900000000001, "end": 5484.1, "text": " what would happen if we took all the passengers on the Titanic and grouped them into males", "tokens": [437, 576, 1051, 498, 321, 1890, 439, 264, 18436, 322, 264, 42183, 293, 41877, 552, 666, 20776], "temperature": 0.0, "avg_logprob": -0.10293497690340368, "compression_ratio": 1.7587939698492463, "no_speech_prob": 1.4970661140978336e-05}, {"id": 1159, "seek": 546822, "start": 5484.1, "end": 5488.62, "text": " and females and let's look at two things the first is let's look at their survival rate", "tokens": [293, 21529, 293, 718, 311, 574, 412, 732, 721, 264, 700, 307, 718, 311, 574, 412, 641, 12559, 3314], "temperature": 0.0, "avg_logprob": -0.10293497690340368, "compression_ratio": 1.7587939698492463, "no_speech_prob": 1.4970661140978336e-05}, {"id": 1160, "seek": 546822, "start": 5488.62, "end": 5496.62, "text": " so about 20 percent survival rate for males and about 75 percent for females and let's", "tokens": [370, 466, 945, 3043, 12559, 3314, 337, 20776, 293, 466, 9562, 3043, 337, 21529, 293, 718, 311], "temperature": 0.0, "avg_logprob": -0.10293497690340368, "compression_ratio": 1.7587939698492463, "no_speech_prob": 1.4970661140978336e-05}, {"id": 1161, "seek": 549662, "start": 5496.62, "end": 5502.0599999999995, "text": " look at the histogram how many of them are there about twice as many males as females", "tokens": [574, 412, 264, 49816, 577, 867, 295, 552, 366, 456, 466, 6091, 382, 867, 20776, 382, 21529], "temperature": 0.0, "avg_logprob": -0.0862582272822314, "compression_ratio": 1.8620689655172413, "no_speech_prob": 2.6687128411140293e-05}, {"id": 1162, "seek": 549662, "start": 5502.0599999999995, "end": 5506.74, "text": " consider what would happen if you created the world's simplest model which was what", "tokens": [1949, 437, 576, 1051, 498, 291, 2942, 264, 1002, 311, 22811, 2316, 597, 390, 437], "temperature": 0.0, "avg_logprob": -0.0862582272822314, "compression_ratio": 1.8620689655172413, "no_speech_prob": 2.6687128411140293e-05}, {"id": 1163, "seek": 549662, "start": 5506.74, "end": 5513.74, "text": " sex are they that wouldn't be bad would it because there's a big difference between the", "tokens": [3260, 366, 436, 300, 2759, 380, 312, 1578, 576, 309, 570, 456, 311, 257, 955, 2649, 1296, 264], "temperature": 0.0, "avg_logprob": -0.0862582272822314, "compression_ratio": 1.8620689655172413, "no_speech_prob": 2.6687128411140293e-05}, {"id": 1164, "seek": 549662, "start": 5513.74, "end": 5520.099999999999, "text": " males and the females a huge difference in survival rate so if we said oh if you're a", "tokens": [20776, 293, 264, 21529, 257, 2603, 2649, 294, 12559, 3314, 370, 498, 321, 848, 1954, 498, 291, 434, 257], "temperature": 0.0, "avg_logprob": -0.0862582272822314, "compression_ratio": 1.8620689655172413, "no_speech_prob": 2.6687128411140293e-05}, {"id": 1165, "seek": 549662, "start": 5520.099999999999, "end": 5524.98, "text": " man you probably died if you're a woman you probably survived or not just a man or a boy", "tokens": [587, 291, 1391, 4539, 498, 291, 434, 257, 3059, 291, 1391, 14433, 420, 406, 445, 257, 587, 420, 257, 3237], "temperature": 0.0, "avg_logprob": -0.0862582272822314, "compression_ratio": 1.8620689655172413, "no_speech_prob": 2.6687128411140293e-05}, {"id": 1166, "seek": 552498, "start": 5524.98, "end": 5531.7, "text": " so male or female that would be a pretty good model because it's done a good job of splitting", "tokens": [370, 7133, 420, 6556, 300, 576, 312, 257, 1238, 665, 2316, 570, 309, 311, 1096, 257, 665, 1691, 295, 30348], "temperature": 0.0, "avg_logprob": -0.0920718252658844, "compression_ratio": 1.7586206896551724, "no_speech_prob": 8.664516826684121e-06}, {"id": 1167, "seek": 552498, "start": 5531.7, "end": 5539.179999999999, "text": " it into two groups that have very different survival rates this is called a binary split", "tokens": [309, 666, 732, 3935, 300, 362, 588, 819, 12559, 6846, 341, 307, 1219, 257, 17434, 7472], "temperature": 0.0, "avg_logprob": -0.0920718252658844, "compression_ratio": 1.7586206896551724, "no_speech_prob": 8.664516826684121e-06}, {"id": 1168, "seek": 552498, "start": 5539.179999999999, "end": 5546.82, "text": " a binary split is something that splits the rows into two groups hence binary let's talk", "tokens": [257, 17434, 7472, 307, 746, 300, 37741, 264, 13241, 666, 732, 3935, 16678, 17434, 718, 311, 751], "temperature": 0.0, "avg_logprob": -0.0920718252658844, "compression_ratio": 1.7586206896551724, "no_speech_prob": 8.664516826684121e-06}, {"id": 1169, "seek": 552498, "start": 5546.82, "end": 5554.82, "text": " about another example of a binary split I'm getting ahead of myself before we do that", "tokens": [466, 1071, 1365, 295, 257, 17434, 7472, 286, 478, 1242, 2286, 295, 2059, 949, 321, 360, 300], "temperature": 0.0, "avg_logprob": -0.0920718252658844, "compression_ratio": 1.7586206896551724, "no_speech_prob": 8.664516826684121e-06}, {"id": 1170, "seek": 555482, "start": 5554.82, "end": 5559.82, "text": " let's look at what would happen if we used this model so if we created a model which", "tokens": [718, 311, 574, 412, 437, 576, 1051, 498, 321, 1143, 341, 2316, 370, 498, 321, 2942, 257, 2316, 597], "temperature": 0.0, "avg_logprob": -0.09214972365986217, "compression_ratio": 1.6878048780487804, "no_speech_prob": 1.5688616258557886e-05}, {"id": 1171, "seek": 555482, "start": 5559.82, "end": 5564.62, "text": " just looked at sex how good would it be so to figure that out we first have to split", "tokens": [445, 2956, 412, 3260, 577, 665, 576, 309, 312, 370, 281, 2573, 300, 484, 321, 700, 362, 281, 7472], "temperature": 0.0, "avg_logprob": -0.09214972365986217, "compression_ratio": 1.6878048780487804, "no_speech_prob": 1.5688616258557886e-05}, {"id": 1172, "seek": 555482, "start": 5564.62, "end": 5573.98, "text": " into training and test sets so let's go ahead and do that and then let's convert all of", "tokens": [666, 3097, 293, 1500, 6352, 370, 718, 311, 352, 2286, 293, 360, 300, 293, 550, 718, 311, 7620, 439, 295], "temperature": 0.0, "avg_logprob": -0.09214972365986217, "compression_ratio": 1.6878048780487804, "no_speech_prob": 1.5688616258557886e-05}, {"id": 1173, "seek": 555482, "start": 5573.98, "end": 5579.9, "text": " our categorical variables into their codes so we've now got 0 1 2 whatever we don't have", "tokens": [527, 19250, 804, 9102, 666, 641, 14211, 370, 321, 600, 586, 658, 1958, 502, 568, 2035, 321, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.09214972365986217, "compression_ratio": 1.6878048780487804, "no_speech_prob": 1.5688616258557886e-05}, {"id": 1174, "seek": 557990, "start": 5579.9, "end": 5592.099999999999, "text": " male female there anymore and let's also create something that returns the independent variables", "tokens": [7133, 6556, 456, 3602, 293, 718, 311, 611, 1884, 746, 300, 11247, 264, 6695, 9102], "temperature": 0.0, "avg_logprob": -0.1074020117521286, "compression_ratio": 1.814569536423841, "no_speech_prob": 6.747848146915203e-06}, {"id": 1175, "seek": 557990, "start": 5592.099999999999, "end": 5599.58, "text": " which will call the X's and the dependent variable which will call Y and so we can now", "tokens": [597, 486, 818, 264, 1783, 311, 293, 264, 12334, 7006, 597, 486, 818, 398, 293, 370, 321, 393, 586], "temperature": 0.0, "avg_logprob": -0.1074020117521286, "compression_ratio": 1.814569536423841, "no_speech_prob": 6.747848146915203e-06}, {"id": 1176, "seek": 557990, "start": 5599.58, "end": 5604.54, "text": " get the X's and the Y for each of the training set and the validation set and so now let's", "tokens": [483, 264, 1783, 311, 293, 264, 398, 337, 1184, 295, 264, 3097, 992, 293, 264, 24071, 992, 293, 370, 586, 718, 311], "temperature": 0.0, "avg_logprob": -0.1074020117521286, "compression_ratio": 1.814569536423841, "no_speech_prob": 6.747848146915203e-06}, {"id": 1177, "seek": 560454, "start": 5604.54, "end": 5611.54, "text": " create some predictions we'll predict that they survived if their sex is zero so if they're", "tokens": [1884, 512, 21264, 321, 603, 6069, 300, 436, 14433, 498, 641, 3260, 307, 4018, 370, 498, 436, 434], "temperature": 0.0, "avg_logprob": -0.12458451588948567, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.1842750609503128e-05}, {"id": 1178, "seek": 560454, "start": 5611.54, "end": 5619.58, "text": " female so how good is that model remember I told you that to calculate mean absolute", "tokens": [6556, 370, 577, 665, 307, 300, 2316, 1604, 286, 1907, 291, 300, 281, 8873, 914, 8236], "temperature": 0.0, "avg_logprob": -0.12458451588948567, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.1842750609503128e-05}, {"id": 1179, "seek": 560454, "start": 5619.58, "end": 5624.54, "text": " error we can get psychic learn or pytorch whatever do it for us instead of doing it", "tokens": [6713, 321, 393, 483, 35406, 1466, 420, 25878, 284, 339, 2035, 360, 309, 337, 505, 2602, 295, 884, 309], "temperature": 0.0, "avg_logprob": -0.12458451588948567, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.1842750609503128e-05}, {"id": 1180, "seek": 560454, "start": 5624.54, "end": 5630.14, "text": " ourselves so just showing you here's how you do it just by importing it directly this is", "tokens": [4175, 370, 445, 4099, 291, 510, 311, 577, 291, 360, 309, 445, 538, 43866, 309, 3838, 341, 307], "temperature": 0.0, "avg_logprob": -0.12458451588948567, "compression_ratio": 1.6384976525821595, "no_speech_prob": 1.1842750609503128e-05}, {"id": 1181, "seek": 563014, "start": 5630.14, "end": 5638.660000000001, "text": " exactly the same as the one we did manually in the last notebook so that's a 21.5% error", "tokens": [2293, 264, 912, 382, 264, 472, 321, 630, 16945, 294, 264, 1036, 21060, 370, 300, 311, 257, 5080, 13, 20, 4, 6713], "temperature": 0.0, "avg_logprob": -0.15609909171488748, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1443858713610098e-05}, {"id": 1182, "seek": 563014, "start": 5638.660000000001, "end": 5648.54, "text": " so that's a pretty good model could we do better well here's another example what about", "tokens": [370, 300, 311, 257, 1238, 665, 2316, 727, 321, 360, 1101, 731, 510, 311, 1071, 1365, 437, 466], "temperature": 0.0, "avg_logprob": -0.15609909171488748, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1443858713610098e-05}, {"id": 1183, "seek": 563014, "start": 5648.54, "end": 5655.9800000000005, "text": " fair so fair is different to sex because fair is continuous or log fair I'll take that we", "tokens": [3143, 370, 3143, 307, 819, 281, 3260, 570, 3143, 307, 10957, 420, 3565, 3143, 286, 603, 747, 300, 321], "temperature": 0.0, "avg_logprob": -0.15609909171488748, "compression_ratio": 1.5375722543352601, "no_speech_prob": 2.1443858713610098e-05}, {"id": 1184, "seek": 565598, "start": 5655.98, "end": 5663.419999999999, "text": " could still split it into two groups so here's for all the people that didn't survive this", "tokens": [727, 920, 7472, 309, 666, 732, 3935, 370, 510, 311, 337, 439, 264, 561, 300, 994, 380, 7867, 341], "temperature": 0.0, "avg_logprob": -0.10860149065653484, "compression_ratio": 2.175757575757576, "no_speech_prob": 1.8630835256772116e-05}, {"id": 1185, "seek": 565598, "start": 5663.419999999999, "end": 5671.94, "text": " is their median fair here and then this is their quartiles for bigger fairs and quartiles", "tokens": [307, 641, 26779, 3143, 510, 293, 550, 341, 307, 641, 20837, 4680, 337, 3801, 3143, 82, 293, 20837, 4680], "temperature": 0.0, "avg_logprob": -0.10860149065653484, "compression_ratio": 2.175757575757576, "no_speech_prob": 1.8630835256772116e-05}, {"id": 1186, "seek": 565598, "start": 5671.94, "end": 5678.5, "text": " for smaller fairs and here's the median fair for those that survived and their quartiles", "tokens": [337, 4356, 3143, 82, 293, 510, 311, 264, 26779, 3143, 337, 729, 300, 14433, 293, 641, 20837, 4680], "temperature": 0.0, "avg_logprob": -0.10860149065653484, "compression_ratio": 2.175757575757576, "no_speech_prob": 1.8630835256772116e-05}, {"id": 1187, "seek": 565598, "start": 5678.5, "end": 5682.66, "text": " so you can see the median fair for those that survived is higher than the median fair for", "tokens": [370, 291, 393, 536, 264, 26779, 3143, 337, 729, 300, 14433, 307, 2946, 813, 264, 26779, 3143, 337], "temperature": 0.0, "avg_logprob": -0.10860149065653484, "compression_ratio": 2.175757575757576, "no_speech_prob": 1.8630835256772116e-05}, {"id": 1188, "seek": 568266, "start": 5682.66, "end": 5691.62, "text": " those that didn't we can't create a histogram exactly for fair because it's continuous we", "tokens": [729, 300, 994, 380, 321, 393, 380, 1884, 257, 49816, 2293, 337, 3143, 570, 309, 311, 10957, 321], "temperature": 0.0, "avg_logprob": -0.09058090448379516, "compression_ratio": 1.8010204081632653, "no_speech_prob": 1.4738595382368658e-05}, {"id": 1189, "seek": 568266, "start": 5691.62, "end": 5695.78, "text": " could bucket it into groups to create a histogram so I guess we can create a histogram that", "tokens": [727, 13058, 309, 666, 3935, 281, 1884, 257, 49816, 370, 286, 2041, 321, 393, 1884, 257, 49816, 300], "temperature": 0.0, "avg_logprob": -0.09058090448379516, "compression_ratio": 1.8010204081632653, "no_speech_prob": 1.4738595382368658e-05}, {"id": 1190, "seek": 568266, "start": 5695.78, "end": 5700.3, "text": " wasn't true what I should say is we could create something better which is a kernel", "tokens": [2067, 380, 2074, 437, 286, 820, 584, 307, 321, 727, 1884, 746, 1101, 597, 307, 257, 28256], "temperature": 0.0, "avg_logprob": -0.09058090448379516, "compression_ratio": 1.8010204081632653, "no_speech_prob": 1.4738595382368658e-05}, {"id": 1191, "seek": 568266, "start": 5700.3, "end": 5705.46, "text": " density plot which is just like a histogram but it's like with infinitely small bins so", "tokens": [10305, 7542, 597, 307, 445, 411, 257, 49816, 457, 309, 311, 411, 365, 36227, 1359, 41275, 370], "temperature": 0.0, "avg_logprob": -0.09058090448379516, "compression_ratio": 1.8010204081632653, "no_speech_prob": 1.4738595382368658e-05}, {"id": 1192, "seek": 570546, "start": 5705.46, "end": 5715.02, "text": " we can see most people have a log fair of about two so what if we split on about a bit", "tokens": [321, 393, 536, 881, 561, 362, 257, 3565, 3143, 295, 466, 732, 370, 437, 498, 321, 7472, 322, 466, 257, 857], "temperature": 0.0, "avg_logprob": -0.07646570354700089, "compression_ratio": 1.6097560975609757, "no_speech_prob": 5.594147751253331e-06}, {"id": 1193, "seek": 570546, "start": 5715.02, "end": 5722.78, "text": " under three you know that seems to be a point at which there's a difference in survival", "tokens": [833, 1045, 291, 458, 300, 2544, 281, 312, 257, 935, 412, 597, 456, 311, 257, 2649, 294, 12559], "temperature": 0.0, "avg_logprob": -0.07646570354700089, "compression_ratio": 1.6097560975609757, "no_speech_prob": 5.594147751253331e-06}, {"id": 1194, "seek": 570546, "start": 5722.78, "end": 5728.58, "text": " between people that are greater than or less than that amount so here's another model log", "tokens": [1296, 561, 300, 366, 5044, 813, 420, 1570, 813, 300, 2372, 370, 510, 311, 1071, 2316, 3565], "temperature": 0.0, "avg_logprob": -0.07646570354700089, "compression_ratio": 1.6097560975609757, "no_speech_prob": 5.594147751253331e-06}, {"id": 1195, "seek": 572858, "start": 5728.58, "end": 5736.54, "text": " fair greater than point two point seven oh much worse point three three six versus point", "tokens": [3143, 5044, 813, 935, 732, 935, 3407, 1954, 709, 5324, 935, 1045, 1045, 2309, 5717, 935], "temperature": 0.0, "avg_logprob": -0.1098885695139567, "compression_ratio": 1.593939393939394, "no_speech_prob": 2.2252479539019987e-06}, {"id": 1196, "seek": 572858, "start": 5736.54, "end": 5745.14, "text": " two one five well I don't know maybe this is something better we could create a little", "tokens": [732, 472, 1732, 731, 286, 500, 380, 458, 1310, 341, 307, 746, 1101, 321, 727, 1884, 257, 707], "temperature": 0.0, "avg_logprob": -0.1098885695139567, "compression_ratio": 1.593939393939394, "no_speech_prob": 2.2252479539019987e-06}, {"id": 1197, "seek": 572858, "start": 5745.14, "end": 5756.3, "text": " interactive tool so what I want is something that can give us a quick score of how good", "tokens": [15141, 2290, 370, 437, 286, 528, 307, 746, 300, 393, 976, 505, 257, 1702, 6175, 295, 577, 665], "temperature": 0.0, "avg_logprob": -0.1098885695139567, "compression_ratio": 1.593939393939394, "no_speech_prob": 2.2252479539019987e-06}, {"id": 1198, "seek": 575630, "start": 5756.3, "end": 5760.6, "text": " a binary split is and I want it to be able to work regardless of whether we're dealing", "tokens": [257, 17434, 7472, 307, 293, 286, 528, 309, 281, 312, 1075, 281, 589, 10060, 295, 1968, 321, 434, 6260], "temperature": 0.0, "avg_logprob": -0.07810216352164027, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.2124759198050015e-05}, {"id": 1199, "seek": 575630, "start": 5760.6, "end": 5769.46, "text": " with categorical or continuous or whatever data so I just came up with a simple little", "tokens": [365, 19250, 804, 420, 10957, 420, 2035, 1412, 370, 286, 445, 1361, 493, 365, 257, 2199, 707], "temperature": 0.0, "avg_logprob": -0.07810216352164027, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.2124759198050015e-05}, {"id": 1200, "seek": 575630, "start": 5769.46, "end": 5776.74, "text": " way of scoring which is I said okay if you split your data into two groups a good split", "tokens": [636, 295, 22358, 597, 307, 286, 848, 1392, 498, 291, 7472, 428, 1412, 666, 732, 3935, 257, 665, 7472], "temperature": 0.0, "avg_logprob": -0.07810216352164027, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.2124759198050015e-05}, {"id": 1201, "seek": 575630, "start": 5776.74, "end": 5783.42, "text": " would be one in which all of the values of the dependent variable on one side are all", "tokens": [576, 312, 472, 294, 597, 439, 295, 264, 4190, 295, 264, 12334, 7006, 322, 472, 1252, 366, 439], "temperature": 0.0, "avg_logprob": -0.07810216352164027, "compression_ratio": 1.6367924528301887, "no_speech_prob": 2.2124759198050015e-05}, {"id": 1202, "seek": 578342, "start": 5783.42, "end": 5786.66, "text": " pretty much the same and all of the dependent variables on the other side are all pretty", "tokens": [1238, 709, 264, 912, 293, 439, 295, 264, 12334, 9102, 322, 264, 661, 1252, 366, 439, 1238], "temperature": 0.0, "avg_logprob": -0.05946299433708191, "compression_ratio": 2.0599078341013826, "no_speech_prob": 1.4510143046209123e-05}, {"id": 1203, "seek": 578342, "start": 5786.66, "end": 5793.82, "text": " much the same for example if pretty much all the males had the same survival outcome which", "tokens": [709, 264, 912, 337, 1365, 498, 1238, 709, 439, 264, 20776, 632, 264, 912, 12559, 9700, 597], "temperature": 0.0, "avg_logprob": -0.05946299433708191, "compression_ratio": 2.0599078341013826, "no_speech_prob": 1.4510143046209123e-05}, {"id": 1204, "seek": 578342, "start": 5793.82, "end": 5798.82, "text": " is didn't survive and all the females had about the same survival outcome which is they", "tokens": [307, 994, 380, 7867, 293, 439, 264, 21529, 632, 466, 264, 912, 12559, 9700, 597, 307, 436], "temperature": 0.0, "avg_logprob": -0.05946299433708191, "compression_ratio": 2.0599078341013826, "no_speech_prob": 1.4510143046209123e-05}, {"id": 1205, "seek": 578342, "start": 5798.82, "end": 5803.74, "text": " did survive that would be a good split right it doesn't just work for categorical variables", "tokens": [630, 7867, 300, 576, 312, 257, 665, 7472, 558, 309, 1177, 380, 445, 589, 337, 19250, 804, 9102], "temperature": 0.0, "avg_logprob": -0.05946299433708191, "compression_ratio": 2.0599078341013826, "no_speech_prob": 1.4510143046209123e-05}, {"id": 1206, "seek": 578342, "start": 5803.74, "end": 5809.3, "text": " it would work if your dependent variable was continuous as well you basically want each", "tokens": [309, 576, 589, 498, 428, 12334, 7006, 390, 10957, 382, 731, 291, 1936, 528, 1184], "temperature": 0.0, "avg_logprob": -0.05946299433708191, "compression_ratio": 2.0599078341013826, "no_speech_prob": 1.4510143046209123e-05}, {"id": 1207, "seek": 580930, "start": 5809.3, "end": 5814.46, "text": " of your groups within group to be as similar as possible on the dependent variable and", "tokens": [295, 428, 3935, 1951, 1594, 281, 312, 382, 2531, 382, 1944, 322, 264, 12334, 7006, 293], "temperature": 0.0, "avg_logprob": -0.07328182759911123, "compression_ratio": 2.3105263157894735, "no_speech_prob": 4.331673699198291e-05}, {"id": 1208, "seek": 580930, "start": 5814.46, "end": 5819.18, "text": " then the other group you want them to be as similar as possible on the dependent variable", "tokens": [550, 264, 661, 1594, 291, 528, 552, 281, 312, 382, 2531, 382, 1944, 322, 264, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.07328182759911123, "compression_ratio": 2.3105263157894735, "no_speech_prob": 4.331673699198291e-05}, {"id": 1209, "seek": 580930, "start": 5819.18, "end": 5825.66, "text": " so how similar is all the things in a group that's a standard deviation so what I want", "tokens": [370, 577, 2531, 307, 439, 264, 721, 294, 257, 1594, 300, 311, 257, 3832, 25163, 370, 437, 286, 528], "temperature": 0.0, "avg_logprob": -0.07328182759911123, "compression_ratio": 2.3105263157894735, "no_speech_prob": 4.331673699198291e-05}, {"id": 1210, "seek": 580930, "start": 5825.66, "end": 5829.7, "text": " to do to get is basically add the standard deviations of the two groups of the dependent", "tokens": [281, 360, 281, 483, 307, 1936, 909, 264, 3832, 31219, 763, 295, 264, 732, 3935, 295, 264, 12334], "temperature": 0.0, "avg_logprob": -0.07328182759911123, "compression_ratio": 2.3105263157894735, "no_speech_prob": 4.331673699198291e-05}, {"id": 1211, "seek": 580930, "start": 5829.7, "end": 5836.900000000001, "text": " variable and then if there's a really small standard deviation but it's a really small", "tokens": [7006, 293, 550, 498, 456, 311, 257, 534, 1359, 3832, 25163, 457, 309, 311, 257, 534, 1359], "temperature": 0.0, "avg_logprob": -0.07328182759911123, "compression_ratio": 2.3105263157894735, "no_speech_prob": 4.331673699198291e-05}, {"id": 1212, "seek": 583690, "start": 5836.9, "end": 5842.5, "text": " group that's not very interesting so I'll multiply it by the size right so this is something", "tokens": [1594, 300, 311, 406, 588, 1880, 370, 286, 603, 12972, 309, 538, 264, 2744, 558, 370, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.06635466161763894, "compression_ratio": 1.9910313901345291, "no_speech_prob": 1.5206675016088411e-05}, {"id": 1213, "seek": 583690, "start": 5842.5, "end": 5848.139999999999, "text": " which says what's the score for one of my groups one of my sides it's the standard deviation", "tokens": [597, 1619, 437, 311, 264, 6175, 337, 472, 295, 452, 3935, 472, 295, 452, 4881, 309, 311, 264, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.06635466161763894, "compression_ratio": 1.9910313901345291, "no_speech_prob": 1.5206675016088411e-05}, {"id": 1214, "seek": 583690, "start": 5848.139999999999, "end": 5855.46, "text": " multiplied by how many things are in that group so the total score is the score for", "tokens": [17207, 538, 577, 867, 721, 366, 294, 300, 1594, 370, 264, 3217, 6175, 307, 264, 6175, 337], "temperature": 0.0, "avg_logprob": -0.06635466161763894, "compression_ratio": 1.9910313901345291, "no_speech_prob": 1.5206675016088411e-05}, {"id": 1215, "seek": 583690, "start": 5855.46, "end": 5860.82, "text": " the left hand side so all the things in one group plus the score for the right hand side", "tokens": [264, 1411, 1011, 1252, 370, 439, 264, 721, 294, 472, 1594, 1804, 264, 6175, 337, 264, 558, 1011, 1252], "temperature": 0.0, "avg_logprob": -0.06635466161763894, "compression_ratio": 1.9910313901345291, "no_speech_prob": 1.5206675016088411e-05}, {"id": 1216, "seek": 583690, "start": 5860.82, "end": 5866.259999999999, "text": " which is tilde means not so not left hand side is right hand side and then we'll just", "tokens": [597, 307, 45046, 1355, 406, 370, 406, 1411, 1011, 1252, 307, 558, 1011, 1252, 293, 550, 321, 603, 445], "temperature": 0.0, "avg_logprob": -0.06635466161763894, "compression_ratio": 1.9910313901345291, "no_speech_prob": 1.5206675016088411e-05}, {"id": 1217, "seek": 586626, "start": 5866.26, "end": 5875.820000000001, "text": " take the average of that so for example if we split by sex is greater than or less than", "tokens": [747, 264, 4274, 295, 300, 370, 337, 1365, 498, 321, 7472, 538, 3260, 307, 5044, 813, 420, 1570, 813], "temperature": 0.0, "avg_logprob": -0.11300352641514369, "compression_ratio": 1.9715909090909092, "no_speech_prob": 1.6964064343483187e-05}, {"id": 1218, "seek": 586626, "start": 5875.820000000001, "end": 5881.46, "text": " point five that'll create two groups males and females and that gives us this score and", "tokens": [935, 1732, 300, 603, 1884, 732, 3935, 20776, 293, 21529, 293, 300, 2709, 505, 341, 6175, 293], "temperature": 0.0, "avg_logprob": -0.11300352641514369, "compression_ratio": 1.9715909090909092, "no_speech_prob": 1.6964064343483187e-05}, {"id": 1219, "seek": 586626, "start": 5881.46, "end": 5886.360000000001, "text": " if we do log fair greater than or less than two point seven it gives us this score and", "tokens": [498, 321, 360, 3565, 3143, 5044, 813, 420, 1570, 813, 732, 935, 3407, 309, 2709, 505, 341, 6175, 293], "temperature": 0.0, "avg_logprob": -0.11300352641514369, "compression_ratio": 1.9715909090909092, "no_speech_prob": 1.6964064343483187e-05}, {"id": 1220, "seek": 586626, "start": 5886.360000000001, "end": 5892.2, "text": " lower scores better so sex is better than log fair so now that we've got that we can", "tokens": [3126, 13444, 1101, 370, 3260, 307, 1101, 813, 3565, 3143, 370, 586, 300, 321, 600, 658, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.11300352641514369, "compression_ratio": 1.9715909090909092, "no_speech_prob": 1.6964064343483187e-05}, {"id": 1221, "seek": 589220, "start": 5892.2, "end": 5901.54, "text": " use our favorite interact tool to create a little gooey and so we can say you know let's", "tokens": [764, 527, 2954, 4648, 2290, 281, 1884, 257, 707, 33192, 2030, 293, 370, 321, 393, 584, 291, 458, 718, 311], "temperature": 0.0, "avg_logprob": -0.14117081268973972, "compression_ratio": 1.4274193548387097, "no_speech_prob": 8.01321857579751e-06}, {"id": 1222, "seek": 589220, "start": 5901.54, "end": 5910.0599999999995, "text": " try like oh what about this one can we uh oops can we find something that's a bit better", "tokens": [853, 411, 1954, 437, 466, 341, 472, 393, 321, 2232, 34166, 393, 321, 915, 746, 300, 311, 257, 857, 1101], "temperature": 0.0, "avg_logprob": -0.14117081268973972, "compression_ratio": 1.4274193548387097, "no_speech_prob": 8.01321857579751e-06}, {"id": 1223, "seek": 591006, "start": 5910.06, "end": 5924.820000000001, "text": " 4.8.485 no not very good what about p-class 0.468.460 so we can fiddle around with these", "tokens": [1017, 13, 23, 13, 13318, 20, 572, 406, 588, 665, 437, 466, 280, 12, 11665, 1958, 13, 16169, 23, 13, 19, 4550, 370, 321, 393, 24553, 2285, 926, 365, 613], "temperature": 0.0, "avg_logprob": -0.21566666689786043, "compression_ratio": 1.3235294117647058, "no_speech_prob": 9.515897545497864e-06}, {"id": 1224, "seek": 591006, "start": 5924.820000000001, "end": 5932.580000000001, "text": " we could do the same thing for the categorical variables so we already know that sex we can", "tokens": [321, 727, 360, 264, 912, 551, 337, 264, 19250, 804, 9102, 370, 321, 1217, 458, 300, 3260, 321, 393], "temperature": 0.0, "avg_logprob": -0.21566666689786043, "compression_ratio": 1.3235294117647058, "no_speech_prob": 9.515897545497864e-06}, {"id": 1225, "seek": 593258, "start": 5932.58, "end": 5942.74, "text": " get to 0.407 what about embarked hmm all right so it looks like sex might be our best well", "tokens": [483, 281, 1958, 13, 5254, 22, 437, 466, 29832, 292, 16478, 439, 558, 370, 309, 1542, 411, 3260, 1062, 312, 527, 1151, 731], "temperature": 0.0, "avg_logprob": -0.13735503308913288, "compression_ratio": 1.5459770114942528, "no_speech_prob": 7.296239346032962e-06}, {"id": 1226, "seek": 593258, "start": 5942.74, "end": 5946.3, "text": " that was pretty inefficient right it would be nice if we could find some automatic way", "tokens": [300, 390, 1238, 43495, 558, 309, 576, 312, 1481, 498, 321, 727, 915, 512, 12509, 636], "temperature": 0.0, "avg_logprob": -0.13735503308913288, "compression_ratio": 1.5459770114942528, "no_speech_prob": 7.296239346032962e-06}, {"id": 1227, "seek": 593258, "start": 5946.3, "end": 5952.54, "text": " to do all that well of course we can for example if we wanted to find what's the best split", "tokens": [281, 360, 439, 300, 731, 295, 1164, 321, 393, 337, 1365, 498, 321, 1415, 281, 915, 437, 311, 264, 1151, 7472], "temperature": 0.0, "avg_logprob": -0.13735503308913288, "compression_ratio": 1.5459770114942528, "no_speech_prob": 7.296239346032962e-06}, {"id": 1228, "seek": 595254, "start": 5952.54, "end": 5963.74, "text": " point for age then we just have to create let's do this again if we want to find the", "tokens": [935, 337, 3205, 550, 321, 445, 362, 281, 1884, 718, 311, 360, 341, 797, 498, 321, 528, 281, 915, 264], "temperature": 0.0, "avg_logprob": -0.07593793027541217, "compression_ratio": 1.64375, "no_speech_prob": 7.296265721379314e-06}, {"id": 1229, "seek": 595254, "start": 5963.74, "end": 5968.86, "text": " best split point for age we could just create a list of all of the unique values of age", "tokens": [1151, 7472, 935, 337, 3205, 321, 727, 445, 1884, 257, 1329, 295, 439, 295, 264, 3845, 4190, 295, 3205], "temperature": 0.0, "avg_logprob": -0.07593793027541217, "compression_ratio": 1.64375, "no_speech_prob": 7.296265721379314e-06}, {"id": 1230, "seek": 595254, "start": 5968.86, "end": 5974.82, "text": " and try each one in turn and see what score we get if we made a binary split on that level", "tokens": [293, 853, 1184, 472, 294, 1261, 293, 536, 437, 6175, 321, 483, 498, 321, 1027, 257, 17434, 7472, 322, 300, 1496], "temperature": 0.0, "avg_logprob": -0.07593793027541217, "compression_ratio": 1.64375, "no_speech_prob": 7.296265721379314e-06}, {"id": 1231, "seek": 597482, "start": 5974.82, "end": 5983.219999999999, "text": " of age so here's a list of all of the possible binary split thresholds for age let's go through", "tokens": [295, 3205, 370, 510, 311, 257, 1329, 295, 439, 295, 264, 1944, 17434, 7472, 14678, 82, 337, 3205, 718, 311, 352, 807], "temperature": 0.0, "avg_logprob": -0.06922296979534093, "compression_ratio": 1.6, "no_speech_prob": 4.28926978202071e-06}, {"id": 1232, "seek": 597482, "start": 5983.219999999999, "end": 5993.5, "text": " all of them for each of them calculate the score and then numpy and pytorch have an", "tokens": [439, 295, 552, 337, 1184, 295, 552, 8873, 264, 6175, 293, 550, 1031, 8200, 293, 25878, 284, 339, 362, 364], "temperature": 0.0, "avg_logprob": -0.06922296979534093, "compression_ratio": 1.6, "no_speech_prob": 4.28926978202071e-06}, {"id": 1233, "seek": 597482, "start": 5993.5, "end": 5999.7, "text": " argmin function which tells you what index into that list is the smallest so just to", "tokens": [3882, 2367, 2445, 597, 5112, 291, 437, 8186, 666, 300, 1329, 307, 264, 16998, 370, 445, 281], "temperature": 0.0, "avg_logprob": -0.06922296979534093, "compression_ratio": 1.6, "no_speech_prob": 4.28926978202071e-06}, {"id": 1234, "seek": 599970, "start": 5999.7, "end": 6020.42, "text": " show you here's the scores and 0 1 2 3 4 5 6 oh sorry 0 1 2 3 4 5 6 so apparently that", "tokens": [855, 291, 510, 311, 264, 13444, 293, 1958, 502, 568, 805, 1017, 1025, 1386, 1954, 2597, 1958, 502, 568, 805, 1017, 1025, 1386, 370, 7970, 300], "temperature": 0.0, "avg_logprob": -0.08772707939147949, "compression_ratio": 1.4705882352941178, "no_speech_prob": 6.854158073110739e-06}, {"id": 1235, "seek": 599970, "start": 6020.42, "end": 6028.78, "text": " that value has the smallest score so that tells us that for age the threshold of 6 would", "tokens": [300, 2158, 575, 264, 16998, 6175, 370, 300, 5112, 505, 300, 337, 3205, 264, 14678, 295, 1386, 576], "temperature": 0.0, "avg_logprob": -0.08772707939147949, "compression_ratio": 1.4705882352941178, "no_speech_prob": 6.854158073110739e-06}, {"id": 1236, "seek": 602878, "start": 6028.78, "end": 6036.7, "text": " be best so here's something that just calculates that for a column it calculates the best split", "tokens": [312, 1151, 370, 510, 311, 746, 300, 445, 4322, 1024, 300, 337, 257, 7738, 309, 4322, 1024, 264, 1151, 7472], "temperature": 0.0, "avg_logprob": -0.09258961341750453, "compression_ratio": 1.8157894736842106, "no_speech_prob": 6.96201504979399e-06}, {"id": 1237, "seek": 602878, "start": 6036.7, "end": 6044.34, "text": " point so here's 6 right and it also tells us what the score is at that point which is", "tokens": [935, 370, 510, 311, 1386, 558, 293, 309, 611, 5112, 505, 437, 264, 6175, 307, 412, 300, 935, 597, 307], "temperature": 0.0, "avg_logprob": -0.09258961341750453, "compression_ratio": 1.8157894736842106, "no_speech_prob": 6.96201504979399e-06}, {"id": 1238, "seek": 602878, "start": 6044.34, "end": 6055.9, "text": " 0.478 so now we can just go through and calculates the score for the best split point for each", "tokens": [1958, 13, 14060, 23, 370, 586, 321, 393, 445, 352, 807, 293, 4322, 1024, 264, 6175, 337, 264, 1151, 7472, 935, 337, 1184], "temperature": 0.0, "avg_logprob": -0.09258961341750453, "compression_ratio": 1.8157894736842106, "no_speech_prob": 6.96201504979399e-06}, {"id": 1239, "seek": 605590, "start": 6055.9, "end": 6071.259999999999, "text": " column and if we do that we find that the lowest score is 6 so that is how we calculate", "tokens": [7738, 293, 498, 321, 360, 300, 321, 915, 300, 264, 12437, 6175, 307, 1386, 370, 300, 307, 577, 321, 8873], "temperature": 0.0, "avg_logprob": -0.1020031308018884, "compression_ratio": 1.4956521739130435, "no_speech_prob": 1.0451336493133567e-05}, {"id": 1240, "seek": 605590, "start": 6071.259999999999, "end": 6081.259999999999, "text": " the best binary split so we now know that the model that we created earlier this one", "tokens": [264, 1151, 17434, 7472, 370, 321, 586, 458, 300, 264, 2316, 300, 321, 2942, 3071, 341, 472], "temperature": 0.0, "avg_logprob": -0.1020031308018884, "compression_ratio": 1.4956521739130435, "no_speech_prob": 1.0451336493133567e-05}, {"id": 1241, "seek": 608126, "start": 6081.26, "end": 6089.860000000001, "text": " is the best single binary split model we can find so next week we're going to be we're", "tokens": [307, 264, 1151, 2167, 17434, 7472, 2316, 321, 393, 915, 370, 958, 1243, 321, 434, 516, 281, 312, 321, 434], "temperature": 0.0, "avg_logprob": -0.08860218234178496, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.647746052592993e-05}, {"id": 1242, "seek": 608126, "start": 6089.860000000001, "end": 6094.54, "text": " going to learn how we can recursively do this to create a decision tree and then do that", "tokens": [516, 281, 1466, 577, 321, 393, 20560, 3413, 360, 341, 281, 1884, 257, 3537, 4230, 293, 550, 360, 300], "temperature": 0.0, "avg_logprob": -0.08860218234178496, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.647746052592993e-05}, {"id": 1243, "seek": 608126, "start": 6094.54, "end": 6102.42, "text": " multiple times to create a random forest but before we do I want to point something out", "tokens": [3866, 1413, 281, 1884, 257, 4974, 6719, 457, 949, 321, 360, 286, 528, 281, 935, 746, 484], "temperature": 0.0, "avg_logprob": -0.08860218234178496, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.647746052592993e-05}, {"id": 1244, "seek": 608126, "start": 6102.42, "end": 6108.780000000001, "text": " which is this ridiculously simple thing which is find a single binary split and stop is", "tokens": [597, 307, 341, 41358, 2199, 551, 597, 307, 915, 257, 2167, 17434, 7472, 293, 1590, 307], "temperature": 0.0, "avg_logprob": -0.08860218234178496, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.647746052592993e-05}, {"id": 1245, "seek": 610878, "start": 6108.78, "end": 6115.62, "text": " a type of model it has a name it's called 1r and the 1r model it turned out in a review", "tokens": [257, 2010, 295, 2316, 309, 575, 257, 1315, 309, 311, 1219, 502, 81, 293, 264, 502, 81, 2316, 309, 3574, 484, 294, 257, 3131], "temperature": 0.0, "avg_logprob": -0.07801000888531025, "compression_ratio": 1.7053140096618358, "no_speech_prob": 1.2218222764204256e-05}, {"id": 1246, "seek": 610878, "start": 6115.62, "end": 6123.219999999999, "text": " of machine learning methods in the 90s turned out to be one of the best if not the best", "tokens": [295, 3479, 2539, 7150, 294, 264, 4289, 82, 3574, 484, 281, 312, 472, 295, 264, 1151, 498, 406, 264, 1151], "temperature": 0.0, "avg_logprob": -0.07801000888531025, "compression_ratio": 1.7053140096618358, "no_speech_prob": 1.2218222764204256e-05}, {"id": 1247, "seek": 610878, "start": 6123.219999999999, "end": 6129.74, "text": " machine learning classifiers for a wide range of real world data sets so that is to say", "tokens": [3479, 2539, 1508, 23463, 337, 257, 4874, 3613, 295, 957, 1002, 1412, 6352, 370, 300, 307, 281, 584], "temperature": 0.0, "avg_logprob": -0.07801000888531025, "compression_ratio": 1.7053140096618358, "no_speech_prob": 1.2218222764204256e-05}, {"id": 1248, "seek": 610878, "start": 6129.74, "end": 6138.0199999999995, "text": " don't assume that you have to go complicated it's not a bad idea to always start creating", "tokens": [500, 380, 6552, 300, 291, 362, 281, 352, 6179, 309, 311, 406, 257, 1578, 1558, 281, 1009, 722, 4084], "temperature": 0.0, "avg_logprob": -0.07801000888531025, "compression_ratio": 1.7053140096618358, "no_speech_prob": 1.2218222764204256e-05}, {"id": 1249, "seek": 613802, "start": 6138.02, "end": 6147.5, "text": " a baseline of 1r a decision tree with a single binary split and in fact for the Titanic competition", "tokens": [257, 20518, 295, 502, 81, 257, 3537, 4230, 365, 257, 2167, 17434, 7472, 293, 294, 1186, 337, 264, 42183, 6211], "temperature": 0.0, "avg_logprob": -0.11234530696162472, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.4284480130299926e-05}, {"id": 1250, "seek": 613802, "start": 6147.5, "end": 6151.4400000000005, "text": " that's exactly what we do if we look at the Titanic competition on Kaggle you'll find", "tokens": [300, 311, 2293, 437, 321, 360, 498, 321, 574, 412, 264, 42183, 6211, 322, 48751, 22631, 291, 603, 915], "temperature": 0.0, "avg_logprob": -0.11234530696162472, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.4284480130299926e-05}, {"id": 1251, "seek": 613802, "start": 6151.4400000000005, "end": 6157.660000000001, "text": " that what we did is our sample submission is one that just splits into male versus female", "tokens": [300, 437, 321, 630, 307, 527, 6889, 23689, 307, 472, 300, 445, 37741, 666, 7133, 5717, 6556], "temperature": 0.0, "avg_logprob": -0.11234530696162472, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.4284480130299926e-05}, {"id": 1252, "seek": 613802, "start": 6157.660000000001, "end": 6164.02, "text": " alright thanks everybody hope you found that interesting and I will see you next lesson", "tokens": [5845, 3231, 2201, 1454, 291, 1352, 300, 1880, 293, 286, 486, 536, 291, 958, 6898], "temperature": 0.0, "avg_logprob": -0.11234530696162472, "compression_ratio": 1.665137614678899, "no_speech_prob": 1.4284480130299926e-05}, {"id": 1253, "seek": 616402, "start": 6164.02, "end": 6168.22, "text": " bye", "tokens": [50364, 6543, 50574], "temperature": 0.0, "avg_logprob": -0.7509469985961914, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.00045531673822551966}], "language": "en"}