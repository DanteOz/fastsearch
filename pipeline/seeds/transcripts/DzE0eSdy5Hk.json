{"text": " So I I don't want to embarrass Rachel, but I'm very excited that Rachel's here, so this is Rachel for those of you That don't know She's not quite back on her feet after her illness But well enough to at least come to at least part with this lesson So don't worry if she can't stay for the whole thing I'm really glad she's here because Rachel actually wrote the vast majority of the lesson We're going to see it's I think it's a really really cool work So I'm glad she's gonna at least see it being taught even if unfortunately she's not teaching it herself so Good Thanksgiving present best Thanksgiving present So we're as we discussed at the end of last lesson we're kind of moving from the sea decision tree ensembles to to neural nets broadly defined and As we discussed, you know random forests and decision trees are limited by the fact in the end that they're basically They're basically doing nearest neighbors Right, you know that all they can do is to get return the average of a bunch of other points And so they can't extrapolate out to you know If you're thinking what happens if I increase my prices by 20% And you've never priced at that level before or what's going to happen to sales next year? And obviously we've never seen next year before It's very hard to extrapolate. It's also hard if it needs to you know, like it's it can only do around log base to N decisions, you know And so if there's like a time series it needs to fit to that takes like four steps to kind of get to the right time Area then suddenly there's not many decisions left for it to make so it's kind of this limited Amount of computation that it can do so there's a limited Complexity of relationship that it can model Yes Prince Can I ask about one more drawback of random forests that yeah, I feel so If we have a data as categorical variable, which are not in sequential order So for random forest we encode them and treat them as numbers Let's say we have 20 cardinality and 1 to 20 So the result that random forest gives is like the split that random forest gives is something like less than 5 less than 6 But if the categories are not sequential not in any order What does that mean? Yeah, so So if you've got like oh Let's go back to bulldozers erops Erops with a C OROPS And a No, whatever right and We arbitrarily label them like this All right And so actually we know that all that really mattered was if it had air conditioning So what's going to happen? Well, it's basically going to say like okay if I group it into Those together and those together that like that's an interesting break Just because it so happens that the air conditioning ones all are going to end up in the right hand side and then having done that Right, it's then going to say okay. Well within the group with the two and three It's going to notice that it's furthermore going to have to split it into two more groups. So eventually it's going to get there It's going to pull out that category. It's just it's going to take more splits than we would ideally like So it's kind of similar to the fact that for it to model a line It can only do it with lots of spits and only approximately For is just fine with categories that are not sequential also. Yeah, so I can do it It's just like in some way it's suboptimal because we just need to do More breakpoints than we would have liked but it gets there. It does a pretty good job. And so even although Random forests, you know do you have some deficiencies? They're incredibly Powerful, you know, particularly because they have so few assumptions They really had to screw up and you know, it's kind of hard to actually win a Kaggle competition with a random forest But it's very easy to get like top 10% So in like in real life where often that third decimal place doesn't matter random forests are often like what you end up doing But for some things like this Ecuadorian groceries Competition, it's very very hard to get a good result with a random forest Because like there's a huge time series component and like nearly everything is these two massively high cardinality Categorical variables which is the store and the item and like so this so there's very little there to even throw out a random forest and the You know the difference between every pair of stores is kind of different in different ways And so, you know, there are some things that are just hard to get even relatively good results with a random forest another example is Recognizing numbers You can get like okay results with a random forest But in the end they're kind of the relationship between you know, like the spatial structure Turns out to be important, right and you kind of want to be able to do like computations like finding edges or whatever that kind of carry forward through through the computations so, you know just doing a Kind of a clever nearest neighbors like a random forest, you know turns out not to be ideal So for stuff like this neural networks turn out that they are ideal Neural networks turn out to be something that works particularly well for both things like the Ecuadorian groceries competition So forecasting sales over time by store and by item and for things like recognizing digits and for things like turning voice into speech and so it's kind of nice between these two things Neural nets and random forests we kind of cover the territory, right? I don't I haven't needed to use anything other than these two things for a very long time And we'll actually learn I don't know what course exactly but at some point we'll learn also how to combine the two because you can combine the two in really cool ways So here's a picture from Adam guide key of an image so an image is just a bunch of numbers right and Each of those numbers is not to 255 and the dark ones are too close to 255 the light ones are close to zero All right. So here is an example of a digit from this M NIST data set M NIST is a really old It's like the hello world of machine of neural networks And so here's an example and so there are 28 by 28 pixels if it was Color there would be three of these one for red one for green one for blue So our job is to look at you know the array of numbers and Figure out that this is the number 8 Which is tricky right? How do we do that? so We're going to use a few a small number of fast AI pieces And we're gradually going to remove more and more and more until by the end. We'll have implemented our own Neural network from stretch our own training loop from scratch and our own matrix multiplication from scratch So we're gradually going to dig in further and further All right, so the data for M NIST which is the name of this very famous data set Is available from here and We have a thing in fast AI Dot IO called get data which will grab it from a URL and store it from your on your computer Unless it's already there in which case it'll just go ahead and use it okay, and Then we've got a little function here called load M NIST which simply loads it up You'll see That it's zipped so we could just use pythons gzip to open it up And then it's also pickled so if you have any kind of Python object at all You can use this built-in. I've been library called pickle to dump it out onto your disk Share it around load it up later, and you get back the same Python object you started with so you've already seen this something like this with like Pandas feather format right pickle is not just for pandas It's not just for anything it works for basically nearly every python object so Which might lead to the question well? Why didn't we use pickle for a pandas data frame right and the answer is pickle works for nearly every python object? But it's probably not like optimal for nearly any python object right so because like we were looking at pandas data frames with like over a hundred million rows We really want to save that quickly and so feather is a format that's specifically designed For that purpose and so it's going to do that really fast if we try to pickle it it would have been taken a lot longer right Also note that pickle files Only for python so you can't give them to somebody else where else like a feather file you can hand around Okay, so it's worth knowing that pickle exists because if you've got some Dictionary or some kind of object floating around that you want to save for later or send to somebody else you can always just Pickle it okay, so in this particular case the folks at deep learning net were kind enough to provide a pickled version Pickle has Changed slightly over time And so old pickle files like this one you actually have to this is a python 2 one So you have to tell it that it was encoded using this particular Python 2 character set but other than that python 2 and 3 you can normally open each other's pickle files Alright, so once we've loaded that in We load it in like so and so this thing which we're doing here This is called destructuring and so destructuring means that load MNIST is giving us back a tuple of tuples and So if we have on the left hand side of the equal sign a tuple of tuples we can fill all these things in So we're given back a tuple of training data a tuple of validation data and a tuple of test data In this case, I don't care about the test data. So I just put it into a variable called underscore which kind of by like People and pick Python people tend to think of underscore as being a special variable Which we put things we're going to throw away into it's actually not special But it's just it's really common if you see something assigned to underscore it probably means you're just throwing it away, right? By the way in a jupyter notebook it does have a special meaning which is the last cell That you calculate is always available in underscore by the way, but that's kind of a separate issue So then the first thing in that tuple is itself a tuple and so we're going to stick that into X and Y for our training data and then the second one goes into X and Y for our validation data Okay, so that's called destructuring and it's pretty common in lots of languages Some languages don't support it, but those that do life becomes a lot easier So as soon as I you know, look at some new data set, I just check out what's what have I got, right? So what's its type? Okay. It's a numpy array What's its shape? It's 50,000 by 7 8 4 and then what about the dependent variables? That's an array its shape is 50,000 so This image is not of length 7 8 4. It's of size 28 by 28 So what happened here? Well, we could guess and we can check on the website It turns out we would be right that all they did was they took the second row and concatenated to the first row and the third Row and concatenated to that and the fourth row and concatenated to that So in other words, they took this whole 28 by 28 and flattened it out into a single 1d array That makes sense. So it's going to be of size 28 squared This is not like normal by any means So don't think like everything you see is going to be like this most of the time when people share images They share them as JPEGs or PNGs you load them up you get back a nice 2d array But in this particular case for whatever reason the thing that they pickled was flattened out to be 784 and this word flatten is very common with You know kind of working with tensors so when you flatten the tensor it just means that you're turning it into a Lower rank tensor than you started with in this case. We started with a rank 2 tensor and matrix For each image and we turned each one into a rank 1 tensor ie a vector So overall the whole thing, you know is a rank 2 matrix Rank 2 tensor rather than a rank 3 tensor. So just to remind us of You know the jargon here This In math we would call a vector Right in computer science, we would call it a 1d array But because deep learning have people have to come across as smarter than everybody else we have to call this a rank 1 1 tensor okay, they all mean the same thing more or less Unless you're a physicist in which case this means something else and you get very angry at the deep learning people because you say it's Not a tensor So there you go. Don't blame me. This is just what people say so this is Either a matrix or a 2d array or a rank 2 tensor and So once we start to get into three dimensions, we start to run out of mathematical names Right, which is why we start to be nice just to say rank 3 tensor And so there's actually nothing special about vectors and matrices that make them in any way more important Than rank 3 tensors or rank 4 tensors or whatever So I try not to use the terms vector and matrix where possible Because I don't really think they're they're any more special than any other rank of tensor Okay, so kind of it's good to get used to thinking of this as a rank 2 tensor Okay, and then the The rows and Columns If it was a if we're computer science people we would call this dimension 0 and dimension 1 But if we're deep learning people we would call this axis 0 or axis 1 Okay, and then just to be really confusing if you're an image person This is the first axis and this is the second axis Right, so if you think about like TVs, you know 1920 by 1080 columns by rows Everybody else including deep learning and mathematicians rows by columns So this is pretty confusing if you use like the Python imaging library You get that columns by rows pretty much everything else rows by columns. So be careful Because they hate us Because they're bad people I Guess I Mean there's a lot of just um Particularly in deep learning like a whole lot of different areas have come together like information theory computer vision statistics signal processing and you've ended up with this hodgepodge of nomenclature and deep learning often like Every version of things will be used. So today we're going to hear about something that's called either negative log likelihood or binomial or categorical cross entropy Depending on where you come from We've already seen something that's called either one-hot encoding or dummy variables depending on where you come from I really is just like the same concept gets kind of somewhat independently invented in different fields and Eventually, they find their way to machine learning and then we don't know what to call them So we call them all of the above something like that So I think that's what's happened with with computer vision rows and columns so There's this idea of normalizing data which is subtracting out the mean and dividing by the standard deviation So a question for you Do you like often it's important to normalize the data so that we can more easily train a model Do you think it would be important to normalize the independent variables? For a random forest if we're training a random forest I'll be honest. I don't know why we don't need to normalize. I just know that we don't we don't okay Does anybody want to think about why? Kara it wouldn't matter because each scaling and Transformation we can have will be applied to each row and we will be computing means as we were doing like local averages and At the end we will of course want to denormalize it back to give so it wouldn't change the results I'm doing that the independent variables not the dependent variable. I thought you asked about Okay, let's have a go Matthew It might be because we just care about the relationship between the independent variables and the dependent variable so scale doesn't really matter Okay, go on How what why what why do we only like because at each split point? We can just divide to see Which Regardless of what scale you're on What minimizes variance and that would right so really the key is that when we're deciding where to split? All that matters is the order like it all that matters is how they're sorted so if we divide by the Subtract the mean and divide by the standard deviation. They're still sorted in the same order So remember when we implemented the random forest we said sort them and then we like it then we completely ignored the values We just said like now add on one thing from the dependent at a time So so random forests only care about the sort order of the independent variables They don't care at all about their size, and so that's why they're wonderfully immune to outliers Because they totally ignore the fact that it's an outlier. They only care about which one's higher than what other thing right so This is an important concept. It doesn't just appear in random forests It occurs in some metrics as well for example area under the ROC curve you come across a lot That area under the ROC curve Completely ignores scale and only cares about sort We saw something else when we did the dendrogram Spearman's correlation is a rank correlation only cares about order not about scale So random forests one of the many wonderful things about them are that we can completely ignore a lot of these statistical distribution issues But we can't for deep learning because for deep learning we're trying to train a parameterized model So we do need to normalize our data If we don't then it's going to be much harder to create a network that trains effectively so we grab the mean and the standard deviation of our training data and Subtract out the mean divide by the standard deviation and that gives us a mean of zero and a standard deviation of one Now for our validation data We need to use the standard deviation and mean from the training data right we have to normalize it the same way Just like Categorical variables we had to make sure they had the same indexes mapped to the same levels for a random forest or Missing values we had to make sure we had the same median used when we were replacing the missing values You need to make sure anything you do in the training set you do exactly the same thing in the test and validation set So here I'm subtracting out the training set mean the training sets down deviation So this is not exactly zero. This is not exactly one, but it's pretty close and So in general if you find you try something on a validation set or a test set and it's like much much much worse Than your training set. It's probably because you Normalized in an inconsistent way or encoded categories in inconsistent way or something like that All right, so let's take a look at some of this data So we've got 10,000 images in the validation set and each one is a rank one tensor of length seven eight four In order to display it. I want to turn it into a rank two tensor of 28 by 28 so there's a NumPy has a reshape function that takes a tensor in and Reshapes it to whatever size tensor you request Now if you think about it, you only need to tell it About if there are D axes you only need to tell it about D minus one of the axes you want Because the last one it can figure out for itself Right so in total there are ten thousand by seven hundred and eighty four numbers here altogether Right so if you say well, I want my last axes to be 28 by 28 Then you can figure out that this must be 10,000 otherwise it's not going to fit It makes sense so if you put minus one it says like make it as big or as small as you have to To make it fit and so you can see here it figured out it has to be 10,000 So you'll see this used in neural net software Pre-processing and stuff like that all the time like I could have written 10,000 here But I try to get into a habit of like any time I'm referring to like how many items are in my input I tend to use minus one because like it just means later on I could like use a subsample This code wouldn't break. I could you know do some kind of stratified sampling. It was unbalanced this code wouldn't break So by using this kind of approach of saying like minus one here for the size It just makes it more resilient to change us later. It's a good habit to get into so this kind of idea of like being able to take tensors and reshape them and and and change axes around and stuff like that is something you need to be like Totally do without thinking Because it's going to happen all the time. So for example, here's one. I tried to read in some images They were flattened. I need to unflatten them into a bunch of matrices. Okay reshape thing I read some I read some images in with open CV and it turns out open CV orders the channels Blue green red everything else expects them to be red green blue. I need to reverse the last axis. How do you do that? I read in some images with Python imaging library. It reads them as You know rows by columns by channels pytorch expects channels by rows by columns. How do I? Transform that so these are all things you need to be able to do Without thinking like straight away because they just it happens all the time and you never want to be sitting there thinking about it For ages, so make sure you spend a lot of time over the week Just practicing with things like all the stuff we're going to see today reshaping slicing reordering dimensions Stuff like that. And so the best way is to create some small Tenses yourself and start thinking like okay, what shall I experiment with so here? Can we pass that over there? Do my for backtrack a little bit, of course, I love it. So back in Normalize you say like you might have gone over this but I'm still like Wrestling with a little bit. Yeah, many machine learning algorithms behave better when the data is normalized Yeah, but you also just said that scales are really matter. So I said it doesn't matter for random forests Okay, yeah, so random forests are just going to spit things based on order And so we love them. We love random forests for the way. They're so Immune to worrying about distributional assumptions, but we're not doing random forests. We're doing deep learning and deep learning does care Can you pass it over there? We have a parametric then we should scale if we have a non parametric then we should not do scale Can we generalize no not quite right because like K nearest neighbors is non parametric and scale matters a hell of a lot So I would say things involving trees Generally, you're just going to split at a point and so probably you don't care about scale But you know, you probably just need to think like is this an algorithm that uses order or does it use specific numbers? Can can you please give us an intuition of why it needs scale just because it's that we may clarify some of the issues not until we get to doing SGD So we're going to get to that. Yeah, so for now, we're just going to say take my word for it Okay, pass it to general So this is probably a dumb question But can you like explain a little bit more what you mean by scale because I guess when I think of scale I'm like, oh all the numbers should be generally the same size That's exactly what we mean But is that like the case like with the cats and dogs that we went over with like the deep learning like you could have A small cat and like a larger cat, but it would still know that those were both cats Oh, I guess you know, this is one of these problems where language gets overloaded. Yeah, so in computer vision when we scale an image We're actually increasing the size of the cat in this case. We're scaling the actual pixel values So in both case scaling means to make something bigger and smaller in this case We're taking numbers from naught to 255 and making them so that they have an average of zero and a standard deviation of one Jeremy Could you please explain us? Is it by column by row? by pixel by pixel so there's a single general when you're scaling In just not thinking about every picture, but I'm kind of an input. Yeah much learning. So, okay. Yeah, sure So, I mean, it's a little bit subtle, but in this case, I've just got a single mean and a single standard deviation Right, so it's basically on average how How much black is there? Right and so on average, you know, we have a mean and a standard deviation across all the pixels In computer vision we would normally do it by channel So we would normally have one number for red one number for green one number for blue in General you you need a different set of normalization Normalization coefficients for each like each thing you would expect to behave differently So if we were doing like a structured data set where we've got like income Distance in kilometers and number of children, but you need three separate normalization coefficients for those They're like very different kinds of things So yeah, it's kind of like a bit domain specific here. It's like in this case all of the pixels are You know levels of gray. So we just got a single scaling number Where else you could imagine if they were red versus green versus blue you could need to scale those channels in different ways So I'm having a bit of trouble imagining what would happen if we don't normalize in this case So when we will get there so for net so we so this is kind of what you net was saying It's like why do we normalize and for now we're normalizing because I say we have to When we get to looking at stochastic gradient descent, we'll basically discover that if you Basically to skip ahead a little bit we're going to be doing a matrix multiply by a bunch of weights We're going to pick those weights in such a way that when we do the matrix multiply we're going to try to keep the numbers at the same scale that this out has and That's going to basically require the initial numbers. We're going to have to know what their scale is So basically it's much easier to create a single kind of neural network architecture That works for lots of different kinds of inputs if we know that they're consistently going to be mean zero standard deviation one That would be the short answer, but we'll learn a lot more about it and if in a couple of lessons You're still not quite sure why let's come back to it because it's a really interesting thing to talk about Yes, I'm just trying to visualize the axes we're working with here so under plots when you when you write so x-valid shape We get 10,000 by 7 8 4. Yeah, I mean that we brought in 10,000 pictures. Yeah of that dimension exactly Okay, and then in the next line When you choose to reshape it is there a reason why you put 28 28 on as a Y or z coordinates or is there a reason why they're in that order? Yeah, there is Pretty much all Neural network libraries assume that the first axis is Like kind of the equivalent of a row. It's like a separate thing. It's a sentence or an image or You know example of sales or whatever So I want each image, you know to be a separate item of the first axis And then so that leaves two more axes for the rows and columns of the images and that's pretty standard. That's totally standard Yeah, I don't think I've ever seen a library that doesn't work that way Can you pass it to our bureau? So while normalizing the validation data I saw you have Used mean of X and standard deviation of X data training data only. Yes So shouldn't we use mean and standard deviation of validation data? You mean like join them together or the separately calculating mean no because you see then you would be normalizing the value of the values Or the separately calculating mean no because you see then you would be normalizing the validation set using different numbers and so now the meaning of like This this pixel has a value of 3 in the validation set has a different meaning to the meaning of 3 in the in the training set it would be like If we had like days of the week Encoded such that Monday was a 1 in the training set and was a 0 in the validation set We've got now two different sets where the same number has a different meaning so we want to make sure That we so let me give you an example Let's say we were doing like full color images and our Tests their training set can contain like green frogs green snakes and gray elephants, right? We're trying to figure out which was which and we normalized using you know the each each channel mean and then we have a Validation set and the test set which are just green frogs and green snakes So if we were to normalize by the validation sets Statistics we would end up saying things on average are green and so we would like remove all the greenness out and So we would now fail to recognize the green frogs and the green Snakes effectively right so we actually want to use the same Normalization coefficients that we were training on and for those of you during the deep learning class We actually go further than that when we use a pre trained network We have to use the same normalization coefficients that the original authors trained on so the idea is that you know that a number Needs to have this consistent meaning across every data set where you use it Now can you pass it to us meter? That means when you're looking at the test set you normalize the test set based on this this mean it's that's right Okay So Here's a you know so so the valid validation y values are just rank one tensor of 10,000 Remember, there's this kind of weird Python thing where a tuple with just one thing in it needs a trailing comma Okay, so this is a rank one tensor of length 10,000, and so here's an example of something from that. It's just the number three So that's our labels, so here's another thing you need to be able to do in your sleep slicing Into a tensor So in this case we're slicing into the first axis With zero so that means we're grabbing the first slice So because this is a single number. This is going to reduce the rank of the tensor by one It's going to turn it from a three dimensional tensor into a two dimensional tensor right so you can see here This is now just a matrix and then we're going to grab 10 through 14 inclusive rows 10 through 14 inclusive columns and here it is right, so this is the kind of thing you need to be super comfortable like grabbing pieces out looking at the numbers and Looking at the picture right so here's an example of a little piece of that first image and so You kind of want to get used to this idea that if you're working with something like pictures or audio You know this is something your brain is really good at interpreting right so like keep showing pictures of what you're doing whenever you can But also remember behind the scenes their numbers So like if something's going weird print out a few of the actual numbers you might find somehow some of them have become infinity Or they're all zero or whatever right so like use this interactive environment And to explore the data as you go Did you have a question where's the box up? Just a quick I guess Semantic question why when it's a tensor of rank 3 is it stored as like xyz instead of? Like to me it would make more sense to store it as like a list of like 2d tensors It's not stored as either right so I'm for the formatting because let's look at this as a 3d Okay, so here's a 3d Right so a 3d tensor is formatted as showing a list of 2d tensors basically But when you're extracting it why isn't it like if you're extracting the first one why isn't it x images? Square bracket zero closed square brackets, and then a second set of square because that has a different meaning right so It's kind of the difference between Tenses and jagged arrays right so basically if you do like something like Something like that that says take the second list item and From it grab the third list item and so we tend to use that when we have something called a jagged array Which is where each sub array may be of a different length right where else we have? like a single object of Three dimensions, and so we're trying to say like which little piece of it Do we want and so the idea is that that is a single slice object to go in and grab that piece out? Okay So here's a example of a few of those images along with their labels and This kind of stuff you want to be able to do pretty quickly with matplotlib It's it's going to help you a lot in in life in your exam So you can have a look at you know what Rachel wrote here when she wrote plots we can use we can use add subplot to basically create those little separate plots and You need to know that I am show is how we basically take a numpy array and draw it as a picture Okay, and then we've also added the title on top So there it is all right, so let's now Take that data and try to build a neural network With it and so a neural network And sorry this is going to be a lot of review for those of you already doing deep learning A neural network is just a particular mathematical function or a class of mathematical functions But it's a really important class because it has the property it supports. What's called the universal approximation theorem Which is that which means that a neural network can? Approximate any other function arbitrarily closely Right so in other words it can do in theory it can do anything as long as we make it big enough So this is very different to a function like 3x plus 5 Right which can only do one thing it's a very specific It's a specific function or the class of functions a x plus B Which can only represent lines of different slopes moving it up and down different amounts or? Even the function a x squared plus B x plus C plus sine D You know again only can represent a very specific subset of relationships The neural network however is a function that can represent any other function to arbitrarily close accuracy, right? So what we're going to do is we're going to learn how to take a function And so let's take work a x plus B And we're going to learn how to find its parameters in this case a and B Which allow it to fit as closely as possible to a set of data? And so this here is showing example from a notebook that we'll be looking at in deep learning course Which basically shows what happens when we use something called stochastic gradient descent to try and set a and B And basically what happens is we're going to pick a random a to start with a random B To start with and then we're going to basically figure out Do I need to increase or decrease a to make it closer the line closer to the dots? Do I need to increase or decrease B to make the line closer to the dots and then just keep? Increasing and decreasing a and B lots and lots of times Okay, so that's what we're going to do and to answer the question do I need to increase or decrease a and B? We're going to take the derivative Right so the derivative of the function with respect to a and B tells us how will that function change as we change a and B? All right, so that's basically what we're going to do, but we're not going to start with just a line The idea is we're going to build up to actually having a neural net and so it's going to be exactly the same idea But because it's an infinitely flexible function We're going to be able to use this exact same technique to fit arbitrarily to arbitrarily complex relationships Now that's basically the idea So then what you need to know is that a neural net is actually a very simple thing a neural net actually is something which Takes as input. Let's say we've got a vector Does a matrix product By that vector right so this is like this is of size. Let's draw this properly so Like if this is size R. This is like R by C a matrix product will spit out something of Size C Right and then we do something called a non-linearity which is basically we're going to throw away all the negative values So it's basically max zero comma X and Then we're going to put that through another matrix multiply And then we're going to put that through another max zero comma X And we're going to put that through another matrix multiply and so on right until eventually we end up with The single vector that we want so in other words each stage of our neural network is the key thing going on is a Matrix multiply so in other words a linear function So basically deep learning most of the calculation is lots and lots of linear functions but between each one We're going to replace the negative numbers with zeros. Can you? Yes, so why are we throwing away the negative numbers as we go through this we'll see right the short answer is If you apply a linear function to a linear function to a linear function. It's still just a linear function So it's totally useless, but if you throw away the negatives That's actually a nonlinear transformation and so it turns out that if you apply a linear function To the thing we threw away the negatives that applied that to a linear function that creates a neural network And it turns out that's the thing that can approximate any other function Arbitrarily closely so this tiny little difference Actually makes all the difference and if you're interested in it check out the deep learning video where we cover this because I actually show a nice visual intuitive proof not something that I created but something that Michael Nielsen created Or if you want to skip straight to his website You could go to Michael Nielsen universal I Think I spelled his name wrong never mind mission theorem There we go neural networks and deep learning chapter 4, and he's got a really nice Walk through basically with lots of animations where you can see why this works One I feel like the the the hardest thing I Feel like the hardest thing with getting started like technical writing on the internet is just like posting your first thing so if you do a search for Rachel Thomas medium blog you'll find this we'll put it on the lesson wiki Where she talks about she actually says the top advice she would give to her younger self would be to start blogging sooner and she has like Both reasons why you should do it Some examples of things that you know examples of places She's bloggers turned out to be great for her and her career, but then some tips about how to get started I remember when I first suggested to Rachel She might think about blogging because she had so much interesting to say and you know at first she was kind of surprised at The idea that like she could blog you know and now people come up to us at conferences And they're like you're Rachel Thomas. I love your writing You know so like I've kind of seen that that transition from like wow could I blog to to be known as a strong? Technical author so yeah, so check out this article If you still need convincing or if you're wondering how to get started and since the first one is the hardest Maybe your first one should be like Something really easy for you to write you know so it could be like you know here's a summary of the first 15 minutes of Lesson three of our machine learning course you know here's why it's interesting. Here's what we learned or it could be like Here's a summary of how I used a random forest to solve a particular problem in my practicum I often get questions like on my practicum my organization. We've got like sensitive commercial data That's fine like you know just find another data set and do it on that instead to show the example or You know anonymize all of the values and change the names of the variables or whatever like you can talk to Your employer or your practicum partner to make sure that they're comfortable With whatever it is you're writing in general though. You know People love it when their interns and staff Blog about what they're working on because it makes them look super cool. You know it's like hey, I'm an You know Intern working at this company, and I wrote this post about this cool analysis I did and then other people would be like wow that looks like a great company to work for so generally speaking You should find people are pretty supportive Of besides which there's lots and lots of data sets out there available So even if you can't base it on the work you're doing you can find something similar for sure All right, so we're going to start building our neural network. We're going to build it Using something called my torch my torch is a library that basically looks a lot like numpy But when you create Some code with pie torch you can run it on the GPU rather than the CPU so the GPU is Something which is basically going to be probably at least an order of magnitude Possibly hundreds of times faster than the code that you might write for the CPU For particularly stuff involving lots of linear algebra So with deep learning neural nets You can if you if you don't have a GPU you can do it on the CPU right, but it's going to be frustratingly slow Your Mac does not have a GPU that we can use for this Because I'm actually advertising today. We need an Nvidia GPU I would actually much prefer that we could use your Macs because competitions great right but Nvidia were really the first ones to create a GPU which did a good job of supporting general-purpose Graphics programming units GP GPU so in other words that means using a GPU for things other than playing computer games They used they created a framework called CUDA CUDA It's it's a very good framework. It's pretty much universally used in deep learning If you don't have an Nvidia GPU you can't use it no no current Macs have an Nvidia GPU Most laptops of any kind don't have an Nvidia GPU if you're interested in doing deep learning on your laptop The good news is that you need to buy one which is really good for playing computer games on There's a place called exotic PC gaming laptops where you can go and buy yourself a great laptop For doing deep learning you can tell your parents that you need the money To do deep learning so could you please have? Yeah, so you'll generally find a whole bunch of laptops with names like predator and viper with pictures of robots and stuff so Stealth Pro Raider leopard anyway Having said that like I don't know that many people that do much deep learning on their laptop most people will log into a cloud environment By far the easiest I know of to use is called Cressel with Cressel You can basically sign up and straight away the first thing you get is a Thrown straight into a jupyter notebook backed by a GPU cost 60 cents an hour With all of the fast AI libraries and data already available So that makes life really easy It's less Flexible and in some ways less fast than using AWS Which is the Amazon web services option? Costs a little bit more 90 cents an hour rather than 60 cents an hour But it's very likely that your employer Is already using that it's like it's good to get to know anyway They've got more different choices around GPUs and it's a good good choice if you google for github student pack If you're a student You can get a hundred and fifty dollars of credits Straight away pretty much and so that's a really good way to get started Daniel. Did you have a question? I just wanted to know your opinion on I know that Intel recently published like an open source like way of like boosting like regular packages that they claim is Equivalent like if you use the bottom tier GPU on your seat like on your CPU if you use their boost packages Like you can get the same performance Do you know anything about that? Yeah, I do it's a good question. So and actually Intel makes some great Numerical programming libraries particularly this one called MKL the matrix kernel library they They definitely make things faster than not using those libraries But if you look at a graph of performance over time GPUs have consistently throughout the last 10 years including now Are about 10 times more floating point operations per second than the equivalent CPU? And they're generally about a fifth of the price for that performance so Yeah, it it and then because of that like Everybody doing anything with deep learning basically does it on Nvidia GPUs and therefore using anything other than Nvidia GPUs is currently very annoying So slower more expensive more annoying I really hope there will be more activity around AMG GPUs in particular in this area But AMD's got like literally years of catching up to do so it might take a while Yeah, so I just wanted to point out that you can also buy things such as like a GPU extender to a laptop Yeah, that's also like kind of like like maybe a first step solution. Yeah, you really want to put something on Yeah, yeah I think for like 300 bucks or so you can buy something that plugs into your thunderbolt port if you have a Mac and then for another Five or six hundred bucks you can buy a GPU to plug into that Having said that for about a thousand bucks you can actually create a pretty good You know GPU based desktop And so if you're considering that the fast AI forums have like lots of threads where people help each other Spec out something at a particular price point Anyway, so to start with I'd say use Cressul and then You know when you're ready to invest a few extra minutes getting going use AWS to use AWS you basically Huh yeah, yeah Yeah, I'm just talking to the folks online as well so Okay, so So AWS when you get there go to EC2 EC2 like there's lots of stuff on AWS EC2 is the bit where we get to like rent computers by the hour right? Now we're going to need a GPU based instance Unfortunately when you first sign up for AWS they don't give you access to them So you have to request that access so go to limits up on the top left right and the main GPU instance will be using is called the P2 so scroll down to P2 and here P2 dot x large You need to make sure that that number is not zero if you've just got a new account It probably is zero which means you won't be allowed to create one so you have to go request limit increase and the trick there is When it asks you why do you want the limit increase type faster AI? Because AWS knows to look out and they know that faster AI people are good people so they'll do it quite quickly That takes a day or two generally speaking to go through So once you get the email saying you've been approved for P2 instances You can then go back here and say launch instance And so we've basically set up one that has everything you need so if you click on community AMI and AMI is an Amazon machine image. It's basically a completely set up Computer right and so if you type fast AI or one word You'll find here fast AI deal part one version two for the P2 right so that's all set up ready to go so if you click on Select and it'll say okay. What kind of computer do you want right and so we have to say all right? I want a GPU compute type and Specifically I want a P2 extra large Right and then you can say review and launch I'm assuming you already know how to deal with SSH keys and all that kind of stuff if you don't check out the introductory tutorials and workshop videos that we have online or Google around for SSH keys Very important skill to know anyway all right, so hopefully you get through all that you have Something running on a GPU with the fast AI repo if you use Cressel just See D fast AI to the repos already there get Paul AWS CD fast AI the repo was already there get Paul If it's your own computer, you'll just have to get clone and then away you go All right, so part of all of those is pytorch is pre-installed and so pytorch basically means we can write code That looks a lot like numpy, but it's going to run really quickly on the GPU secondly Since we need to know like which direction and how much to move our parameters to improve our loss We need to know the derivative of functions Pytorch has this amazing thing where any code you write using the pytorch library It can automatically take the derivative of that for you So we're not going to look at any calculus in this course And I don't look at any calculus in any of my courses or in any of my work Basically ever in terms of like actually calculating derivatives myself because I've never had to It's done for me by the library so as long as you write the Python code It's the derivative is done So the only calculus you really need to know to be an effective practitioner is like what is it? What does it mean to be a derivative? And you also need to know the chain rule which will come to You all right, so we're going to start out kind of top-down Create a neural net and we're going to assume a whole bunch of stuff and gradually we're going to dig into each piece Right so to create neural nets we need to import the pytorch neural net library Pytorch funnily enough is not called pytorch. It's called torch. Okay, so torch dot nn is the pytorch Fastsection that's responsible for neural nets Okay, so we'll call that nn and then we're going to import a few bits out of fast AI just to make life a bit easier for us So here is how you create a neural network in pytorch the simplest possible neural network You say sequential and sequential means I am now going to give you a list of the layers that I want in my neural network Right so in this case my list has two things in it the first thing says I want a linear layer so a linear layer is something that's basically going to do y equals ax plus b right but Matrix matrix multiply not not univariate obviously So it's going to do a matrix product basically So the input to the matrix product is going to be a vector of length 28 times 28 because that's how many Pixels we have and the output needs to be of size 10 We'll talk about why in a moment, but for now, you know, this is how we define a linear layer And then again, we're going to dig into this in detail But every linear layer just about in neural nets has to have a non-linearity after it and we're going to learn about this particular Non-linearity in a moment. It's called the softmax and if you've done the DL course, you've already seen this So that's how we define a neural net. This is a two layer neural net There's also kind of an implicit additional first layer, which is the input But with pytorch you don't have to explicitly mention the input that normally we think conceptually like the input image Is kind of also a layer Because we're kind of doing things pretty manually With pytorch we're not taking advantage of any of the conveniences in fast AI for building this stuff We have to then write dot CUDA which tells pytorch to copy this neural network across to the GPU So now from now on that network is going to be actually running on the GPU If we didn't say that it would run on the CPU So that gives us back a neural net very simple neural net so we're then going to try and Fit the neural net to some data so we need some data So fast AI has this concept of a model data object, which is basically something that wraps up training data validation data and optionally test data and so to create a Model data object you can just say I want to create some image classifier data. I'm going to grab it from some arrays Right and you just say okay. This is the path that I'm going to save any temporary files This is my training data Arrays, and this is my validation data arrays, okay, and so that just returns an object That's going to wrap that all up and so we're going to be able to fit to that data So now that we have a neural net and we have some data We're going to come back to this in a moment But we basically say what loss function do we want to use what optimizer do we want to use and then we say? Fit we say fit this network To this data going over every image once Using this loss function and this optimizer and print out these metrics Bang okay, and this says here. This is 91 point eight percent accurate So that's like the simplest possible neuron it so what that's doing is It's creating a matrix multiplication Followed by a non-linearity, and then it's trying to find the values for this matrix Which cause which basically that fit the data as well as possible that? That end up predicting this is a 1 this is a 9 this is a 3 and So we need some definition for as well as possible And so the general term for that thing is called the loss function so the loss function is the function that's going to be Lower if this is better right just like with random forests We have this concept of information gain, and we got to like pick what function Do you want to use to define information gain, and we were mainly looking at root mean squared error, right? Most machine learning algorithms we call something very similar that loss right so the loss is How do we score how good we are and so in the end? we're going to calculate the derivative of the loss with respect to the The weight matrix that we're multiplying by to figure out how to update it right so We're going to use something called negative log likelihood loss So negative log likelihood loss is also known as cross entropy They're literally the same thing. There's two versions one called binary cross entropy or Binary negative log likelihood and another called categorical cross entropy now the same thing One is for when you've only got a zero or one dependent the other is if you've got like cat dog Airplane or horse or zero one through nine or so forth so what we've got here is the binary version of Cross entropy and so here is the definition I Think maybe the easiest way to understand this definition is to look at an example So let's say we're trying to predict cat versus dog One is cat zero is dog so here we've got cat dog dog cat and Here are our predictions. We said 90% sure it's a cat 90% sure it's a dog 80% sure it's a dog 80% sure it's a cat right, so we can then calculate the Binary cross entropy by calling our function, so it's going to say okay for the first one We've got y equals 1 so it's going to be 1 times log of point 9 Plus 1 minus y 1 minus 1 is 0 so that's going to be skipped Okay, and then the second one is going to be a 0 so it's going to be 0 times something so that's going to be skipped and The second part will be 1 minus 0 so this is 1 times log of 1 minus p 1 minus point 1 is point 9 So in other words the first piece and the second piece of this are going to give exactly the same number Which makes sense because the first one we said we were 90% confident It was a cat and it was and the second we said we were 90% confident It was a dog and it was so in each case the loss is coming from the fact that you know We could have been more confident. Yeah, so if we said we're a hundred percent confident the loss would have been zero Right so let's look at that in Excel So here's our Point 9 point 1 point 2 point 8 right and here's our predictions 1001 so here's 1 minus the prediction Right here is log of our prediction Here is log of 1 minus our prediction And so then here is our sum Okay So if you think about it And I want you to think about this during the week you could replace this with an if statement Rather than why because why is always 1 or 0? Right then it's only ever going to use either this or this so you could replace this with an if statement So I'd like you during the week to try to rewrite this with an if statement okay, and then see if you can then Scale it out to be a categorical cross entropy so categorical cross entropy works this way. Let's say we were trying to predict 3 and then 6 and then 7 and then 2 so if we were trying to predict 3 and The actual thing that was predicted was like 4.7 Right Versus like well actually think of this way we're trying to predict 3 and we actually predicted 5 Or we're trying to predict 3 and we accidentally predicted 9 Like being 5 instead of 3 is no better than being 9 instead of 3 So we're not actually going to say like how far away is the actual number we're going to express it differently Or to put it another way what if we're trying to predict cats dogs horses and airplanes You can't like how far away is cat from horse, so we're going to express these a little bit differently rather than thinking What is a 3 let's think of it as a? vector With a 1 in the third location And rather than thinking it was a 6 let's think of it as a vector of zeros with a 1 in the 6th Location so in other words one hot encoding right so let's one hot encode a dependent variable And so that way now rather than predicting trying to predict a single number. Let's predict 10 numbers Let's predict. What's the probability that it's a zero? What's the probability? It's a one What's the probability is a two and so forth right and so let's say we're trying to predict the two Right then here is our binary cross entropy sorry categorical cross entropy, so it's just saying okay Did this one predict correctly or not how far off was it and so forth for each one? Right and so add them all up so categorical cross entropy is identical to binary cross entropy We just have to add it up across all of the categories So try and turn the binary cross entropy Function in Python into a categorical cross entropy Python and maybe create both the version with the if statement and the version with the sum And the product right? All right, so that's why in our pie torch We had 10 as the output as the output dimensionality for this matrix Because when we multiply by a matrix with 10 columns We're going to end up with something of length 10, which is what we want we want to have 10 predictions Okay so So that's the loss function that we're using All right, so then we can fit the model And what it does is it goes through every image this many times so in this case, it's just looking at every image once and going to slightly update the values in that weight matrix based on those gradients and So once we've trained it we can then say predict Using this model on the validation set Right and now that spits out something of 10,000 by 10 Can somebody tell me why is this of shape these predictions? Why are they of shape 10,000 by 10? Go for it Chris. It's right next to you Well, it's because we have 10,000 images What we're training on 10,000 images training on so that's what we're validating on Or this but the same thing so 10,000 we're validating on so that's the first axis That's the first and then the second axis is because we actually make 10 predictions per image good good exactly So each one of these rows is the probabilities that it's a naught that it's a one that is a two that's three and so forth Okay, very good So in math there's a really common Operation we do called arg max and what I say it's common. It's funny like At high school I never saw arg max First year undergrad I never saw arg max but somehow after university Everything's about arg max so it's one of these things that's for some reason not really taught at school But it actually turns out to be super critical And so arg max is both something that you'll see in math, and it's just written out in full arg max It's in numpy it's in pytorch It's super important and what it does is it says let's take this array of preds Right and let's figure out on this axis remember axis one is columns right so across as Chris said the 10 predictions for each one for each row Let's find which prediction has the highest value and return not that if it just said max it would return the value arg max returns the index Of the value right so by saying arg max axis equals one it's going to return The index which is actually the number itself right so let's grab the first five Okay, so for the first one it thinks as a three then it thinks next one's an eight next one's a six the next one's a Nine next one's a six again, okay, so that's how we can convert our probabilities back into predictions All right, so if we save that away call it preds we can then say okay when does preds equal? The ground truth right so that's going to return an array of balls Which we can treat as ones and zeros and the mean of a bunch of ones and zeros Is just the average so that gives us the accuracy So there's our 91.8 percent and so you want to be able to like replicate the numbers you see and here it is There's our 91.8 percent All right, so when we train this it tells us the last thing it tells us is whatever metric we asked for and we asked for Accuracy okay, so the last thing it tells us is our metric which is accuracy and then before that we get the Training set loss and the loss is again whatever odds we asked for negative log likelihood And the second thing is the validation set loss Pi torch doesn't use the word loss they use the word criterion so you'll see here crit Okay, so that's criterion equals loss. This is what loss function do we want to use they call that the criterion same thing, okay? So here is how we can recreate that accuracy So now we can go ahead and plot eight of the images along with their predictions and we've got three eight six nine wrong Five wrong okay, and you can see like why they're wrong like this is pretty close to a nine It's just missing a little cross at the top This is pretty close to a five. It's got a little bit of the extra here right so we've made a start and And all we've done so far is we haven't actually created a deep neural net We've actually got only one layer So what we've actually done is we've created a logistic regression Okay, so a logistic regression is is literally what we just built and you could try and replicate this with SK learns logistic regression Package when I did it I got Similar accuracy, but this version ran much faster because this is running on the GPU Where else SK learn runs on the CPU? Okay, so even for something like logistic regression we can you know implement it very quickly with pytorch. How can you pass that to him? So when we're when we're creating our net we have to do dot CUDA What would be the consequence of not doing that would it just not run it wouldn't run quickly? Yeah, it'll run on the CPU Can you pass it to Jay? So maybe the neural network why is that we have to do linear and followed by a nonlinear So the short answer is because that's what the universal approximation theorem says is the structure which can give you Arbitrarily accurate functions for any functional form, you know So the long answer is the details of why the universal approximation theorem works Another version of the short answer is that's the definition of a neural network So the definition of a neural network is a linear layer followed by a Activation function followed by a linear layer followed by an activation function Etc We go into a lot more detail of this in the deep learning course But you know for this purpose. It's it's enough to know like that it works So far of course we haven't actually built a deep neural net at all. We've just built a logistic regression and so at this point if you think about it all we're doing is we're taking every input pixel and Multiplying it by a weight For each possible outcome, right? So we're basically saying, you know on average the number one You know has these pixels turned on the number two has these pixels turned on and that's why it's not terribly accurate, right? That's that's not how Digit recognition works in real life, but that's that's always built so far Okay, can you pass that to Devon? So you keep saying this universal approximation theorem. Yeah, did you define that? Yeah, but let's cover it again because it's worth talking about so All right, so Michael Nielsen has this great website called neural networks and deep learning and his chapter 4 is Is actually kind of famous now and in it he does this walkthrough of basically showing that a neural network can Can approximate any other function to arbitrarily close Accuracy as long as it's big enough and we walk through this in a lot of detail in the deep learning course but the basic trick is that he shows that with a Few different numbers you can basically kind of cause these things to kind of create little boxes You can move the boxes up and down you can move them around You can join them together to eventually basically create like connections of towers Which you can like use to approximate any kind of surface, right? so that's you know, that's basically the the trick and And so all we need to do given given that is to kind of find the parameters for each of the linear functions In that neural network so to find the weights in each of the in each of the matrices and so so far We've got just one Matrix and so we've just built a simple logistic regression so far I'm gonna do your question. Yeah, just a small doubt I just want to confirm that when you showed images of the examples of the images which were misclassified Yeah, they look rectangular. So it's just that while rendering the pixels are being scaled differently So are they still 28 by 28 square 28 by 28? I Think that's where I think they just look rectangular because they've got titles on the top. I'm not sure I don't know. Anyway, they are square and like Matplotlib. Yeah, it does often fiddle around with you know what it considers black versus white and you know Having different size axes and stuff. So yeah, you do have to be a bit careful there sometimes Okay, so Hopefully this will now make more sense because what we're going to do is like dig in a layer deeper and define Logistic regression without using an end dot sequential without using an end dot linear without using an end dot log softmax So we're going to do Nearly all of the layer definition from scratch. Okay, so to do that We're going to have to define a pytorch module a pytorch module is basically either a neural net or a layer in a neural net Which is actually kind of a powerful concept of itself Basically anything that can kind of behave like a neural net can itself be part of another neural net And so this is like how we can construct particularly powerful architectures combining lots of other pieces So to create a pytorch module just create a Python class But it has to inherit from an end module. So we haven't done inheritance before Other than that, this is all the same concepts we've seen in owo already Basically, if you put something in parentheses here, what it means is that our class gets all of the functionality of this class For free it's called subclassing it So we're going to get all of the capabilities of a neural network module that the pytorch authors have provided and then we're going to add additional functionality to it When you create a subclass there is one key thing you need to remember to do which is when you initialize your class You have to first of all initialize the super class. That's how the super class is the nn dot module So the nn dot module has to be built before you can start adding your pieces to it And so this is just like something you can copy and paste into every one of your modules You just say super dot in it. This just means Construct the super class first. Okay so Having done that we can now go ahead and define our weights and our bias. So our weights is The the weight matrix is the actual matrix that we're going to multiply our data by and as we discussed it's going to have 28 times 28 rows and 10 columns And that's because if we take an image Which we flattened out into a 28 by 28 length vector right, then we can multiply it by this weight matrix to get back out a length 10 Vector which we can then use to Consider as a set of predictions So that's our weight matrix now The problem is that we don't just want y equals ax we want y equals ax plus b so the plus B in Neural nets is called bias and so as well as defining weights We're also going to find bias and so since this thing is going to spit out For every image something of length 10 That means that we need to create a vector of length 10 to be our Biases in other words for everything nought one two three up to nine. We're going to have a different Plus B that would be adding right, so We've got our Data matrix here, which is of length 10,000 by 28 times 28 All right, and then we've got our weight matrix Which is 28 by 28 Rows by 10, so if we multiply those together We get something of size 10,000 by 10 Right and then we want to add on our bias Sorry wrong way around add on our bias Okay like so and so when we add on and we're going to learn a lot more about this later, but when we add on a Vector like this it basically is going to get added to every row Okay, so the bias is going to get added to every row So we first of all define those and so to define them we've created a tiny little function called get weights Which is over here, right? Which basically just creates some normally distributed random numbers So torch dot Rand n returns a tensor filled with random numbers from a normal distribution We have to be a bit careful though when we do deep learning like when we add more linear layers later, I imagine if we have a Matrix which on average tends to increase the size of the inputs we give to it if we then Multiply by lots of matrices of that size. It's going to make the numbers bigger and bigger and bigger like exponentially bigger Or what if it made them a bit smaller? It's going to make them smaller and smaller and smaller exponentially smaller So like because a deep network applies lots of linear layers If on average they result in things a bit bigger than they started with or a bit smaller than they started with it's going to like exponentially multiply that difference So we need to make sure that the weight matrix is of an appropriate size that the Inputs to it the kind of the mean of the inputs basically is not going to change so it turns out that if you use normally distributed random numbers and divided by The number of rows in the weight matrix It turns out that particular random initialization Keeps your numbers at about the right scale, right? So this idea that like if you've done linear algebra Basically if the eigenvalue the first eigenvalue is like bigger than one or smaller than one It's going to cause the gradients to like get bigger and bigger or smaller and smaller. That's called gradient explosion, right? So We'll talk more about this in the deep learning course but if you're interested you can look up timing her initialization and Read all about This concept right but for now, you know, it's probably just enough to know that if you use this Type of random number generation you're going to get random numbers that are Nicely behaved you're going to start out with an input which is mean zero standard deviation one Once you put it through this set of random numbers, you'll still have something that's about mean zero standard deviation one That's basically the goal. Okay? One nice thing about pie torch is that you can play with this stuff, right? So torch dot random like try it out like every time you see a function being used run it All right, and take a look and so you'll see it looks a lot like numpy Right, but it doesn't return a numpy array. It returns a tensor and in fact Now I'm GPU programming, okay, like put docuda and now it's doing it on the GPU so like I Just multiplied that matrix by three very quickly on the GPU, right? So that's how we do GPU programming with pie torch, right? So This this is our weight matrix So we create as I said we create 128 by 28 by 10 one is just rank one of 10 for the biases We have to make them a parameter. This is basically telling pie torch which things to update when it does SGD That's very minor technical detail. So having created the weight matrices We then define a special method with the name forward. This is a special method The word the name forward has a special meaning in pie torch a method called forward in pie torch Is the name of the method that will get called when your layer is calculated? Okay, so if you create a neural net or a layer you have to define Forward and it's going to get past the data from the previous layer So our definition is to do a matrix multiplication of our input data times our weights and add on the biases So that's it. That's what happened earlier on when we said n n dot linear it created this This thing for us Okay now unfortunately, though, we're not getting a 28 by 28 long vector we're getting a 28 row by 28 column matrix So we have to flatten it Unfortunately in torch pie torch they tend to rename things They they spell resize reshape. They spell it view. Okay, so view means reshape So you can see here we end up with something where the number of images we're going to leave the same and Then we're going to replace row by column with a single Axis again negative one meaning as long as required. Okay, so this is how we Flatten something using pie torch. So we flatten it do a matrix multiply and then finally We do a soft max. So soft max is the activation function we use If you look in the deep learning repo, you'll find something called entropy example Where you'll see an example of soft max, but a soft max simply takes the outputs from our final layer so we get our outputs from our from our linear layer and What we do is we go e to the power of? for each output and Then we take that number and we divide by the sum of the e to the power ofs That's called soft max. Why do we do that? Well because we're dividing this by the sum That means that the sum of those itself must add to one, right? And that's what we want. We want the probabilities of all the possible outcomes add to one Furthermore because we're using a to the power of that means we know that every one of these is between 0 and 1 and Probabilities we know should be between 0 and 1 And then finally because we're using a to the power of it tends to mean that slightly bigger values In the input turn into much bigger values in the output So you'll see generally speaking my soft max there was going to be one big number and lots of small numbers and that's what we want Right because we know that the output is one hot encoded So in other words a soft max Activation function the soft max non-linearity is something that returns things that behave like probabilities and Where one of those probabilities is more likely to be kind of high and the other ones that were more likely to be low and We know that's what we want for a to map to our one hot encoding So a soft max is a great activation function to use to kind of help the neural net make it easier for the neural net to to map To the output that you wanted This is what we generally want when we're kind of designing neural networks We try to come up with little architectural tweaks that make it as easy for it as possible to Match the output that we know we want so That's basically it right like rather than doing sequential You know and using an end dot linear and in dot soft max we have to find it from scratch We can now say just like before our net is equal to that class dot cuda And we can say dot fit and we get to within a slight random deviation exactly the same output Okay, so what I'd like you to do during the week is to play around with like Torch dot random to generate some random tensors torch dot mat mole to start multiplying them together adding them up Try to make sure that you can rewrite soft max yourself from scratch You know like try to fiddle around a bit with you know reshaping view all that kind of stuff so that by the time you come Back next week you feel like pretty comfortable with pytorch and if you google for pytorch tutorial You'll see there's a lot of great material Actually on the pytorch website To help you along basically showing you how to create tensors and modify them and do operations on them Alright great Yes, you had a question. Can you pass it over? So I see that the forward is the layer that gets applied after each of the linear layers So not quite the forward is just the definition of the module So this is like how we're this is how we're implementing linear So does that mean after each linear layer we have to apply the same function. Let's say we can do a Log soft max after layer one and then apply some other function after layer two if we have like a Multi-layer neural network So normally we define neural networks Normally we define neural networks like so we just say here is a list of the layers we want Right we don't you don't have to write Your own forward right all we did just now was to say like okay instead of doing this Let's not use any of this at all, but write it all by hand ourselves ourselves right so you can you can write as many layers as you like in what any order you like here the Point was that? Here we're not using any of that. We've written our own matmul plus bias our own Soft max so this is like this is this is just Python code you can write whatever Python code Inside forward that you like To define your own neural net so like you won't normally do this yourself Normally you'll just use the layers that pytorch provides and you're used dot sequential to put them together or even more likely You'll download a predefined architecture and use that we're just doing this to learn how it works behind the scenes All right great. Thanks everybody", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.8, "text": " So I I don't want to embarrass Rachel, but I'm very excited that Rachel's here, so this is Rachel for those of you", "tokens": [407, 286, 286, 500, 380, 528, 281, 9187, 14246, 11, 457, 286, 478, 588, 2919, 300, 14246, 311, 510, 11, 370, 341, 307, 14246, 337, 729, 295, 291], "temperature": 0.0, "avg_logprob": -0.14378573313480666, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.04601277410984039}, {"id": 1, "seek": 0, "start": 8.8, "end": 10.8, "text": " That don't know", "tokens": [663, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.14378573313480666, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.04601277410984039}, {"id": 2, "seek": 0, "start": 11.48, "end": 14.48, "text": " She's not quite back on her feet after her illness", "tokens": [1240, 311, 406, 1596, 646, 322, 720, 3521, 934, 720, 10152], "temperature": 0.0, "avg_logprob": -0.14378573313480666, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.04601277410984039}, {"id": 3, "seek": 0, "start": 14.48, "end": 17.32, "text": " But well enough to at least come to at least part with this lesson", "tokens": [583, 731, 1547, 281, 412, 1935, 808, 281, 412, 1935, 644, 365, 341, 6898], "temperature": 0.0, "avg_logprob": -0.14378573313480666, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.04601277410984039}, {"id": 4, "seek": 0, "start": 17.32, "end": 19.44, "text": " So don't worry if she can't stay for the whole thing", "tokens": [407, 500, 380, 3292, 498, 750, 393, 380, 1754, 337, 264, 1379, 551], "temperature": 0.0, "avg_logprob": -0.14378573313480666, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.04601277410984039}, {"id": 5, "seek": 0, "start": 19.44, "end": 24.28, "text": " I'm really glad she's here because Rachel actually wrote the vast majority of the lesson", "tokens": [286, 478, 534, 5404, 750, 311, 510, 570, 14246, 767, 4114, 264, 8369, 6286, 295, 264, 6898], "temperature": 0.0, "avg_logprob": -0.14378573313480666, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.04601277410984039}, {"id": 6, "seek": 0, "start": 24.28, "end": 26.560000000000002, "text": " We're going to see it's I think it's a really really cool work", "tokens": [492, 434, 516, 281, 536, 309, 311, 286, 519, 309, 311, 257, 534, 534, 1627, 589], "temperature": 0.0, "avg_logprob": -0.14378573313480666, "compression_ratio": 1.7290076335877862, "no_speech_prob": 0.04601277410984039}, {"id": 7, "seek": 2656, "start": 26.56, "end": 32.8, "text": " So I'm glad she's gonna at least see it being taught even if unfortunately she's not teaching it herself", "tokens": [407, 286, 478, 5404, 750, 311, 799, 412, 1935, 536, 309, 885, 5928, 754, 498, 7015, 750, 311, 406, 4571, 309, 7530], "temperature": 0.0, "avg_logprob": -0.22272580290493899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.609384142095223e-05}, {"id": 8, "seek": 2656, "start": 33.879999999999995, "end": 35.879999999999995, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.22272580290493899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.609384142095223e-05}, {"id": 9, "seek": 2656, "start": 37.2, "end": 39.68, "text": " Good Thanksgiving present best Thanksgiving present", "tokens": [2205, 21230, 1974, 1151, 21230, 1974], "temperature": 0.0, "avg_logprob": -0.22272580290493899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.609384142095223e-05}, {"id": 10, "seek": 2656, "start": 41.28, "end": 48.68, "text": " So we're as we discussed at the end of last lesson we're kind of moving from the sea decision tree ensembles to", "tokens": [407, 321, 434, 382, 321, 7152, 412, 264, 917, 295, 1036, 6898, 321, 434, 733, 295, 2684, 490, 264, 4158, 3537, 4230, 12567, 2504, 904, 281], "temperature": 0.0, "avg_logprob": -0.22272580290493899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.609384142095223e-05}, {"id": 11, "seek": 2656, "start": 49.519999999999996, "end": 51.92, "text": " to neural nets broadly defined and", "tokens": [281, 18161, 36170, 19511, 7642, 293], "temperature": 0.0, "avg_logprob": -0.22272580290493899, "compression_ratio": 1.6363636363636365, "no_speech_prob": 9.609384142095223e-05}, {"id": 12, "seek": 5192, "start": 51.92, "end": 56.2, "text": " As we discussed, you know random forests and decision trees", "tokens": [1018, 321, 7152, 11, 291, 458, 4974, 21700, 293, 3537, 5852], "temperature": 0.0, "avg_logprob": -0.16714643122075679, "compression_ratio": 1.6116071428571428, "no_speech_prob": 8.939477993408218e-06}, {"id": 13, "seek": 5192, "start": 57.120000000000005, "end": 58.96, "text": " are limited", "tokens": [366, 5567], "temperature": 0.0, "avg_logprob": -0.16714643122075679, "compression_ratio": 1.6116071428571428, "no_speech_prob": 8.939477993408218e-06}, {"id": 14, "seek": 5192, "start": 58.96, "end": 61.120000000000005, "text": " by the fact in the end that they're basically", "tokens": [538, 264, 1186, 294, 264, 917, 300, 436, 434, 1936], "temperature": 0.0, "avg_logprob": -0.16714643122075679, "compression_ratio": 1.6116071428571428, "no_speech_prob": 8.939477993408218e-06}, {"id": 15, "seek": 5192, "start": 63.800000000000004, "end": 65.88, "text": " They're basically doing nearest neighbors", "tokens": [814, 434, 1936, 884, 23831, 12512], "temperature": 0.0, "avg_logprob": -0.16714643122075679, "compression_ratio": 1.6116071428571428, "no_speech_prob": 8.939477993408218e-06}, {"id": 16, "seek": 5192, "start": 66.4, "end": 71.08, "text": " Right, you know that all they can do is to get return the average of a bunch of other points", "tokens": [1779, 11, 291, 458, 300, 439, 436, 393, 360, 307, 281, 483, 2736, 264, 4274, 295, 257, 3840, 295, 661, 2793], "temperature": 0.0, "avg_logprob": -0.16714643122075679, "compression_ratio": 1.6116071428571428, "no_speech_prob": 8.939477993408218e-06}, {"id": 17, "seek": 5192, "start": 71.08, "end": 73.56, "text": " And so they can't extrapolate out to you know", "tokens": [400, 370, 436, 393, 380, 48224, 473, 484, 281, 291, 458], "temperature": 0.0, "avg_logprob": -0.16714643122075679, "compression_ratio": 1.6116071428571428, "no_speech_prob": 8.939477993408218e-06}, {"id": 18, "seek": 5192, "start": 73.56, "end": 76.56, "text": " If you're thinking what happens if I increase my prices by 20%", "tokens": [759, 291, 434, 1953, 437, 2314, 498, 286, 3488, 452, 7901, 538, 945, 4], "temperature": 0.0, "avg_logprob": -0.16714643122075679, "compression_ratio": 1.6116071428571428, "no_speech_prob": 8.939477993408218e-06}, {"id": 19, "seek": 7656, "start": 76.56, "end": 82.56, "text": " And you've never priced at that level before or what's going to happen to sales next year?", "tokens": [400, 291, 600, 1128, 30349, 412, 300, 1496, 949, 420, 437, 311, 516, 281, 1051, 281, 5763, 958, 1064, 30], "temperature": 0.0, "avg_logprob": -0.19518570761078768, "compression_ratio": 1.6853448275862069, "no_speech_prob": 1.9947187865909655e-06}, {"id": 20, "seek": 7656, "start": 82.56, "end": 84.64, "text": " And obviously we've never seen next year before", "tokens": [400, 2745, 321, 600, 1128, 1612, 958, 1064, 949], "temperature": 0.0, "avg_logprob": -0.19518570761078768, "compression_ratio": 1.6853448275862069, "no_speech_prob": 1.9947187865909655e-06}, {"id": 21, "seek": 7656, "start": 85.32000000000001, "end": 93.6, "text": " It's very hard to extrapolate. It's also hard if it needs to you know, like it's it can only do around log base to N", "tokens": [467, 311, 588, 1152, 281, 48224, 473, 13, 467, 311, 611, 1152, 498, 309, 2203, 281, 291, 458, 11, 411, 309, 311, 309, 393, 787, 360, 926, 3565, 3096, 281, 426], "temperature": 0.0, "avg_logprob": -0.19518570761078768, "compression_ratio": 1.6853448275862069, "no_speech_prob": 1.9947187865909655e-06}, {"id": 22, "seek": 7656, "start": 94.64, "end": 96.0, "text": " decisions, you know", "tokens": [5327, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.19518570761078768, "compression_ratio": 1.6853448275862069, "no_speech_prob": 1.9947187865909655e-06}, {"id": 23, "seek": 7656, "start": 96.0, "end": 102.36, "text": " And so if there's like a time series it needs to fit to that takes like four steps to kind of get to the right time", "tokens": [400, 370, 498, 456, 311, 411, 257, 565, 2638, 309, 2203, 281, 3318, 281, 300, 2516, 411, 1451, 4439, 281, 733, 295, 483, 281, 264, 558, 565], "temperature": 0.0, "avg_logprob": -0.19518570761078768, "compression_ratio": 1.6853448275862069, "no_speech_prob": 1.9947187865909655e-06}, {"id": 24, "seek": 10236, "start": 102.36, "end": 106.84, "text": " Area then suddenly there's not many decisions left for it to make so it's kind of this limited", "tokens": [19405, 550, 5800, 456, 311, 406, 867, 5327, 1411, 337, 309, 281, 652, 370, 309, 311, 733, 295, 341, 5567], "temperature": 0.0, "avg_logprob": -0.22977657318115235, "compression_ratio": 1.5701754385964912, "no_speech_prob": 3.340516968819429e-06}, {"id": 25, "seek": 10236, "start": 107.52, "end": 111.16, "text": " Amount of computation that it can do so there's a limited", "tokens": [2012, 792, 295, 24903, 300, 309, 393, 360, 370, 456, 311, 257, 5567], "temperature": 0.0, "avg_logprob": -0.22977657318115235, "compression_ratio": 1.5701754385964912, "no_speech_prob": 3.340516968819429e-06}, {"id": 26, "seek": 10236, "start": 112.12, "end": 114.12, "text": " Complexity of relationship that it can model", "tokens": [41184, 507, 295, 2480, 300, 309, 393, 2316], "temperature": 0.0, "avg_logprob": -0.22977657318115235, "compression_ratio": 1.5701754385964912, "no_speech_prob": 3.340516968819429e-06}, {"id": 27, "seek": 10236, "start": 115.36, "end": 117.36, "text": " Yes Prince", "tokens": [1079, 9821], "temperature": 0.0, "avg_logprob": -0.22977657318115235, "compression_ratio": 1.5701754385964912, "no_speech_prob": 3.340516968819429e-06}, {"id": 28, "seek": 10236, "start": 120.48, "end": 125.72, "text": " Can I ask about one more drawback of random forests that yeah, I feel so", "tokens": [1664, 286, 1029, 466, 472, 544, 2642, 3207, 295, 4974, 21700, 300, 1338, 11, 286, 841, 370], "temperature": 0.0, "avg_logprob": -0.22977657318115235, "compression_ratio": 1.5701754385964912, "no_speech_prob": 3.340516968819429e-06}, {"id": 29, "seek": 10236, "start": 126.28, "end": 131.42000000000002, "text": " If we have a data as categorical variable, which are not in sequential order", "tokens": [759, 321, 362, 257, 1412, 382, 19250, 804, 7006, 11, 597, 366, 406, 294, 42881, 1668], "temperature": 0.0, "avg_logprob": -0.22977657318115235, "compression_ratio": 1.5701754385964912, "no_speech_prob": 3.340516968819429e-06}, {"id": 30, "seek": 13142, "start": 131.42, "end": 135.45999999999998, "text": " So for random forest we encode them and treat them as numbers", "tokens": [407, 337, 4974, 6719, 321, 2058, 1429, 552, 293, 2387, 552, 382, 3547], "temperature": 0.0, "avg_logprob": -0.19543757968478734, "compression_ratio": 1.6748768472906403, "no_speech_prob": 4.859985438088188e-06}, {"id": 31, "seek": 13142, "start": 135.5, "end": 139.29999999999998, "text": " Let's say we have 20 cardinality and 1 to 20", "tokens": [961, 311, 584, 321, 362, 945, 2920, 259, 1860, 293, 502, 281, 945], "temperature": 0.0, "avg_logprob": -0.19543757968478734, "compression_ratio": 1.6748768472906403, "no_speech_prob": 4.859985438088188e-06}, {"id": 32, "seek": 13142, "start": 139.54, "end": 146.94, "text": " So the result that random forest gives is like the split that random forest gives is something like less than 5 less than 6", "tokens": [407, 264, 1874, 300, 4974, 6719, 2709, 307, 411, 264, 7472, 300, 4974, 6719, 2709, 307, 746, 411, 1570, 813, 1025, 1570, 813, 1386], "temperature": 0.0, "avg_logprob": -0.19543757968478734, "compression_ratio": 1.6748768472906403, "no_speech_prob": 4.859985438088188e-06}, {"id": 33, "seek": 13142, "start": 147.22, "end": 150.66, "text": " But if the categories are not sequential not in any order", "tokens": [583, 498, 264, 10479, 366, 406, 42881, 406, 294, 604, 1668], "temperature": 0.0, "avg_logprob": -0.19543757968478734, "compression_ratio": 1.6748768472906403, "no_speech_prob": 4.859985438088188e-06}, {"id": 34, "seek": 13142, "start": 151.7, "end": 153.7, "text": " What does that mean?", "tokens": [708, 775, 300, 914, 30], "temperature": 0.0, "avg_logprob": -0.19543757968478734, "compression_ratio": 1.6748768472906403, "no_speech_prob": 4.859985438088188e-06}, {"id": 35, "seek": 13142, "start": 154.01999999999998, "end": 156.01999999999998, "text": " Yeah, so", "tokens": [865, 11, 370], "temperature": 0.0, "avg_logprob": -0.19543757968478734, "compression_ratio": 1.6748768472906403, "no_speech_prob": 4.859985438088188e-06}, {"id": 36, "seek": 15602, "start": 156.02, "end": 160.3, "text": " So if you've got like oh", "tokens": [407, 498, 291, 600, 658, 411, 1954], "temperature": 0.0, "avg_logprob": -0.547366020896218, "compression_ratio": 1.180327868852459, "no_speech_prob": 3.611957481552963e-06}, {"id": 37, "seek": 15602, "start": 162.14000000000001, "end": 164.98000000000002, "text": " Let's go back to bulldozers erops", "tokens": [961, 311, 352, 646, 281, 4693, 2595, 41698, 1189, 3370], "temperature": 0.0, "avg_logprob": -0.547366020896218, "compression_ratio": 1.180327868852459, "no_speech_prob": 3.611957481552963e-06}, {"id": 38, "seek": 15602, "start": 167.42000000000002, "end": 169.9, "text": " Erops with a", "tokens": [462, 340, 1878, 365, 257], "temperature": 0.0, "avg_logprob": -0.547366020896218, "compression_ratio": 1.180327868852459, "no_speech_prob": 3.611957481552963e-06}, {"id": 39, "seek": 15602, "start": 171.5, "end": 173.5, "text": " C OROPS", "tokens": [383, 422, 7142, 6273], "temperature": 0.0, "avg_logprob": -0.547366020896218, "compression_ratio": 1.180327868852459, "no_speech_prob": 3.611957481552963e-06}, {"id": 40, "seek": 15602, "start": 174.86, "end": 176.86, "text": " And a", "tokens": [400, 257], "temperature": 0.0, "avg_logprob": -0.547366020896218, "compression_ratio": 1.180327868852459, "no_speech_prob": 3.611957481552963e-06}, {"id": 41, "seek": 15602, "start": 178.38, "end": 180.38, "text": " No, whatever right and", "tokens": [883, 11, 2035, 558, 293], "temperature": 0.0, "avg_logprob": -0.547366020896218, "compression_ratio": 1.180327868852459, "no_speech_prob": 3.611957481552963e-06}, {"id": 42, "seek": 18038, "start": 180.38, "end": 185.1, "text": " We arbitrarily label them like this", "tokens": [492, 19071, 3289, 7645, 552, 411, 341], "temperature": 0.0, "avg_logprob": -0.24329540091501156, "compression_ratio": 1.5783783783783785, "no_speech_prob": 2.0904458324366715e-06}, {"id": 43, "seek": 18038, "start": 186.74, "end": 188.14, "text": " All right", "tokens": [1057, 558], "temperature": 0.0, "avg_logprob": -0.24329540091501156, "compression_ratio": 1.5783783783783785, "no_speech_prob": 2.0904458324366715e-06}, {"id": 44, "seek": 18038, "start": 188.14, "end": 193.1, "text": " And so actually we know that all that really mattered was if it had air conditioning", "tokens": [400, 370, 767, 321, 458, 300, 439, 300, 534, 44282, 390, 498, 309, 632, 1988, 21901], "temperature": 0.0, "avg_logprob": -0.24329540091501156, "compression_ratio": 1.5783783783783785, "no_speech_prob": 2.0904458324366715e-06}, {"id": 45, "seek": 18038, "start": 193.42, "end": 199.18, "text": " So what's going to happen? Well, it's basically going to say like okay if I group it into", "tokens": [407, 437, 311, 516, 281, 1051, 30, 1042, 11, 309, 311, 1936, 516, 281, 584, 411, 1392, 498, 286, 1594, 309, 666], "temperature": 0.0, "avg_logprob": -0.24329540091501156, "compression_ratio": 1.5783783783783785, "no_speech_prob": 2.0904458324366715e-06}, {"id": 46, "seek": 18038, "start": 200.1, "end": 205.42, "text": " Those together and those together that like that's an interesting break", "tokens": [3950, 1214, 293, 729, 1214, 300, 411, 300, 311, 364, 1880, 1821], "temperature": 0.0, "avg_logprob": -0.24329540091501156, "compression_ratio": 1.5783783783783785, "no_speech_prob": 2.0904458324366715e-06}, {"id": 47, "seek": 20542, "start": 205.42, "end": 212.42, "text": " Just because it so happens that the air conditioning ones all are going to end up in the right hand side and then having done that", "tokens": [1449, 570, 309, 370, 2314, 300, 264, 1988, 21901, 2306, 439, 366, 516, 281, 917, 493, 294, 264, 558, 1011, 1252, 293, 550, 1419, 1096, 300], "temperature": 0.0, "avg_logprob": -0.15360264551071895, "compression_ratio": 1.875, "no_speech_prob": 1.018807097352692e-06}, {"id": 48, "seek": 20542, "start": 212.85999999999999, "end": 218.14, "text": " Right, it's then going to say okay. Well within the group with the two and three", "tokens": [1779, 11, 309, 311, 550, 516, 281, 584, 1392, 13, 1042, 1951, 264, 1594, 365, 264, 732, 293, 1045], "temperature": 0.0, "avg_logprob": -0.15360264551071895, "compression_ratio": 1.875, "no_speech_prob": 1.018807097352692e-06}, {"id": 49, "seek": 20542, "start": 218.22, "end": 223.33999999999997, "text": " It's going to notice that it's furthermore going to have to split it into two more groups. So eventually it's going to get there", "tokens": [467, 311, 516, 281, 3449, 300, 309, 311, 3052, 3138, 516, 281, 362, 281, 7472, 309, 666, 732, 544, 3935, 13, 407, 4728, 309, 311, 516, 281, 483, 456], "temperature": 0.0, "avg_logprob": -0.15360264551071895, "compression_ratio": 1.875, "no_speech_prob": 1.018807097352692e-06}, {"id": 50, "seek": 20542, "start": 223.94, "end": 230.01999999999998, "text": " It's going to pull out that category. It's just it's going to take more splits than we would ideally like", "tokens": [467, 311, 516, 281, 2235, 484, 300, 7719, 13, 467, 311, 445, 309, 311, 516, 281, 747, 544, 37741, 813, 321, 576, 22915, 411], "temperature": 0.0, "avg_logprob": -0.15360264551071895, "compression_ratio": 1.875, "no_speech_prob": 1.018807097352692e-06}, {"id": 51, "seek": 20542, "start": 230.17999999999998, "end": 234.06, "text": " So it's kind of similar to the fact that for it to model a line", "tokens": [407, 309, 311, 733, 295, 2531, 281, 264, 1186, 300, 337, 309, 281, 2316, 257, 1622], "temperature": 0.0, "avg_logprob": -0.15360264551071895, "compression_ratio": 1.875, "no_speech_prob": 1.018807097352692e-06}, {"id": 52, "seek": 23406, "start": 234.06, "end": 237.74, "text": " It can only do it with lots of spits and only approximately", "tokens": [467, 393, 787, 360, 309, 365, 3195, 295, 637, 1208, 293, 787, 10447], "temperature": 0.0, "avg_logprob": -0.2296020540140443, "compression_ratio": 1.5938566552901023, "no_speech_prob": 3.726612703758292e-06}, {"id": 53, "seek": 23406, "start": 238.78, "end": 243.3, "text": " For is just fine with categories that are not sequential also. Yeah, so I can do it", "tokens": [1171, 307, 445, 2489, 365, 10479, 300, 366, 406, 42881, 611, 13, 865, 11, 370, 286, 393, 360, 309], "temperature": 0.0, "avg_logprob": -0.2296020540140443, "compression_ratio": 1.5938566552901023, "no_speech_prob": 3.726612703758292e-06}, {"id": 54, "seek": 23406, "start": 243.3, "end": 246.72, "text": " It's just like in some way it's suboptimal because we just need to do", "tokens": [467, 311, 445, 411, 294, 512, 636, 309, 311, 1422, 5747, 10650, 570, 321, 445, 643, 281, 360], "temperature": 0.0, "avg_logprob": -0.2296020540140443, "compression_ratio": 1.5938566552901023, "no_speech_prob": 3.726612703758292e-06}, {"id": 55, "seek": 23406, "start": 247.26, "end": 252.98000000000002, "text": " More breakpoints than we would have liked but it gets there. It does a pretty good job. And so even although", "tokens": [5048, 1821, 20552, 813, 321, 576, 362, 4501, 457, 309, 2170, 456, 13, 467, 775, 257, 1238, 665, 1691, 13, 400, 370, 754, 4878], "temperature": 0.0, "avg_logprob": -0.2296020540140443, "compression_ratio": 1.5938566552901023, "no_speech_prob": 3.726612703758292e-06}, {"id": 56, "seek": 23406, "start": 253.66, "end": 256.26, "text": " Random forests, you know do you have some deficiencies?", "tokens": [37603, 21700, 11, 291, 458, 360, 291, 362, 512, 19248, 31294, 30], "temperature": 0.0, "avg_logprob": -0.2296020540140443, "compression_ratio": 1.5938566552901023, "no_speech_prob": 3.726612703758292e-06}, {"id": 57, "seek": 23406, "start": 257.1, "end": 258.98, "text": " They're incredibly", "tokens": [814, 434, 6252], "temperature": 0.0, "avg_logprob": -0.2296020540140443, "compression_ratio": 1.5938566552901023, "no_speech_prob": 3.726612703758292e-06}, {"id": 58, "seek": 23406, "start": 258.98, "end": 262.38, "text": " Powerful, you know, particularly because they have so few assumptions", "tokens": [7086, 906, 11, 291, 458, 11, 4098, 570, 436, 362, 370, 1326, 17695], "temperature": 0.0, "avg_logprob": -0.2296020540140443, "compression_ratio": 1.5938566552901023, "no_speech_prob": 3.726612703758292e-06}, {"id": 59, "seek": 26238, "start": 262.38, "end": 268.42, "text": " They really had to screw up and you know, it's kind of hard to actually win a Kaggle competition with a random forest", "tokens": [814, 534, 632, 281, 5630, 493, 293, 291, 458, 11, 309, 311, 733, 295, 1152, 281, 767, 1942, 257, 48751, 22631, 6211, 365, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.18250488512443774, "compression_ratio": 1.715481171548117, "no_speech_prob": 7.296323019545525e-06}, {"id": 60, "seek": 26238, "start": 269.42, "end": 271.65999999999997, "text": " But it's very easy to get like top 10%", "tokens": [583, 309, 311, 588, 1858, 281, 483, 411, 1192, 1266, 4], "temperature": 0.0, "avg_logprob": -0.18250488512443774, "compression_ratio": 1.715481171548117, "no_speech_prob": 7.296323019545525e-06}, {"id": 61, "seek": 26238, "start": 271.65999999999997, "end": 278.94, "text": " So in like in real life where often that third decimal place doesn't matter random forests are often like what you end up doing", "tokens": [407, 294, 411, 294, 957, 993, 689, 2049, 300, 2636, 26601, 1081, 1177, 380, 1871, 4974, 21700, 366, 2049, 411, 437, 291, 917, 493, 884], "temperature": 0.0, "avg_logprob": -0.18250488512443774, "compression_ratio": 1.715481171548117, "no_speech_prob": 7.296323019545525e-06}, {"id": 62, "seek": 26238, "start": 280.78, "end": 283.44, "text": " But for some things like this Ecuadorian groceries", "tokens": [583, 337, 512, 721, 411, 341, 41558, 952, 31391], "temperature": 0.0, "avg_logprob": -0.18250488512443774, "compression_ratio": 1.715481171548117, "no_speech_prob": 7.296323019545525e-06}, {"id": 63, "seek": 26238, "start": 283.98, "end": 287.78, "text": " Competition, it's very very hard to get a good result with a random forest", "tokens": [43634, 11, 309, 311, 588, 588, 1152, 281, 483, 257, 665, 1874, 365, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.18250488512443774, "compression_ratio": 1.715481171548117, "no_speech_prob": 7.296323019545525e-06}, {"id": 64, "seek": 28778, "start": 287.78, "end": 294.44, "text": " Because like there's a huge time series component and like nearly everything is these two massively high cardinality", "tokens": [1436, 411, 456, 311, 257, 2603, 565, 2638, 6542, 293, 411, 6217, 1203, 307, 613, 732, 29379, 1090, 2920, 259, 1860], "temperature": 0.0, "avg_logprob": -0.17099769828245812, "compression_ratio": 1.7325102880658436, "no_speech_prob": 5.255336873233318e-06}, {"id": 65, "seek": 28778, "start": 294.82, "end": 301.97999999999996, "text": " Categorical variables which is the store and the item and like so this so there's very little there to even throw out a random forest", "tokens": [383, 2968, 284, 804, 9102, 597, 307, 264, 3531, 293, 264, 3174, 293, 411, 370, 341, 370, 456, 311, 588, 707, 456, 281, 754, 3507, 484, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.17099769828245812, "compression_ratio": 1.7325102880658436, "no_speech_prob": 5.255336873233318e-06}, {"id": 66, "seek": 28778, "start": 301.97999999999996, "end": 303.41999999999996, "text": " and the", "tokens": [293, 264], "temperature": 0.0, "avg_logprob": -0.17099769828245812, "compression_ratio": 1.7325102880658436, "no_speech_prob": 5.255336873233318e-06}, {"id": 67, "seek": 28778, "start": 303.41999999999996, "end": 308.73999999999995, "text": " You know the difference between every pair of stores is kind of different in different ways", "tokens": [509, 458, 264, 2649, 1296, 633, 6119, 295, 9512, 307, 733, 295, 819, 294, 819, 2098], "temperature": 0.0, "avg_logprob": -0.17099769828245812, "compression_ratio": 1.7325102880658436, "no_speech_prob": 5.255336873233318e-06}, {"id": 68, "seek": 28778, "start": 308.82, "end": 313.34, "text": " And so, you know, there are some things that are just hard to get even", "tokens": [400, 370, 11, 291, 458, 11, 456, 366, 512, 721, 300, 366, 445, 1152, 281, 483, 754], "temperature": 0.0, "avg_logprob": -0.17099769828245812, "compression_ratio": 1.7325102880658436, "no_speech_prob": 5.255336873233318e-06}, {"id": 69, "seek": 31334, "start": 313.34, "end": 317.85999999999996, "text": " relatively good results with a random forest another example is", "tokens": [7226, 665, 3542, 365, 257, 4974, 6719, 1071, 1365, 307], "temperature": 0.0, "avg_logprob": -0.20711672041151258, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.5294056083803298e-06}, {"id": 70, "seek": 31334, "start": 320.46, "end": 322.21999999999997, "text": " Recognizing numbers", "tokens": [44682, 3319, 3547], "temperature": 0.0, "avg_logprob": -0.20711672041151258, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.5294056083803298e-06}, {"id": 71, "seek": 31334, "start": 322.21999999999997, "end": 325.21999999999997, "text": " You can get like okay results with a random forest", "tokens": [509, 393, 483, 411, 1392, 3542, 365, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.20711672041151258, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.5294056083803298e-06}, {"id": 72, "seek": 31334, "start": 325.21999999999997, "end": 330.97999999999996, "text": " But in the end they're kind of the relationship between you know, like the spatial structure", "tokens": [583, 294, 264, 917, 436, 434, 733, 295, 264, 2480, 1296, 291, 458, 11, 411, 264, 23598, 3877], "temperature": 0.0, "avg_logprob": -0.20711672041151258, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.5294056083803298e-06}, {"id": 73, "seek": 31334, "start": 331.7, "end": 335.7, "text": " Turns out to be important, right and you kind of want to be able to do like", "tokens": [29524, 484, 281, 312, 1021, 11, 558, 293, 291, 733, 295, 528, 281, 312, 1075, 281, 360, 411], "temperature": 0.0, "avg_logprob": -0.20711672041151258, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.5294056083803298e-06}, {"id": 74, "seek": 31334, "start": 336.5, "end": 342.5, "text": " computations like finding edges or whatever that kind of carry forward through through the computations", "tokens": [2807, 763, 411, 5006, 8819, 420, 2035, 300, 733, 295, 3985, 2128, 807, 807, 264, 2807, 763], "temperature": 0.0, "avg_logprob": -0.20711672041151258, "compression_ratio": 1.7543103448275863, "no_speech_prob": 1.5294056083803298e-06}, {"id": 75, "seek": 34250, "start": 342.5, "end": 344.5, "text": " so, you know just doing a", "tokens": [370, 11, 291, 458, 445, 884, 257], "temperature": 0.0, "avg_logprob": -0.13637998492218728, "compression_ratio": 1.75, "no_speech_prob": 1.9947192413383164e-06}, {"id": 76, "seek": 34250, "start": 345.42, "end": 351.1, "text": " Kind of a clever nearest neighbors like a random forest, you know turns out not to be ideal", "tokens": [9242, 295, 257, 13494, 23831, 12512, 411, 257, 4974, 6719, 11, 291, 458, 4523, 484, 406, 281, 312, 7157], "temperature": 0.0, "avg_logprob": -0.13637998492218728, "compression_ratio": 1.75, "no_speech_prob": 1.9947192413383164e-06}, {"id": 77, "seek": 34250, "start": 352.86, "end": 356.38, "text": " So for stuff like this neural networks turn out that they are ideal", "tokens": [407, 337, 1507, 411, 341, 18161, 9590, 1261, 484, 300, 436, 366, 7157], "temperature": 0.0, "avg_logprob": -0.13637998492218728, "compression_ratio": 1.75, "no_speech_prob": 1.9947192413383164e-06}, {"id": 78, "seek": 34250, "start": 356.94, "end": 364.14, "text": " Neural networks turn out to be something that works particularly well for both things like the Ecuadorian groceries competition", "tokens": [1734, 1807, 9590, 1261, 484, 281, 312, 746, 300, 1985, 4098, 731, 337, 1293, 721, 411, 264, 41558, 952, 31391, 6211], "temperature": 0.0, "avg_logprob": -0.13637998492218728, "compression_ratio": 1.75, "no_speech_prob": 1.9947192413383164e-06}, {"id": 79, "seek": 34250, "start": 364.14, "end": 370.14, "text": " So forecasting sales over time by store and by item and for things like", "tokens": [407, 44331, 5763, 670, 565, 538, 3531, 293, 538, 3174, 293, 337, 721, 411], "temperature": 0.0, "avg_logprob": -0.13637998492218728, "compression_ratio": 1.75, "no_speech_prob": 1.9947192413383164e-06}, {"id": 80, "seek": 37014, "start": 370.14, "end": 377.18, "text": " recognizing digits and for things like turning voice into speech and so it's kind of nice between these two things", "tokens": [18538, 27011, 293, 337, 721, 411, 6246, 3177, 666, 6218, 293, 370, 309, 311, 733, 295, 1481, 1296, 613, 732, 721], "temperature": 0.0, "avg_logprob": -0.16302101634373176, "compression_ratio": 1.7265625, "no_speech_prob": 3.393132146811695e-06}, {"id": 81, "seek": 37014, "start": 377.62, "end": 382.14, "text": " Neural nets and random forests we kind of cover the territory, right?", "tokens": [1734, 1807, 36170, 293, 4974, 21700, 321, 733, 295, 2060, 264, 11360, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16302101634373176, "compression_ratio": 1.7265625, "no_speech_prob": 3.393132146811695e-06}, {"id": 82, "seek": 37014, "start": 382.14, "end": 385.34, "text": " I don't I haven't needed to use anything other than these two things", "tokens": [286, 500, 380, 286, 2378, 380, 2978, 281, 764, 1340, 661, 813, 613, 732, 721], "temperature": 0.0, "avg_logprob": -0.16302101634373176, "compression_ratio": 1.7265625, "no_speech_prob": 3.393132146811695e-06}, {"id": 83, "seek": 37014, "start": 385.97999999999996, "end": 387.9, "text": " for a very long time", "tokens": [337, 257, 588, 938, 565], "temperature": 0.0, "avg_logprob": -0.16302101634373176, "compression_ratio": 1.7265625, "no_speech_prob": 3.393132146811695e-06}, {"id": 84, "seek": 37014, "start": 387.9, "end": 389.9, "text": " And we'll actually learn", "tokens": [400, 321, 603, 767, 1466], "temperature": 0.0, "avg_logprob": -0.16302101634373176, "compression_ratio": 1.7265625, "no_speech_prob": 3.393132146811695e-06}, {"id": 85, "seek": 37014, "start": 389.97999999999996, "end": 395.58, "text": " I don't know what course exactly but at some point we'll learn also how to combine the two because you can combine the two in", "tokens": [286, 500, 380, 458, 437, 1164, 2293, 457, 412, 512, 935, 321, 603, 1466, 611, 577, 281, 10432, 264, 732, 570, 291, 393, 10432, 264, 732, 294], "temperature": 0.0, "avg_logprob": -0.16302101634373176, "compression_ratio": 1.7265625, "no_speech_prob": 3.393132146811695e-06}, {"id": 86, "seek": 37014, "start": 395.58, "end": 397.58, "text": " really cool ways", "tokens": [534, 1627, 2098], "temperature": 0.0, "avg_logprob": -0.16302101634373176, "compression_ratio": 1.7265625, "no_speech_prob": 3.393132146811695e-06}, {"id": 87, "seek": 39758, "start": 397.58, "end": 403.65999999999997, "text": " So here's a picture from Adam guide key of an image", "tokens": [407, 510, 311, 257, 3036, 490, 7938, 5934, 2141, 295, 364, 3256], "temperature": 0.0, "avg_logprob": -0.2564844544400874, "compression_ratio": 1.6227272727272728, "no_speech_prob": 1.095282641472295e-05}, {"id": 88, "seek": 39758, "start": 403.65999999999997, "end": 407.26, "text": " so an image is just a bunch of numbers right and", "tokens": [370, 364, 3256, 307, 445, 257, 3840, 295, 3547, 558, 293], "temperature": 0.0, "avg_logprob": -0.2564844544400874, "compression_ratio": 1.6227272727272728, "no_speech_prob": 1.095282641472295e-05}, {"id": 89, "seek": 39758, "start": 407.82, "end": 414.53999999999996, "text": " Each of those numbers is not to 255 and the dark ones are too close to 255 the light ones are close to zero", "tokens": [6947, 295, 729, 3547, 307, 406, 281, 3552, 20, 293, 264, 2877, 2306, 366, 886, 1998, 281, 3552, 20, 264, 1442, 2306, 366, 1998, 281, 4018], "temperature": 0.0, "avg_logprob": -0.2564844544400874, "compression_ratio": 1.6227272727272728, "no_speech_prob": 1.095282641472295e-05}, {"id": 90, "seek": 39758, "start": 414.53999999999996, "end": 422.28, "text": " All right. So here is an example of a digit from this M NIST data set M NIST is a really old", "tokens": [1057, 558, 13, 407, 510, 307, 364, 1365, 295, 257, 14293, 490, 341, 376, 426, 19756, 1412, 992, 376, 426, 19756, 307, 257, 534, 1331], "temperature": 0.0, "avg_logprob": -0.2564844544400874, "compression_ratio": 1.6227272727272728, "no_speech_prob": 1.095282641472295e-05}, {"id": 91, "seek": 39758, "start": 422.28, "end": 425.21999999999997, "text": " It's like the hello world of machine of neural networks", "tokens": [467, 311, 411, 264, 7751, 1002, 295, 3479, 295, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.2564844544400874, "compression_ratio": 1.6227272727272728, "no_speech_prob": 1.095282641472295e-05}, {"id": 92, "seek": 42522, "start": 425.22, "end": 430.28000000000003, "text": " And so here's an example and so there are 28 by 28", "tokens": [400, 370, 510, 311, 364, 1365, 293, 370, 456, 366, 7562, 538, 7562], "temperature": 0.0, "avg_logprob": -0.19861078836831703, "compression_ratio": 1.5380434782608696, "no_speech_prob": 3.966956683143508e-06}, {"id": 93, "seek": 42522, "start": 431.14000000000004, "end": 432.78000000000003, "text": " pixels if it was", "tokens": [18668, 498, 309, 390], "temperature": 0.0, "avg_logprob": -0.19861078836831703, "compression_ratio": 1.5380434782608696, "no_speech_prob": 3.966956683143508e-06}, {"id": 94, "seek": 42522, "start": 432.78000000000003, "end": 436.90000000000003, "text": " Color there would be three of these one for red one for green one for blue", "tokens": [10458, 456, 576, 312, 1045, 295, 613, 472, 337, 2182, 472, 337, 3092, 472, 337, 3344], "temperature": 0.0, "avg_logprob": -0.19861078836831703, "compression_ratio": 1.5380434782608696, "no_speech_prob": 3.966956683143508e-06}, {"id": 95, "seek": 42522, "start": 439.18, "end": 443.62, "text": " So our job is to look at you know the array of numbers and", "tokens": [407, 527, 1691, 307, 281, 574, 412, 291, 458, 264, 10225, 295, 3547, 293], "temperature": 0.0, "avg_logprob": -0.19861078836831703, "compression_ratio": 1.5380434782608696, "no_speech_prob": 3.966956683143508e-06}, {"id": 96, "seek": 42522, "start": 444.42, "end": 446.42, "text": " Figure out that this is the number 8", "tokens": [43225, 484, 300, 341, 307, 264, 1230, 1649], "temperature": 0.0, "avg_logprob": -0.19861078836831703, "compression_ratio": 1.5380434782608696, "no_speech_prob": 3.966956683143508e-06}, {"id": 97, "seek": 42522, "start": 446.74, "end": 449.74, "text": " Which is tricky right? How do we do that?", "tokens": [3013, 307, 12414, 558, 30, 1012, 360, 321, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.19861078836831703, "compression_ratio": 1.5380434782608696, "no_speech_prob": 3.966956683143508e-06}, {"id": 98, "seek": 42522, "start": 451.3, "end": 453.3, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.19861078836831703, "compression_ratio": 1.5380434782608696, "no_speech_prob": 3.966956683143508e-06}, {"id": 99, "seek": 45330, "start": 453.3, "end": 458.02000000000004, "text": " We're going to use a few a small number of fast AI pieces", "tokens": [492, 434, 516, 281, 764, 257, 1326, 257, 1359, 1230, 295, 2370, 7318, 3755], "temperature": 0.0, "avg_logprob": -0.16281441279820033, "compression_ratio": 1.7196652719665273, "no_speech_prob": 3.0894741485099075e-06}, {"id": 100, "seek": 45330, "start": 458.02000000000004, "end": 463.3, "text": " And we're gradually going to remove more and more and more until by the end. We'll have implemented our own", "tokens": [400, 321, 434, 13145, 516, 281, 4159, 544, 293, 544, 293, 544, 1826, 538, 264, 917, 13, 492, 603, 362, 12270, 527, 1065], "temperature": 0.0, "avg_logprob": -0.16281441279820033, "compression_ratio": 1.7196652719665273, "no_speech_prob": 3.0894741485099075e-06}, {"id": 101, "seek": 45330, "start": 463.98, "end": 468.86, "text": " Neural network from stretch our own training loop from scratch and our own matrix multiplication from scratch", "tokens": [1734, 1807, 3209, 490, 5985, 527, 1065, 3097, 6367, 490, 8459, 293, 527, 1065, 8141, 27290, 490, 8459], "temperature": 0.0, "avg_logprob": -0.16281441279820033, "compression_ratio": 1.7196652719665273, "no_speech_prob": 3.0894741485099075e-06}, {"id": 102, "seek": 45330, "start": 469.98, "end": 472.18, "text": " So we're gradually going to dig in further and further", "tokens": [407, 321, 434, 13145, 516, 281, 2528, 294, 3052, 293, 3052], "temperature": 0.0, "avg_logprob": -0.16281441279820033, "compression_ratio": 1.7196652719665273, "no_speech_prob": 3.0894741485099075e-06}, {"id": 103, "seek": 45330, "start": 474.82, "end": 480.1, "text": " All right, so the data for M NIST which is the name of this very famous data set", "tokens": [1057, 558, 11, 370, 264, 1412, 337, 376, 426, 19756, 597, 307, 264, 1315, 295, 341, 588, 4618, 1412, 992], "temperature": 0.0, "avg_logprob": -0.16281441279820033, "compression_ratio": 1.7196652719665273, "no_speech_prob": 3.0894741485099075e-06}, {"id": 104, "seek": 48010, "start": 480.1, "end": 482.1, "text": " Is", "tokens": [1119], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 105, "seek": 48010, "start": 482.14000000000004, "end": 483.82000000000005, "text": " available from here and", "tokens": [2435, 490, 510, 293], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 106, "seek": 48010, "start": 483.82000000000005, "end": 485.82000000000005, "text": " We have a thing in fast AI", "tokens": [492, 362, 257, 551, 294, 2370, 7318], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 107, "seek": 48010, "start": 486.14000000000004, "end": 491.38, "text": " Dot IO called get data which will grab it from a URL and store it from your on your computer", "tokens": [38753, 39839, 1219, 483, 1412, 597, 486, 4444, 309, 490, 257, 12905, 293, 3531, 309, 490, 428, 322, 428, 3820], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 108, "seek": 48010, "start": 491.62, "end": 495.1, "text": " Unless it's already there in which case it'll just go ahead and use it okay, and", "tokens": [16581, 309, 311, 1217, 456, 294, 597, 1389, 309, 603, 445, 352, 2286, 293, 764, 309, 1392, 11, 293], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 109, "seek": 48010, "start": 496.06, "end": 500.82000000000005, "text": " Then we've got a little function here called load M NIST which simply loads it up", "tokens": [1396, 321, 600, 658, 257, 707, 2445, 510, 1219, 3677, 376, 426, 19756, 597, 2935, 12668, 309, 493], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 110, "seek": 48010, "start": 501.70000000000005, "end": 503.34000000000003, "text": " You'll see", "tokens": [509, 603, 536], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 111, "seek": 48010, "start": 503.34000000000003, "end": 507.62, "text": " That it's zipped so we could just use pythons gzip to open it up", "tokens": [663, 309, 311, 710, 5529, 370, 321, 727, 445, 764, 10664, 392, 892, 290, 27268, 281, 1269, 309, 493], "temperature": 0.0, "avg_logprob": -0.24504532070334897, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.747963197994977e-06}, {"id": 112, "seek": 50762, "start": 507.62, "end": 513.46, "text": " And then it's also pickled so if you have any kind of Python object at all", "tokens": [400, 550, 309, 311, 611, 38076, 370, 498, 291, 362, 604, 733, 295, 15329, 2657, 412, 439], "temperature": 0.0, "avg_logprob": -0.15975431295541617, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.443982566546765e-06}, {"id": 113, "seek": 50762, "start": 514.1, "end": 521.3, "text": " You can use this built-in. I've been library called pickle to dump it out onto your disk", "tokens": [509, 393, 764, 341, 3094, 12, 259, 13, 286, 600, 668, 6405, 1219, 31433, 281, 11430, 309, 484, 3911, 428, 12355], "temperature": 0.0, "avg_logprob": -0.15975431295541617, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.443982566546765e-06}, {"id": 114, "seek": 50762, "start": 522.62, "end": 530.54, "text": " Share it around load it up later, and you get back the same Python object you started with so you've already seen this something like this with", "tokens": [14945, 309, 926, 3677, 309, 493, 1780, 11, 293, 291, 483, 646, 264, 912, 15329, 2657, 291, 1409, 365, 370, 291, 600, 1217, 1612, 341, 746, 411, 341, 365], "temperature": 0.0, "avg_logprob": -0.15975431295541617, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.443982566546765e-06}, {"id": 115, "seek": 50762, "start": 530.54, "end": 532.26, "text": " like", "tokens": [411], "temperature": 0.0, "avg_logprob": -0.15975431295541617, "compression_ratio": 1.5294117647058822, "no_speech_prob": 2.443982566546765e-06}, {"id": 116, "seek": 53226, "start": 532.26, "end": 537.86, "text": " Pandas feather format right pickle is not just for pandas", "tokens": [16995, 296, 25852, 7877, 558, 31433, 307, 406, 445, 337, 4565, 296], "temperature": 0.0, "avg_logprob": -0.15046584129333496, "compression_ratio": 1.8608695652173912, "no_speech_prob": 8.579214636483812e-07}, {"id": 117, "seek": 53226, "start": 537.86, "end": 542.1, "text": " It's not just for anything it works for basically nearly every python object", "tokens": [467, 311, 406, 445, 337, 1340, 309, 1985, 337, 1936, 6217, 633, 38797, 2657], "temperature": 0.0, "avg_logprob": -0.15046584129333496, "compression_ratio": 1.8608695652173912, "no_speech_prob": 8.579214636483812e-07}, {"id": 118, "seek": 53226, "start": 542.86, "end": 544.02, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.15046584129333496, "compression_ratio": 1.8608695652173912, "no_speech_prob": 8.579214636483812e-07}, {"id": 119, "seek": 53226, "start": 544.02, "end": 545.54, "text": " Which might lead to the question well?", "tokens": [3013, 1062, 1477, 281, 264, 1168, 731, 30], "temperature": 0.0, "avg_logprob": -0.15046584129333496, "compression_ratio": 1.8608695652173912, "no_speech_prob": 8.579214636483812e-07}, {"id": 120, "seek": 53226, "start": 545.54, "end": 553.4, "text": " Why didn't we use pickle for a pandas data frame right and the answer is pickle works for nearly every python object?", "tokens": [1545, 994, 380, 321, 764, 31433, 337, 257, 4565, 296, 1412, 3920, 558, 293, 264, 1867, 307, 31433, 1985, 337, 6217, 633, 38797, 2657, 30], "temperature": 0.0, "avg_logprob": -0.15046584129333496, "compression_ratio": 1.8608695652173912, "no_speech_prob": 8.579214636483812e-07}, {"id": 121, "seek": 53226, "start": 553.74, "end": 561.9399999999999, "text": " But it's probably not like optimal for nearly any python object right so because like we were looking at pandas data frames with like", "tokens": [583, 309, 311, 1391, 406, 411, 16252, 337, 6217, 604, 38797, 2657, 558, 370, 570, 411, 321, 645, 1237, 412, 4565, 296, 1412, 12083, 365, 411], "temperature": 0.0, "avg_logprob": -0.15046584129333496, "compression_ratio": 1.8608695652173912, "no_speech_prob": 8.579214636483812e-07}, {"id": 122, "seek": 56194, "start": 561.94, "end": 563.94, "text": " over a hundred million rows", "tokens": [670, 257, 3262, 2459, 13241], "temperature": 0.0, "avg_logprob": -0.18429998115256982, "compression_ratio": 1.6840148698884758, "no_speech_prob": 1.1544601647983654e-06}, {"id": 123, "seek": 56194, "start": 563.94, "end": 568.74, "text": " We really want to save that quickly and so feather is a format that's specifically designed", "tokens": [492, 534, 528, 281, 3155, 300, 2661, 293, 370, 25852, 307, 257, 7877, 300, 311, 4682, 4761], "temperature": 0.0, "avg_logprob": -0.18429998115256982, "compression_ratio": 1.6840148698884758, "no_speech_prob": 1.1544601647983654e-06}, {"id": 124, "seek": 56194, "start": 569.0600000000001, "end": 574.6600000000001, "text": " For that purpose and so it's going to do that really fast if we try to pickle it it would have been taken a lot", "tokens": [1171, 300, 4334, 293, 370, 309, 311, 516, 281, 360, 300, 534, 2370, 498, 321, 853, 281, 31433, 309, 309, 576, 362, 668, 2726, 257, 688], "temperature": 0.0, "avg_logprob": -0.18429998115256982, "compression_ratio": 1.6840148698884758, "no_speech_prob": 1.1544601647983654e-06}, {"id": 125, "seek": 56194, "start": 574.6600000000001, "end": 576.6600000000001, "text": " longer right", "tokens": [2854, 558], "temperature": 0.0, "avg_logprob": -0.18429998115256982, "compression_ratio": 1.6840148698884758, "no_speech_prob": 1.1544601647983654e-06}, {"id": 126, "seek": 56194, "start": 577.86, "end": 579.86, "text": " Also note that pickle files", "tokens": [2743, 3637, 300, 31433, 7098], "temperature": 0.0, "avg_logprob": -0.18429998115256982, "compression_ratio": 1.6840148698884758, "no_speech_prob": 1.1544601647983654e-06}, {"id": 127, "seek": 56194, "start": 580.62, "end": 586.7800000000001, "text": " Only for python so you can't give them to somebody else where else like a feather file you can hand around", "tokens": [5686, 337, 38797, 370, 291, 393, 380, 976, 552, 281, 2618, 1646, 689, 1646, 411, 257, 25852, 3991, 291, 393, 1011, 926], "temperature": 0.0, "avg_logprob": -0.18429998115256982, "compression_ratio": 1.6840148698884758, "no_speech_prob": 1.1544601647983654e-06}, {"id": 128, "seek": 58678, "start": 586.78, "end": 591.3399999999999, "text": " Okay, so it's worth knowing that pickle exists because if you've got some", "tokens": [1033, 11, 370, 309, 311, 3163, 5276, 300, 31433, 8198, 570, 498, 291, 600, 658, 512], "temperature": 0.0, "avg_logprob": -0.1969699637834416, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.4465583667042665e-06}, {"id": 129, "seek": 58678, "start": 592.14, "end": 599.6999999999999, "text": " Dictionary or some kind of object floating around that you want to save for later or send to somebody else you can always just", "tokens": [413, 4105, 822, 420, 512, 733, 295, 2657, 12607, 926, 300, 291, 528, 281, 3155, 337, 1780, 420, 2845, 281, 2618, 1646, 291, 393, 1009, 445], "temperature": 0.0, "avg_logprob": -0.1969699637834416, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.4465583667042665e-06}, {"id": 130, "seek": 58678, "start": 600.4599999999999, "end": 608.26, "text": " Pickle it okay, so in this particular case the folks at deep learning net were kind enough to provide a pickled version", "tokens": [14129, 306, 309, 1392, 11, 370, 294, 341, 1729, 1389, 264, 4024, 412, 2452, 2539, 2533, 645, 733, 1547, 281, 2893, 257, 38076, 3037], "temperature": 0.0, "avg_logprob": -0.1969699637834416, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.4465583667042665e-06}, {"id": 131, "seek": 58678, "start": 610.9, "end": 612.54, "text": " Pickle has", "tokens": [14129, 306, 575], "temperature": 0.0, "avg_logprob": -0.1969699637834416, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.4465583667042665e-06}, {"id": 132, "seek": 58678, "start": 612.54, "end": 615.06, "text": " Changed slightly over time", "tokens": [761, 10296, 4748, 670, 565], "temperature": 0.0, "avg_logprob": -0.1969699637834416, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.4465583667042665e-06}, {"id": 133, "seek": 61506, "start": 615.06, "end": 620.4599999999999, "text": " And so old pickle files like this one you actually have to this is a python 2 one", "tokens": [400, 370, 1331, 31433, 7098, 411, 341, 472, 291, 767, 362, 281, 341, 307, 257, 38797, 568, 472], "temperature": 0.0, "avg_logprob": -0.20390113898083173, "compression_ratio": 1.7588932806324111, "no_speech_prob": 2.0580412183335284e-06}, {"id": 134, "seek": 61506, "start": 620.4599999999999, "end": 623.38, "text": " So you have to tell it that it was encoded using this particular", "tokens": [407, 291, 362, 281, 980, 309, 300, 309, 390, 2058, 12340, 1228, 341, 1729], "temperature": 0.0, "avg_logprob": -0.20390113898083173, "compression_ratio": 1.7588932806324111, "no_speech_prob": 2.0580412183335284e-06}, {"id": 135, "seek": 61506, "start": 623.9799999999999, "end": 631.14, "text": " Python 2 character set but other than that python 2 and 3 you can normally open each other's pickle files", "tokens": [15329, 568, 2517, 992, 457, 661, 813, 300, 38797, 568, 293, 805, 291, 393, 5646, 1269, 1184, 661, 311, 31433, 7098], "temperature": 0.0, "avg_logprob": -0.20390113898083173, "compression_ratio": 1.7588932806324111, "no_speech_prob": 2.0580412183335284e-06}, {"id": 136, "seek": 61506, "start": 631.5, "end": 633.5, "text": " Alright, so once we've loaded that in", "tokens": [2798, 11, 370, 1564, 321, 600, 13210, 300, 294], "temperature": 0.0, "avg_logprob": -0.20390113898083173, "compression_ratio": 1.7588932806324111, "no_speech_prob": 2.0580412183335284e-06}, {"id": 137, "seek": 61506, "start": 635.42, "end": 638.6999999999999, "text": " We load it in like so and so this thing which we're doing here", "tokens": [492, 3677, 309, 294, 411, 370, 293, 370, 341, 551, 597, 321, 434, 884, 510], "temperature": 0.0, "avg_logprob": -0.20390113898083173, "compression_ratio": 1.7588932806324111, "no_speech_prob": 2.0580412183335284e-06}, {"id": 138, "seek": 61506, "start": 638.6999999999999, "end": 643.78, "text": " This is called destructuring and so destructuring means that load MNIST is giving us back a", "tokens": [639, 307, 1219, 2677, 1757, 1345, 293, 370, 2677, 1757, 1345, 1355, 300, 3677, 376, 45, 19756, 307, 2902, 505, 646, 257], "temperature": 0.0, "avg_logprob": -0.20390113898083173, "compression_ratio": 1.7588932806324111, "no_speech_prob": 2.0580412183335284e-06}, {"id": 139, "seek": 64378, "start": 643.78, "end": 645.78, "text": " tuple of tuples and", "tokens": [2604, 781, 295, 2604, 2622, 293], "temperature": 0.0, "avg_logprob": -0.15400796135266623, "compression_ratio": 1.7222222222222223, "no_speech_prob": 2.9023019578744425e-06}, {"id": 140, "seek": 64378, "start": 646.5, "end": 652.42, "text": " So if we have on the left hand side of the equal sign a tuple of tuples we can fill all these things in", "tokens": [407, 498, 321, 362, 322, 264, 1411, 1011, 1252, 295, 264, 2681, 1465, 257, 2604, 781, 295, 2604, 2622, 321, 393, 2836, 439, 613, 721, 294], "temperature": 0.0, "avg_logprob": -0.15400796135266623, "compression_ratio": 1.7222222222222223, "no_speech_prob": 2.9023019578744425e-06}, {"id": 141, "seek": 64378, "start": 652.62, "end": 659.3399999999999, "text": " So we're given back a tuple of training data a tuple of validation data and a tuple of test data", "tokens": [407, 321, 434, 2212, 646, 257, 2604, 781, 295, 3097, 1412, 257, 2604, 781, 295, 24071, 1412, 293, 257, 2604, 781, 295, 1500, 1412], "temperature": 0.0, "avg_logprob": -0.15400796135266623, "compression_ratio": 1.7222222222222223, "no_speech_prob": 2.9023019578744425e-06}, {"id": 142, "seek": 64378, "start": 659.8199999999999, "end": 665.5, "text": " In this case, I don't care about the test data. So I just put it into a variable called underscore which", "tokens": [682, 341, 1389, 11, 286, 500, 380, 1127, 466, 264, 1500, 1412, 13, 407, 286, 445, 829, 309, 666, 257, 7006, 1219, 37556, 597], "temperature": 0.0, "avg_logprob": -0.15400796135266623, "compression_ratio": 1.7222222222222223, "no_speech_prob": 2.9023019578744425e-06}, {"id": 143, "seek": 64378, "start": 666.66, "end": 668.66, "text": " kind of by like", "tokens": [733, 295, 538, 411], "temperature": 0.0, "avg_logprob": -0.15400796135266623, "compression_ratio": 1.7222222222222223, "no_speech_prob": 2.9023019578744425e-06}, {"id": 144, "seek": 66866, "start": 668.66, "end": 673.4599999999999, "text": " People and pick Python people tend to think of underscore as being a special variable", "tokens": [3432, 293, 1888, 15329, 561, 3928, 281, 519, 295, 37556, 382, 885, 257, 2121, 7006], "temperature": 0.0, "avg_logprob": -0.16690044574909382, "compression_ratio": 1.740072202166065, "no_speech_prob": 3.089479605478118e-06}, {"id": 145, "seek": 66866, "start": 673.5799999999999, "end": 677.38, "text": " Which we put things we're going to throw away into it's actually not special", "tokens": [3013, 321, 829, 721, 321, 434, 516, 281, 3507, 1314, 666, 309, 311, 767, 406, 2121], "temperature": 0.0, "avg_logprob": -0.16690044574909382, "compression_ratio": 1.740072202166065, "no_speech_prob": 3.089479605478118e-06}, {"id": 146, "seek": 66866, "start": 677.86, "end": 683.5, "text": " But it's just it's really common if you see something assigned to underscore it probably means you're just throwing it away, right?", "tokens": [583, 309, 311, 445, 309, 311, 534, 2689, 498, 291, 536, 746, 13279, 281, 37556, 309, 1391, 1355, 291, 434, 445, 10238, 309, 1314, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16690044574909382, "compression_ratio": 1.740072202166065, "no_speech_prob": 3.089479605478118e-06}, {"id": 147, "seek": 66866, "start": 684.66, "end": 689.14, "text": " By the way in a jupyter notebook it does have a special meaning which is the last cell", "tokens": [3146, 264, 636, 294, 257, 361, 1010, 88, 391, 21060, 309, 775, 362, 257, 2121, 3620, 597, 307, 264, 1036, 2815], "temperature": 0.0, "avg_logprob": -0.16690044574909382, "compression_ratio": 1.740072202166065, "no_speech_prob": 3.089479605478118e-06}, {"id": 148, "seek": 66866, "start": 689.4599999999999, "end": 694.9, "text": " That you calculate is always available in underscore by the way, but that's kind of a separate issue", "tokens": [663, 291, 8873, 307, 1009, 2435, 294, 37556, 538, 264, 636, 11, 457, 300, 311, 733, 295, 257, 4994, 2734], "temperature": 0.0, "avg_logprob": -0.16690044574909382, "compression_ratio": 1.740072202166065, "no_speech_prob": 3.089479605478118e-06}, {"id": 149, "seek": 69490, "start": 694.9, "end": 702.26, "text": " So then the first thing in that tuple is itself a tuple and so we're going to stick that into", "tokens": [407, 550, 264, 700, 551, 294, 300, 2604, 781, 307, 2564, 257, 2604, 781, 293, 370, 321, 434, 516, 281, 2897, 300, 666], "temperature": 0.0, "avg_logprob": -0.1653853594246557, "compression_ratio": 1.7099236641221374, "no_speech_prob": 3.7266252093104413e-06}, {"id": 150, "seek": 69490, "start": 702.5799999999999, "end": 708.78, "text": " X and Y for our training data and then the second one goes into X and Y for our validation data", "tokens": [1783, 293, 398, 337, 527, 3097, 1412, 293, 550, 264, 1150, 472, 1709, 666, 1783, 293, 398, 337, 527, 24071, 1412], "temperature": 0.0, "avg_logprob": -0.1653853594246557, "compression_ratio": 1.7099236641221374, "no_speech_prob": 3.7266252093104413e-06}, {"id": 151, "seek": 69490, "start": 709.9, "end": 714.22, "text": " Okay, so that's called destructuring and it's pretty common in lots of languages", "tokens": [1033, 11, 370, 300, 311, 1219, 2677, 1757, 1345, 293, 309, 311, 1238, 2689, 294, 3195, 295, 8650], "temperature": 0.0, "avg_logprob": -0.1653853594246557, "compression_ratio": 1.7099236641221374, "no_speech_prob": 3.7266252093104413e-06}, {"id": 152, "seek": 69490, "start": 715.14, "end": 718.62, "text": " Some languages don't support it, but those that do life becomes a lot easier", "tokens": [2188, 8650, 500, 380, 1406, 309, 11, 457, 729, 300, 360, 993, 3643, 257, 688, 3571], "temperature": 0.0, "avg_logprob": -0.1653853594246557, "compression_ratio": 1.7099236641221374, "no_speech_prob": 3.7266252093104413e-06}, {"id": 153, "seek": 69490, "start": 719.26, "end": 724.34, "text": " So as soon as I you know, look at some new data set, I just check out what's what have I got, right?", "tokens": [407, 382, 2321, 382, 286, 291, 458, 11, 574, 412, 512, 777, 1412, 992, 11, 286, 445, 1520, 484, 437, 311, 437, 362, 286, 658, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1653853594246557, "compression_ratio": 1.7099236641221374, "no_speech_prob": 3.7266252093104413e-06}, {"id": 154, "seek": 72434, "start": 724.34, "end": 726.86, "text": " So what's its type? Okay. It's a numpy array", "tokens": [407, 437, 311, 1080, 2010, 30, 1033, 13, 467, 311, 257, 1031, 8200, 10225], "temperature": 0.0, "avg_logprob": -0.19222285083888732, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.041586751351133e-06}, {"id": 155, "seek": 72434, "start": 727.82, "end": 729.46, "text": " What's its shape?", "tokens": [708, 311, 1080, 3909, 30], "temperature": 0.0, "avg_logprob": -0.19222285083888732, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.041586751351133e-06}, {"id": 156, "seek": 72434, "start": 729.46, "end": 735.74, "text": " It's 50,000 by 7 8 4 and then what about the dependent variables? That's an array its shape is", "tokens": [467, 311, 2625, 11, 1360, 538, 1614, 1649, 1017, 293, 550, 437, 466, 264, 12334, 9102, 30, 663, 311, 364, 10225, 1080, 3909, 307], "temperature": 0.0, "avg_logprob": -0.19222285083888732, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.041586751351133e-06}, {"id": 157, "seek": 72434, "start": 736.38, "end": 738.0600000000001, "text": " 50,000", "tokens": [2625, 11, 1360], "temperature": 0.0, "avg_logprob": -0.19222285083888732, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.041586751351133e-06}, {"id": 158, "seek": 72434, "start": 738.0600000000001, "end": 740.0600000000001, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.19222285083888732, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.041586751351133e-06}, {"id": 159, "seek": 72434, "start": 740.34, "end": 747.44, "text": " This image is not of length 7 8 4. It's of size 28 by 28", "tokens": [639, 3256, 307, 406, 295, 4641, 1614, 1649, 1017, 13, 467, 311, 295, 2744, 7562, 538, 7562], "temperature": 0.0, "avg_logprob": -0.19222285083888732, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.041586751351133e-06}, {"id": 160, "seek": 72434, "start": 747.94, "end": 752.86, "text": " So what happened here? Well, we could guess and we can check on the website", "tokens": [407, 437, 2011, 510, 30, 1042, 11, 321, 727, 2041, 293, 321, 393, 1520, 322, 264, 3144], "temperature": 0.0, "avg_logprob": -0.19222285083888732, "compression_ratio": 1.5228426395939085, "no_speech_prob": 3.041586751351133e-06}, {"id": 161, "seek": 75286, "start": 752.86, "end": 759.0, "text": " It turns out we would be right that all they did was they took the second row and concatenated to the first row and the third", "tokens": [467, 4523, 484, 321, 576, 312, 558, 300, 439, 436, 630, 390, 436, 1890, 264, 1150, 5386, 293, 1588, 7186, 770, 281, 264, 700, 5386, 293, 264, 2636], "temperature": 0.0, "avg_logprob": -0.14791276886707216, "compression_ratio": 1.811808118081181, "no_speech_prob": 1.7880586256069364e-06}, {"id": 162, "seek": 75286, "start": 759.0, "end": 761.82, "text": " Row and concatenated to that and the fourth row and concatenated to that", "tokens": [20309, 293, 1588, 7186, 770, 281, 300, 293, 264, 6409, 5386, 293, 1588, 7186, 770, 281, 300], "temperature": 0.0, "avg_logprob": -0.14791276886707216, "compression_ratio": 1.811808118081181, "no_speech_prob": 1.7880586256069364e-06}, {"id": 163, "seek": 75286, "start": 761.82, "end": 768.1800000000001, "text": " So in other words, they took this whole 28 by 28 and flattened it out into a single 1d array", "tokens": [407, 294, 661, 2283, 11, 436, 1890, 341, 1379, 7562, 538, 7562, 293, 24183, 292, 309, 484, 666, 257, 2167, 502, 67, 10225], "temperature": 0.0, "avg_logprob": -0.14791276886707216, "compression_ratio": 1.811808118081181, "no_speech_prob": 1.7880586256069364e-06}, {"id": 164, "seek": 75286, "start": 768.58, "end": 770.9, "text": " That makes sense. So it's going to be of size", "tokens": [663, 1669, 2020, 13, 407, 309, 311, 516, 281, 312, 295, 2744], "temperature": 0.0, "avg_logprob": -0.14791276886707216, "compression_ratio": 1.811808118081181, "no_speech_prob": 1.7880586256069364e-06}, {"id": 165, "seek": 75286, "start": 771.58, "end": 773.58, "text": " 28 squared", "tokens": [7562, 8889], "temperature": 0.0, "avg_logprob": -0.14791276886707216, "compression_ratio": 1.811808118081181, "no_speech_prob": 1.7880586256069364e-06}, {"id": 166, "seek": 75286, "start": 774.1800000000001, "end": 777.38, "text": " This is not like normal by any means", "tokens": [639, 307, 406, 411, 2710, 538, 604, 1355], "temperature": 0.0, "avg_logprob": -0.14791276886707216, "compression_ratio": 1.811808118081181, "no_speech_prob": 1.7880586256069364e-06}, {"id": 167, "seek": 77738, "start": 777.38, "end": 782.86, "text": " So don't think like everything you see is going to be like this most of the time when people share images", "tokens": [407, 500, 380, 519, 411, 1203, 291, 536, 307, 516, 281, 312, 411, 341, 881, 295, 264, 565, 562, 561, 2073, 5267], "temperature": 0.0, "avg_logprob": -0.16925137093726625, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.6280454246953013e-06}, {"id": 168, "seek": 77738, "start": 783.02, "end": 788.9399999999999, "text": " They share them as JPEGs or PNGs you load them up you get back a nice 2d array", "tokens": [814, 2073, 552, 382, 508, 5208, 33715, 420, 430, 30237, 82, 291, 3677, 552, 493, 291, 483, 646, 257, 1481, 568, 67, 10225], "temperature": 0.0, "avg_logprob": -0.16925137093726625, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.6280454246953013e-06}, {"id": 169, "seek": 77738, "start": 789.9399999999999, "end": 796.26, "text": " But in this particular case for whatever reason the thing that they pickled was flattened out to be", "tokens": [583, 294, 341, 1729, 1389, 337, 2035, 1778, 264, 551, 300, 436, 38076, 390, 24183, 292, 484, 281, 312], "temperature": 0.0, "avg_logprob": -0.16925137093726625, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.6280454246953013e-06}, {"id": 170, "seek": 77738, "start": 797.18, "end": 801.64, "text": " 784 and this word flatten is very common with", "tokens": [1614, 25494, 293, 341, 1349, 24183, 307, 588, 2689, 365], "temperature": 0.0, "avg_logprob": -0.16925137093726625, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.6280454246953013e-06}, {"id": 171, "seek": 77738, "start": 802.54, "end": 804.38, "text": " You know kind of working with tensors", "tokens": [509, 458, 733, 295, 1364, 365, 10688, 830], "temperature": 0.0, "avg_logprob": -0.16925137093726625, "compression_ratio": 1.5593220338983051, "no_speech_prob": 1.6280454246953013e-06}, {"id": 172, "seek": 80438, "start": 804.38, "end": 808.26, "text": " so when you flatten the tensor it just means that you're turning it into a", "tokens": [370, 562, 291, 24183, 264, 40863, 309, 445, 1355, 300, 291, 434, 6246, 309, 666, 257], "temperature": 0.0, "avg_logprob": -0.2183917260939075, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.118938939725922e-06}, {"id": 173, "seek": 80438, "start": 809.34, "end": 815.18, "text": " Lower rank tensor than you started with in this case. We started with a rank 2 tensor and matrix", "tokens": [25523, 6181, 40863, 813, 291, 1409, 365, 294, 341, 1389, 13, 492, 1409, 365, 257, 6181, 568, 40863, 293, 8141], "temperature": 0.0, "avg_logprob": -0.2183917260939075, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.118938939725922e-06}, {"id": 174, "seek": 80438, "start": 815.7, "end": 821.26, "text": " For each image and we turned each one into a rank 1 tensor ie a vector", "tokens": [1171, 1184, 3256, 293, 321, 3574, 1184, 472, 666, 257, 6181, 502, 40863, 43203, 257, 8062], "temperature": 0.0, "avg_logprob": -0.2183917260939075, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.118938939725922e-06}, {"id": 175, "seek": 80438, "start": 821.26, "end": 826.42, "text": " So overall the whole thing, you know is a rank 2 matrix", "tokens": [407, 4787, 264, 1379, 551, 11, 291, 458, 307, 257, 6181, 568, 8141], "temperature": 0.0, "avg_logprob": -0.2183917260939075, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.118938939725922e-06}, {"id": 176, "seek": 80438, "start": 826.98, "end": 831.54, "text": " Rank 2 tensor rather than a rank 3 tensor. So just to remind us of", "tokens": [35921, 568, 40863, 2831, 813, 257, 6181, 805, 40863, 13, 407, 445, 281, 4160, 505, 295], "temperature": 0.0, "avg_logprob": -0.2183917260939075, "compression_ratio": 1.7718446601941749, "no_speech_prob": 1.118938939725922e-06}, {"id": 177, "seek": 83154, "start": 831.54, "end": 834.18, "text": " You know the jargon here", "tokens": [509, 458, 264, 15181, 10660, 510], "temperature": 0.0, "avg_logprob": -0.25634447220833073, "compression_ratio": 1.471698113207547, "no_speech_prob": 9.570760539645562e-07}, {"id": 178, "seek": 83154, "start": 837.9, "end": 839.9, "text": " This", "tokens": [639], "temperature": 0.0, "avg_logprob": -0.25634447220833073, "compression_ratio": 1.471698113207547, "no_speech_prob": 9.570760539645562e-07}, {"id": 179, "seek": 83154, "start": 840.06, "end": 842.42, "text": " In math we would call a vector", "tokens": [682, 5221, 321, 576, 818, 257, 8062], "temperature": 0.0, "avg_logprob": -0.25634447220833073, "compression_ratio": 1.471698113207547, "no_speech_prob": 9.570760539645562e-07}, {"id": 180, "seek": 83154, "start": 843.74, "end": 848.0, "text": " Right in computer science, we would call it a 1d array", "tokens": [1779, 294, 3820, 3497, 11, 321, 576, 818, 309, 257, 502, 67, 10225], "temperature": 0.0, "avg_logprob": -0.25634447220833073, "compression_ratio": 1.471698113207547, "no_speech_prob": 9.570760539645562e-07}, {"id": 181, "seek": 83154, "start": 850.26, "end": 858.26, "text": " But because deep learning have people have to come across as smarter than everybody else we have to call this a rank 1", "tokens": [583, 570, 2452, 2539, 362, 561, 362, 281, 808, 2108, 382, 20294, 813, 2201, 1646, 321, 362, 281, 818, 341, 257, 6181, 502], "temperature": 0.0, "avg_logprob": -0.25634447220833073, "compression_ratio": 1.471698113207547, "no_speech_prob": 9.570760539645562e-07}, {"id": 182, "seek": 85826, "start": 858.26, "end": 863.36, "text": " 1 tensor okay, they all mean the same thing more or less", "tokens": [502, 40863, 1392, 11, 436, 439, 914, 264, 912, 551, 544, 420, 1570], "temperature": 0.0, "avg_logprob": -0.23810340033637153, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.6425684609421296e-06}, {"id": 183, "seek": 85826, "start": 863.8199999999999, "end": 869.2, "text": " Unless you're a physicist in which case this means something else and you get very angry at the deep learning people because you say it's", "tokens": [16581, 291, 434, 257, 42466, 294, 597, 1389, 341, 1355, 746, 1646, 293, 291, 483, 588, 6884, 412, 264, 2452, 2539, 561, 570, 291, 584, 309, 311], "temperature": 0.0, "avg_logprob": -0.23810340033637153, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.6425684609421296e-06}, {"id": 184, "seek": 85826, "start": 869.2, "end": 870.66, "text": " Not a tensor", "tokens": [1726, 257, 40863], "temperature": 0.0, "avg_logprob": -0.23810340033637153, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.6425684609421296e-06}, {"id": 185, "seek": 85826, "start": 870.66, "end": 874.38, "text": " So there you go. Don't blame me. This is just what people say", "tokens": [407, 456, 291, 352, 13, 1468, 380, 10127, 385, 13, 639, 307, 445, 437, 561, 584], "temperature": 0.0, "avg_logprob": -0.23810340033637153, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.6425684609421296e-06}, {"id": 186, "seek": 85826, "start": 875.34, "end": 877.34, "text": " so this is", "tokens": [370, 341, 307], "temperature": 0.0, "avg_logprob": -0.23810340033637153, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.6425684609421296e-06}, {"id": 187, "seek": 85826, "start": 878.1, "end": 882.58, "text": " Either a matrix or a 2d array or", "tokens": [13746, 257, 8141, 420, 257, 568, 67, 10225, 420], "temperature": 0.0, "avg_logprob": -0.23810340033637153, "compression_ratio": 1.5666666666666667, "no_speech_prob": 2.6425684609421296e-06}, {"id": 188, "seek": 88258, "start": 882.58, "end": 887.82, "text": " a rank 2 tensor and", "tokens": [257, 6181, 568, 40863, 293], "temperature": 0.0, "avg_logprob": -0.1617859398446432, "compression_ratio": 1.66, "no_speech_prob": 7.18321098247543e-06}, {"id": 189, "seek": 88258, "start": 888.34, "end": 892.1800000000001, "text": " So once we start to get into three dimensions, we start to run out of mathematical names", "tokens": [407, 1564, 321, 722, 281, 483, 666, 1045, 12819, 11, 321, 722, 281, 1190, 484, 295, 18894, 5288], "temperature": 0.0, "avg_logprob": -0.1617859398446432, "compression_ratio": 1.66, "no_speech_prob": 7.18321098247543e-06}, {"id": 190, "seek": 88258, "start": 894.26, "end": 897.98, "text": " Right, which is why we start to be nice just to say rank 3 tensor", "tokens": [1779, 11, 597, 307, 983, 321, 722, 281, 312, 1481, 445, 281, 584, 6181, 805, 40863], "temperature": 0.0, "avg_logprob": -0.1617859398446432, "compression_ratio": 1.66, "no_speech_prob": 7.18321098247543e-06}, {"id": 191, "seek": 88258, "start": 897.98, "end": 904.6600000000001, "text": " And so there's actually nothing special about vectors and matrices that make them in any way more important", "tokens": [400, 370, 456, 311, 767, 1825, 2121, 466, 18875, 293, 32284, 300, 652, 552, 294, 604, 636, 544, 1021], "temperature": 0.0, "avg_logprob": -0.1617859398446432, "compression_ratio": 1.66, "no_speech_prob": 7.18321098247543e-06}, {"id": 192, "seek": 88258, "start": 905.4200000000001, "end": 908.2, "text": " Than rank 3 tensors or rank 4 tensors or whatever", "tokens": [18289, 6181, 805, 10688, 830, 420, 6181, 1017, 10688, 830, 420, 2035], "temperature": 0.0, "avg_logprob": -0.1617859398446432, "compression_ratio": 1.66, "no_speech_prob": 7.18321098247543e-06}, {"id": 193, "seek": 90820, "start": 908.2, "end": 912.9000000000001, "text": " So I try not to use the terms vector and matrix where possible", "tokens": [407, 286, 853, 406, 281, 764, 264, 2115, 8062, 293, 8141, 689, 1944], "temperature": 0.0, "avg_logprob": -0.15111891428629556, "compression_ratio": 1.5027624309392265, "no_speech_prob": 4.965268090018071e-07}, {"id": 194, "seek": 90820, "start": 913.9000000000001, "end": 919.1, "text": " Because I don't really think they're they're any more special than any other rank of tensor", "tokens": [1436, 286, 500, 380, 534, 519, 436, 434, 436, 434, 604, 544, 2121, 813, 604, 661, 6181, 295, 40863], "temperature": 0.0, "avg_logprob": -0.15111891428629556, "compression_ratio": 1.5027624309392265, "no_speech_prob": 4.965268090018071e-07}, {"id": 195, "seek": 90820, "start": 919.1800000000001, "end": 924.6600000000001, "text": " Okay, so kind of it's good to get used to thinking of this as a rank 2 tensor", "tokens": [1033, 11, 370, 733, 295, 309, 311, 665, 281, 483, 1143, 281, 1953, 295, 341, 382, 257, 6181, 568, 40863], "temperature": 0.0, "avg_logprob": -0.15111891428629556, "compression_ratio": 1.5027624309392265, "no_speech_prob": 4.965268090018071e-07}, {"id": 196, "seek": 90820, "start": 925.1, "end": 927.1, "text": " Okay, and then", "tokens": [1033, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.15111891428629556, "compression_ratio": 1.5027624309392265, "no_speech_prob": 4.965268090018071e-07}, {"id": 197, "seek": 90820, "start": 927.98, "end": 929.98, "text": " the", "tokens": [264], "temperature": 0.0, "avg_logprob": -0.15111891428629556, "compression_ratio": 1.5027624309392265, "no_speech_prob": 4.965268090018071e-07}, {"id": 198, "seek": 90820, "start": 931.94, "end": 933.94, "text": " The rows and", "tokens": [440, 13241, 293], "temperature": 0.0, "avg_logprob": -0.15111891428629556, "compression_ratio": 1.5027624309392265, "no_speech_prob": 4.965268090018071e-07}, {"id": 199, "seek": 93394, "start": 933.94, "end": 936.4200000000001, "text": " Columns", "tokens": [4004, 449, 3695], "temperature": 0.0, "avg_logprob": -0.22588786794178523, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.3496978681359906e-06}, {"id": 200, "seek": 93394, "start": 938.62, "end": 945.58, "text": " If it was a if we're computer science people we would call this dimension 0 and dimension 1", "tokens": [759, 309, 390, 257, 498, 321, 434, 3820, 3497, 561, 321, 576, 818, 341, 10139, 1958, 293, 10139, 502], "temperature": 0.0, "avg_logprob": -0.22588786794178523, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.3496978681359906e-06}, {"id": 201, "seek": 93394, "start": 947.82, "end": 952.86, "text": " But if we're deep learning people we would call this axis 0 or", "tokens": [583, 498, 321, 434, 2452, 2539, 561, 321, 576, 818, 341, 10298, 1958, 420], "temperature": 0.0, "avg_logprob": -0.22588786794178523, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.3496978681359906e-06}, {"id": 202, "seek": 93394, "start": 953.9000000000001, "end": 955.1, "text": " axis", "tokens": [10298], "temperature": 0.0, "avg_logprob": -0.22588786794178523, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.3496978681359906e-06}, {"id": 203, "seek": 93394, "start": 955.1, "end": 956.2600000000001, "text": " 1", "tokens": [502], "temperature": 0.0, "avg_logprob": -0.22588786794178523, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.3496978681359906e-06}, {"id": 204, "seek": 93394, "start": 956.2600000000001, "end": 960.94, "text": " Okay, and then just to be really confusing if you're an image person", "tokens": [1033, 11, 293, 550, 445, 281, 312, 534, 13181, 498, 291, 434, 364, 3256, 954], "temperature": 0.0, "avg_logprob": -0.22588786794178523, "compression_ratio": 1.5354838709677419, "no_speech_prob": 1.3496978681359906e-06}, {"id": 205, "seek": 96094, "start": 960.94, "end": 964.5, "text": " This is the first axis and this is the second axis", "tokens": [639, 307, 264, 700, 10298, 293, 341, 307, 264, 1150, 10298], "temperature": 0.0, "avg_logprob": -0.21005314872378394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.482353013670945e-06}, {"id": 206, "seek": 96094, "start": 965.1800000000001, "end": 967.82, "text": " Right, so if you think about like TVs, you know", "tokens": [1779, 11, 370, 498, 291, 519, 466, 411, 38085, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.21005314872378394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.482353013670945e-06}, {"id": 207, "seek": 96094, "start": 968.3800000000001, "end": 970.94, "text": " 1920 by 1080 columns by rows", "tokens": [22003, 538, 24547, 13766, 538, 13241], "temperature": 0.0, "avg_logprob": -0.21005314872378394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.482353013670945e-06}, {"id": 208, "seek": 96094, "start": 972.0600000000001, "end": 976.32, "text": " Everybody else including deep learning and mathematicians rows by columns", "tokens": [7646, 1646, 3009, 2452, 2539, 293, 32811, 2567, 13241, 538, 13766], "temperature": 0.0, "avg_logprob": -0.21005314872378394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.482353013670945e-06}, {"id": 209, "seek": 96094, "start": 976.7800000000001, "end": 980.74, "text": " So this is pretty confusing if you use like the Python imaging library", "tokens": [407, 341, 307, 1238, 13181, 498, 291, 764, 411, 264, 15329, 25036, 6405], "temperature": 0.0, "avg_logprob": -0.21005314872378394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.482353013670945e-06}, {"id": 210, "seek": 96094, "start": 981.4200000000001, "end": 987.82, "text": " You get that columns by rows pretty much everything else rows by columns. So be careful", "tokens": [509, 483, 300, 13766, 538, 13241, 1238, 709, 1203, 1646, 13241, 538, 13766, 13, 407, 312, 5026], "temperature": 0.0, "avg_logprob": -0.21005314872378394, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.482353013670945e-06}, {"id": 211, "seek": 98782, "start": 987.82, "end": 989.82, "text": " Because they hate us", "tokens": [1436, 436, 4700, 505], "temperature": 0.0, "avg_logprob": -0.27257528184335444, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.5008254144486273e-06}, {"id": 212, "seek": 98782, "start": 992.1, "end": 994.1, "text": " Because they're bad people I", "tokens": [1436, 436, 434, 1578, 561, 286], "temperature": 0.0, "avg_logprob": -0.27257528184335444, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.5008254144486273e-06}, {"id": 213, "seek": 98782, "start": 995.58, "end": 997.58, "text": " Guess I", "tokens": [17795, 286], "temperature": 0.0, "avg_logprob": -0.27257528184335444, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.5008254144486273e-06}, {"id": 214, "seek": 98782, "start": 999.9000000000001, "end": 1001.98, "text": " Mean there's a lot of just um", "tokens": [12302, 456, 311, 257, 688, 295, 445, 1105], "temperature": 0.0, "avg_logprob": -0.27257528184335444, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.5008254144486273e-06}, {"id": 215, "seek": 98782, "start": 1003.5400000000001, "end": 1009.74, "text": " Particularly in deep learning like a whole lot of different areas have come together like information theory computer vision statistics", "tokens": [32281, 294, 2452, 2539, 411, 257, 1379, 688, 295, 819, 3179, 362, 808, 1214, 411, 1589, 5261, 3820, 5201, 12523], "temperature": 0.0, "avg_logprob": -0.27257528184335444, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.5008254144486273e-06}, {"id": 216, "seek": 98782, "start": 1010.1400000000001, "end": 1013.5200000000001, "text": " signal processing and you've ended up with this hodgepodge of", "tokens": [6358, 9007, 293, 291, 600, 4590, 493, 365, 341, 276, 19315, 79, 19315, 295], "temperature": 0.0, "avg_logprob": -0.27257528184335444, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.5008254144486273e-06}, {"id": 217, "seek": 98782, "start": 1014.1, "end": 1016.4000000000001, "text": " nomenclature and deep learning often like", "tokens": [297, 4726, 3474, 1503, 293, 2452, 2539, 2049, 411], "temperature": 0.0, "avg_logprob": -0.27257528184335444, "compression_ratio": 1.5645933014354068, "no_speech_prob": 3.5008254144486273e-06}, {"id": 218, "seek": 101640, "start": 1016.4, "end": 1022.04, "text": " Every version of things will be used. So today we're going to hear about something that's called either", "tokens": [2048, 3037, 295, 721, 486, 312, 1143, 13, 407, 965, 321, 434, 516, 281, 1568, 466, 746, 300, 311, 1219, 2139], "temperature": 0.0, "avg_logprob": -0.16355349110291068, "compression_ratio": 1.772108843537415, "no_speech_prob": 5.771880296379095e-06}, {"id": 219, "seek": 101640, "start": 1022.76, "end": 1024.76, "text": " negative log likelihood or", "tokens": [3671, 3565, 22119, 420], "temperature": 0.0, "avg_logprob": -0.16355349110291068, "compression_ratio": 1.772108843537415, "no_speech_prob": 5.771880296379095e-06}, {"id": 220, "seek": 101640, "start": 1025.16, "end": 1027.16, "text": " binomial or categorical cross entropy", "tokens": [5171, 47429, 420, 19250, 804, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.16355349110291068, "compression_ratio": 1.772108843537415, "no_speech_prob": 5.771880296379095e-06}, {"id": 221, "seek": 101640, "start": 1027.36, "end": 1029.04, "text": " Depending on where you come from", "tokens": [22539, 322, 689, 291, 808, 490], "temperature": 0.0, "avg_logprob": -0.16355349110291068, "compression_ratio": 1.772108843537415, "no_speech_prob": 5.771880296379095e-06}, {"id": 222, "seek": 101640, "start": 1029.04, "end": 1034.44, "text": " We've already seen something that's called either one-hot encoding or dummy variables depending on where you come from", "tokens": [492, 600, 1217, 1612, 746, 300, 311, 1219, 2139, 472, 12, 12194, 43430, 420, 35064, 9102, 5413, 322, 689, 291, 808, 490], "temperature": 0.0, "avg_logprob": -0.16355349110291068, "compression_ratio": 1.772108843537415, "no_speech_prob": 5.771880296379095e-06}, {"id": 223, "seek": 101640, "start": 1034.44, "end": 1040.28, "text": " I really is just like the same concept gets kind of somewhat independently invented in different fields and", "tokens": [286, 534, 307, 445, 411, 264, 912, 3410, 2170, 733, 295, 8344, 21761, 14479, 294, 819, 7909, 293], "temperature": 0.0, "avg_logprob": -0.16355349110291068, "compression_ratio": 1.772108843537415, "no_speech_prob": 5.771880296379095e-06}, {"id": 224, "seek": 101640, "start": 1040.76, "end": 1044.76, "text": " Eventually, they find their way to machine learning and then we don't know what to call them", "tokens": [17586, 11, 436, 915, 641, 636, 281, 3479, 2539, 293, 550, 321, 500, 380, 458, 437, 281, 818, 552], "temperature": 0.0, "avg_logprob": -0.16355349110291068, "compression_ratio": 1.772108843537415, "no_speech_prob": 5.771880296379095e-06}, {"id": 225, "seek": 104476, "start": 1044.76, "end": 1048.4, "text": " So we call them all of the above something like that", "tokens": [407, 321, 818, 552, 439, 295, 264, 3673, 746, 411, 300], "temperature": 0.0, "avg_logprob": -0.14984743935721262, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.4285370525612962e-05}, {"id": 226, "seek": 104476, "start": 1049.32, "end": 1052.68, "text": " So I think that's what's happened with with computer vision rows and columns", "tokens": [407, 286, 519, 300, 311, 437, 311, 2011, 365, 365, 3820, 5201, 13241, 293, 13766], "temperature": 0.0, "avg_logprob": -0.14984743935721262, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.4285370525612962e-05}, {"id": 227, "seek": 104476, "start": 1054.12, "end": 1056.12, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.14984743935721262, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.4285370525612962e-05}, {"id": 228, "seek": 104476, "start": 1057.8799999999999, "end": 1063.24, "text": " There's this idea of normalizing data which is subtracting out the mean and dividing by the standard deviation", "tokens": [821, 311, 341, 1558, 295, 2710, 3319, 1412, 597, 307, 16390, 278, 484, 264, 914, 293, 26764, 538, 264, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.14984743935721262, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.4285370525612962e-05}, {"id": 229, "seek": 104476, "start": 1066.32, "end": 1068.32, "text": " So a question for you", "tokens": [407, 257, 1168, 337, 291], "temperature": 0.0, "avg_logprob": -0.14984743935721262, "compression_ratio": 1.6291079812206573, "no_speech_prob": 1.4285370525612962e-05}, {"id": 230, "seek": 106832, "start": 1068.32, "end": 1075.4399999999998, "text": " Do you like often it's important to normalize the data so that we can more easily train a model", "tokens": [1144, 291, 411, 2049, 309, 311, 1021, 281, 2710, 1125, 264, 1412, 370, 300, 321, 393, 544, 3612, 3847, 257, 2316], "temperature": 0.0, "avg_logprob": -0.18090434827302632, "compression_ratio": 1.7621359223300972, "no_speech_prob": 4.710863777290797e-06}, {"id": 231, "seek": 106832, "start": 1076.08, "end": 1080.3799999999999, "text": " Do you think it would be important to normalize the independent variables?", "tokens": [1144, 291, 519, 309, 576, 312, 1021, 281, 2710, 1125, 264, 6695, 9102, 30], "temperature": 0.0, "avg_logprob": -0.18090434827302632, "compression_ratio": 1.7621359223300972, "no_speech_prob": 4.710863777290797e-06}, {"id": 232, "seek": 106832, "start": 1080.96, "end": 1083.6799999999998, "text": " For a random forest if we're training a random forest", "tokens": [1171, 257, 4974, 6719, 498, 321, 434, 3097, 257, 4974, 6719], "temperature": 0.0, "avg_logprob": -0.18090434827302632, "compression_ratio": 1.7621359223300972, "no_speech_prob": 4.710863777290797e-06}, {"id": 233, "seek": 106832, "start": 1087.52, "end": 1094.04, "text": " I'll be honest. I don't know why we don't need to normalize. I just know that we don't we don't okay", "tokens": [286, 603, 312, 3245, 13, 286, 500, 380, 458, 983, 321, 500, 380, 643, 281, 2710, 1125, 13, 286, 445, 458, 300, 321, 500, 380, 321, 500, 380, 1392], "temperature": 0.0, "avg_logprob": -0.18090434827302632, "compression_ratio": 1.7621359223300972, "no_speech_prob": 4.710863777290797e-06}, {"id": 234, "seek": 106832, "start": 1094.3999999999999, "end": 1097.0, "text": " Does anybody want to think about why?", "tokens": [4402, 4472, 528, 281, 519, 466, 983, 30], "temperature": 0.0, "avg_logprob": -0.18090434827302632, "compression_ratio": 1.7621359223300972, "no_speech_prob": 4.710863777290797e-06}, {"id": 235, "seek": 109700, "start": 1097.0, "end": 1099.52, "text": " Kara it wouldn't matter because", "tokens": [34838, 309, 2759, 380, 1871, 570], "temperature": 0.0, "avg_logprob": -0.39487496289339935, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.93100001726998e-05}, {"id": 236, "seek": 109700, "start": 1100.84, "end": 1102.84, "text": " each scaling and", "tokens": [1184, 21589, 293], "temperature": 0.0, "avg_logprob": -0.39487496289339935, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.93100001726998e-05}, {"id": 237, "seek": 109700, "start": 1103.76, "end": 1110.4, "text": " Transformation we can have will be applied to each row and we will be computing means as we were doing", "tokens": [6531, 8663, 321, 393, 362, 486, 312, 6456, 281, 1184, 5386, 293, 321, 486, 312, 15866, 1355, 382, 321, 645, 884], "temperature": 0.0, "avg_logprob": -0.39487496289339935, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.93100001726998e-05}, {"id": 238, "seek": 109700, "start": 1110.64, "end": 1112.64, "text": " like local averages and", "tokens": [411, 2654, 42257, 293], "temperature": 0.0, "avg_logprob": -0.39487496289339935, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.93100001726998e-05}, {"id": 239, "seek": 109700, "start": 1112.92, "end": 1119.4, "text": " At the end we will of course want to denormalize it back to give so it wouldn't change the results", "tokens": [1711, 264, 917, 321, 486, 295, 1164, 528, 281, 1441, 24440, 1125, 309, 646, 281, 976, 370, 309, 2759, 380, 1319, 264, 3542], "temperature": 0.0, "avg_logprob": -0.39487496289339935, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.93100001726998e-05}, {"id": 240, "seek": 109700, "start": 1119.4, "end": 1123.84, "text": " I'm doing that the independent variables not the dependent variable. I thought you asked about", "tokens": [286, 478, 884, 300, 264, 6695, 9102, 406, 264, 12334, 7006, 13, 286, 1194, 291, 2351, 466], "temperature": 0.0, "avg_logprob": -0.39487496289339935, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.93100001726998e-05}, {"id": 241, "seek": 112384, "start": 1123.84, "end": 1126.3999999999999, "text": " Okay, let's have a go Matthew", "tokens": [1033, 11, 718, 311, 362, 257, 352, 12434], "temperature": 0.0, "avg_logprob": -0.3332071039411757, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5205026102194097e-05}, {"id": 242, "seek": 112384, "start": 1131.52, "end": 1138.24, "text": " It might be because we just care about the relationship between the independent variables and the dependent variable so scale doesn't really matter", "tokens": [467, 1062, 312, 570, 321, 445, 1127, 466, 264, 2480, 1296, 264, 6695, 9102, 293, 264, 12334, 7006, 370, 4373, 1177, 380, 534, 1871], "temperature": 0.0, "avg_logprob": -0.3332071039411757, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5205026102194097e-05}, {"id": 243, "seek": 112384, "start": 1139.36, "end": 1141.36, "text": " Okay, go on", "tokens": [1033, 11, 352, 322], "temperature": 0.0, "avg_logprob": -0.3332071039411757, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5205026102194097e-05}, {"id": 244, "seek": 112384, "start": 1141.36, "end": 1145.36, "text": " How what why what why do we only like because at each split point?", "tokens": [1012, 437, 983, 437, 983, 360, 321, 787, 411, 570, 412, 1184, 7472, 935, 30], "temperature": 0.0, "avg_logprob": -0.3332071039411757, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5205026102194097e-05}, {"id": 245, "seek": 112384, "start": 1146.1999999999998, "end": 1148.1999999999998, "text": " We can just divide to see", "tokens": [492, 393, 445, 9845, 281, 536], "temperature": 0.0, "avg_logprob": -0.3332071039411757, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5205026102194097e-05}, {"id": 246, "seek": 112384, "start": 1149.48, "end": 1151.48, "text": " Which", "tokens": [3013], "temperature": 0.0, "avg_logprob": -0.3332071039411757, "compression_ratio": 1.5483870967741935, "no_speech_prob": 1.5205026102194097e-05}, {"id": 247, "seek": 115148, "start": 1151.48, "end": 1153.64, "text": " Regardless of what scale you're on", "tokens": [25148, 295, 437, 4373, 291, 434, 322], "temperature": 0.0, "avg_logprob": -0.18484052622093344, "compression_ratio": 1.8188976377952757, "no_speech_prob": 4.936956429446582e-06}, {"id": 248, "seek": 115148, "start": 1154.48, "end": 1161.1, "text": " What minimizes variance and that would right so really the key is that when we're deciding where to split?", "tokens": [708, 4464, 5660, 21977, 293, 300, 576, 558, 370, 534, 264, 2141, 307, 300, 562, 321, 434, 17990, 689, 281, 7472, 30], "temperature": 0.0, "avg_logprob": -0.18484052622093344, "compression_ratio": 1.8188976377952757, "no_speech_prob": 4.936956429446582e-06}, {"id": 249, "seek": 115148, "start": 1161.6, "end": 1167.1200000000001, "text": " All that matters is the order like it all that matters is how they're sorted so if we divide by the", "tokens": [1057, 300, 7001, 307, 264, 1668, 411, 309, 439, 300, 7001, 307, 577, 436, 434, 25462, 370, 498, 321, 9845, 538, 264], "temperature": 0.0, "avg_logprob": -0.18484052622093344, "compression_ratio": 1.8188976377952757, "no_speech_prob": 4.936956429446582e-06}, {"id": 250, "seek": 115148, "start": 1168.2, "end": 1171.64, "text": " Subtract the mean and divide by the standard deviation. They're still sorted in the same order", "tokens": [8511, 83, 1897, 264, 914, 293, 9845, 538, 264, 3832, 25163, 13, 814, 434, 920, 25462, 294, 264, 912, 1668], "temperature": 0.0, "avg_logprob": -0.18484052622093344, "compression_ratio": 1.8188976377952757, "no_speech_prob": 4.936956429446582e-06}, {"id": 251, "seek": 115148, "start": 1171.64, "end": 1178.7, "text": " So remember when we implemented the random forest we said sort them and then we like it then we completely ignored the values", "tokens": [407, 1604, 562, 321, 12270, 264, 4974, 6719, 321, 848, 1333, 552, 293, 550, 321, 411, 309, 550, 321, 2584, 19735, 264, 4190], "temperature": 0.0, "avg_logprob": -0.18484052622093344, "compression_ratio": 1.8188976377952757, "no_speech_prob": 4.936956429446582e-06}, {"id": 252, "seek": 117870, "start": 1178.7, "end": 1182.68, "text": " We just said like now add on one thing from the dependent at a time", "tokens": [492, 445, 848, 411, 586, 909, 322, 472, 551, 490, 264, 12334, 412, 257, 565], "temperature": 0.0, "avg_logprob": -0.13186222773331863, "compression_ratio": 1.7192307692307693, "no_speech_prob": 3.576356846224371e-07}, {"id": 253, "seek": 117870, "start": 1183.02, "end": 1189.44, "text": " So so random forests only care about the sort order of the independent variables", "tokens": [407, 370, 4974, 21700, 787, 1127, 466, 264, 1333, 1668, 295, 264, 6695, 9102], "temperature": 0.0, "avg_logprob": -0.13186222773331863, "compression_ratio": 1.7192307692307693, "no_speech_prob": 3.576356846224371e-07}, {"id": 254, "seek": 117870, "start": 1189.44, "end": 1195.1000000000001, "text": " They don't care at all about their size, and so that's why they're wonderfully immune to outliers", "tokens": [814, 500, 380, 1127, 412, 439, 466, 641, 2744, 11, 293, 370, 300, 311, 983, 436, 434, 38917, 11992, 281, 484, 23646], "temperature": 0.0, "avg_logprob": -0.13186222773331863, "compression_ratio": 1.7192307692307693, "no_speech_prob": 3.576356846224371e-07}, {"id": 255, "seek": 117870, "start": 1195.64, "end": 1202.3600000000001, "text": " Because they totally ignore the fact that it's an outlier. They only care about which one's higher than what other thing right so", "tokens": [1436, 436, 3879, 11200, 264, 1186, 300, 309, 311, 364, 484, 2753, 13, 814, 787, 1127, 466, 597, 472, 311, 2946, 813, 437, 661, 551, 558, 370], "temperature": 0.0, "avg_logprob": -0.13186222773331863, "compression_ratio": 1.7192307692307693, "no_speech_prob": 3.576356846224371e-07}, {"id": 256, "seek": 117870, "start": 1202.6000000000001, "end": 1205.72, "text": " This is an important concept. It doesn't just appear in random forests", "tokens": [639, 307, 364, 1021, 3410, 13, 467, 1177, 380, 445, 4204, 294, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.13186222773331863, "compression_ratio": 1.7192307692307693, "no_speech_prob": 3.576356846224371e-07}, {"id": 257, "seek": 120572, "start": 1205.72, "end": 1211.52, "text": " It occurs in some metrics as well for example area under the ROC curve you come across a lot", "tokens": [467, 11843, 294, 512, 16367, 382, 731, 337, 1365, 1859, 833, 264, 9025, 34, 7605, 291, 808, 2108, 257, 688], "temperature": 0.0, "avg_logprob": -0.16389340852436265, "compression_ratio": 1.7866666666666666, "no_speech_prob": 2.5612769150029635e-06}, {"id": 258, "seek": 120572, "start": 1211.84, "end": 1213.84, "text": " That area under the ROC curve", "tokens": [663, 1859, 833, 264, 9025, 34, 7605], "temperature": 0.0, "avg_logprob": -0.16389340852436265, "compression_ratio": 1.7866666666666666, "no_speech_prob": 2.5612769150029635e-06}, {"id": 259, "seek": 120572, "start": 1214.64, "end": 1217.94, "text": " Completely ignores scale and only cares about sort", "tokens": [39978, 5335, 2706, 4373, 293, 787, 12310, 466, 1333], "temperature": 0.0, "avg_logprob": -0.16389340852436265, "compression_ratio": 1.7866666666666666, "no_speech_prob": 2.5612769150029635e-06}, {"id": 260, "seek": 120572, "start": 1218.58, "end": 1220.92, "text": " We saw something else when we did the dendrogram", "tokens": [492, 1866, 746, 1646, 562, 321, 630, 264, 274, 521, 340, 1342], "temperature": 0.0, "avg_logprob": -0.16389340852436265, "compression_ratio": 1.7866666666666666, "no_speech_prob": 2.5612769150029635e-06}, {"id": 261, "seek": 120572, "start": 1222.04, "end": 1227.66, "text": " Spearman's correlation is a rank correlation only cares about order not about scale", "tokens": [3550, 289, 1601, 311, 20009, 307, 257, 6181, 20009, 787, 12310, 466, 1668, 406, 466, 4373], "temperature": 0.0, "avg_logprob": -0.16389340852436265, "compression_ratio": 1.7866666666666666, "no_speech_prob": 2.5612769150029635e-06}, {"id": 262, "seek": 120572, "start": 1228.76, "end": 1234.6000000000001, "text": " So random forests one of the many wonderful things about them are that we can completely ignore", "tokens": [407, 4974, 21700, 472, 295, 264, 867, 3715, 721, 466, 552, 366, 300, 321, 393, 2584, 11200], "temperature": 0.0, "avg_logprob": -0.16389340852436265, "compression_ratio": 1.7866666666666666, "no_speech_prob": 2.5612769150029635e-06}, {"id": 263, "seek": 123460, "start": 1234.6, "end": 1237.8, "text": " a lot of these statistical distribution issues", "tokens": [257, 688, 295, 613, 22820, 7316, 2663], "temperature": 0.0, "avg_logprob": -0.1490099719751661, "compression_ratio": 1.8525896414342629, "no_speech_prob": 4.936961886414792e-06}, {"id": 264, "seek": 123460, "start": 1238.52, "end": 1244.8, "text": " But we can't for deep learning because for deep learning we're trying to train a parameterized model", "tokens": [583, 321, 393, 380, 337, 2452, 2539, 570, 337, 2452, 2539, 321, 434, 1382, 281, 3847, 257, 13075, 1602, 2316], "temperature": 0.0, "avg_logprob": -0.1490099719751661, "compression_ratio": 1.8525896414342629, "no_speech_prob": 4.936961886414792e-06}, {"id": 265, "seek": 123460, "start": 1245.0, "end": 1247.0, "text": " So we do need to normalize our data", "tokens": [407, 321, 360, 643, 281, 2710, 1125, 527, 1412], "temperature": 0.0, "avg_logprob": -0.1490099719751661, "compression_ratio": 1.8525896414342629, "no_speech_prob": 4.936961886414792e-06}, {"id": 266, "seek": 123460, "start": 1247.6399999999999, "end": 1253.7199999999998, "text": " If we don't then it's going to be much harder to create a network that trains effectively", "tokens": [759, 321, 500, 380, 550, 309, 311, 516, 281, 312, 709, 6081, 281, 1884, 257, 3209, 300, 16329, 8659], "temperature": 0.0, "avg_logprob": -0.1490099719751661, "compression_ratio": 1.8525896414342629, "no_speech_prob": 4.936961886414792e-06}, {"id": 267, "seek": 123460, "start": 1254.04, "end": 1257.4399999999998, "text": " so we grab the mean and the standard deviation of our training data and", "tokens": [370, 321, 4444, 264, 914, 293, 264, 3832, 25163, 295, 527, 3097, 1412, 293], "temperature": 0.0, "avg_logprob": -0.1490099719751661, "compression_ratio": 1.8525896414342629, "no_speech_prob": 4.936961886414792e-06}, {"id": 268, "seek": 125744, "start": 1257.44, "end": 1264.4, "text": " Subtract out the mean divide by the standard deviation and that gives us a mean of zero and a standard deviation of one", "tokens": [8511, 83, 1897, 484, 264, 914, 9845, 538, 264, 3832, 25163, 293, 300, 2709, 505, 257, 914, 295, 4018, 293, 257, 3832, 25163, 295, 472], "temperature": 0.0, "avg_logprob": -0.13336955860096922, "compression_ratio": 1.8169014084507042, "no_speech_prob": 2.3320662876358256e-06}, {"id": 269, "seek": 125744, "start": 1265.3200000000002, "end": 1267.3200000000002, "text": " Now for our validation data", "tokens": [823, 337, 527, 24071, 1412], "temperature": 0.0, "avg_logprob": -0.13336955860096922, "compression_ratio": 1.8169014084507042, "no_speech_prob": 2.3320662876358256e-06}, {"id": 270, "seek": 125744, "start": 1267.52, "end": 1274.3200000000002, "text": " We need to use the standard deviation and mean from the training data right we have to normalize it the same way", "tokens": [492, 643, 281, 764, 264, 3832, 25163, 293, 914, 490, 264, 3097, 1412, 558, 321, 362, 281, 2710, 1125, 309, 264, 912, 636], "temperature": 0.0, "avg_logprob": -0.13336955860096922, "compression_ratio": 1.8169014084507042, "no_speech_prob": 2.3320662876358256e-06}, {"id": 271, "seek": 125744, "start": 1274.72, "end": 1276.3200000000002, "text": " Just like", "tokens": [1449, 411], "temperature": 0.0, "avg_logprob": -0.13336955860096922, "compression_ratio": 1.8169014084507042, "no_speech_prob": 2.3320662876358256e-06}, {"id": 272, "seek": 125744, "start": 1276.3200000000002, "end": 1282.8400000000001, "text": " Categorical variables we had to make sure they had the same indexes mapped to the same levels for a random forest or", "tokens": [383, 2968, 284, 804, 9102, 321, 632, 281, 652, 988, 436, 632, 264, 912, 8186, 279, 33318, 281, 264, 912, 4358, 337, 257, 4974, 6719, 420], "temperature": 0.0, "avg_logprob": -0.13336955860096922, "compression_ratio": 1.8169014084507042, "no_speech_prob": 2.3320662876358256e-06}, {"id": 273, "seek": 128284, "start": 1282.84, "end": 1290.12, "text": " Missing values we had to make sure we had the same median used when we were replacing the missing values", "tokens": [5275, 278, 4190, 321, 632, 281, 652, 988, 321, 632, 264, 912, 26779, 1143, 562, 321, 645, 19139, 264, 5361, 4190], "temperature": 0.0, "avg_logprob": -0.16665915138701087, "compression_ratio": 1.996, "no_speech_prob": 6.179379283821618e-07}, {"id": 274, "seek": 128284, "start": 1290.28, "end": 1296.08, "text": " You need to make sure anything you do in the training set you do exactly the same thing in the test and validation set", "tokens": [509, 643, 281, 652, 988, 1340, 291, 360, 294, 264, 3097, 992, 291, 360, 2293, 264, 912, 551, 294, 264, 1500, 293, 24071, 992], "temperature": 0.0, "avg_logprob": -0.16665915138701087, "compression_ratio": 1.996, "no_speech_prob": 6.179379283821618e-07}, {"id": 275, "seek": 128284, "start": 1296.08, "end": 1299.58, "text": " So here I'm subtracting out the training set mean the training sets down deviation", "tokens": [407, 510, 286, 478, 16390, 278, 484, 264, 3097, 992, 914, 264, 3097, 6352, 760, 25163], "temperature": 0.0, "avg_logprob": -0.16665915138701087, "compression_ratio": 1.996, "no_speech_prob": 6.179379283821618e-07}, {"id": 276, "seek": 128284, "start": 1299.58, "end": 1304.1999999999998, "text": " So this is not exactly zero. This is not exactly one, but it's pretty close and", "tokens": [407, 341, 307, 406, 2293, 4018, 13, 639, 307, 406, 2293, 472, 11, 457, 309, 311, 1238, 1998, 293], "temperature": 0.0, "avg_logprob": -0.16665915138701087, "compression_ratio": 1.996, "no_speech_prob": 6.179379283821618e-07}, {"id": 277, "seek": 128284, "start": 1305.12, "end": 1312.24, "text": " So in general if you find you try something on a validation set or a test set and it's like much much much worse", "tokens": [407, 294, 2674, 498, 291, 915, 291, 853, 746, 322, 257, 24071, 992, 420, 257, 1500, 992, 293, 309, 311, 411, 709, 709, 709, 5324], "temperature": 0.0, "avg_logprob": -0.16665915138701087, "compression_ratio": 1.996, "no_speech_prob": 6.179379283821618e-07}, {"id": 278, "seek": 131224, "start": 1312.24, "end": 1315.8, "text": " Than your training set. It's probably because you", "tokens": [18289, 428, 3097, 992, 13, 467, 311, 1391, 570, 291], "temperature": 0.0, "avg_logprob": -0.19321137245255288, "compression_ratio": 1.5708502024291497, "no_speech_prob": 3.3075897931666987e-07}, {"id": 279, "seek": 131224, "start": 1317.0, "end": 1321.36, "text": " Normalized in an inconsistent way or encoded categories in inconsistent way or something like that", "tokens": [21277, 1602, 294, 364, 36891, 636, 420, 2058, 12340, 10479, 294, 36891, 636, 420, 746, 411, 300], "temperature": 0.0, "avg_logprob": -0.19321137245255288, "compression_ratio": 1.5708502024291497, "no_speech_prob": 3.3075897931666987e-07}, {"id": 280, "seek": 131224, "start": 1323.6, "end": 1326.16, "text": " All right, so let's take a look at some of this data", "tokens": [1057, 558, 11, 370, 718, 311, 747, 257, 574, 412, 512, 295, 341, 1412], "temperature": 0.0, "avg_logprob": -0.19321137245255288, "compression_ratio": 1.5708502024291497, "no_speech_prob": 3.3075897931666987e-07}, {"id": 281, "seek": 131224, "start": 1327.32, "end": 1329.2, "text": " So we've got", "tokens": [407, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.19321137245255288, "compression_ratio": 1.5708502024291497, "no_speech_prob": 3.3075897931666987e-07}, {"id": 282, "seek": 131224, "start": 1329.2, "end": 1335.36, "text": " 10,000 images in the validation set and each one is a rank one tensor of length seven eight four", "tokens": [1266, 11, 1360, 5267, 294, 264, 24071, 992, 293, 1184, 472, 307, 257, 6181, 472, 40863, 295, 4641, 3407, 3180, 1451], "temperature": 0.0, "avg_logprob": -0.19321137245255288, "compression_ratio": 1.5708502024291497, "no_speech_prob": 3.3075897931666987e-07}, {"id": 283, "seek": 133536, "start": 1335.36, "end": 1341.3999999999999, "text": " In order to display it. I want to turn it into a rank two tensor of 28 by 28", "tokens": [682, 1668, 281, 4674, 309, 13, 286, 528, 281, 1261, 309, 666, 257, 6181, 732, 40863, 295, 7562, 538, 7562], "temperature": 0.0, "avg_logprob": -0.17741759618123373, "compression_ratio": 1.4411764705882353, "no_speech_prob": 8.851557140587829e-07}, {"id": 284, "seek": 133536, "start": 1342.6, "end": 1344.32, "text": " so there's a", "tokens": [370, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.17741759618123373, "compression_ratio": 1.4411764705882353, "no_speech_prob": 8.851557140587829e-07}, {"id": 285, "seek": 133536, "start": 1344.32, "end": 1346.32, "text": " NumPy has a reshape function", "tokens": [22592, 47, 88, 575, 257, 725, 42406, 2445], "temperature": 0.0, "avg_logprob": -0.17741759618123373, "compression_ratio": 1.4411764705882353, "no_speech_prob": 8.851557140587829e-07}, {"id": 286, "seek": 133536, "start": 1346.6399999999999, "end": 1348.6399999999999, "text": " that takes a", "tokens": [300, 2516, 257], "temperature": 0.0, "avg_logprob": -0.17741759618123373, "compression_ratio": 1.4411764705882353, "no_speech_prob": 8.851557140587829e-07}, {"id": 287, "seek": 133536, "start": 1349.56, "end": 1351.56, "text": " tensor in and", "tokens": [40863, 294, 293], "temperature": 0.0, "avg_logprob": -0.17741759618123373, "compression_ratio": 1.4411764705882353, "no_speech_prob": 8.851557140587829e-07}, {"id": 288, "seek": 133536, "start": 1351.76, "end": 1354.6399999999999, "text": " Reshapes it to whatever size tensor you request", "tokens": [5015, 71, 569, 279, 309, 281, 2035, 2744, 40863, 291, 5308], "temperature": 0.0, "avg_logprob": -0.17741759618123373, "compression_ratio": 1.4411764705882353, "no_speech_prob": 8.851557140587829e-07}, {"id": 289, "seek": 133536, "start": 1355.3999999999999, "end": 1359.12, "text": " Now if you think about it, you only need to tell it", "tokens": [823, 498, 291, 519, 466, 309, 11, 291, 787, 643, 281, 980, 309], "temperature": 0.0, "avg_logprob": -0.17741759618123373, "compression_ratio": 1.4411764705882353, "no_speech_prob": 8.851557140587829e-07}, {"id": 290, "seek": 135912, "start": 1359.12, "end": 1365.52, "text": " About if there are D axes you only need to tell it about D minus one of the axes you want", "tokens": [7769, 498, 456, 366, 413, 35387, 291, 787, 643, 281, 980, 309, 466, 413, 3175, 472, 295, 264, 35387, 291, 528], "temperature": 0.0, "avg_logprob": -0.20648047388816365, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.9333529053255916e-06}, {"id": 291, "seek": 135912, "start": 1365.6399999999999, "end": 1367.6399999999999, "text": " Because the last one it can figure out for itself", "tokens": [1436, 264, 1036, 472, 309, 393, 2573, 484, 337, 2564], "temperature": 0.0, "avg_logprob": -0.20648047388816365, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.9333529053255916e-06}, {"id": 292, "seek": 135912, "start": 1368.08, "end": 1374.9199999999998, "text": " Right so in total there are ten thousand by seven hundred and eighty four numbers here altogether", "tokens": [1779, 370, 294, 3217, 456, 366, 2064, 4714, 538, 3407, 3262, 293, 26348, 1451, 3547, 510, 19051], "temperature": 0.0, "avg_logprob": -0.20648047388816365, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.9333529053255916e-06}, {"id": 293, "seek": 135912, "start": 1375.28, "end": 1379.8, "text": " Right so if you say well, I want my last axes to be 28 by 28", "tokens": [1779, 370, 498, 291, 584, 731, 11, 286, 528, 452, 1036, 35387, 281, 312, 7562, 538, 7562], "temperature": 0.0, "avg_logprob": -0.20648047388816365, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.9333529053255916e-06}, {"id": 294, "seek": 135912, "start": 1380.4799999999998, "end": 1383.0, "text": " Then you can figure out that this must be", "tokens": [1396, 291, 393, 2573, 484, 300, 341, 1633, 312], "temperature": 0.0, "avg_logprob": -0.20648047388816365, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.9333529053255916e-06}, {"id": 295, "seek": 135912, "start": 1384.08, "end": 1386.1599999999999, "text": " 10,000 otherwise it's not going to fit", "tokens": [1266, 11, 1360, 5911, 309, 311, 406, 516, 281, 3318], "temperature": 0.0, "avg_logprob": -0.20648047388816365, "compression_ratio": 1.669603524229075, "no_speech_prob": 1.9333529053255916e-06}, {"id": 296, "seek": 138616, "start": 1386.16, "end": 1392.5600000000002, "text": " It makes sense so if you put minus one it says like make it as big or as small as you have to", "tokens": [467, 1669, 2020, 370, 498, 291, 829, 3175, 472, 309, 1619, 411, 652, 309, 382, 955, 420, 382, 1359, 382, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 297, "seek": 138616, "start": 1392.68, "end": 1396.52, "text": " To make it fit and so you can see here it figured out it has to be 10,000", "tokens": [1407, 652, 309, 3318, 293, 370, 291, 393, 536, 510, 309, 8932, 484, 309, 575, 281, 312, 1266, 11, 1360], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 298, "seek": 138616, "start": 1397.68, "end": 1399.68, "text": " So you'll see this used in", "tokens": [407, 291, 603, 536, 341, 1143, 294], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 299, "seek": 138616, "start": 1400.68, "end": 1402.1200000000001, "text": " neural net", "tokens": [18161, 2533], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 300, "seek": 138616, "start": 1402.1200000000001, "end": 1403.64, "text": " software", "tokens": [4722], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 301, "seek": 138616, "start": 1403.64, "end": 1407.3600000000001, "text": " Pre-processing and stuff like that all the time like I could have written 10,000 here", "tokens": [6001, 12, 41075, 278, 293, 1507, 411, 300, 439, 264, 565, 411, 286, 727, 362, 3720, 1266, 11, 1360, 510], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 302, "seek": 138616, "start": 1407.3600000000001, "end": 1409.92, "text": " But I try to get into a habit of like any time", "tokens": [583, 286, 853, 281, 483, 666, 257, 7164, 295, 411, 604, 565], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 303, "seek": 138616, "start": 1409.92, "end": 1413.64, "text": " I'm referring to like how many items are in my input", "tokens": [286, 478, 13761, 281, 411, 577, 867, 4754, 366, 294, 452, 4846], "temperature": 0.0, "avg_logprob": -0.15622077610181725, "compression_ratio": 1.646090534979424, "no_speech_prob": 6.17938667346607e-07}, {"id": 304, "seek": 141364, "start": 1413.64, "end": 1418.96, "text": " I tend to use minus one because like it just means later on I could like use a subsample", "tokens": [286, 3928, 281, 764, 3175, 472, 570, 411, 309, 445, 1355, 1780, 322, 286, 727, 411, 764, 257, 2090, 335, 781], "temperature": 0.0, "avg_logprob": -0.1408429187277089, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.4144726492304471e-06}, {"id": 305, "seek": 141364, "start": 1419.5200000000002, "end": 1425.68, "text": " This code wouldn't break. I could you know do some kind of stratified sampling. It was unbalanced this code wouldn't break", "tokens": [639, 3089, 2759, 380, 1821, 13, 286, 727, 291, 458, 360, 512, 733, 295, 23674, 2587, 21179, 13, 467, 390, 517, 40251, 341, 3089, 2759, 380, 1821], "temperature": 0.0, "avg_logprob": -0.1408429187277089, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.4144726492304471e-06}, {"id": 306, "seek": 141364, "start": 1425.76, "end": 1430.3200000000002, "text": " So by using this kind of approach of saying like minus one here for the size", "tokens": [407, 538, 1228, 341, 733, 295, 3109, 295, 1566, 411, 3175, 472, 510, 337, 264, 2744], "temperature": 0.0, "avg_logprob": -0.1408429187277089, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.4144726492304471e-06}, {"id": 307, "seek": 141364, "start": 1430.5600000000002, "end": 1434.8400000000001, "text": " It just makes it more resilient to change us later. It's a good habit to get into", "tokens": [467, 445, 1669, 309, 544, 23699, 281, 1319, 505, 1780, 13, 467, 311, 257, 665, 7164, 281, 483, 666], "temperature": 0.0, "avg_logprob": -0.1408429187277089, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.4144726492304471e-06}, {"id": 308, "seek": 141364, "start": 1436.2800000000002, "end": 1441.68, "text": " so this kind of idea of like being able to take tensors and reshape them and and and", "tokens": [370, 341, 733, 295, 1558, 295, 411, 885, 1075, 281, 747, 10688, 830, 293, 725, 42406, 552, 293, 293, 293], "temperature": 0.0, "avg_logprob": -0.1408429187277089, "compression_ratio": 1.7843137254901962, "no_speech_prob": 1.4144726492304471e-06}, {"id": 309, "seek": 144168, "start": 1441.68, "end": 1446.92, "text": " change axes around and stuff like that is something you need to be like", "tokens": [1319, 35387, 926, 293, 1507, 411, 300, 307, 746, 291, 643, 281, 312, 411], "temperature": 0.0, "avg_logprob": -0.2070256435509884, "compression_ratio": 1.620253164556962, "no_speech_prob": 1.209863398798916e-06}, {"id": 310, "seek": 144168, "start": 1448.04, "end": 1450.3600000000001, "text": " Totally do without thinking", "tokens": [22837, 360, 1553, 1953], "temperature": 0.0, "avg_logprob": -0.2070256435509884, "compression_ratio": 1.620253164556962, "no_speech_prob": 1.209863398798916e-06}, {"id": 311, "seek": 144168, "start": 1451.16, "end": 1456.3, "text": " Because it's going to happen all the time. So for example, here's one. I tried to read in some images", "tokens": [1436, 309, 311, 516, 281, 1051, 439, 264, 565, 13, 407, 337, 1365, 11, 510, 311, 472, 13, 286, 3031, 281, 1401, 294, 512, 5267], "temperature": 0.0, "avg_logprob": -0.2070256435509884, "compression_ratio": 1.620253164556962, "no_speech_prob": 1.209863398798916e-06}, {"id": 312, "seek": 144168, "start": 1456.64, "end": 1462.04, "text": " They were flattened. I need to unflatten them into a bunch of matrices. Okay reshape thing", "tokens": [814, 645, 24183, 292, 13, 286, 643, 281, 517, 3423, 32733, 552, 666, 257, 3840, 295, 32284, 13, 1033, 725, 42406, 551], "temperature": 0.0, "avg_logprob": -0.2070256435509884, "compression_ratio": 1.620253164556962, "no_speech_prob": 1.209863398798916e-06}, {"id": 313, "seek": 144168, "start": 1462.64, "end": 1468.1200000000001, "text": " I read some I read some images in with open CV and it turns out open CV", "tokens": [286, 1401, 512, 286, 1401, 512, 5267, 294, 365, 1269, 22995, 293, 309, 4523, 484, 1269, 22995], "temperature": 0.0, "avg_logprob": -0.2070256435509884, "compression_ratio": 1.620253164556962, "no_speech_prob": 1.209863398798916e-06}, {"id": 314, "seek": 144168, "start": 1468.8400000000001, "end": 1470.3200000000002, "text": " orders the channels", "tokens": [9470, 264, 9235], "temperature": 0.0, "avg_logprob": -0.2070256435509884, "compression_ratio": 1.620253164556962, "no_speech_prob": 1.209863398798916e-06}, {"id": 315, "seek": 147032, "start": 1470.32, "end": 1477.52, "text": " Blue green red everything else expects them to be red green blue. I need to reverse the last axis. How do you do that?", "tokens": [8510, 3092, 2182, 1203, 1646, 33280, 552, 281, 312, 2182, 3092, 3344, 13, 286, 643, 281, 9943, 264, 1036, 10298, 13, 1012, 360, 291, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.19030521392822267, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.8448193941367208e-06}, {"id": 316, "seek": 147032, "start": 1478.36, "end": 1483.08, "text": " I read in some images with Python imaging library. It reads them as", "tokens": [286, 1401, 294, 512, 5267, 365, 15329, 25036, 6405, 13, 467, 15700, 552, 382], "temperature": 0.0, "avg_logprob": -0.19030521392822267, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.8448193941367208e-06}, {"id": 317, "seek": 147032, "start": 1483.48, "end": 1490.62, "text": " You know rows by columns by channels pytorch expects channels by rows by columns. How do I?", "tokens": [509, 458, 13241, 538, 13766, 538, 9235, 25878, 284, 339, 33280, 9235, 538, 13241, 538, 13766, 13, 1012, 360, 286, 30], "temperature": 0.0, "avg_logprob": -0.19030521392822267, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.8448193941367208e-06}, {"id": 318, "seek": 147032, "start": 1491.52, "end": 1495.24, "text": " Transform that so these are all things you need to be able to do", "tokens": [27938, 300, 370, 613, 366, 439, 721, 291, 643, 281, 312, 1075, 281, 360], "temperature": 0.0, "avg_logprob": -0.19030521392822267, "compression_ratio": 1.6935483870967742, "no_speech_prob": 1.8448193941367208e-06}, {"id": 319, "seek": 149524, "start": 1495.24, "end": 1502.94, "text": " Without thinking like straight away because they just it happens all the time and you never want to be sitting there thinking about it", "tokens": [9129, 1953, 411, 2997, 1314, 570, 436, 445, 309, 2314, 439, 264, 565, 293, 291, 1128, 528, 281, 312, 3798, 456, 1953, 466, 309], "temperature": 0.0, "avg_logprob": -0.1604233838002616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 6.240891252673464e-06}, {"id": 320, "seek": 149524, "start": 1502.94, "end": 1506.72, "text": " For ages, so make sure you spend a lot of time over the week", "tokens": [1171, 12357, 11, 370, 652, 988, 291, 3496, 257, 688, 295, 565, 670, 264, 1243], "temperature": 0.0, "avg_logprob": -0.1604233838002616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 6.240891252673464e-06}, {"id": 321, "seek": 149524, "start": 1506.72, "end": 1510.66, "text": " Just practicing with things like all the stuff we're going to see today reshaping", "tokens": [1449, 11350, 365, 721, 411, 439, 264, 1507, 321, 434, 516, 281, 536, 965, 725, 71, 569, 278], "temperature": 0.0, "avg_logprob": -0.1604233838002616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 6.240891252673464e-06}, {"id": 322, "seek": 149524, "start": 1511.44, "end": 1512.68, "text": " slicing", "tokens": [46586], "temperature": 0.0, "avg_logprob": -0.1604233838002616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 6.240891252673464e-06}, {"id": 323, "seek": 149524, "start": 1512.68, "end": 1514.4, "text": " reordering dimensions", "tokens": [319, 765, 1794, 12819], "temperature": 0.0, "avg_logprob": -0.1604233838002616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 6.240891252673464e-06}, {"id": 324, "seek": 149524, "start": 1514.4, "end": 1517.6, "text": " Stuff like that. And so the best way is to create some small", "tokens": [31347, 411, 300, 13, 400, 370, 264, 1151, 636, 307, 281, 1884, 512, 1359], "temperature": 0.0, "avg_logprob": -0.1604233838002616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 6.240891252673464e-06}, {"id": 325, "seek": 149524, "start": 1518.64, "end": 1523.68, "text": " Tenses yourself and start thinking like okay, what shall I experiment with so here?", "tokens": [314, 9085, 1803, 293, 722, 1953, 411, 1392, 11, 437, 4393, 286, 5120, 365, 370, 510, 30], "temperature": 0.0, "avg_logprob": -0.1604233838002616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 6.240891252673464e-06}, {"id": 326, "seek": 152368, "start": 1523.68, "end": 1526.3600000000001, "text": " Can we pass that over there?", "tokens": [1664, 321, 1320, 300, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.27288888630114105, "compression_ratio": 1.6751824817518248, "no_speech_prob": 1.7502508853795007e-05}, {"id": 327, "seek": 152368, "start": 1527.68, "end": 1531.0, "text": " Do my for backtrack a little bit, of course, I love it. So back in", "tokens": [1144, 452, 337, 646, 19466, 257, 707, 857, 11, 295, 1164, 11, 286, 959, 309, 13, 407, 646, 294], "temperature": 0.0, "avg_logprob": -0.27288888630114105, "compression_ratio": 1.6751824817518248, "no_speech_prob": 1.7502508853795007e-05}, {"id": 328, "seek": 152368, "start": 1531.64, "end": 1535.92, "text": " Normalize you say like you might have gone over this but I'm still like", "tokens": [21277, 1125, 291, 584, 411, 291, 1062, 362, 2780, 670, 341, 457, 286, 478, 920, 411], "temperature": 0.0, "avg_logprob": -0.27288888630114105, "compression_ratio": 1.6751824817518248, "no_speech_prob": 1.7502508853795007e-05}, {"id": 329, "seek": 152368, "start": 1536.6000000000001, "end": 1540.72, "text": " Wrestling with a little bit. Yeah, many machine learning algorithms behave better when the data is normalized", "tokens": [43508, 365, 257, 707, 857, 13, 865, 11, 867, 3479, 2539, 14642, 15158, 1101, 562, 264, 1412, 307, 48704], "temperature": 0.0, "avg_logprob": -0.27288888630114105, "compression_ratio": 1.6751824817518248, "no_speech_prob": 1.7502508853795007e-05}, {"id": 330, "seek": 152368, "start": 1540.72, "end": 1547.24, "text": " Yeah, but you also just said that scales are really matter. So I said it doesn't matter for random forests", "tokens": [865, 11, 457, 291, 611, 445, 848, 300, 17408, 366, 534, 1871, 13, 407, 286, 848, 309, 1177, 380, 1871, 337, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.27288888630114105, "compression_ratio": 1.6751824817518248, "no_speech_prob": 1.7502508853795007e-05}, {"id": 331, "seek": 152368, "start": 1547.24, "end": 1551.92, "text": " Okay, yeah, so random forests are just going to spit things based on order", "tokens": [1033, 11, 1338, 11, 370, 4974, 21700, 366, 445, 516, 281, 22127, 721, 2361, 322, 1668], "temperature": 0.0, "avg_logprob": -0.27288888630114105, "compression_ratio": 1.6751824817518248, "no_speech_prob": 1.7502508853795007e-05}, {"id": 332, "seek": 155192, "start": 1551.92, "end": 1556.18, "text": " And so we love them. We love random forests for the way. They're so", "tokens": [400, 370, 321, 959, 552, 13, 492, 959, 4974, 21700, 337, 264, 636, 13, 814, 434, 370], "temperature": 0.0, "avg_logprob": -0.24885876395485618, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.6700909327482805e-05}, {"id": 333, "seek": 155192, "start": 1556.96, "end": 1562.88, "text": " Immune to worrying about distributional assumptions, but we're not doing random forests. We're doing deep learning and deep learning does care", "tokens": [17322, 2613, 281, 18788, 466, 7316, 304, 17695, 11, 457, 321, 434, 406, 884, 4974, 21700, 13, 492, 434, 884, 2452, 2539, 293, 2452, 2539, 775, 1127], "temperature": 0.0, "avg_logprob": -0.24885876395485618, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.6700909327482805e-05}, {"id": 334, "seek": 155192, "start": 1564.8000000000002, "end": 1566.8000000000002, "text": " Can you pass it over there?", "tokens": [1664, 291, 1320, 309, 670, 456, 30], "temperature": 0.0, "avg_logprob": -0.24885876395485618, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.6700909327482805e-05}, {"id": 335, "seek": 155192, "start": 1567.76, "end": 1573.0800000000002, "text": " We have a parametric then we should scale if we have a non parametric then we should not do scale", "tokens": [492, 362, 257, 6220, 17475, 550, 321, 820, 4373, 498, 321, 362, 257, 2107, 6220, 17475, 550, 321, 820, 406, 360, 4373], "temperature": 0.0, "avg_logprob": -0.24885876395485618, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.6700909327482805e-05}, {"id": 336, "seek": 155192, "start": 1573.0800000000002, "end": 1580.04, "text": " Can we generalize no not quite right because like K nearest neighbors is non parametric and scale matters a hell of a lot", "tokens": [1664, 321, 2674, 1125, 572, 406, 1596, 558, 570, 411, 591, 23831, 12512, 307, 2107, 6220, 17475, 293, 4373, 7001, 257, 4921, 295, 257, 688], "temperature": 0.0, "avg_logprob": -0.24885876395485618, "compression_ratio": 1.8102766798418972, "no_speech_prob": 1.6700909327482805e-05}, {"id": 337, "seek": 158004, "start": 1580.04, "end": 1583.12, "text": " So I would say things involving trees", "tokens": [407, 286, 576, 584, 721, 17030, 5852], "temperature": 0.0, "avg_logprob": -0.1669495019568018, "compression_ratio": 1.5765765765765767, "no_speech_prob": 7.52793039282551e-06}, {"id": 338, "seek": 158004, "start": 1583.76, "end": 1587.46, "text": " Generally, you're just going to split at a point and so probably you don't care about scale", "tokens": [21082, 11, 291, 434, 445, 516, 281, 7472, 412, 257, 935, 293, 370, 1391, 291, 500, 380, 1127, 466, 4373], "temperature": 0.0, "avg_logprob": -0.1669495019568018, "compression_ratio": 1.5765765765765767, "no_speech_prob": 7.52793039282551e-06}, {"id": 339, "seek": 158004, "start": 1588.32, "end": 1595.76, "text": " But you know, you probably just need to think like is this an algorithm that uses order or does it use specific numbers?", "tokens": [583, 291, 458, 11, 291, 1391, 445, 643, 281, 519, 411, 307, 341, 364, 9284, 300, 4960, 1668, 420, 775, 309, 764, 2685, 3547, 30], "temperature": 0.0, "avg_logprob": -0.1669495019568018, "compression_ratio": 1.5765765765765767, "no_speech_prob": 7.52793039282551e-06}, {"id": 340, "seek": 158004, "start": 1598.28, "end": 1604.1599999999999, "text": " Can can you please give us an intuition of why it needs scale just because it's that we may clarify", "tokens": [1664, 393, 291, 1767, 976, 505, 364, 24002, 295, 983, 309, 2203, 4373, 445, 570, 309, 311, 300, 321, 815, 17594], "temperature": 0.0, "avg_logprob": -0.1669495019568018, "compression_ratio": 1.5765765765765767, "no_speech_prob": 7.52793039282551e-06}, {"id": 341, "seek": 160416, "start": 1604.16, "end": 1608.96, "text": " some of the issues not until we get to doing SGD", "tokens": [512, 295, 264, 2663, 406, 1826, 321, 483, 281, 884, 34520, 35], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 342, "seek": 160416, "start": 1608.96, "end": 1613.0, "text": " So we're going to get to that. Yeah, so for now, we're just going to say take my word for it", "tokens": [407, 321, 434, 516, 281, 483, 281, 300, 13, 865, 11, 370, 337, 586, 11, 321, 434, 445, 516, 281, 584, 747, 452, 1349, 337, 309], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 343, "seek": 160416, "start": 1613.0, "end": 1615.0, "text": " Okay, pass it to general", "tokens": [1033, 11, 1320, 309, 281, 2674], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 344, "seek": 160416, "start": 1615.0, "end": 1617.0, "text": " So this is probably a dumb question", "tokens": [407, 341, 307, 1391, 257, 10316, 1168], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 345, "seek": 160416, "start": 1617.0, "end": 1622.6000000000001, "text": " But can you like explain a little bit more what you mean by scale because I guess when I think of scale", "tokens": [583, 393, 291, 411, 2903, 257, 707, 857, 544, 437, 291, 914, 538, 4373, 570, 286, 2041, 562, 286, 519, 295, 4373], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 346, "seek": 160416, "start": 1622.6000000000001, "end": 1625.24, "text": " I'm like, oh all the numbers should be generally the same size", "tokens": [286, 478, 411, 11, 1954, 439, 264, 3547, 820, 312, 5101, 264, 912, 2744], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 347, "seek": 160416, "start": 1626.3600000000001, "end": 1627.5600000000002, "text": " That's exactly what we mean", "tokens": [663, 311, 2293, 437, 321, 914], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 348, "seek": 160416, "start": 1627.5600000000002, "end": 1633.24, "text": " But is that like the case like with the cats and dogs that we went over with like the deep learning like you could have", "tokens": [583, 307, 300, 411, 264, 1389, 411, 365, 264, 11111, 293, 7197, 300, 321, 1437, 670, 365, 411, 264, 2452, 2539, 411, 291, 727, 362], "temperature": 0.0, "avg_logprob": -0.15852827184340534, "compression_ratio": 1.7062706270627064, "no_speech_prob": 1.0783127436297946e-05}, {"id": 349, "seek": 163324, "start": 1633.24, "end": 1637.44, "text": " A small cat and like a larger cat, but it would still know that those were both cats", "tokens": [316, 1359, 3857, 293, 411, 257, 4833, 3857, 11, 457, 309, 576, 920, 458, 300, 729, 645, 1293, 11111], "temperature": 0.0, "avg_logprob": -0.13614248460338962, "compression_ratio": 1.6948051948051948, "no_speech_prob": 2.5215585992555134e-06}, {"id": 350, "seek": 163324, "start": 1637.44, "end": 1644.88, "text": " Oh, I guess you know, this is one of these problems where language gets overloaded. Yeah, so in computer vision when we scale an image", "tokens": [876, 11, 286, 2041, 291, 458, 11, 341, 307, 472, 295, 613, 2740, 689, 2856, 2170, 28777, 292, 13, 865, 11, 370, 294, 3820, 5201, 562, 321, 4373, 364, 3256], "temperature": 0.0, "avg_logprob": -0.13614248460338962, "compression_ratio": 1.6948051948051948, "no_speech_prob": 2.5215585992555134e-06}, {"id": 351, "seek": 163324, "start": 1644.88, "end": 1649.6, "text": " We're actually increasing the size of the cat in this case. We're scaling the actual", "tokens": [492, 434, 767, 5662, 264, 2744, 295, 264, 3857, 294, 341, 1389, 13, 492, 434, 21589, 264, 3539], "temperature": 0.0, "avg_logprob": -0.13614248460338962, "compression_ratio": 1.6948051948051948, "no_speech_prob": 2.5215585992555134e-06}, {"id": 352, "seek": 163324, "start": 1650.28, "end": 1651.76, "text": " pixel values", "tokens": [19261, 4190], "temperature": 0.0, "avg_logprob": -0.13614248460338962, "compression_ratio": 1.6948051948051948, "no_speech_prob": 2.5215585992555134e-06}, {"id": 353, "seek": 163324, "start": 1651.76, "end": 1655.84, "text": " So in both case scaling means to make something bigger and smaller in this case", "tokens": [407, 294, 1293, 1389, 21589, 1355, 281, 652, 746, 3801, 293, 4356, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.13614248460338962, "compression_ratio": 1.6948051948051948, "no_speech_prob": 2.5215585992555134e-06}, {"id": 354, "seek": 163324, "start": 1655.84, "end": 1662.44, "text": " We're taking numbers from naught to 255 and making them so that they have an average of zero and a standard deviation of one", "tokens": [492, 434, 1940, 3547, 490, 13138, 281, 3552, 20, 293, 1455, 552, 370, 300, 436, 362, 364, 4274, 295, 4018, 293, 257, 3832, 25163, 295, 472], "temperature": 0.0, "avg_logprob": -0.13614248460338962, "compression_ratio": 1.6948051948051948, "no_speech_prob": 2.5215585992555134e-06}, {"id": 355, "seek": 166244, "start": 1662.44, "end": 1664.44, "text": " Jeremy", "tokens": [17809], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 356, "seek": 166244, "start": 1664.76, "end": 1668.3200000000002, "text": " Could you please explain us? Is it by column by row?", "tokens": [7497, 291, 1767, 2903, 505, 30, 1119, 309, 538, 7738, 538, 5386, 30], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 357, "seek": 166244, "start": 1669.0, "end": 1670.6000000000001, "text": " by pixel", "tokens": [538, 19261], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 358, "seek": 166244, "start": 1670.6000000000001, "end": 1675.0, "text": " by pixel so there's a single general when you're scaling", "tokens": [538, 19261, 370, 456, 311, 257, 2167, 2674, 562, 291, 434, 21589], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 359, "seek": 166244, "start": 1675.76, "end": 1682.42, "text": " In just not thinking about every picture, but I'm kind of an input. Yeah much learning. So, okay. Yeah, sure", "tokens": [682, 445, 406, 1953, 466, 633, 3036, 11, 457, 286, 478, 733, 295, 364, 4846, 13, 865, 709, 2539, 13, 407, 11, 1392, 13, 865, 11, 988], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 360, "seek": 166244, "start": 1682.42, "end": 1686.96, "text": " So, I mean, it's a little bit subtle, but in this case, I've just got a single mean and a single standard deviation", "tokens": [407, 11, 286, 914, 11, 309, 311, 257, 707, 857, 13743, 11, 457, 294, 341, 1389, 11, 286, 600, 445, 658, 257, 2167, 914, 293, 257, 2167, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 361, "seek": 166244, "start": 1687.4, "end": 1689.4, "text": " Right, so it's basically on average", "tokens": [1779, 11, 370, 309, 311, 1936, 322, 4274], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 362, "seek": 166244, "start": 1690.28, "end": 1691.68, "text": " how", "tokens": [577], "temperature": 0.0, "avg_logprob": -0.2923707360619897, "compression_ratio": 1.5662650602409638, "no_speech_prob": 2.24737905227812e-05}, {"id": 363, "seek": 169168, "start": 1691.68, "end": 1693.5600000000002, "text": " How much black is there?", "tokens": [1012, 709, 2211, 307, 456, 30], "temperature": 0.0, "avg_logprob": -0.25681815912694106, "compression_ratio": 1.6391752577319587, "no_speech_prob": 2.5215647383447504e-06}, {"id": 364, "seek": 169168, "start": 1693.5600000000002, "end": 1699.5600000000002, "text": " Right and so on average, you know, we have a mean and a standard deviation", "tokens": [1779, 293, 370, 322, 4274, 11, 291, 458, 11, 321, 362, 257, 914, 293, 257, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.25681815912694106, "compression_ratio": 1.6391752577319587, "no_speech_prob": 2.5215647383447504e-06}, {"id": 365, "seek": 169168, "start": 1700.5600000000002, "end": 1702.5600000000002, "text": " across all the pixels", "tokens": [2108, 439, 264, 18668], "temperature": 0.0, "avg_logprob": -0.25681815912694106, "compression_ratio": 1.6391752577319587, "no_speech_prob": 2.5215647383447504e-06}, {"id": 366, "seek": 169168, "start": 1702.6000000000001, "end": 1705.64, "text": " In computer vision we would normally do it by channel", "tokens": [682, 3820, 5201, 321, 576, 5646, 360, 309, 538, 2269], "temperature": 0.0, "avg_logprob": -0.25681815912694106, "compression_ratio": 1.6391752577319587, "no_speech_prob": 2.5215647383447504e-06}, {"id": 367, "seek": 169168, "start": 1705.72, "end": 1709.96, "text": " So we would normally have one number for red one number for green one number for blue", "tokens": [407, 321, 576, 5646, 362, 472, 1230, 337, 2182, 472, 1230, 337, 3092, 472, 1230, 337, 3344], "temperature": 0.0, "avg_logprob": -0.25681815912694106, "compression_ratio": 1.6391752577319587, "no_speech_prob": 2.5215647383447504e-06}, {"id": 368, "seek": 169168, "start": 1711.0800000000002, "end": 1713.0800000000002, "text": " in", "tokens": [294], "temperature": 0.0, "avg_logprob": -0.25681815912694106, "compression_ratio": 1.6391752577319587, "no_speech_prob": 2.5215647383447504e-06}, {"id": 369, "seek": 169168, "start": 1713.44, "end": 1717.24, "text": " General you you need a different set of normalization", "tokens": [6996, 291, 291, 643, 257, 819, 992, 295, 2710, 2144], "temperature": 0.0, "avg_logprob": -0.25681815912694106, "compression_ratio": 1.6391752577319587, "no_speech_prob": 2.5215647383447504e-06}, {"id": 370, "seek": 171724, "start": 1717.24, "end": 1722.4, "text": " Normalization coefficients for each like each thing you would expect to behave differently", "tokens": [21277, 2144, 31994, 337, 1184, 411, 1184, 551, 291, 576, 2066, 281, 15158, 7614], "temperature": 0.0, "avg_logprob": -0.14715259273846945, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.4144713986752322e-06}, {"id": 371, "seek": 171724, "start": 1722.6, "end": 1725.92, "text": " So if we were doing like a structured data set where we've got like", "tokens": [407, 498, 321, 645, 884, 411, 257, 18519, 1412, 992, 689, 321, 600, 658, 411], "temperature": 0.0, "avg_logprob": -0.14715259273846945, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.4144713986752322e-06}, {"id": 372, "seek": 171724, "start": 1726.76, "end": 1728.32, "text": " income", "tokens": [5742], "temperature": 0.0, "avg_logprob": -0.14715259273846945, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.4144713986752322e-06}, {"id": 373, "seek": 171724, "start": 1728.32, "end": 1734.6, "text": " Distance in kilometers and number of children, but you need three separate normalization coefficients for those", "tokens": [9840, 719, 294, 13904, 293, 1230, 295, 2227, 11, 457, 291, 643, 1045, 4994, 2710, 2144, 31994, 337, 729], "temperature": 0.0, "avg_logprob": -0.14715259273846945, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.4144713986752322e-06}, {"id": 374, "seek": 171724, "start": 1734.6, "end": 1736.6, "text": " They're like very different kinds of things", "tokens": [814, 434, 411, 588, 819, 3685, 295, 721], "temperature": 0.0, "avg_logprob": -0.14715259273846945, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.4144713986752322e-06}, {"id": 375, "seek": 171724, "start": 1737.08, "end": 1743.8, "text": " So yeah, it's kind of like a bit domain specific here. It's like in this case all of the pixels are", "tokens": [407, 1338, 11, 309, 311, 733, 295, 411, 257, 857, 9274, 2685, 510, 13, 467, 311, 411, 294, 341, 1389, 439, 295, 264, 18668, 366], "temperature": 0.0, "avg_logprob": -0.14715259273846945, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.4144713986752322e-06}, {"id": 376, "seek": 174380, "start": 1743.8, "end": 1748.76, "text": " You know levels of gray. So we just got a single scaling number", "tokens": [509, 458, 4358, 295, 10855, 13, 407, 321, 445, 658, 257, 2167, 21589, 1230], "temperature": 0.0, "avg_logprob": -0.2190961516305302, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.384579354635207e-05}, {"id": 377, "seek": 174380, "start": 1749.04, "end": 1754.76, "text": " Where else you could imagine if they were red versus green versus blue you could need to scale those channels in different ways", "tokens": [2305, 1646, 291, 727, 3811, 498, 436, 645, 2182, 5717, 3092, 5717, 3344, 291, 727, 643, 281, 4373, 729, 9235, 294, 819, 2098], "temperature": 0.0, "avg_logprob": -0.2190961516305302, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.384579354635207e-05}, {"id": 378, "seek": 174380, "start": 1759.28, "end": 1764.34, "text": " So I'm having a bit of trouble imagining what would happen if we don't normalize in this case", "tokens": [407, 286, 478, 1419, 257, 857, 295, 5253, 27798, 437, 576, 1051, 498, 321, 500, 380, 2710, 1125, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.2190961516305302, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.384579354635207e-05}, {"id": 379, "seek": 174380, "start": 1765.1599999999999, "end": 1769.48, "text": " So when we will get there so for net so we so this is kind of what you net was saying", "tokens": [407, 562, 321, 486, 483, 456, 370, 337, 2533, 370, 321, 370, 341, 307, 733, 295, 437, 291, 2533, 390, 1566], "temperature": 0.0, "avg_logprob": -0.2190961516305302, "compression_ratio": 1.5854700854700854, "no_speech_prob": 1.384579354635207e-05}, {"id": 380, "seek": 176948, "start": 1769.48, "end": 1773.8, "text": " It's like why do we normalize and for now we're normalizing because I say we have to", "tokens": [467, 311, 411, 983, 360, 321, 2710, 1125, 293, 337, 586, 321, 434, 2710, 3319, 570, 286, 584, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.16274317470165567, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.4510258552036248e-05}, {"id": 381, "seek": 176948, "start": 1774.8600000000001, "end": 1780.68, "text": " When we get to looking at stochastic gradient descent, we'll basically discover that if you", "tokens": [1133, 321, 483, 281, 1237, 412, 342, 8997, 2750, 16235, 23475, 11, 321, 603, 1936, 4411, 300, 498, 291], "temperature": 0.0, "avg_logprob": -0.16274317470165567, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.4510258552036248e-05}, {"id": 382, "seek": 176948, "start": 1782.88, "end": 1787.24, "text": " Basically to skip ahead a little bit we're going to be doing a matrix multiply by a bunch of weights", "tokens": [8537, 281, 10023, 2286, 257, 707, 857, 321, 434, 516, 281, 312, 884, 257, 8141, 12972, 538, 257, 3840, 295, 17443], "temperature": 0.0, "avg_logprob": -0.16274317470165567, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.4510258552036248e-05}, {"id": 383, "seek": 176948, "start": 1787.52, "end": 1791.76, "text": " We're going to pick those weights in such a way that when we do the matrix multiply", "tokens": [492, 434, 516, 281, 1888, 729, 17443, 294, 1270, 257, 636, 300, 562, 321, 360, 264, 8141, 12972], "temperature": 0.0, "avg_logprob": -0.16274317470165567, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.4510258552036248e-05}, {"id": 384, "seek": 176948, "start": 1791.76, "end": 1795.88, "text": " we're going to try to keep the numbers at the same scale that this out has and", "tokens": [321, 434, 516, 281, 853, 281, 1066, 264, 3547, 412, 264, 912, 4373, 300, 341, 484, 575, 293], "temperature": 0.0, "avg_logprob": -0.16274317470165567, "compression_ratio": 1.746031746031746, "no_speech_prob": 1.4510258552036248e-05}, {"id": 385, "seek": 179588, "start": 1795.88, "end": 1801.48, "text": " That's going to basically require the initial numbers. We're going to have to know what their scale is", "tokens": [663, 311, 516, 281, 1936, 3651, 264, 5883, 3547, 13, 492, 434, 516, 281, 362, 281, 458, 437, 641, 4373, 307], "temperature": 0.0, "avg_logprob": -0.13709167741302752, "compression_ratio": 1.7236842105263157, "no_speech_prob": 3.2887364795897156e-06}, {"id": 386, "seek": 179588, "start": 1802.0, "end": 1806.4, "text": " So basically it's much easier to create a single kind of neural network architecture", "tokens": [407, 1936, 309, 311, 709, 3571, 281, 1884, 257, 2167, 733, 295, 18161, 3209, 9482], "temperature": 0.0, "avg_logprob": -0.13709167741302752, "compression_ratio": 1.7236842105263157, "no_speech_prob": 3.2887364795897156e-06}, {"id": 387, "seek": 179588, "start": 1807.0800000000002, "end": 1814.24, "text": " That works for lots of different kinds of inputs if we know that they're consistently going to be mean zero standard deviation one", "tokens": [663, 1985, 337, 3195, 295, 819, 3685, 295, 15743, 498, 321, 458, 300, 436, 434, 14961, 516, 281, 312, 914, 4018, 3832, 25163, 472], "temperature": 0.0, "avg_logprob": -0.13709167741302752, "compression_ratio": 1.7236842105263157, "no_speech_prob": 3.2887364795897156e-06}, {"id": 388, "seek": 179588, "start": 1814.8000000000002, "end": 1819.8400000000001, "text": " That would be the short answer, but we'll learn a lot more about it and if in a couple of lessons", "tokens": [663, 576, 312, 264, 2099, 1867, 11, 457, 321, 603, 1466, 257, 688, 544, 466, 309, 293, 498, 294, 257, 1916, 295, 8820], "temperature": 0.0, "avg_logprob": -0.13709167741302752, "compression_ratio": 1.7236842105263157, "no_speech_prob": 3.2887364795897156e-06}, {"id": 389, "seek": 181984, "start": 1819.84, "end": 1825.86, "text": " You're still not quite sure why let's come back to it because it's a really interesting thing to talk about", "tokens": [509, 434, 920, 406, 1596, 988, 983, 718, 311, 808, 646, 281, 309, 570, 309, 311, 257, 534, 1880, 551, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.19376719727808114, "compression_ratio": 1.5, "no_speech_prob": 2.2473512217402458e-05}, {"id": 390, "seek": 181984, "start": 1827.84, "end": 1834.9599999999998, "text": " Yes, I'm just trying to visualize the axes we're working with here so under plots when you when you write so x-valid shape", "tokens": [1079, 11, 286, 478, 445, 1382, 281, 23273, 264, 35387, 321, 434, 1364, 365, 510, 370, 833, 28609, 562, 291, 562, 291, 2464, 370, 2031, 12, 3337, 327, 3909], "temperature": 0.0, "avg_logprob": -0.19376719727808114, "compression_ratio": 1.5, "no_speech_prob": 2.2473512217402458e-05}, {"id": 391, "seek": 181984, "start": 1834.9599999999998, "end": 1842.1399999999999, "text": " We get 10,000 by 7 8 4. Yeah, I mean that we brought in 10,000 pictures. Yeah of that dimension exactly", "tokens": [492, 483, 1266, 11, 1360, 538, 1614, 1649, 1017, 13, 865, 11, 286, 914, 300, 321, 3038, 294, 1266, 11, 1360, 5242, 13, 865, 295, 300, 10139, 2293], "temperature": 0.0, "avg_logprob": -0.19376719727808114, "compression_ratio": 1.5, "no_speech_prob": 2.2473512217402458e-05}, {"id": 392, "seek": 181984, "start": 1842.1599999999999, "end": 1844.1599999999999, "text": " Okay, and then in the next line", "tokens": [1033, 11, 293, 550, 294, 264, 958, 1622], "temperature": 0.0, "avg_logprob": -0.19376719727808114, "compression_ratio": 1.5, "no_speech_prob": 2.2473512217402458e-05}, {"id": 393, "seek": 184416, "start": 1844.16, "end": 1849.42, "text": " When you choose to reshape it is there a reason why you put 28 28 on as a", "tokens": [1133, 291, 2826, 281, 725, 42406, 309, 307, 456, 257, 1778, 983, 291, 829, 7562, 7562, 322, 382, 257], "temperature": 0.0, "avg_logprob": -0.23339605823005596, "compression_ratio": 1.5695652173913044, "no_speech_prob": 3.3405160593247274e-06}, {"id": 394, "seek": 184416, "start": 1849.98, "end": 1854.5, "text": " Y or z coordinates or is there a reason why they're in that order? Yeah, there is", "tokens": [398, 420, 710, 21056, 420, 307, 456, 257, 1778, 983, 436, 434, 294, 300, 1668, 30, 865, 11, 456, 307], "temperature": 0.0, "avg_logprob": -0.23339605823005596, "compression_ratio": 1.5695652173913044, "no_speech_prob": 3.3405160593247274e-06}, {"id": 395, "seek": 184416, "start": 1856.14, "end": 1858.14, "text": " Pretty much all", "tokens": [10693, 709, 439], "temperature": 0.0, "avg_logprob": -0.23339605823005596, "compression_ratio": 1.5695652173913044, "no_speech_prob": 3.3405160593247274e-06}, {"id": 396, "seek": 184416, "start": 1858.3000000000002, "end": 1862.02, "text": " Neural network libraries assume that the first axis is", "tokens": [1734, 1807, 3209, 15148, 6552, 300, 264, 700, 10298, 307], "temperature": 0.0, "avg_logprob": -0.23339605823005596, "compression_ratio": 1.5695652173913044, "no_speech_prob": 3.3405160593247274e-06}, {"id": 397, "seek": 184416, "start": 1862.7, "end": 1867.8200000000002, "text": " Like kind of the equivalent of a row. It's like a separate thing. It's a sentence or an image or", "tokens": [1743, 733, 295, 264, 10344, 295, 257, 5386, 13, 467, 311, 411, 257, 4994, 551, 13, 467, 311, 257, 8174, 420, 364, 3256, 420], "temperature": 0.0, "avg_logprob": -0.23339605823005596, "compression_ratio": 1.5695652173913044, "no_speech_prob": 3.3405160593247274e-06}, {"id": 398, "seek": 184416, "start": 1868.7, "end": 1872.22, "text": " You know example of sales or whatever", "tokens": [509, 458, 1365, 295, 5763, 420, 2035], "temperature": 0.0, "avg_logprob": -0.23339605823005596, "compression_ratio": 1.5695652173913044, "no_speech_prob": 3.3405160593247274e-06}, {"id": 399, "seek": 187222, "start": 1872.22, "end": 1878.54, "text": " So I want each image, you know to be a separate item of the first axis", "tokens": [407, 286, 528, 1184, 3256, 11, 291, 458, 281, 312, 257, 4994, 3174, 295, 264, 700, 10298], "temperature": 0.0, "avg_logprob": -0.1503150553642949, "compression_ratio": 1.5175879396984924, "no_speech_prob": 3.844896582450019e-06}, {"id": 400, "seek": 187222, "start": 1879.42, "end": 1885.46, "text": " And then so that leaves two more axes for the rows and columns of the images and that's pretty standard. That's totally standard", "tokens": [400, 550, 370, 300, 5510, 732, 544, 35387, 337, 264, 13241, 293, 13766, 295, 264, 5267, 293, 300, 311, 1238, 3832, 13, 663, 311, 3879, 3832], "temperature": 0.0, "avg_logprob": -0.1503150553642949, "compression_ratio": 1.5175879396984924, "no_speech_prob": 3.844896582450019e-06}, {"id": 401, "seek": 187222, "start": 1885.46, "end": 1888.22, "text": " Yeah, I don't think I've ever seen a library that doesn't work that way", "tokens": [865, 11, 286, 500, 380, 519, 286, 600, 1562, 1612, 257, 6405, 300, 1177, 380, 589, 300, 636], "temperature": 0.0, "avg_logprob": -0.1503150553642949, "compression_ratio": 1.5175879396984924, "no_speech_prob": 3.844896582450019e-06}, {"id": 402, "seek": 187222, "start": 1889.06, "end": 1891.06, "text": " Can you pass it to our bureau?", "tokens": [1664, 291, 1320, 309, 281, 527, 35343, 30], "temperature": 0.0, "avg_logprob": -0.1503150553642949, "compression_ratio": 1.5175879396984924, "no_speech_prob": 3.844896582450019e-06}, {"id": 403, "seek": 189106, "start": 1891.06, "end": 1896.1399999999999, "text": " So while normalizing the validation data I saw you have", "tokens": [407, 1339, 2710, 3319, 264, 24071, 1412, 286, 1866, 291, 362], "temperature": 0.0, "avg_logprob": -0.5119131469726562, "compression_ratio": 1.7591623036649215, "no_speech_prob": 6.400766869774088e-05}, {"id": 404, "seek": 189106, "start": 1896.86, "end": 1902.34, "text": " Used mean of X and standard deviation of X data training data only. Yes", "tokens": [43237, 914, 295, 1783, 293, 3832, 25163, 295, 1783, 1412, 3097, 1412, 787, 13, 1079], "temperature": 0.0, "avg_logprob": -0.5119131469726562, "compression_ratio": 1.7591623036649215, "no_speech_prob": 6.400766869774088e-05}, {"id": 405, "seek": 189106, "start": 1902.34, "end": 1906.34, "text": " So shouldn't we use mean and standard deviation of validation data?", "tokens": [407, 4659, 380, 321, 764, 914, 293, 3832, 25163, 295, 24071, 1412, 30], "temperature": 0.0, "avg_logprob": -0.5119131469726562, "compression_ratio": 1.7591623036649215, "no_speech_prob": 6.400766869774088e-05}, {"id": 406, "seek": 189106, "start": 1907.4199999999998, "end": 1915.58, "text": " You mean like join them together or the separately calculating mean no because you see then you would be normalizing the value of the values", "tokens": [509, 914, 411, 3917, 552, 1214, 420, 264, 14759, 28258, 914, 572, 570, 291, 536, 550, 291, 576, 312, 2710, 3319, 264, 2158, 295, 264, 4190], "temperature": 0.0, "avg_logprob": -0.5119131469726562, "compression_ratio": 1.7591623036649215, "no_speech_prob": 6.400766869774088e-05}, {"id": 407, "seek": 191558, "start": 1915.58, "end": 1921.8999999999999, "text": " Or the separately calculating mean no because you see then you would be normalizing the validation set", "tokens": [1610, 264, 14759, 28258, 914, 572, 570, 291, 536, 550, 291, 576, 312, 2710, 3319, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.17199231138323792, "compression_ratio": 1.9308755760368663, "no_speech_prob": 6.438994205382187e-06}, {"id": 408, "seek": 191558, "start": 1922.22, "end": 1925.06, "text": " using different numbers and so now the meaning of like", "tokens": [1228, 819, 3547, 293, 370, 586, 264, 3620, 295, 411], "temperature": 0.0, "avg_logprob": -0.17199231138323792, "compression_ratio": 1.9308755760368663, "no_speech_prob": 6.438994205382187e-06}, {"id": 409, "seek": 191558, "start": 1925.4199999999998, "end": 1933.22, "text": " This this pixel has a value of 3 in the validation set has a different meaning to the meaning of 3 in the in the training", "tokens": [639, 341, 19261, 575, 257, 2158, 295, 805, 294, 264, 24071, 992, 575, 257, 819, 3620, 281, 264, 3620, 295, 805, 294, 264, 294, 264, 3097], "temperature": 0.0, "avg_logprob": -0.17199231138323792, "compression_ratio": 1.9308755760368663, "no_speech_prob": 6.438994205382187e-06}, {"id": 410, "seek": 191558, "start": 1933.22, "end": 1935.22, "text": " set it would be like", "tokens": [992, 309, 576, 312, 411], "temperature": 0.0, "avg_logprob": -0.17199231138323792, "compression_ratio": 1.9308755760368663, "no_speech_prob": 6.438994205382187e-06}, {"id": 411, "seek": 191558, "start": 1935.5, "end": 1937.96, "text": " If we had like days of the week", "tokens": [759, 321, 632, 411, 1708, 295, 264, 1243], "temperature": 0.0, "avg_logprob": -0.17199231138323792, "compression_ratio": 1.9308755760368663, "no_speech_prob": 6.438994205382187e-06}, {"id": 412, "seek": 191558, "start": 1938.54, "end": 1944.82, "text": " Encoded such that Monday was a 1 in the training set and was a 0 in the validation set", "tokens": [29584, 12340, 1270, 300, 8138, 390, 257, 502, 294, 264, 3097, 992, 293, 390, 257, 1958, 294, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.17199231138323792, "compression_ratio": 1.9308755760368663, "no_speech_prob": 6.438994205382187e-06}, {"id": 413, "seek": 194482, "start": 1944.82, "end": 1950.22, "text": " We've got now two different sets where the same number has a different meaning so we want to make sure", "tokens": [492, 600, 658, 586, 732, 819, 6352, 689, 264, 912, 1230, 575, 257, 819, 3620, 370, 321, 528, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.19801366447222116, "compression_ratio": 1.6374501992031874, "no_speech_prob": 5.255361429590266e-06}, {"id": 414, "seek": 194482, "start": 1950.82, "end": 1952.82, "text": " That we so let me give you an example", "tokens": [663, 321, 370, 718, 385, 976, 291, 364, 1365], "temperature": 0.0, "avg_logprob": -0.19801366447222116, "compression_ratio": 1.6374501992031874, "no_speech_prob": 5.255361429590266e-06}, {"id": 415, "seek": 194482, "start": 1953.46, "end": 1956.2, "text": " Let's say we were doing like full color images", "tokens": [961, 311, 584, 321, 645, 884, 411, 1577, 2017, 5267], "temperature": 0.0, "avg_logprob": -0.19801366447222116, "compression_ratio": 1.6374501992031874, "no_speech_prob": 5.255361429590266e-06}, {"id": 416, "seek": 194482, "start": 1956.82, "end": 1958.82, "text": " and our", "tokens": [293, 527], "temperature": 0.0, "avg_logprob": -0.19801366447222116, "compression_ratio": 1.6374501992031874, "no_speech_prob": 5.255361429590266e-06}, {"id": 417, "seek": 194482, "start": 1958.86, "end": 1965.34, "text": " Tests their training set can contain like green frogs green snakes and gray elephants, right?", "tokens": [314, 4409, 641, 3097, 992, 393, 5304, 411, 3092, 37107, 3092, 21817, 293, 10855, 33015, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19801366447222116, "compression_ratio": 1.6374501992031874, "no_speech_prob": 5.255361429590266e-06}, {"id": 418, "seek": 194482, "start": 1965.34, "end": 1971.06, "text": " We're trying to figure out which was which and we normalized using you know the each each channel mean and", "tokens": [492, 434, 1382, 281, 2573, 484, 597, 390, 597, 293, 321, 48704, 1228, 291, 458, 264, 1184, 1184, 2269, 914, 293], "temperature": 0.0, "avg_logprob": -0.19801366447222116, "compression_ratio": 1.6374501992031874, "no_speech_prob": 5.255361429590266e-06}, {"id": 419, "seek": 194482, "start": 1971.78, "end": 1973.78, "text": " then we have a", "tokens": [550, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.19801366447222116, "compression_ratio": 1.6374501992031874, "no_speech_prob": 5.255361429590266e-06}, {"id": 420, "seek": 197378, "start": 1973.78, "end": 1978.46, "text": " Validation set and the test set which are just green frogs and green snakes", "tokens": [7188, 327, 399, 992, 293, 264, 1500, 992, 597, 366, 445, 3092, 37107, 293, 3092, 21817], "temperature": 0.0, "avg_logprob": -0.17714878608440532, "compression_ratio": 1.8208955223880596, "no_speech_prob": 3.340524017403368e-06}, {"id": 421, "seek": 197378, "start": 1979.3799999999999, "end": 1982.82, "text": " So if we were to normalize by the validation sets", "tokens": [407, 498, 321, 645, 281, 2710, 1125, 538, 264, 24071, 6352], "temperature": 0.0, "avg_logprob": -0.17714878608440532, "compression_ratio": 1.8208955223880596, "no_speech_prob": 3.340524017403368e-06}, {"id": 422, "seek": 197378, "start": 1984.22, "end": 1990.22, "text": " Statistics we would end up saying things on average are green and so we would like remove all the greenness out and", "tokens": [49226, 321, 576, 917, 493, 1566, 721, 322, 4274, 366, 3092, 293, 370, 321, 576, 411, 4159, 439, 264, 3092, 1287, 484, 293], "temperature": 0.0, "avg_logprob": -0.17714878608440532, "compression_ratio": 1.8208955223880596, "no_speech_prob": 3.340524017403368e-06}, {"id": 423, "seek": 197378, "start": 1990.74, "end": 1994.42, "text": " So we would now fail to recognize the green frogs and the green", "tokens": [407, 321, 576, 586, 3061, 281, 5521, 264, 3092, 37107, 293, 264, 3092], "temperature": 0.0, "avg_logprob": -0.17714878608440532, "compression_ratio": 1.8208955223880596, "no_speech_prob": 3.340524017403368e-06}, {"id": 424, "seek": 197378, "start": 1994.94, "end": 1998.74, "text": " Snakes effectively right so we actually want to use the same", "tokens": [9264, 3419, 8659, 558, 370, 321, 767, 528, 281, 764, 264, 912], "temperature": 0.0, "avg_logprob": -0.17714878608440532, "compression_ratio": 1.8208955223880596, "no_speech_prob": 3.340524017403368e-06}, {"id": 425, "seek": 199874, "start": 1998.74, "end": 2003.94, "text": " Normalization coefficients that we were training on and for those of you during the deep learning class", "tokens": [21277, 2144, 31994, 300, 321, 645, 3097, 322, 293, 337, 729, 295, 291, 1830, 264, 2452, 2539, 1508], "temperature": 0.0, "avg_logprob": -0.15814310709635418, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.726589739017072e-06}, {"id": 426, "seek": 199874, "start": 2004.18, "end": 2007.16, "text": " We actually go further than that when we use a pre trained network", "tokens": [492, 767, 352, 3052, 813, 300, 562, 321, 764, 257, 659, 8895, 3209], "temperature": 0.0, "avg_logprob": -0.15814310709635418, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.726589739017072e-06}, {"id": 427, "seek": 199874, "start": 2007.22, "end": 2014.9, "text": " We have to use the same normalization coefficients that the original authors trained on so the idea is that you know that a number", "tokens": [492, 362, 281, 764, 264, 912, 2710, 2144, 31994, 300, 264, 3380, 16552, 8895, 322, 370, 264, 1558, 307, 300, 291, 458, 300, 257, 1230], "temperature": 0.0, "avg_logprob": -0.15814310709635418, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.726589739017072e-06}, {"id": 428, "seek": 199874, "start": 2015.5, "end": 2020.54, "text": " Needs to have this consistent meaning across every data set where you use it", "tokens": [1734, 5147, 281, 362, 341, 8398, 3620, 2108, 633, 1412, 992, 689, 291, 764, 309], "temperature": 0.0, "avg_logprob": -0.15814310709635418, "compression_ratio": 1.777292576419214, "no_speech_prob": 3.726589739017072e-06}, {"id": 429, "seek": 202054, "start": 2020.54, "end": 2027.54, "text": " Now can you pass it to us meter?", "tokens": [823, 393, 291, 1320, 309, 281, 505, 9255, 30], "temperature": 0.0, "avg_logprob": -0.27992552518844604, "compression_ratio": 1.5, "no_speech_prob": 5.954796506557614e-06}, {"id": 430, "seek": 202054, "start": 2028.94, "end": 2035.68, "text": " That means when you're looking at the test set you normalize the test set based on this this mean it's that's right", "tokens": [663, 1355, 562, 291, 434, 1237, 412, 264, 1500, 992, 291, 2710, 1125, 264, 1500, 992, 2361, 322, 341, 341, 914, 309, 311, 300, 311, 558], "temperature": 0.0, "avg_logprob": -0.27992552518844604, "compression_ratio": 1.5, "no_speech_prob": 5.954796506557614e-06}, {"id": 431, "seek": 202054, "start": 2037.3, "end": 2039.3, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.27992552518844604, "compression_ratio": 1.5, "no_speech_prob": 5.954796506557614e-06}, {"id": 432, "seek": 202054, "start": 2043.34, "end": 2044.58, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.27992552518844604, "compression_ratio": 1.5, "no_speech_prob": 5.954796506557614e-06}, {"id": 433, "seek": 204458, "start": 2044.58, "end": 2051.86, "text": " Here's a you know so so the valid validation y values are just rank one tensor of 10,000", "tokens": [1692, 311, 257, 291, 458, 370, 370, 264, 7363, 24071, 288, 4190, 366, 445, 6181, 472, 40863, 295, 1266, 11, 1360], "temperature": 0.0, "avg_logprob": -0.15123559747423446, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.443978473820607e-06}, {"id": 434, "seek": 204458, "start": 2052.02, "end": 2058.34, "text": " Remember, there's this kind of weird Python thing where a tuple with just one thing in it needs a trailing comma", "tokens": [5459, 11, 456, 311, 341, 733, 295, 3657, 15329, 551, 689, 257, 2604, 781, 365, 445, 472, 551, 294, 309, 2203, 257, 944, 4883, 22117], "temperature": 0.0, "avg_logprob": -0.15123559747423446, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.443978473820607e-06}, {"id": 435, "seek": 204458, "start": 2058.98, "end": 2065.7799999999997, "text": " Okay, so this is a rank one tensor of length 10,000, and so here's an example of something from that. It's just the number three", "tokens": [1033, 11, 370, 341, 307, 257, 6181, 472, 40863, 295, 4641, 1266, 11, 1360, 11, 293, 370, 510, 311, 364, 1365, 295, 746, 490, 300, 13, 467, 311, 445, 264, 1230, 1045], "temperature": 0.0, "avg_logprob": -0.15123559747423446, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.443978473820607e-06}, {"id": 436, "seek": 204458, "start": 2066.74, "end": 2071.2799999999997, "text": " So that's our labels, so here's another thing you need to be able to do in your sleep", "tokens": [407, 300, 311, 527, 16949, 11, 370, 510, 311, 1071, 551, 291, 643, 281, 312, 1075, 281, 360, 294, 428, 2817], "temperature": 0.0, "avg_logprob": -0.15123559747423446, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.443978473820607e-06}, {"id": 437, "seek": 204458, "start": 2072.22, "end": 2073.58, "text": " slicing", "tokens": [46586], "temperature": 0.0, "avg_logprob": -0.15123559747423446, "compression_ratio": 1.7096774193548387, "no_speech_prob": 2.443978473820607e-06}, {"id": 438, "seek": 207358, "start": 2073.58, "end": 2075.58, "text": " Into a tensor", "tokens": [23373, 257, 40863], "temperature": 0.0, "avg_logprob": -0.17736615525915267, "compression_ratio": 1.776190476190476, "no_speech_prob": 6.540410140587483e-06}, {"id": 439, "seek": 207358, "start": 2075.58, "end": 2078.38, "text": " So in this case we're slicing into the first axis", "tokens": [407, 294, 341, 1389, 321, 434, 46586, 666, 264, 700, 10298], "temperature": 0.0, "avg_logprob": -0.17736615525915267, "compression_ratio": 1.776190476190476, "no_speech_prob": 6.540410140587483e-06}, {"id": 440, "seek": 207358, "start": 2079.18, "end": 2082.7799999999997, "text": " With zero so that means we're grabbing the first slice", "tokens": [2022, 4018, 370, 300, 1355, 321, 434, 23771, 264, 700, 13153], "temperature": 0.0, "avg_logprob": -0.17736615525915267, "compression_ratio": 1.776190476190476, "no_speech_prob": 6.540410140587483e-06}, {"id": 441, "seek": 207358, "start": 2083.8199999999997, "end": 2087.98, "text": " So because this is a single number. This is going to reduce the rank of the tensor by one", "tokens": [407, 570, 341, 307, 257, 2167, 1230, 13, 639, 307, 516, 281, 5407, 264, 6181, 295, 264, 40863, 538, 472], "temperature": 0.0, "avg_logprob": -0.17736615525915267, "compression_ratio": 1.776190476190476, "no_speech_prob": 6.540410140587483e-06}, {"id": 442, "seek": 207358, "start": 2088.1, "end": 2093.54, "text": " It's going to turn it from a three dimensional tensor into a two dimensional tensor right so you can see here", "tokens": [467, 311, 516, 281, 1261, 309, 490, 257, 1045, 18795, 40863, 666, 257, 732, 18795, 40863, 558, 370, 291, 393, 536, 510], "temperature": 0.0, "avg_logprob": -0.17736615525915267, "compression_ratio": 1.776190476190476, "no_speech_prob": 6.540410140587483e-06}, {"id": 443, "seek": 207358, "start": 2093.54, "end": 2097.56, "text": " This is now just a matrix and then we're going to grab", "tokens": [639, 307, 586, 445, 257, 8141, 293, 550, 321, 434, 516, 281, 4444], "temperature": 0.0, "avg_logprob": -0.17736615525915267, "compression_ratio": 1.776190476190476, "no_speech_prob": 6.540410140587483e-06}, {"id": 444, "seek": 209756, "start": 2097.56, "end": 2104.7999999999997, "text": " 10 through 14 inclusive rows 10 through 14 inclusive columns and here it is right, so this is the kind of thing", "tokens": [1266, 807, 3499, 13429, 13241, 1266, 807, 3499, 13429, 13766, 293, 510, 309, 307, 558, 11, 370, 341, 307, 264, 733, 295, 551], "temperature": 0.0, "avg_logprob": -0.1658318959749662, "compression_ratio": 1.7807692307692307, "no_speech_prob": 7.690368875046261e-07}, {"id": 445, "seek": 209756, "start": 2105.36, "end": 2109.7599999999998, "text": " you need to be super comfortable like grabbing pieces out looking at the numbers and", "tokens": [291, 643, 281, 312, 1687, 4619, 411, 23771, 3755, 484, 1237, 412, 264, 3547, 293], "temperature": 0.0, "avg_logprob": -0.1658318959749662, "compression_ratio": 1.7807692307692307, "no_speech_prob": 7.690368875046261e-07}, {"id": 446, "seek": 209756, "start": 2111.0, "end": 2116.68, "text": " Looking at the picture right so here's an example of a little piece of that first image", "tokens": [11053, 412, 264, 3036, 558, 370, 510, 311, 364, 1365, 295, 257, 707, 2522, 295, 300, 700, 3256], "temperature": 0.0, "avg_logprob": -0.1658318959749662, "compression_ratio": 1.7807692307692307, "no_speech_prob": 7.690368875046261e-07}, {"id": 447, "seek": 209756, "start": 2117.4, "end": 2119.4, "text": " and so", "tokens": [293, 370], "temperature": 0.0, "avg_logprob": -0.1658318959749662, "compression_ratio": 1.7807692307692307, "no_speech_prob": 7.690368875046261e-07}, {"id": 448, "seek": 209756, "start": 2119.96, "end": 2124.04, "text": " You kind of want to get used to this idea that if you're working with something like pictures or audio", "tokens": [509, 733, 295, 528, 281, 483, 1143, 281, 341, 1558, 300, 498, 291, 434, 1364, 365, 746, 411, 5242, 420, 6278], "temperature": 0.0, "avg_logprob": -0.1658318959749662, "compression_ratio": 1.7807692307692307, "no_speech_prob": 7.690368875046261e-07}, {"id": 449, "seek": 212404, "start": 2124.04, "end": 2131.56, "text": " You know this is something your brain is really good at interpreting right so like keep showing pictures of what you're doing whenever you can", "tokens": [509, 458, 341, 307, 746, 428, 3567, 307, 534, 665, 412, 37395, 558, 370, 411, 1066, 4099, 5242, 295, 437, 291, 434, 884, 5699, 291, 393], "temperature": 0.0, "avg_logprob": -0.16158175724808888, "compression_ratio": 1.7193675889328064, "no_speech_prob": 1.209862148243701e-06}, {"id": 450, "seek": 212404, "start": 2132.6, "end": 2135.02, "text": " But also remember behind the scenes their numbers", "tokens": [583, 611, 1604, 2261, 264, 8026, 641, 3547], "temperature": 0.0, "avg_logprob": -0.16158175724808888, "compression_ratio": 1.7193675889328064, "no_speech_prob": 1.209862148243701e-06}, {"id": 451, "seek": 212404, "start": 2135.32, "end": 2141.7599999999998, "text": " So like if something's going weird print out a few of the actual numbers you might find somehow some of them have become infinity", "tokens": [407, 411, 498, 746, 311, 516, 3657, 4482, 484, 257, 1326, 295, 264, 3539, 3547, 291, 1062, 915, 6063, 512, 295, 552, 362, 1813, 13202], "temperature": 0.0, "avg_logprob": -0.16158175724808888, "compression_ratio": 1.7193675889328064, "no_speech_prob": 1.209862148243701e-06}, {"id": 452, "seek": 212404, "start": 2141.7599999999998, "end": 2147.6, "text": " Or they're all zero or whatever right so like use this interactive environment", "tokens": [1610, 436, 434, 439, 4018, 420, 2035, 558, 370, 411, 764, 341, 15141, 2823], "temperature": 0.0, "avg_logprob": -0.16158175724808888, "compression_ratio": 1.7193675889328064, "no_speech_prob": 1.209862148243701e-06}, {"id": 453, "seek": 212404, "start": 2148.04, "end": 2151.04, "text": " And to explore the data as you go", "tokens": [400, 281, 6839, 264, 1412, 382, 291, 352], "temperature": 0.0, "avg_logprob": -0.16158175724808888, "compression_ratio": 1.7193675889328064, "no_speech_prob": 1.209862148243701e-06}, {"id": 454, "seek": 215104, "start": 2151.04, "end": 2156.04, "text": " Did you have a question where's the box up?", "tokens": [2589, 291, 362, 257, 1168, 689, 311, 264, 2424, 493, 30], "temperature": 0.0, "avg_logprob": -0.3123260883802778, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.028935639624251e-05}, {"id": 455, "seek": 215104, "start": 2157.96, "end": 2159.96, "text": " Just a quick I guess", "tokens": [1449, 257, 1702, 286, 2041], "temperature": 0.0, "avg_logprob": -0.3123260883802778, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.028935639624251e-05}, {"id": 456, "seek": 215104, "start": 2161.24, "end": 2168.84, "text": " Semantic question why when it's a tensor of rank 3 is it stored as like xyz instead of?", "tokens": [14421, 7128, 1168, 983, 562, 309, 311, 257, 40863, 295, 6181, 805, 307, 309, 12187, 382, 411, 2031, 37433, 2602, 295, 30], "temperature": 0.0, "avg_logprob": -0.3123260883802778, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.028935639624251e-05}, {"id": 457, "seek": 215104, "start": 2168.92, "end": 2174.48, "text": " Like to me it would make more sense to store it as like a list of like 2d tensors", "tokens": [1743, 281, 385, 309, 576, 652, 544, 2020, 281, 3531, 309, 382, 411, 257, 1329, 295, 411, 568, 67, 10688, 830], "temperature": 0.0, "avg_logprob": -0.3123260883802778, "compression_ratio": 1.5870646766169154, "no_speech_prob": 1.028935639624251e-05}, {"id": 458, "seek": 217448, "start": 2174.48, "end": 2181.72, "text": " It's not stored as either right so I'm for the formatting because let's look at this as a 3d", "tokens": [467, 311, 406, 12187, 382, 2139, 558, 370, 286, 478, 337, 264, 39366, 570, 718, 311, 574, 412, 341, 382, 257, 805, 67], "temperature": 0.0, "avg_logprob": -0.1663759343764361, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.2603182969760383e-06}, {"id": 459, "seek": 217448, "start": 2182.6, "end": 2184.52, "text": " Okay, so here's a 3d", "tokens": [1033, 11, 370, 510, 311, 257, 805, 67], "temperature": 0.0, "avg_logprob": -0.1663759343764361, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.2603182969760383e-06}, {"id": 460, "seek": 217448, "start": 2184.52, "end": 2191.28, "text": " Right so a 3d tensor is formatted as showing a list of 2d tensors basically", "tokens": [1779, 370, 257, 805, 67, 40863, 307, 1254, 32509, 382, 4099, 257, 1329, 295, 568, 67, 10688, 830, 1936], "temperature": 0.0, "avg_logprob": -0.1663759343764361, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.2603182969760383e-06}, {"id": 461, "seek": 217448, "start": 2191.28, "end": 2197.96, "text": " But when you're extracting it why isn't it like if you're extracting the first one why isn't it x images?", "tokens": [583, 562, 291, 434, 49844, 309, 983, 1943, 380, 309, 411, 498, 291, 434, 49844, 264, 700, 472, 983, 1943, 380, 309, 2031, 5267, 30], "temperature": 0.0, "avg_logprob": -0.1663759343764361, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.2603182969760383e-06}, {"id": 462, "seek": 219796, "start": 2197.96, "end": 2206.28, "text": " Square bracket zero closed square brackets, and then a second set of square because that has a different meaning right so", "tokens": [16463, 16904, 4018, 5395, 3732, 26179, 11, 293, 550, 257, 1150, 992, 295, 3732, 570, 300, 575, 257, 819, 3620, 558, 370], "temperature": 0.0, "avg_logprob": -0.17981741163465711, "compression_ratio": 1.8767772511848342, "no_speech_prob": 1.6028053551053745e-06}, {"id": 463, "seek": 219796, "start": 2207.52, "end": 2209.52, "text": " It's kind of the difference between", "tokens": [467, 311, 733, 295, 264, 2649, 1296], "temperature": 0.0, "avg_logprob": -0.17981741163465711, "compression_ratio": 1.8767772511848342, "no_speech_prob": 1.6028053551053745e-06}, {"id": 464, "seek": 219796, "start": 2211.0, "end": 2215.12, "text": " Tenses and jagged arrays right so basically if you do like something like", "tokens": [314, 9085, 293, 6368, 3004, 41011, 558, 370, 1936, 498, 291, 360, 411, 746, 411], "temperature": 0.0, "avg_logprob": -0.17981741163465711, "compression_ratio": 1.8767772511848342, "no_speech_prob": 1.6028053551053745e-06}, {"id": 465, "seek": 219796, "start": 2217.44, "end": 2221.28, "text": " Something like that that says take the second list item and", "tokens": [6595, 411, 300, 300, 1619, 747, 264, 1150, 1329, 3174, 293], "temperature": 0.0, "avg_logprob": -0.17981741163465711, "compression_ratio": 1.8767772511848342, "no_speech_prob": 1.6028053551053745e-06}, {"id": 466, "seek": 219796, "start": 2221.52, "end": 2226.2400000000002, "text": " From it grab the third list item and so we tend to use that when we have something called a jagged array", "tokens": [3358, 309, 4444, 264, 2636, 1329, 3174, 293, 370, 321, 3928, 281, 764, 300, 562, 321, 362, 746, 1219, 257, 6368, 3004, 10225], "temperature": 0.0, "avg_logprob": -0.17981741163465711, "compression_ratio": 1.8767772511848342, "no_speech_prob": 1.6028053551053745e-06}, {"id": 467, "seek": 222624, "start": 2226.24, "end": 2231.3399999999997, "text": " Which is where each sub array may be of a different length right where else we have?", "tokens": [3013, 307, 689, 1184, 1422, 10225, 815, 312, 295, 257, 819, 4641, 558, 689, 1646, 321, 362, 30], "temperature": 0.0, "avg_logprob": -0.17268086734570956, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.5056974689287017e-06}, {"id": 468, "seek": 222624, "start": 2232.4799999999996, "end": 2234.8399999999997, "text": " like a single object of", "tokens": [411, 257, 2167, 2657, 295], "temperature": 0.0, "avg_logprob": -0.17268086734570956, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.5056974689287017e-06}, {"id": 469, "seek": 222624, "start": 2235.56, "end": 2240.64, "text": " Three dimensions, and so we're trying to say like which little piece of it", "tokens": [6244, 12819, 11, 293, 370, 321, 434, 1382, 281, 584, 411, 597, 707, 2522, 295, 309], "temperature": 0.0, "avg_logprob": -0.17268086734570956, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.5056974689287017e-06}, {"id": 470, "seek": 222624, "start": 2240.72, "end": 2248.6, "text": " Do we want and so the idea is that that is a single slice object to go in and grab that piece out?", "tokens": [1144, 321, 528, 293, 370, 264, 1558, 307, 300, 300, 307, 257, 2167, 13153, 2657, 281, 352, 294, 293, 4444, 300, 2522, 484, 30], "temperature": 0.0, "avg_logprob": -0.17268086734570956, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.5056974689287017e-06}, {"id": 471, "seek": 222624, "start": 2252.72, "end": 2254.72, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.17268086734570956, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.5056974689287017e-06}, {"id": 472, "seek": 225472, "start": 2254.72, "end": 2260.48, "text": " So here's a example of a few of those images along with their", "tokens": [407, 510, 311, 257, 1365, 295, 257, 1326, 295, 729, 5267, 2051, 365, 641], "temperature": 0.0, "avg_logprob": -0.1700569757140509, "compression_ratio": 1.65625, "no_speech_prob": 3.966956228396157e-06}, {"id": 473, "seek": 225472, "start": 2261.3199999999997, "end": 2262.8399999999997, "text": " labels and", "tokens": [16949, 293], "temperature": 0.0, "avg_logprob": -0.1700569757140509, "compression_ratio": 1.65625, "no_speech_prob": 3.966956228396157e-06}, {"id": 474, "seek": 225472, "start": 2262.8399999999997, "end": 2266.2999999999997, "text": " This kind of stuff you want to be able to do pretty quickly with matplotlib", "tokens": [639, 733, 295, 1507, 291, 528, 281, 312, 1075, 281, 360, 1238, 2661, 365, 3803, 564, 310, 38270], "temperature": 0.0, "avg_logprob": -0.1700569757140509, "compression_ratio": 1.65625, "no_speech_prob": 3.966956228396157e-06}, {"id": 475, "seek": 225472, "start": 2266.7599999999998, "end": 2270.3799999999997, "text": " It's it's going to help you a lot in in life in your exam", "tokens": [467, 311, 309, 311, 516, 281, 854, 291, 257, 688, 294, 294, 993, 294, 428, 1139], "temperature": 0.0, "avg_logprob": -0.1700569757140509, "compression_ratio": 1.65625, "no_speech_prob": 3.966956228396157e-06}, {"id": 476, "seek": 225472, "start": 2271.0, "end": 2274.7599999999998, "text": " So you can have a look at you know what Rachel wrote here when she wrote plots", "tokens": [407, 291, 393, 362, 257, 574, 412, 291, 458, 437, 14246, 4114, 510, 562, 750, 4114, 28609], "temperature": 0.0, "avg_logprob": -0.1700569757140509, "compression_ratio": 1.65625, "no_speech_prob": 3.966956228396157e-06}, {"id": 477, "seek": 225472, "start": 2275.8999999999996, "end": 2277.8999999999996, "text": " we can use", "tokens": [321, 393, 764], "temperature": 0.0, "avg_logprob": -0.1700569757140509, "compression_ratio": 1.65625, "no_speech_prob": 3.966956228396157e-06}, {"id": 478, "seek": 225472, "start": 2278.2799999999997, "end": 2282.68, "text": " we can use add subplot to basically create those little separate plots and", "tokens": [321, 393, 764, 909, 1422, 564, 310, 281, 1936, 1884, 729, 707, 4994, 28609, 293], "temperature": 0.0, "avg_logprob": -0.1700569757140509, "compression_ratio": 1.65625, "no_speech_prob": 3.966956228396157e-06}, {"id": 479, "seek": 228268, "start": 2282.68, "end": 2289.7799999999997, "text": " You need to know that I am show is how we basically take a numpy array and draw it as a picture", "tokens": [509, 643, 281, 458, 300, 286, 669, 855, 307, 577, 321, 1936, 747, 257, 1031, 8200, 10225, 293, 2642, 309, 382, 257, 3036], "temperature": 0.0, "avg_logprob": -0.20930642551845974, "compression_ratio": 1.4320987654320987, "no_speech_prob": 2.5215647383447504e-06}, {"id": 480, "seek": 228268, "start": 2290.2999999999997, "end": 2292.2999999999997, "text": " Okay, and then we've also", "tokens": [1033, 11, 293, 550, 321, 600, 611], "temperature": 0.0, "avg_logprob": -0.20930642551845974, "compression_ratio": 1.4320987654320987, "no_speech_prob": 2.5215647383447504e-06}, {"id": 481, "seek": 228268, "start": 2292.7999999999997, "end": 2294.7999999999997, "text": " added the title on", "tokens": [3869, 264, 4876, 322], "temperature": 0.0, "avg_logprob": -0.20930642551845974, "compression_ratio": 1.4320987654320987, "no_speech_prob": 2.5215647383447504e-06}, {"id": 482, "seek": 228268, "start": 2295.48, "end": 2297.48, "text": " top", "tokens": [1192], "temperature": 0.0, "avg_logprob": -0.20930642551845974, "compression_ratio": 1.4320987654320987, "no_speech_prob": 2.5215647383447504e-06}, {"id": 483, "seek": 228268, "start": 2298.04, "end": 2302.0, "text": " So there it is all right, so let's now", "tokens": [407, 456, 309, 307, 439, 558, 11, 370, 718, 311, 586], "temperature": 0.0, "avg_logprob": -0.20930642551845974, "compression_ratio": 1.4320987654320987, "no_speech_prob": 2.5215647383447504e-06}, {"id": 484, "seek": 228268, "start": 2305.3999999999996, "end": 2308.16, "text": " Take that data and try to build a", "tokens": [3664, 300, 1412, 293, 853, 281, 1322, 257], "temperature": 0.0, "avg_logprob": -0.20930642551845974, "compression_ratio": 1.4320987654320987, "no_speech_prob": 2.5215647383447504e-06}, {"id": 485, "seek": 228268, "start": 2308.8399999999997, "end": 2310.52, "text": " neural network", "tokens": [18161, 3209], "temperature": 0.0, "avg_logprob": -0.20930642551845974, "compression_ratio": 1.4320987654320987, "no_speech_prob": 2.5215647383447504e-06}, {"id": 486, "seek": 231052, "start": 2310.52, "end": 2313.16, "text": " With it and so a neural network", "tokens": [2022, 309, 293, 370, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.2001342388114544, "compression_ratio": 1.7707509881422925, "no_speech_prob": 5.507562491402496e-06}, {"id": 487, "seek": 231052, "start": 2314.04, "end": 2317.4, "text": " And sorry this is going to be a lot of review for those of you already doing deep learning", "tokens": [400, 2597, 341, 307, 516, 281, 312, 257, 688, 295, 3131, 337, 729, 295, 291, 1217, 884, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.2001342388114544, "compression_ratio": 1.7707509881422925, "no_speech_prob": 5.507562491402496e-06}, {"id": 488, "seek": 231052, "start": 2317.84, "end": 2323.28, "text": " A neural network is just a particular mathematical function or a class of mathematical functions", "tokens": [316, 18161, 3209, 307, 445, 257, 1729, 18894, 2445, 420, 257, 1508, 295, 18894, 6828], "temperature": 0.0, "avg_logprob": -0.2001342388114544, "compression_ratio": 1.7707509881422925, "no_speech_prob": 5.507562491402496e-06}, {"id": 489, "seek": 231052, "start": 2323.32, "end": 2329.28, "text": " But it's a really important class because it has the property it supports. What's called the universal approximation theorem", "tokens": [583, 309, 311, 257, 534, 1021, 1508, 570, 309, 575, 264, 4707, 309, 9346, 13, 708, 311, 1219, 264, 11455, 28023, 20904], "temperature": 0.0, "avg_logprob": -0.2001342388114544, "compression_ratio": 1.7707509881422925, "no_speech_prob": 5.507562491402496e-06}, {"id": 490, "seek": 231052, "start": 2329.88, "end": 2332.98, "text": " Which is that which means that a neural network can?", "tokens": [3013, 307, 300, 597, 1355, 300, 257, 18161, 3209, 393, 30], "temperature": 0.0, "avg_logprob": -0.2001342388114544, "compression_ratio": 1.7707509881422925, "no_speech_prob": 5.507562491402496e-06}, {"id": 491, "seek": 231052, "start": 2333.72, "end": 2335.72, "text": " Approximate any other function", "tokens": [29551, 87, 2905, 604, 661, 2445], "temperature": 0.0, "avg_logprob": -0.2001342388114544, "compression_ratio": 1.7707509881422925, "no_speech_prob": 5.507562491402496e-06}, {"id": 492, "seek": 231052, "start": 2336.0, "end": 2337.52, "text": " arbitrarily closely", "tokens": [19071, 3289, 8185], "temperature": 0.0, "avg_logprob": -0.2001342388114544, "compression_ratio": 1.7707509881422925, "no_speech_prob": 5.507562491402496e-06}, {"id": 493, "seek": 233752, "start": 2337.52, "end": 2343.48, "text": " Right so in other words it can do in theory it can do anything as long as we make it big enough", "tokens": [1779, 370, 294, 661, 2283, 309, 393, 360, 294, 5261, 309, 393, 360, 1340, 382, 938, 382, 321, 652, 309, 955, 1547], "temperature": 0.0, "avg_logprob": -0.20528418223063152, "compression_ratio": 1.723809523809524, "no_speech_prob": 2.0580439468176337e-06}, {"id": 494, "seek": 233752, "start": 2345.28, "end": 2349.74, "text": " So this is very different to a function like 3x plus 5", "tokens": [407, 341, 307, 588, 819, 281, 257, 2445, 411, 805, 87, 1804, 1025], "temperature": 0.0, "avg_logprob": -0.20528418223063152, "compression_ratio": 1.723809523809524, "no_speech_prob": 2.0580439468176337e-06}, {"id": 495, "seek": 233752, "start": 2350.32, "end": 2353.96, "text": " Right which can only do one thing it's a very specific", "tokens": [1779, 597, 393, 787, 360, 472, 551, 309, 311, 257, 588, 2685], "temperature": 0.0, "avg_logprob": -0.20528418223063152, "compression_ratio": 1.723809523809524, "no_speech_prob": 2.0580439468176337e-06}, {"id": 496, "seek": 233752, "start": 2353.96, "end": 2358.2, "text": " It's a specific function or the class of functions a x plus B", "tokens": [467, 311, 257, 2685, 2445, 420, 264, 1508, 295, 6828, 257, 2031, 1804, 363], "temperature": 0.0, "avg_logprob": -0.20528418223063152, "compression_ratio": 1.723809523809524, "no_speech_prob": 2.0580439468176337e-06}, {"id": 497, "seek": 233752, "start": 2358.8, "end": 2364.84, "text": " Which can only represent lines of different slopes moving it up and down different amounts or?", "tokens": [3013, 393, 787, 2906, 3876, 295, 819, 37725, 2684, 309, 493, 293, 760, 819, 11663, 420, 30], "temperature": 0.0, "avg_logprob": -0.20528418223063152, "compression_ratio": 1.723809523809524, "no_speech_prob": 2.0580439468176337e-06}, {"id": 498, "seek": 236484, "start": 2364.84, "end": 2369.98, "text": " Even the function a x squared plus B x plus C plus sine D", "tokens": [2754, 264, 2445, 257, 2031, 8889, 1804, 363, 2031, 1804, 383, 1804, 18609, 413], "temperature": 0.0, "avg_logprob": -0.22475723050675303, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.2289159485590062e-06}, {"id": 499, "seek": 236484, "start": 2370.84, "end": 2375.44, "text": " You know again only can represent a very specific subset of relationships", "tokens": [509, 458, 797, 787, 393, 2906, 257, 588, 2685, 25993, 295, 6159], "temperature": 0.0, "avg_logprob": -0.22475723050675303, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.2289159485590062e-06}, {"id": 500, "seek": 236484, "start": 2376.08, "end": 2384.0, "text": " The neural network however is a function that can represent any other function to arbitrarily close accuracy, right?", "tokens": [440, 18161, 3209, 4461, 307, 257, 2445, 300, 393, 2906, 604, 661, 2445, 281, 19071, 3289, 1998, 14170, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22475723050675303, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.2289159485590062e-06}, {"id": 501, "seek": 236484, "start": 2384.56, "end": 2387.6400000000003, "text": " So what we're going to do is we're going to learn how to take a function", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1466, 577, 281, 747, 257, 2445], "temperature": 0.0, "avg_logprob": -0.22475723050675303, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.2289159485590062e-06}, {"id": 502, "seek": 236484, "start": 2387.88, "end": 2389.88, "text": " And so let's take work a x plus B", "tokens": [400, 370, 718, 311, 747, 589, 257, 2031, 1804, 363], "temperature": 0.0, "avg_logprob": -0.22475723050675303, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.2289159485590062e-06}, {"id": 503, "seek": 236484, "start": 2390.0, "end": 2394.1600000000003, "text": " And we're going to learn how to find its parameters in this case a and B", "tokens": [400, 321, 434, 516, 281, 1466, 577, 281, 915, 1080, 9834, 294, 341, 1389, 257, 293, 363], "temperature": 0.0, "avg_logprob": -0.22475723050675303, "compression_ratio": 1.7833333333333334, "no_speech_prob": 1.2289159485590062e-06}, {"id": 504, "seek": 239416, "start": 2394.16, "end": 2397.44, "text": " Which allow it to fit as closely as possible to a set of data?", "tokens": [3013, 2089, 309, 281, 3318, 382, 8185, 382, 1944, 281, 257, 992, 295, 1412, 30], "temperature": 0.0, "avg_logprob": -0.16870325088500976, "compression_ratio": 1.7848101265822784, "no_speech_prob": 4.785070814250503e-06}, {"id": 505, "seek": 239416, "start": 2397.44, "end": 2404.96, "text": " And so this here is showing example from a notebook that we'll be looking at in deep learning course", "tokens": [400, 370, 341, 510, 307, 4099, 1365, 490, 257, 21060, 300, 321, 603, 312, 1237, 412, 294, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.16870325088500976, "compression_ratio": 1.7848101265822784, "no_speech_prob": 4.785070814250503e-06}, {"id": 506, "seek": 239416, "start": 2405.16, "end": 2411.08, "text": " Which basically shows what happens when we use something called stochastic gradient descent to try and set a and B", "tokens": [3013, 1936, 3110, 437, 2314, 562, 321, 764, 746, 1219, 342, 8997, 2750, 16235, 23475, 281, 853, 293, 992, 257, 293, 363], "temperature": 0.0, "avg_logprob": -0.16870325088500976, "compression_ratio": 1.7848101265822784, "no_speech_prob": 4.785070814250503e-06}, {"id": 507, "seek": 239416, "start": 2411.08, "end": 2416.42, "text": " And basically what happens is we're going to pick a random a to start with a random B", "tokens": [400, 1936, 437, 2314, 307, 321, 434, 516, 281, 1888, 257, 4974, 257, 281, 722, 365, 257, 4974, 363], "temperature": 0.0, "avg_logprob": -0.16870325088500976, "compression_ratio": 1.7848101265822784, "no_speech_prob": 4.785070814250503e-06}, {"id": 508, "seek": 239416, "start": 2416.48, "end": 2419.92, "text": " To start with and then we're going to basically figure out", "tokens": [1407, 722, 365, 293, 550, 321, 434, 516, 281, 1936, 2573, 484], "temperature": 0.0, "avg_logprob": -0.16870325088500976, "compression_ratio": 1.7848101265822784, "no_speech_prob": 4.785070814250503e-06}, {"id": 509, "seek": 241992, "start": 2419.92, "end": 2425.36, "text": " Do I need to increase or decrease a to make it closer the line closer to the dots?", "tokens": [1144, 286, 643, 281, 3488, 420, 11514, 257, 281, 652, 309, 4966, 264, 1622, 4966, 281, 264, 15026, 30], "temperature": 0.0, "avg_logprob": -0.14229675353042723, "compression_ratio": 2.192982456140351, "no_speech_prob": 1.6028049003580236e-06}, {"id": 510, "seek": 241992, "start": 2425.56, "end": 2431.04, "text": " Do I need to increase or decrease B to make the line closer to the dots and then just keep?", "tokens": [1144, 286, 643, 281, 3488, 420, 11514, 363, 281, 652, 264, 1622, 4966, 281, 264, 15026, 293, 550, 445, 1066, 30], "temperature": 0.0, "avg_logprob": -0.14229675353042723, "compression_ratio": 2.192982456140351, "no_speech_prob": 1.6028049003580236e-06}, {"id": 511, "seek": 241992, "start": 2431.2000000000003, "end": 2433.84, "text": " Increasing and decreasing a and B lots and lots of times", "tokens": [30367, 3349, 293, 23223, 257, 293, 363, 3195, 293, 3195, 295, 1413], "temperature": 0.0, "avg_logprob": -0.14229675353042723, "compression_ratio": 2.192982456140351, "no_speech_prob": 1.6028049003580236e-06}, {"id": 512, "seek": 241992, "start": 2434.04, "end": 2439.32, "text": " Okay, so that's what we're going to do and to answer the question do I need to increase or decrease a and B?", "tokens": [1033, 11, 370, 300, 311, 437, 321, 434, 516, 281, 360, 293, 281, 1867, 264, 1168, 360, 286, 643, 281, 3488, 420, 11514, 257, 293, 363, 30], "temperature": 0.0, "avg_logprob": -0.14229675353042723, "compression_ratio": 2.192982456140351, "no_speech_prob": 1.6028049003580236e-06}, {"id": 513, "seek": 241992, "start": 2439.4, "end": 2440.96, "text": " We're going to take the derivative", "tokens": [492, 434, 516, 281, 747, 264, 13760], "temperature": 0.0, "avg_logprob": -0.14229675353042723, "compression_ratio": 2.192982456140351, "no_speech_prob": 1.6028049003580236e-06}, {"id": 514, "seek": 241992, "start": 2440.96, "end": 2448.84, "text": " Right so the derivative of the function with respect to a and B tells us how will that function change as we change a and B?", "tokens": [1779, 370, 264, 13760, 295, 264, 2445, 365, 3104, 281, 257, 293, 363, 5112, 505, 577, 486, 300, 2445, 1319, 382, 321, 1319, 257, 293, 363, 30], "temperature": 0.0, "avg_logprob": -0.14229675353042723, "compression_ratio": 2.192982456140351, "no_speech_prob": 1.6028049003580236e-06}, {"id": 515, "seek": 244884, "start": 2448.84, "end": 2454.2000000000003, "text": " All right, so that's basically what we're going to do, but we're not going to start with just a line", "tokens": [1057, 558, 11, 370, 300, 311, 1936, 437, 321, 434, 516, 281, 360, 11, 457, 321, 434, 406, 516, 281, 722, 365, 445, 257, 1622], "temperature": 0.0, "avg_logprob": -0.1324501633644104, "compression_ratio": 1.8502024291497976, "no_speech_prob": 3.2887364795897156e-06}, {"id": 516, "seek": 244884, "start": 2454.84, "end": 2460.7400000000002, "text": " The idea is we're going to build up to actually having a neural net and so it's going to be exactly the same idea", "tokens": [440, 1558, 307, 321, 434, 516, 281, 1322, 493, 281, 767, 1419, 257, 18161, 2533, 293, 370, 309, 311, 516, 281, 312, 2293, 264, 912, 1558], "temperature": 0.0, "avg_logprob": -0.1324501633644104, "compression_ratio": 1.8502024291497976, "no_speech_prob": 3.2887364795897156e-06}, {"id": 517, "seek": 244884, "start": 2460.8, "end": 2463.1600000000003, "text": " But because it's an infinitely flexible function", "tokens": [583, 570, 309, 311, 364, 36227, 11358, 2445], "temperature": 0.0, "avg_logprob": -0.1324501633644104, "compression_ratio": 1.8502024291497976, "no_speech_prob": 3.2887364795897156e-06}, {"id": 518, "seek": 244884, "start": 2463.6000000000004, "end": 2470.58, "text": " We're going to be able to use this exact same technique to fit arbitrarily to arbitrarily complex relationships", "tokens": [492, 434, 516, 281, 312, 1075, 281, 764, 341, 1900, 912, 6532, 281, 3318, 19071, 3289, 281, 19071, 3289, 3997, 6159], "temperature": 0.0, "avg_logprob": -0.1324501633644104, "compression_ratio": 1.8502024291497976, "no_speech_prob": 3.2887364795897156e-06}, {"id": 519, "seek": 244884, "start": 2470.92, "end": 2472.92, "text": " Now that's basically the idea", "tokens": [823, 300, 311, 1936, 264, 1558], "temperature": 0.0, "avg_logprob": -0.1324501633644104, "compression_ratio": 1.8502024291497976, "no_speech_prob": 3.2887364795897156e-06}, {"id": 520, "seek": 247292, "start": 2472.92, "end": 2480.2000000000003, "text": " So then what you need to know is that a neural net is actually a very simple thing", "tokens": [407, 550, 437, 291, 643, 281, 458, 307, 300, 257, 18161, 2533, 307, 767, 257, 588, 2199, 551], "temperature": 0.0, "avg_logprob": -0.21097586495535714, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.328774260400678e-06}, {"id": 521, "seek": 247292, "start": 2480.84, "end": 2483.32, "text": " a neural net actually is something which", "tokens": [257, 18161, 2533, 767, 307, 746, 597], "temperature": 0.0, "avg_logprob": -0.21097586495535714, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.328774260400678e-06}, {"id": 522, "seek": 247292, "start": 2486.08, "end": 2492.16, "text": " Takes as input. Let's say we've got a vector", "tokens": [44347, 382, 4846, 13, 961, 311, 584, 321, 600, 658, 257, 8062], "temperature": 0.0, "avg_logprob": -0.21097586495535714, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.328774260400678e-06}, {"id": 523, "seek": 247292, "start": 2494.4, "end": 2496.4, "text": " Does a matrix product", "tokens": [4402, 257, 8141, 1674], "temperature": 0.0, "avg_logprob": -0.21097586495535714, "compression_ratio": 1.5730994152046784, "no_speech_prob": 1.328774260400678e-06}, {"id": 524, "seek": 249640, "start": 2496.4, "end": 2502.2400000000002, "text": " By that vector right so this is like this is of size. Let's draw this properly so", "tokens": [3146, 300, 8062, 558, 370, 341, 307, 411, 341, 307, 295, 2744, 13, 961, 311, 2642, 341, 6108, 370], "temperature": 0.0, "avg_logprob": -0.23662151760525174, "compression_ratio": 1.6018957345971565, "no_speech_prob": 3.6119708965998143e-06}, {"id": 525, "seek": 249640, "start": 2504.6, "end": 2510.2000000000003, "text": " Like if this is size R. This is like R by C a matrix product will spit out something of", "tokens": [1743, 498, 341, 307, 2744, 497, 13, 639, 307, 411, 497, 538, 383, 257, 8141, 1674, 486, 22127, 484, 746, 295], "temperature": 0.0, "avg_logprob": -0.23662151760525174, "compression_ratio": 1.6018957345971565, "no_speech_prob": 3.6119708965998143e-06}, {"id": 526, "seek": 249640, "start": 2513.6800000000003, "end": 2515.6800000000003, "text": " Size C", "tokens": [35818, 383], "temperature": 0.0, "avg_logprob": -0.23662151760525174, "compression_ratio": 1.6018957345971565, "no_speech_prob": 3.6119708965998143e-06}, {"id": 527, "seek": 249640, "start": 2515.76, "end": 2521.7200000000003, "text": " Right and then we do something called a non-linearity which is basically we're going to throw away all the negative values", "tokens": [1779, 293, 550, 321, 360, 746, 1219, 257, 2107, 12, 1889, 17409, 597, 307, 1936, 321, 434, 516, 281, 3507, 1314, 439, 264, 3671, 4190], "temperature": 0.0, "avg_logprob": -0.23662151760525174, "compression_ratio": 1.6018957345971565, "no_speech_prob": 3.6119708965998143e-06}, {"id": 528, "seek": 249640, "start": 2521.7200000000003, "end": 2523.7200000000003, "text": " So it's basically max", "tokens": [407, 309, 311, 1936, 11469], "temperature": 0.0, "avg_logprob": -0.23662151760525174, "compression_ratio": 1.6018957345971565, "no_speech_prob": 3.6119708965998143e-06}, {"id": 529, "seek": 249640, "start": 2524.1600000000003, "end": 2526.1600000000003, "text": " zero comma X and", "tokens": [4018, 22117, 1783, 293], "temperature": 0.0, "avg_logprob": -0.23662151760525174, "compression_ratio": 1.6018957345971565, "no_speech_prob": 3.6119708965998143e-06}, {"id": 530, "seek": 252616, "start": 2526.16, "end": 2529.08, "text": " Then we're going to put that through another matrix multiply", "tokens": [1396, 321, 434, 516, 281, 829, 300, 807, 1071, 8141, 12972], "temperature": 0.0, "avg_logprob": -0.19533311167070944, "compression_ratio": 2.078125, "no_speech_prob": 4.710887878900394e-06}, {"id": 531, "seek": 252616, "start": 2530.2, "end": 2534.24, "text": " And then we're going to put that through another max zero comma X", "tokens": [400, 550, 321, 434, 516, 281, 829, 300, 807, 1071, 11469, 4018, 22117, 1783], "temperature": 0.0, "avg_logprob": -0.19533311167070944, "compression_ratio": 2.078125, "no_speech_prob": 4.710887878900394e-06}, {"id": 532, "seek": 252616, "start": 2534.64, "end": 2540.64, "text": " And we're going to put that through another matrix multiply and so on right until eventually we end up with", "tokens": [400, 321, 434, 516, 281, 829, 300, 807, 1071, 8141, 12972, 293, 370, 322, 558, 1826, 4728, 321, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.19533311167070944, "compression_ratio": 2.078125, "no_speech_prob": 4.710887878900394e-06}, {"id": 533, "seek": 252616, "start": 2541.08, "end": 2543.08, "text": " The single vector that we want", "tokens": [440, 2167, 8062, 300, 321, 528], "temperature": 0.0, "avg_logprob": -0.19533311167070944, "compression_ratio": 2.078125, "no_speech_prob": 4.710887878900394e-06}, {"id": 534, "seek": 252616, "start": 2543.64, "end": 2549.64, "text": " so in other words each stage of our neural network is the key thing going on is a", "tokens": [370, 294, 661, 2283, 1184, 3233, 295, 527, 18161, 3209, 307, 264, 2141, 551, 516, 322, 307, 257], "temperature": 0.0, "avg_logprob": -0.19533311167070944, "compression_ratio": 2.078125, "no_speech_prob": 4.710887878900394e-06}, {"id": 535, "seek": 252616, "start": 2551.2799999999997, "end": 2554.68, "text": " Matrix multiply so in other words a linear function", "tokens": [36274, 12972, 370, 294, 661, 2283, 257, 8213, 2445], "temperature": 0.0, "avg_logprob": -0.19533311167070944, "compression_ratio": 2.078125, "no_speech_prob": 4.710887878900394e-06}, {"id": 536, "seek": 255468, "start": 2554.68, "end": 2561.06, "text": " So basically deep learning most of the calculation is lots and lots of linear functions", "tokens": [407, 1936, 2452, 2539, 881, 295, 264, 17108, 307, 3195, 293, 3195, 295, 8213, 6828], "temperature": 0.0, "avg_logprob": -0.27128205370547165, "compression_ratio": 1.532258064516129, "no_speech_prob": 2.368787818340934e-06}, {"id": 537, "seek": 255468, "start": 2561.44, "end": 2563.44, "text": " but between each one", "tokens": [457, 1296, 1184, 472], "temperature": 0.0, "avg_logprob": -0.27128205370547165, "compression_ratio": 1.532258064516129, "no_speech_prob": 2.368787818340934e-06}, {"id": 538, "seek": 255468, "start": 2563.6, "end": 2568.5, "text": " We're going to replace the negative numbers with zeros. Can you?", "tokens": [492, 434, 516, 281, 7406, 264, 3671, 3547, 365, 35193, 13, 1664, 291, 30], "temperature": 0.0, "avg_logprob": -0.27128205370547165, "compression_ratio": 1.532258064516129, "no_speech_prob": 2.368787818340934e-06}, {"id": 539, "seek": 255468, "start": 2572.04, "end": 2580.18, "text": " Yes, so why are we throwing away the negative numbers as we go through this we'll see right the short answer is", "tokens": [1079, 11, 370, 983, 366, 321, 10238, 1314, 264, 3671, 3547, 382, 321, 352, 807, 341, 321, 603, 536, 558, 264, 2099, 1867, 307], "temperature": 0.0, "avg_logprob": -0.27128205370547165, "compression_ratio": 1.532258064516129, "no_speech_prob": 2.368787818340934e-06}, {"id": 540, "seek": 258018, "start": 2580.18, "end": 2585.8599999999997, "text": " If you apply a linear function to a linear function to a linear function. It's still just a linear function", "tokens": [759, 291, 3079, 257, 8213, 2445, 281, 257, 8213, 2445, 281, 257, 8213, 2445, 13, 467, 311, 920, 445, 257, 8213, 2445], "temperature": 0.0, "avg_logprob": -0.19668754577636718, "compression_ratio": 2.1476190476190475, "no_speech_prob": 1.1911042747669853e-06}, {"id": 541, "seek": 258018, "start": 2587.3399999999997, "end": 2590.62, "text": " So it's totally useless, but if you throw away the negatives", "tokens": [407, 309, 311, 3879, 14115, 11, 457, 498, 291, 3507, 1314, 264, 40019], "temperature": 0.0, "avg_logprob": -0.19668754577636718, "compression_ratio": 2.1476190476190475, "no_speech_prob": 1.1911042747669853e-06}, {"id": 542, "seek": 258018, "start": 2590.7799999999997, "end": 2596.98, "text": " That's actually a nonlinear transformation and so it turns out that if you apply a linear function", "tokens": [663, 311, 767, 257, 2107, 28263, 9887, 293, 370, 309, 4523, 484, 300, 498, 291, 3079, 257, 8213, 2445], "temperature": 0.0, "avg_logprob": -0.19668754577636718, "compression_ratio": 2.1476190476190475, "no_speech_prob": 1.1911042747669853e-06}, {"id": 543, "seek": 258018, "start": 2597.18, "end": 2602.2599999999998, "text": " To the thing we threw away the negatives that applied that to a linear function that creates a neural network", "tokens": [1407, 264, 551, 321, 11918, 1314, 264, 40019, 300, 6456, 300, 281, 257, 8213, 2445, 300, 7829, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.19668754577636718, "compression_ratio": 2.1476190476190475, "no_speech_prob": 1.1911042747669853e-06}, {"id": 544, "seek": 258018, "start": 2602.3399999999997, "end": 2606.58, "text": " And it turns out that's the thing that can approximate any other function", "tokens": [400, 309, 4523, 484, 300, 311, 264, 551, 300, 393, 30874, 604, 661, 2445], "temperature": 0.0, "avg_logprob": -0.19668754577636718, "compression_ratio": 2.1476190476190475, "no_speech_prob": 1.1911042747669853e-06}, {"id": 545, "seek": 260658, "start": 2606.58, "end": 2609.98, "text": " Arbitrarily closely so this tiny little difference", "tokens": [1587, 5260, 81, 3289, 8185, 370, 341, 5870, 707, 2649], "temperature": 0.0, "avg_logprob": -0.2087776396009657, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.3496986639438546e-06}, {"id": 546, "seek": 260658, "start": 2610.66, "end": 2617.5, "text": " Actually makes all the difference and if you're interested in it check out the deep learning video where we cover this because I actually", "tokens": [5135, 1669, 439, 264, 2649, 293, 498, 291, 434, 3102, 294, 309, 1520, 484, 264, 2452, 2539, 960, 689, 321, 2060, 341, 570, 286, 767], "temperature": 0.0, "avg_logprob": -0.2087776396009657, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.3496986639438546e-06}, {"id": 547, "seek": 260658, "start": 2617.8199999999997, "end": 2619.8199999999997, "text": " show a", "tokens": [855, 257], "temperature": 0.0, "avg_logprob": -0.2087776396009657, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.3496986639438546e-06}, {"id": 548, "seek": 260658, "start": 2619.94, "end": 2626.42, "text": " nice visual intuitive proof not something that I created but something that Michael Nielsen created", "tokens": [1481, 5056, 21769, 8177, 406, 746, 300, 286, 2942, 457, 746, 300, 5116, 426, 1187, 6748, 2942], "temperature": 0.0, "avg_logprob": -0.2087776396009657, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.3496986639438546e-06}, {"id": 549, "seek": 260658, "start": 2626.98, "end": 2628.98, "text": " Or if you want to skip straight to his website", "tokens": [1610, 498, 291, 528, 281, 10023, 2997, 281, 702, 3144], "temperature": 0.0, "avg_logprob": -0.2087776396009657, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.3496986639438546e-06}, {"id": 550, "seek": 260658, "start": 2629.86, "end": 2631.86, "text": " You could go to Michael Nielsen", "tokens": [509, 727, 352, 281, 5116, 426, 1187, 6748], "temperature": 0.0, "avg_logprob": -0.2087776396009657, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.3496986639438546e-06}, {"id": 551, "seek": 260658, "start": 2632.7799999999997, "end": 2634.7799999999997, "text": " universal I", "tokens": [11455, 286], "temperature": 0.0, "avg_logprob": -0.2087776396009657, "compression_ratio": 1.6929824561403508, "no_speech_prob": 1.3496986639438546e-06}, {"id": 552, "seek": 263478, "start": 2634.78, "end": 2638.1800000000003, "text": " Think I spelled his name wrong never mind mission theorem", "tokens": [6557, 286, 34388, 702, 1315, 2085, 1128, 1575, 4447, 20904], "temperature": 0.0, "avg_logprob": -0.2422561184052498, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.2805246115021873e-05}, {"id": 553, "seek": 263478, "start": 2641.98, "end": 2646.84, "text": " There we go neural networks and deep learning chapter 4, and he's got a really nice", "tokens": [821, 321, 352, 18161, 9590, 293, 2452, 2539, 7187, 1017, 11, 293, 415, 311, 658, 257, 534, 1481], "temperature": 0.0, "avg_logprob": -0.2422561184052498, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.2805246115021873e-05}, {"id": 554, "seek": 263478, "start": 2648.0600000000004, "end": 2653.34, "text": " Walk through basically with lots of animations where you can see why this works", "tokens": [10818, 807, 1936, 365, 3195, 295, 22868, 689, 291, 393, 536, 983, 341, 1985], "temperature": 0.0, "avg_logprob": -0.2422561184052498, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.2805246115021873e-05}, {"id": 555, "seek": 263478, "start": 2656.1000000000004, "end": 2660.7400000000002, "text": " One I feel like the the the hardest thing I", "tokens": [1485, 286, 841, 411, 264, 264, 264, 13158, 551, 286], "temperature": 0.0, "avg_logprob": -0.2422561184052498, "compression_ratio": 1.4480874316939891, "no_speech_prob": 1.2805246115021873e-05}, {"id": 556, "seek": 266074, "start": 2660.74, "end": 2668.06, "text": " Feel like the hardest thing with getting started like technical writing on the internet is just like", "tokens": [14113, 411, 264, 13158, 551, 365, 1242, 1409, 411, 6191, 3579, 322, 264, 4705, 307, 445, 411], "temperature": 0.0, "avg_logprob": -0.18413623949376548, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.646403901162557e-06}, {"id": 557, "seek": 266074, "start": 2668.8199999999997, "end": 2670.8199999999997, "text": " posting your first thing", "tokens": [15978, 428, 700, 551], "temperature": 0.0, "avg_logprob": -0.18413623949376548, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.646403901162557e-06}, {"id": 558, "seek": 266074, "start": 2671.66, "end": 2673.4199999999996, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.18413623949376548, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.646403901162557e-06}, {"id": 559, "seek": 266074, "start": 2673.4199999999996, "end": 2675.4199999999996, "text": " if you do a search for", "tokens": [498, 291, 360, 257, 3164, 337], "temperature": 0.0, "avg_logprob": -0.18413623949376548, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.646403901162557e-06}, {"id": 560, "seek": 266074, "start": 2675.7799999999997, "end": 2680.74, "text": " Rachel Thomas medium blog you'll find this we'll put it on the lesson wiki", "tokens": [14246, 8500, 6399, 6968, 291, 603, 915, 341, 321, 603, 829, 309, 322, 264, 6898, 261, 9850], "temperature": 0.0, "avg_logprob": -0.18413623949376548, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.646403901162557e-06}, {"id": 561, "seek": 266074, "start": 2681.8999999999996, "end": 2687.8599999999997, "text": " Where she talks about she actually says the top advice she would give to her younger self would be to start blogging", "tokens": [2305, 750, 6686, 466, 750, 767, 1619, 264, 1192, 5192, 750, 576, 976, 281, 720, 7037, 2698, 576, 312, 281, 722, 6968, 3249], "temperature": 0.0, "avg_logprob": -0.18413623949376548, "compression_ratio": 1.6411483253588517, "no_speech_prob": 7.646403901162557e-06}, {"id": 562, "seek": 268786, "start": 2687.86, "end": 2690.94, "text": " sooner and she has like", "tokens": [15324, 293, 750, 575, 411], "temperature": 0.0, "avg_logprob": -0.1632826349756739, "compression_ratio": 1.7185185185185186, "no_speech_prob": 2.5215490495611448e-06}, {"id": 563, "seek": 268786, "start": 2691.94, "end": 2694.2200000000003, "text": " Both reasons why you should do it", "tokens": [6767, 4112, 983, 291, 820, 360, 309], "temperature": 0.0, "avg_logprob": -0.1632826349756739, "compression_ratio": 1.7185185185185186, "no_speech_prob": 2.5215490495611448e-06}, {"id": 564, "seek": 268786, "start": 2695.1400000000003, "end": 2698.3, "text": " Some examples of things that you know examples of places", "tokens": [2188, 5110, 295, 721, 300, 291, 458, 5110, 295, 3190], "temperature": 0.0, "avg_logprob": -0.1632826349756739, "compression_ratio": 1.7185185185185186, "no_speech_prob": 2.5215490495611448e-06}, {"id": 565, "seek": 268786, "start": 2698.3, "end": 2704.08, "text": " She's bloggers turned out to be great for her and her career, but then some tips about how to get started", "tokens": [1240, 311, 6968, 9458, 3574, 484, 281, 312, 869, 337, 720, 293, 720, 3988, 11, 457, 550, 512, 6082, 466, 577, 281, 483, 1409], "temperature": 0.0, "avg_logprob": -0.1632826349756739, "compression_ratio": 1.7185185185185186, "no_speech_prob": 2.5215490495611448e-06}, {"id": 566, "seek": 268786, "start": 2704.08, "end": 2707.38, "text": " I remember when I first suggested to Rachel", "tokens": [286, 1604, 562, 286, 700, 10945, 281, 14246], "temperature": 0.0, "avg_logprob": -0.1632826349756739, "compression_ratio": 1.7185185185185186, "no_speech_prob": 2.5215490495611448e-06}, {"id": 567, "seek": 268786, "start": 2707.38, "end": 2712.82, "text": " She might think about blogging because she had so much interesting to say and you know at first she was kind of surprised at", "tokens": [1240, 1062, 519, 466, 6968, 3249, 570, 750, 632, 370, 709, 1880, 281, 584, 293, 291, 458, 412, 700, 750, 390, 733, 295, 6100, 412], "temperature": 0.0, "avg_logprob": -0.1632826349756739, "compression_ratio": 1.7185185185185186, "no_speech_prob": 2.5215490495611448e-06}, {"id": 568, "seek": 271282, "start": 2712.82, "end": 2718.5800000000004, "text": " The idea that like she could blog you know and now people come up to us at conferences", "tokens": [440, 1558, 300, 411, 750, 727, 6968, 291, 458, 293, 586, 561, 808, 493, 281, 505, 412, 22032], "temperature": 0.0, "avg_logprob": -0.16653583119216475, "compression_ratio": 1.62015503875969, "no_speech_prob": 2.5215576897608116e-06}, {"id": 569, "seek": 271282, "start": 2718.5800000000004, "end": 2721.7400000000002, "text": " And they're like you're Rachel Thomas. I love your writing", "tokens": [400, 436, 434, 411, 291, 434, 14246, 8500, 13, 286, 959, 428, 3579], "temperature": 0.0, "avg_logprob": -0.16653583119216475, "compression_ratio": 1.62015503875969, "no_speech_prob": 2.5215576897608116e-06}, {"id": 570, "seek": 271282, "start": 2721.7400000000002, "end": 2729.5, "text": " You know so like I've kind of seen that that transition from like wow could I blog to to be known as a strong?", "tokens": [509, 458, 370, 411, 286, 600, 733, 295, 1612, 300, 300, 6034, 490, 411, 6076, 727, 286, 6968, 281, 281, 312, 2570, 382, 257, 2068, 30], "temperature": 0.0, "avg_logprob": -0.16653583119216475, "compression_ratio": 1.62015503875969, "no_speech_prob": 2.5215576897608116e-06}, {"id": 571, "seek": 271282, "start": 2730.06, "end": 2732.78, "text": " Technical author so yeah, so check out this", "tokens": [35512, 3793, 370, 1338, 11, 370, 1520, 484, 341], "temperature": 0.0, "avg_logprob": -0.16653583119216475, "compression_ratio": 1.62015503875969, "no_speech_prob": 2.5215576897608116e-06}, {"id": 572, "seek": 271282, "start": 2733.82, "end": 2734.82, "text": " article", "tokens": [7222], "temperature": 0.0, "avg_logprob": -0.16653583119216475, "compression_ratio": 1.62015503875969, "no_speech_prob": 2.5215576897608116e-06}, {"id": 573, "seek": 271282, "start": 2734.82, "end": 2740.82, "text": " If you still need convincing or if you're wondering how to get started and since the first one is the hardest", "tokens": [759, 291, 920, 643, 24823, 420, 498, 291, 434, 6359, 577, 281, 483, 1409, 293, 1670, 264, 700, 472, 307, 264, 13158], "temperature": 0.0, "avg_logprob": -0.16653583119216475, "compression_ratio": 1.62015503875969, "no_speech_prob": 2.5215576897608116e-06}, {"id": 574, "seek": 274082, "start": 2740.82, "end": 2743.1800000000003, "text": " Maybe your first one should be like", "tokens": [2704, 428, 700, 472, 820, 312, 411], "temperature": 0.0, "avg_logprob": -0.1624777442530582, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.963794375114958e-06}, {"id": 575, "seek": 274082, "start": 2743.94, "end": 2747.86, "text": " Something really easy for you to write you know so it could be like", "tokens": [6595, 534, 1858, 337, 291, 281, 2464, 291, 458, 370, 309, 727, 312, 411], "temperature": 0.0, "avg_logprob": -0.1624777442530582, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.963794375114958e-06}, {"id": 576, "seek": 274082, "start": 2748.54, "end": 2750.54, "text": " you know here's a summary of", "tokens": [291, 458, 510, 311, 257, 12691, 295], "temperature": 0.0, "avg_logprob": -0.1624777442530582, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.963794375114958e-06}, {"id": 577, "seek": 274082, "start": 2751.1800000000003, "end": 2753.1800000000003, "text": " the first 15 minutes of", "tokens": [264, 700, 2119, 2077, 295], "temperature": 0.0, "avg_logprob": -0.1624777442530582, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.963794375114958e-06}, {"id": 578, "seek": 274082, "start": 2754.26, "end": 2759.98, "text": " Lesson three of our machine learning course you know here's why it's interesting. Here's what we learned or it could be like", "tokens": [18649, 266, 1045, 295, 527, 3479, 2539, 1164, 291, 458, 510, 311, 983, 309, 311, 1880, 13, 1692, 311, 437, 321, 3264, 420, 309, 727, 312, 411], "temperature": 0.0, "avg_logprob": -0.1624777442530582, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.963794375114958e-06}, {"id": 579, "seek": 274082, "start": 2763.26, "end": 2769.1800000000003, "text": " Here's a summary of how I used a random forest to solve a particular problem in my practicum", "tokens": [1692, 311, 257, 12691, 295, 577, 286, 1143, 257, 4974, 6719, 281, 5039, 257, 1729, 1154, 294, 452, 1927, 299, 449], "temperature": 0.0, "avg_logprob": -0.1624777442530582, "compression_ratio": 1.755868544600939, "no_speech_prob": 1.963794375114958e-06}, {"id": 580, "seek": 276918, "start": 2769.18, "end": 2772.98, "text": " I often get questions like on my practicum my organization. We've got like", "tokens": [286, 2049, 483, 1651, 411, 322, 452, 1927, 299, 449, 452, 4475, 13, 492, 600, 658, 411], "temperature": 0.0, "avg_logprob": -0.15986402573124056, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.643382675974863e-06}, {"id": 581, "seek": 276918, "start": 2773.62, "end": 2775.3799999999997, "text": " sensitive commercial data", "tokens": [9477, 6841, 1412], "temperature": 0.0, "avg_logprob": -0.15986402573124056, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.643382675974863e-06}, {"id": 582, "seek": 276918, "start": 2775.3799999999997, "end": 2777.58, "text": " That's fine like you know just", "tokens": [663, 311, 2489, 411, 291, 458, 445], "temperature": 0.0, "avg_logprob": -0.15986402573124056, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.643382675974863e-06}, {"id": 583, "seek": 276918, "start": 2778.8199999999997, "end": 2782.98, "text": " find another data set and do it on that instead to show the example or", "tokens": [915, 1071, 1412, 992, 293, 360, 309, 322, 300, 2602, 281, 855, 264, 1365, 420], "temperature": 0.0, "avg_logprob": -0.15986402573124056, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.643382675974863e-06}, {"id": 584, "seek": 276918, "start": 2784.7799999999997, "end": 2790.7, "text": " You know anonymize all of the values and change the names of the variables or whatever like you can talk to", "tokens": [509, 458, 37293, 1125, 439, 295, 264, 4190, 293, 1319, 264, 5288, 295, 264, 9102, 420, 2035, 411, 291, 393, 751, 281], "temperature": 0.0, "avg_logprob": -0.15986402573124056, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.643382675974863e-06}, {"id": 585, "seek": 276918, "start": 2791.54, "end": 2796.5, "text": " Your employer or your practicum partner to make sure that they're comfortable", "tokens": [2260, 16205, 420, 428, 1927, 299, 449, 4975, 281, 652, 988, 300, 436, 434, 4619], "temperature": 0.0, "avg_logprob": -0.15986402573124056, "compression_ratio": 1.6581196581196582, "no_speech_prob": 6.643382675974863e-06}, {"id": 586, "seek": 279650, "start": 2796.5, "end": 2800.14, "text": " With whatever it is you're writing in general though. You know", "tokens": [2022, 2035, 309, 307, 291, 434, 3579, 294, 2674, 1673, 13, 509, 458], "temperature": 0.0, "avg_logprob": -0.1769611923782914, "compression_ratio": 1.7338403041825095, "no_speech_prob": 1.5056950815051096e-06}, {"id": 587, "seek": 279650, "start": 2802.1, "end": 2804.66, "text": " People love it when their interns and staff", "tokens": [3432, 959, 309, 562, 641, 46145, 293, 3525], "temperature": 0.0, "avg_logprob": -0.1769611923782914, "compression_ratio": 1.7338403041825095, "no_speech_prob": 1.5056950815051096e-06}, {"id": 588, "seek": 279650, "start": 2805.18, "end": 2809.54, "text": " Blog about what they're working on because it makes them look super cool. You know it's like hey, I'm an", "tokens": [46693, 466, 437, 436, 434, 1364, 322, 570, 309, 1669, 552, 574, 1687, 1627, 13, 509, 458, 309, 311, 411, 4177, 11, 286, 478, 364], "temperature": 0.0, "avg_logprob": -0.1769611923782914, "compression_ratio": 1.7338403041825095, "no_speech_prob": 1.5056950815051096e-06}, {"id": 589, "seek": 279650, "start": 2810.38, "end": 2812.02, "text": " You know", "tokens": [509, 458], "temperature": 0.0, "avg_logprob": -0.1769611923782914, "compression_ratio": 1.7338403041825095, "no_speech_prob": 1.5056950815051096e-06}, {"id": 590, "seek": 279650, "start": 2812.02, "end": 2815.86, "text": " Intern working at this company, and I wrote this post about this cool analysis", "tokens": [4844, 1364, 412, 341, 2237, 11, 293, 286, 4114, 341, 2183, 466, 341, 1627, 5215], "temperature": 0.0, "avg_logprob": -0.1769611923782914, "compression_ratio": 1.7338403041825095, "no_speech_prob": 1.5056950815051096e-06}, {"id": 591, "seek": 279650, "start": 2815.86, "end": 2820.14, "text": " I did and then other people would be like wow that looks like a great company to work for so generally speaking", "tokens": [286, 630, 293, 550, 661, 561, 576, 312, 411, 6076, 300, 1542, 411, 257, 869, 2237, 281, 589, 337, 370, 5101, 4124], "temperature": 0.0, "avg_logprob": -0.1769611923782914, "compression_ratio": 1.7338403041825095, "no_speech_prob": 1.5056950815051096e-06}, {"id": 592, "seek": 279650, "start": 2820.58, "end": 2822.58, "text": " You should find people are pretty supportive", "tokens": [509, 820, 915, 561, 366, 1238, 14435], "temperature": 0.0, "avg_logprob": -0.1769611923782914, "compression_ratio": 1.7338403041825095, "no_speech_prob": 1.5056950815051096e-06}, {"id": 593, "seek": 282258, "start": 2822.58, "end": 2827.2599999999998, "text": " Of besides which there's lots and lots of data sets out there available", "tokens": [2720, 11868, 597, 456, 311, 3195, 293, 3195, 295, 1412, 6352, 484, 456, 2435], "temperature": 0.0, "avg_logprob": -0.19985946741971103, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.785050805367064e-06}, {"id": 594, "seek": 282258, "start": 2828.86, "end": 2834.34, "text": " So even if you can't base it on the work you're doing you can find something similar for sure", "tokens": [407, 754, 498, 291, 393, 380, 3096, 309, 322, 264, 589, 291, 434, 884, 291, 393, 915, 746, 2531, 337, 988], "temperature": 0.0, "avg_logprob": -0.19985946741971103, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.785050805367064e-06}, {"id": 595, "seek": 282258, "start": 2835.2999999999997, "end": 2839.34, "text": " All right, so we're going to start building our neural network. We're going to build it", "tokens": [1057, 558, 11, 370, 321, 434, 516, 281, 722, 2390, 527, 18161, 3209, 13, 492, 434, 516, 281, 1322, 309], "temperature": 0.0, "avg_logprob": -0.19985946741971103, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.785050805367064e-06}, {"id": 596, "seek": 282258, "start": 2840.2999999999997, "end": 2846.66, "text": " Using something called my torch my torch is a library that basically looks a lot like numpy", "tokens": [11142, 746, 1219, 452, 27822, 452, 27822, 307, 257, 6405, 300, 1936, 1542, 257, 688, 411, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.19985946741971103, "compression_ratio": 1.6367713004484306, "no_speech_prob": 4.785050805367064e-06}, {"id": 597, "seek": 284666, "start": 2846.66, "end": 2851.8199999999997, "text": " But when you create", "tokens": [583, 562, 291, 1884], "temperature": 0.0, "avg_logprob": -0.29863842795876894, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.6119490687269717e-06}, {"id": 598, "seek": 284666, "start": 2852.8599999999997, "end": 2856.46, "text": " Some code with pie torch you can run it on the GPU", "tokens": [2188, 3089, 365, 1730, 27822, 291, 393, 1190, 309, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.29863842795876894, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.6119490687269717e-06}, {"id": 599, "seek": 284666, "start": 2857.46, "end": 2859.46, "text": " rather than the CPU", "tokens": [2831, 813, 264, 13199], "temperature": 0.0, "avg_logprob": -0.29863842795876894, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.6119490687269717e-06}, {"id": 600, "seek": 284666, "start": 2859.46, "end": 2861.46, "text": " so the GPU is", "tokens": [370, 264, 18407, 307], "temperature": 0.0, "avg_logprob": -0.29863842795876894, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.6119490687269717e-06}, {"id": 601, "seek": 284666, "start": 2864.46, "end": 2869.3799999999997, "text": " Something which is basically going to be probably at least an order of magnitude", "tokens": [6595, 597, 307, 1936, 516, 281, 312, 1391, 412, 1935, 364, 1668, 295, 15668], "temperature": 0.0, "avg_logprob": -0.29863842795876894, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.6119490687269717e-06}, {"id": 602, "seek": 284666, "start": 2869.94, "end": 2874.02, "text": " Possibly hundreds of times faster than the code that you might write for the CPU", "tokens": [33112, 3545, 6779, 295, 1413, 4663, 813, 264, 3089, 300, 291, 1062, 2464, 337, 264, 13199], "temperature": 0.0, "avg_logprob": -0.29863842795876894, "compression_ratio": 1.5739644970414202, "no_speech_prob": 3.6119490687269717e-06}, {"id": 603, "seek": 287402, "start": 2874.02, "end": 2876.94, "text": " For particularly stuff involving lots of linear algebra", "tokens": [1171, 4098, 1507, 17030, 3195, 295, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.21347130669487846, "compression_ratio": 1.483568075117371, "no_speech_prob": 4.565938070300035e-06}, {"id": 604, "seek": 287402, "start": 2878.2599999999998, "end": 2880.5, "text": " So with deep learning neural nets", "tokens": [407, 365, 2452, 2539, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.21347130669487846, "compression_ratio": 1.483568075117371, "no_speech_prob": 4.565938070300035e-06}, {"id": 605, "seek": 287402, "start": 2882.02, "end": 2889.22, "text": " You can if you if you don't have a GPU you can do it on the CPU right, but it's going to be frustratingly slow", "tokens": [509, 393, 498, 291, 498, 291, 500, 380, 362, 257, 18407, 291, 393, 360, 309, 322, 264, 13199, 558, 11, 457, 309, 311, 516, 281, 312, 16522, 356, 2964], "temperature": 0.0, "avg_logprob": -0.21347130669487846, "compression_ratio": 1.483568075117371, "no_speech_prob": 4.565938070300035e-06}, {"id": 606, "seek": 287402, "start": 2892.54, "end": 2893.98, "text": " Your", "tokens": [2260], "temperature": 0.0, "avg_logprob": -0.21347130669487846, "compression_ratio": 1.483568075117371, "no_speech_prob": 4.565938070300035e-06}, {"id": 607, "seek": 287402, "start": 2893.98, "end": 2898.78, "text": " Mac does not have a GPU that we can use for this", "tokens": [5707, 775, 406, 362, 257, 18407, 300, 321, 393, 764, 337, 341], "temperature": 0.0, "avg_logprob": -0.21347130669487846, "compression_ratio": 1.483568075117371, "no_speech_prob": 4.565938070300035e-06}, {"id": 608, "seek": 289878, "start": 2898.78, "end": 2903.2200000000003, "text": " Because I'm actually advertising today. We need an Nvidia GPU", "tokens": [1436, 286, 478, 767, 13097, 965, 13, 492, 643, 364, 46284, 18407], "temperature": 0.0, "avg_logprob": -0.2467477217964504, "compression_ratio": 1.578358208955224, "no_speech_prob": 6.339079845929518e-06}, {"id": 609, "seek": 289878, "start": 2904.2200000000003, "end": 2909.82, "text": " I would actually much prefer that we could use your Macs because competitions great right but", "tokens": [286, 576, 767, 709, 4382, 300, 321, 727, 764, 428, 5707, 82, 570, 26185, 869, 558, 457], "temperature": 0.0, "avg_logprob": -0.2467477217964504, "compression_ratio": 1.578358208955224, "no_speech_prob": 6.339079845929518e-06}, {"id": 610, "seek": 289878, "start": 2910.3, "end": 2915.3, "text": " Nvidia were really the first ones to create a GPU which did a good job of supporting general-purpose", "tokens": [46284, 645, 534, 264, 700, 2306, 281, 1884, 257, 18407, 597, 630, 257, 665, 1691, 295, 7231, 2674, 12, 42601], "temperature": 0.0, "avg_logprob": -0.2467477217964504, "compression_ratio": 1.578358208955224, "no_speech_prob": 6.339079845929518e-06}, {"id": 611, "seek": 289878, "start": 2916.46, "end": 2922.42, "text": " Graphics programming units GP GPU so in other words that means using a GPU for things other than playing computer games", "tokens": [21884, 1167, 9410, 6815, 26039, 18407, 370, 294, 661, 2283, 300, 1355, 1228, 257, 18407, 337, 721, 661, 813, 2433, 3820, 2813], "temperature": 0.0, "avg_logprob": -0.2467477217964504, "compression_ratio": 1.578358208955224, "no_speech_prob": 6.339079845929518e-06}, {"id": 612, "seek": 289878, "start": 2923.78, "end": 2926.5800000000004, "text": " They used they created a framework called CUDA", "tokens": [814, 1143, 436, 2942, 257, 8388, 1219, 29777, 7509], "temperature": 0.0, "avg_logprob": -0.2467477217964504, "compression_ratio": 1.578358208955224, "no_speech_prob": 6.339079845929518e-06}, {"id": 613, "seek": 292658, "start": 2926.58, "end": 2928.58, "text": " CUDA", "tokens": [29777, 7509], "temperature": 0.0, "avg_logprob": -0.1692296680651213, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809995360789e-06}, {"id": 614, "seek": 292658, "start": 2928.9, "end": 2933.2999999999997, "text": " It's it's a very good framework. It's pretty much universally used in deep learning", "tokens": [467, 311, 309, 311, 257, 588, 665, 8388, 13, 467, 311, 1238, 709, 43995, 1143, 294, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.1692296680651213, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809995360789e-06}, {"id": 615, "seek": 292658, "start": 2933.86, "end": 2939.94, "text": " If you don't have an Nvidia GPU you can't use it no no current Macs have an Nvidia GPU", "tokens": [759, 291, 500, 380, 362, 364, 46284, 18407, 291, 393, 380, 764, 309, 572, 572, 2190, 5707, 82, 362, 364, 46284, 18407], "temperature": 0.0, "avg_logprob": -0.1692296680651213, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809995360789e-06}, {"id": 616, "seek": 292658, "start": 2941.34, "end": 2947.74, "text": " Most laptops of any kind don't have an Nvidia GPU if you're interested in doing deep learning on your laptop", "tokens": [4534, 27642, 295, 604, 733, 500, 380, 362, 364, 46284, 18407, 498, 291, 434, 3102, 294, 884, 2452, 2539, 322, 428, 10732], "temperature": 0.0, "avg_logprob": -0.1692296680651213, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809995360789e-06}, {"id": 617, "seek": 292658, "start": 2947.94, "end": 2953.02, "text": " The good news is that you need to buy one which is really good for playing computer games on", "tokens": [440, 665, 2583, 307, 300, 291, 643, 281, 2256, 472, 597, 307, 534, 665, 337, 2433, 3820, 2813, 322], "temperature": 0.0, "avg_logprob": -0.1692296680651213, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809995360789e-06}, {"id": 618, "seek": 295302, "start": 2953.02, "end": 2960.86, "text": " There's a place called exotic PC gaming laptops where you can go and buy yourself a great laptop", "tokens": [821, 311, 257, 1081, 1219, 27063, 6465, 9703, 27642, 689, 291, 393, 352, 293, 2256, 1803, 257, 869, 10732], "temperature": 0.0, "avg_logprob": -0.36650609970092773, "compression_ratio": 1.5583756345177664, "no_speech_prob": 1.994709919017623e-06}, {"id": 619, "seek": 295302, "start": 2961.34, "end": 2965.58, "text": " For doing deep learning you can tell your parents that you need the money", "tokens": [1171, 884, 2452, 2539, 291, 393, 980, 428, 3152, 300, 291, 643, 264, 1460], "temperature": 0.0, "avg_logprob": -0.36650609970092773, "compression_ratio": 1.5583756345177664, "no_speech_prob": 1.994709919017623e-06}, {"id": 620, "seek": 295302, "start": 2966.62, "end": 2969.94, "text": " To do deep learning so could you please have?", "tokens": [1407, 360, 2452, 2539, 370, 727, 291, 1767, 362, 30], "temperature": 0.0, "avg_logprob": -0.36650609970092773, "compression_ratio": 1.5583756345177664, "no_speech_prob": 1.994709919017623e-06}, {"id": 621, "seek": 295302, "start": 2972.7, "end": 2978.62, "text": " Yeah, so you'll generally find a whole bunch of laptops with names like predator and viper", "tokens": [865, 11, 370, 291, 603, 5101, 915, 257, 1379, 3840, 295, 27642, 365, 5288, 411, 35377, 293, 1932, 610], "temperature": 0.0, "avg_logprob": -0.36650609970092773, "compression_ratio": 1.5583756345177664, "no_speech_prob": 1.994709919017623e-06}, {"id": 622, "seek": 297862, "start": 2978.62, "end": 2984.42, "text": " with pictures of robots and stuff so", "tokens": [365, 5242, 295, 14733, 293, 1507, 370], "temperature": 0.0, "avg_logprob": -0.2870139807042941, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.966933036281262e-06}, {"id": 623, "seek": 297862, "start": 2987.7799999999997, "end": 2991.7799999999997, "text": " Stealth Pro Raider leopard anyway", "tokens": [3592, 1302, 1705, 7591, 1438, 47161, 4033], "temperature": 0.0, "avg_logprob": -0.2870139807042941, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.966933036281262e-06}, {"id": 624, "seek": 297862, "start": 2993.66, "end": 2999.5, "text": " Having said that like I don't know that many people that do much deep learning on their laptop most people will log into a cloud environment", "tokens": [10222, 848, 300, 411, 286, 500, 380, 458, 300, 867, 561, 300, 360, 709, 2452, 2539, 322, 641, 10732, 881, 561, 486, 3565, 666, 257, 4588, 2823], "temperature": 0.0, "avg_logprob": -0.2870139807042941, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.966933036281262e-06}, {"id": 625, "seek": 297862, "start": 3000.66, "end": 3003.64, "text": " By far the easiest I know of to use is called Cressel", "tokens": [3146, 1400, 264, 12889, 286, 458, 295, 281, 764, 307, 1219, 383, 735, 338], "temperature": 0.0, "avg_logprob": -0.2870139807042941, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.966933036281262e-06}, {"id": 626, "seek": 297862, "start": 3005.02, "end": 3006.58, "text": " with Cressel", "tokens": [365, 383, 735, 338], "temperature": 0.0, "avg_logprob": -0.2870139807042941, "compression_ratio": 1.4946236559139785, "no_speech_prob": 3.966933036281262e-06}, {"id": 627, "seek": 300658, "start": 3006.58, "end": 3011.38, "text": " You can basically sign up and straight away the first thing you get is a", "tokens": [509, 393, 1936, 1465, 493, 293, 2997, 1314, 264, 700, 551, 291, 483, 307, 257], "temperature": 0.0, "avg_logprob": -0.24164542668982397, "compression_ratio": 1.463768115942029, "no_speech_prob": 4.157313469477231e-06}, {"id": 628, "seek": 300658, "start": 3012.22, "end": 3017.62, "text": " Thrown straight into a jupyter notebook backed by a GPU cost 60 cents an hour", "tokens": [41645, 648, 2997, 666, 257, 361, 1010, 88, 391, 21060, 20391, 538, 257, 18407, 2063, 4060, 14941, 364, 1773], "temperature": 0.0, "avg_logprob": -0.24164542668982397, "compression_ratio": 1.463768115942029, "no_speech_prob": 4.157313469477231e-06}, {"id": 629, "seek": 300658, "start": 3018.22, "end": 3022.54, "text": " With all of the fast AI libraries and data already available", "tokens": [2022, 439, 295, 264, 2370, 7318, 15148, 293, 1412, 1217, 2435], "temperature": 0.0, "avg_logprob": -0.24164542668982397, "compression_ratio": 1.463768115942029, "no_speech_prob": 4.157313469477231e-06}, {"id": 630, "seek": 300658, "start": 3023.2999999999997, "end": 3025.7, "text": " So that makes life really easy", "tokens": [407, 300, 1669, 993, 534, 1858], "temperature": 0.0, "avg_logprob": -0.24164542668982397, "compression_ratio": 1.463768115942029, "no_speech_prob": 4.157313469477231e-06}, {"id": 631, "seek": 300658, "start": 3027.2599999999998, "end": 3029.2599999999998, "text": " It's less", "tokens": [467, 311, 1570], "temperature": 0.0, "avg_logprob": -0.24164542668982397, "compression_ratio": 1.463768115942029, "no_speech_prob": 4.157313469477231e-06}, {"id": 632, "seek": 300658, "start": 3029.54, "end": 3034.2999999999997, "text": " Flexible and in some ways less fast than using AWS", "tokens": [29208, 964, 293, 294, 512, 2098, 1570, 2370, 813, 1228, 17650], "temperature": 0.0, "avg_logprob": -0.24164542668982397, "compression_ratio": 1.463768115942029, "no_speech_prob": 4.157313469477231e-06}, {"id": 633, "seek": 303430, "start": 3034.3, "end": 3037.6600000000003, "text": " Which is the Amazon web services option?", "tokens": [3013, 307, 264, 6795, 3670, 3328, 3614, 30], "temperature": 0.0, "avg_logprob": -0.18920018122746393, "compression_ratio": 1.5701357466063348, "no_speech_prob": 9.368609426019248e-06}, {"id": 634, "seek": 303430, "start": 3039.26, "end": 3043.02, "text": " Costs a little bit more 90 cents an hour rather than 60 cents an hour", "tokens": [20863, 82, 257, 707, 857, 544, 4289, 14941, 364, 1773, 2831, 813, 4060, 14941, 364, 1773], "temperature": 0.0, "avg_logprob": -0.18920018122746393, "compression_ratio": 1.5701357466063348, "no_speech_prob": 9.368609426019248e-06}, {"id": 635, "seek": 303430, "start": 3044.1800000000003, "end": 3047.1800000000003, "text": " But it's very likely that your employer", "tokens": [583, 309, 311, 588, 3700, 300, 428, 16205], "temperature": 0.0, "avg_logprob": -0.18920018122746393, "compression_ratio": 1.5701357466063348, "no_speech_prob": 9.368609426019248e-06}, {"id": 636, "seek": 303430, "start": 3048.0600000000004, "end": 3051.4, "text": " Is already using that it's like it's good to get to know anyway", "tokens": [1119, 1217, 1228, 300, 309, 311, 411, 309, 311, 665, 281, 483, 281, 458, 4033], "temperature": 0.0, "avg_logprob": -0.18920018122746393, "compression_ratio": 1.5701357466063348, "no_speech_prob": 9.368609426019248e-06}, {"id": 637, "seek": 303430, "start": 3052.0600000000004, "end": 3058.86, "text": " They've got more different choices around GPUs and it's a good good choice if you google for github student pack", "tokens": [814, 600, 658, 544, 819, 7994, 926, 18407, 82, 293, 309, 311, 257, 665, 665, 3922, 498, 291, 20742, 337, 290, 355, 836, 3107, 2844], "temperature": 0.0, "avg_logprob": -0.18920018122746393, "compression_ratio": 1.5701357466063348, "no_speech_prob": 9.368609426019248e-06}, {"id": 638, "seek": 303430, "start": 3059.6200000000003, "end": 3061.38, "text": " If you're a student", "tokens": [759, 291, 434, 257, 3107], "temperature": 0.0, "avg_logprob": -0.18920018122746393, "compression_ratio": 1.5701357466063348, "no_speech_prob": 9.368609426019248e-06}, {"id": 639, "seek": 306138, "start": 3061.38, "end": 3064.52, "text": " You can get a hundred and fifty dollars of credits", "tokens": [509, 393, 483, 257, 3262, 293, 13442, 3808, 295, 16816], "temperature": 0.0, "avg_logprob": -0.21284006719719872, "compression_ratio": 1.5142857142857142, "no_speech_prob": 6.2408248595602345e-06}, {"id": 640, "seek": 306138, "start": 3065.42, "end": 3070.42, "text": " Straight away pretty much and so that's a really good way to get started Daniel. Did you have a question?", "tokens": [26908, 1314, 1238, 709, 293, 370, 300, 311, 257, 534, 665, 636, 281, 483, 1409, 8033, 13, 2589, 291, 362, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.21284006719719872, "compression_ratio": 1.5142857142857142, "no_speech_prob": 6.2408248595602345e-06}, {"id": 641, "seek": 306138, "start": 3073.7000000000003, "end": 3080.62, "text": " I just wanted to know your opinion on I know that Intel recently published like an open source like", "tokens": [286, 445, 1415, 281, 458, 428, 4800, 322, 286, 458, 300, 19762, 3938, 6572, 411, 364, 1269, 4009, 411], "temperature": 0.0, "avg_logprob": -0.21284006719719872, "compression_ratio": 1.5142857142857142, "no_speech_prob": 6.2408248595602345e-06}, {"id": 642, "seek": 306138, "start": 3081.02, "end": 3085.06, "text": " way of like boosting like regular packages that they claim is", "tokens": [636, 295, 411, 43117, 411, 3890, 17401, 300, 436, 3932, 307], "temperature": 0.0, "avg_logprob": -0.21284006719719872, "compression_ratio": 1.5142857142857142, "no_speech_prob": 6.2408248595602345e-06}, {"id": 643, "seek": 308506, "start": 3085.06, "end": 3092.06, "text": " Equivalent like if you use the bottom tier GPU on your seat like on your CPU if you use their boost packages", "tokens": [15624, 3576, 317, 411, 498, 291, 764, 264, 2767, 12362, 18407, 322, 428, 6121, 411, 322, 428, 13199, 498, 291, 764, 641, 9194, 17401], "temperature": 0.0, "avg_logprob": -0.168485521015368, "compression_ratio": 1.558490566037736, "no_speech_prob": 1.903372208289511e-06}, {"id": 644, "seek": 308506, "start": 3092.22, "end": 3094.22, "text": " Like you can get the same performance", "tokens": [1743, 291, 393, 483, 264, 912, 3389], "temperature": 0.0, "avg_logprob": -0.168485521015368, "compression_ratio": 1.558490566037736, "no_speech_prob": 1.903372208289511e-06}, {"id": 645, "seek": 308506, "start": 3094.7, "end": 3099.86, "text": " Do you know anything about that? Yeah, I do it's a good question. So and actually Intel makes some great", "tokens": [1144, 291, 458, 1340, 466, 300, 30, 865, 11, 286, 360, 309, 311, 257, 665, 1168, 13, 407, 293, 767, 19762, 1669, 512, 869], "temperature": 0.0, "avg_logprob": -0.168485521015368, "compression_ratio": 1.558490566037736, "no_speech_prob": 1.903372208289511e-06}, {"id": 646, "seek": 308506, "start": 3100.7799999999997, "end": 3105.74, "text": " Numerical programming libraries particularly this one called MKL the matrix kernel library", "tokens": [426, 15583, 804, 9410, 15148, 4098, 341, 472, 1219, 30770, 43, 264, 8141, 28256, 6405], "temperature": 0.0, "avg_logprob": -0.168485521015368, "compression_ratio": 1.558490566037736, "no_speech_prob": 1.903372208289511e-06}, {"id": 647, "seek": 308506, "start": 3107.06, "end": 3108.82, "text": " they", "tokens": [436], "temperature": 0.0, "avg_logprob": -0.168485521015368, "compression_ratio": 1.558490566037736, "no_speech_prob": 1.903372208289511e-06}, {"id": 648, "seek": 308506, "start": 3108.82, "end": 3112.2599999999998, "text": " They definitely make things faster than not using those libraries", "tokens": [814, 2138, 652, 721, 4663, 813, 406, 1228, 729, 15148], "temperature": 0.0, "avg_logprob": -0.168485521015368, "compression_ratio": 1.558490566037736, "no_speech_prob": 1.903372208289511e-06}, {"id": 649, "seek": 311226, "start": 3112.26, "end": 3116.42, "text": " But if you look at a graph of performance over time", "tokens": [583, 498, 291, 574, 412, 257, 4295, 295, 3389, 670, 565], "temperature": 0.0, "avg_logprob": -0.25485084533691404, "compression_ratio": 1.5, "no_speech_prob": 5.4221659411268774e-06}, {"id": 650, "seek": 311226, "start": 3117.3, "end": 3121.34, "text": " GPUs have consistently throughout the last 10 years including now", "tokens": [18407, 82, 362, 14961, 3710, 264, 1036, 1266, 924, 3009, 586], "temperature": 0.0, "avg_logprob": -0.25485084533691404, "compression_ratio": 1.5, "no_speech_prob": 5.4221659411268774e-06}, {"id": 651, "seek": 311226, "start": 3121.94, "end": 3128.38, "text": " Are about 10 times more floating point operations per second than the equivalent CPU?", "tokens": [2014, 466, 1266, 1413, 544, 12607, 935, 7705, 680, 1150, 813, 264, 10344, 13199, 30], "temperature": 0.0, "avg_logprob": -0.25485084533691404, "compression_ratio": 1.5, "no_speech_prob": 5.4221659411268774e-06}, {"id": 652, "seek": 311226, "start": 3129.1800000000003, "end": 3133.86, "text": " And they're generally about a fifth of the price for that performance", "tokens": [400, 436, 434, 5101, 466, 257, 9266, 295, 264, 3218, 337, 300, 3389], "temperature": 0.0, "avg_logprob": -0.25485084533691404, "compression_ratio": 1.5, "no_speech_prob": 5.4221659411268774e-06}, {"id": 653, "seek": 311226, "start": 3135.2200000000003, "end": 3136.34, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.25485084533691404, "compression_ratio": 1.5, "no_speech_prob": 5.4221659411268774e-06}, {"id": 654, "seek": 311226, "start": 3136.34, "end": 3140.9, "text": " Yeah, it it and then because of that like", "tokens": [865, 11, 309, 309, 293, 550, 570, 295, 300, 411], "temperature": 0.0, "avg_logprob": -0.25485084533691404, "compression_ratio": 1.5, "no_speech_prob": 5.4221659411268774e-06}, {"id": 655, "seek": 314090, "start": 3140.9, "end": 3148.98, "text": " Everybody doing anything with deep learning basically does it on Nvidia GPUs and therefore using anything other than Nvidia GPUs is", "tokens": [7646, 884, 1340, 365, 2452, 2539, 1936, 775, 309, 322, 46284, 18407, 82, 293, 4412, 1228, 1340, 661, 813, 46284, 18407, 82, 307], "temperature": 0.0, "avg_logprob": -0.19116972684860228, "compression_ratio": 1.6143497757847534, "no_speech_prob": 3.647714402177371e-05}, {"id": 656, "seek": 314090, "start": 3149.38, "end": 3151.38, "text": " currently very annoying", "tokens": [4362, 588, 11304], "temperature": 0.0, "avg_logprob": -0.19116972684860228, "compression_ratio": 1.6143497757847534, "no_speech_prob": 3.647714402177371e-05}, {"id": 657, "seek": 314090, "start": 3151.58, "end": 3153.58, "text": " So slower more expensive more annoying", "tokens": [407, 14009, 544, 5124, 544, 11304], "temperature": 0.0, "avg_logprob": -0.19116972684860228, "compression_ratio": 1.6143497757847534, "no_speech_prob": 3.647714402177371e-05}, {"id": 658, "seek": 314090, "start": 3153.58, "end": 3159.6600000000003, "text": " I really hope there will be more activity around AMG GPUs in particular in this area", "tokens": [286, 534, 1454, 456, 486, 312, 544, 5191, 926, 6475, 38, 18407, 82, 294, 1729, 294, 341, 1859], "temperature": 0.0, "avg_logprob": -0.19116972684860228, "compression_ratio": 1.6143497757847534, "no_speech_prob": 3.647714402177371e-05}, {"id": 659, "seek": 314090, "start": 3159.6600000000003, "end": 3163.94, "text": " But AMD's got like literally years of catching up to do so it might take a while", "tokens": [583, 34808, 311, 658, 411, 3736, 924, 295, 16124, 493, 281, 360, 370, 309, 1062, 747, 257, 1339], "temperature": 0.0, "avg_logprob": -0.19116972684860228, "compression_ratio": 1.6143497757847534, "no_speech_prob": 3.647714402177371e-05}, {"id": 660, "seek": 316394, "start": 3163.94, "end": 3171.7400000000002, "text": " Yeah, so I just wanted to point out that you can also buy things such as like a GPU extender to a laptop", "tokens": [865, 11, 370, 286, 445, 1415, 281, 935, 484, 300, 291, 393, 611, 2256, 721, 1270, 382, 411, 257, 18407, 1279, 3216, 281, 257, 10732], "temperature": 0.0, "avg_logprob": -0.19838280831613847, "compression_ratio": 1.7649122807017543, "no_speech_prob": 5.594187769020209e-06}, {"id": 661, "seek": 316394, "start": 3171.7400000000002, "end": 3176.34, "text": " Yeah, that's also like kind of like like maybe a first step solution. Yeah, you really want to put something on", "tokens": [865, 11, 300, 311, 611, 411, 733, 295, 411, 411, 1310, 257, 700, 1823, 3827, 13, 865, 11, 291, 534, 528, 281, 829, 746, 322], "temperature": 0.0, "avg_logprob": -0.19838280831613847, "compression_ratio": 1.7649122807017543, "no_speech_prob": 5.594187769020209e-06}, {"id": 662, "seek": 316394, "start": 3176.46, "end": 3177.46, "text": " Yeah, yeah", "tokens": [865, 11, 1338], "temperature": 0.0, "avg_logprob": -0.19838280831613847, "compression_ratio": 1.7649122807017543, "no_speech_prob": 5.594187769020209e-06}, {"id": 663, "seek": 316394, "start": 3177.46, "end": 3183.94, "text": " I think for like 300 bucks or so you can buy something that plugs into your thunderbolt port if you have a Mac and then for another", "tokens": [286, 519, 337, 411, 6641, 11829, 420, 370, 291, 393, 2256, 746, 300, 33899, 666, 428, 19898, 39477, 2436, 498, 291, 362, 257, 5707, 293, 550, 337, 1071], "temperature": 0.0, "avg_logprob": -0.19838280831613847, "compression_ratio": 1.7649122807017543, "no_speech_prob": 5.594187769020209e-06}, {"id": 664, "seek": 316394, "start": 3183.94, "end": 3186.54, "text": " Five or six hundred bucks you can buy a GPU to plug into that", "tokens": [9436, 420, 2309, 3262, 11829, 291, 393, 2256, 257, 18407, 281, 5452, 666, 300], "temperature": 0.0, "avg_logprob": -0.19838280831613847, "compression_ratio": 1.7649122807017543, "no_speech_prob": 5.594187769020209e-06}, {"id": 665, "seek": 316394, "start": 3187.1, "end": 3191.62, "text": " Having said that for about a thousand bucks you can actually create a pretty good", "tokens": [10222, 848, 300, 337, 466, 257, 4714, 11829, 291, 393, 767, 1884, 257, 1238, 665], "temperature": 0.0, "avg_logprob": -0.19838280831613847, "compression_ratio": 1.7649122807017543, "no_speech_prob": 5.594187769020209e-06}, {"id": 666, "seek": 319162, "start": 3191.62, "end": 3194.1, "text": " You know GPU based desktop", "tokens": [509, 458, 18407, 2361, 14502], "temperature": 0.0, "avg_logprob": -0.24149096289346383, "compression_ratio": 1.4933920704845816, "no_speech_prob": 5.862736088602105e-06}, {"id": 667, "seek": 319162, "start": 3194.66, "end": 3200.58, "text": " And so if you're considering that the fast AI forums have like lots of threads where people help each other", "tokens": [400, 370, 498, 291, 434, 8079, 300, 264, 2370, 7318, 26998, 362, 411, 3195, 295, 19314, 689, 561, 854, 1184, 661], "temperature": 0.0, "avg_logprob": -0.24149096289346383, "compression_ratio": 1.4933920704845816, "no_speech_prob": 5.862736088602105e-06}, {"id": 668, "seek": 319162, "start": 3200.8599999999997, "end": 3203.3399999999997, "text": " Spec out something at a particular price point", "tokens": [20484, 484, 746, 412, 257, 1729, 3218, 935], "temperature": 0.0, "avg_logprob": -0.24149096289346383, "compression_ratio": 1.4933920704845816, "no_speech_prob": 5.862736088602105e-06}, {"id": 669, "seek": 319162, "start": 3204.3399999999997, "end": 3206.5, "text": " Anyway, so to start with I'd say use Cressul", "tokens": [5684, 11, 370, 281, 722, 365, 286, 1116, 584, 764, 383, 735, 425], "temperature": 0.0, "avg_logprob": -0.24149096289346383, "compression_ratio": 1.4933920704845816, "no_speech_prob": 5.862736088602105e-06}, {"id": 670, "seek": 319162, "start": 3207.1, "end": 3209.06, "text": " and then", "tokens": [293, 550], "temperature": 0.0, "avg_logprob": -0.24149096289346383, "compression_ratio": 1.4933920704845816, "no_speech_prob": 5.862736088602105e-06}, {"id": 671, "seek": 319162, "start": 3209.06, "end": 3215.54, "text": " You know when you're ready to invest a few extra minutes getting going use AWS", "tokens": [509, 458, 562, 291, 434, 1919, 281, 1963, 257, 1326, 2857, 2077, 1242, 516, 764, 17650], "temperature": 0.0, "avg_logprob": -0.24149096289346383, "compression_ratio": 1.4933920704845816, "no_speech_prob": 5.862736088602105e-06}, {"id": 672, "seek": 319162, "start": 3216.66, "end": 3219.02, "text": " to use AWS you basically", "tokens": [281, 764, 17650, 291, 1936], "temperature": 0.0, "avg_logprob": -0.24149096289346383, "compression_ratio": 1.4933920704845816, "no_speech_prob": 5.862736088602105e-06}, {"id": 673, "seek": 321902, "start": 3219.02, "end": 3222.42, "text": " Huh yeah, yeah", "tokens": [8063, 1338, 11, 1338], "temperature": 0.0, "avg_logprob": -0.3115517436594203, "compression_ratio": 1.3987341772151898, "no_speech_prob": 8.139483725244645e-06}, {"id": 674, "seek": 321902, "start": 3225.2599999999998, "end": 3228.32, "text": " Yeah, I'm just talking to the folks online as well so", "tokens": [865, 11, 286, 478, 445, 1417, 281, 264, 4024, 2950, 382, 731, 370], "temperature": 0.0, "avg_logprob": -0.3115517436594203, "compression_ratio": 1.3987341772151898, "no_speech_prob": 8.139483725244645e-06}, {"id": 675, "seek": 321902, "start": 3232.78, "end": 3234.78, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.3115517436594203, "compression_ratio": 1.3987341772151898, "no_speech_prob": 8.139483725244645e-06}, {"id": 676, "seek": 321902, "start": 3235.06, "end": 3238.94, "text": " So AWS when you get there go to EC2", "tokens": [407, 17650, 562, 291, 483, 456, 352, 281, 19081, 17], "temperature": 0.0, "avg_logprob": -0.3115517436594203, "compression_ratio": 1.3987341772151898, "no_speech_prob": 8.139483725244645e-06}, {"id": 677, "seek": 321902, "start": 3240.42, "end": 3247.2599999999998, "text": " EC2 like there's lots of stuff on AWS EC2 is the bit where we get to like rent computers by the hour right?", "tokens": [19081, 17, 411, 456, 311, 3195, 295, 1507, 322, 17650, 19081, 17, 307, 264, 857, 689, 321, 483, 281, 411, 6214, 10807, 538, 264, 1773, 558, 30], "temperature": 0.0, "avg_logprob": -0.3115517436594203, "compression_ratio": 1.3987341772151898, "no_speech_prob": 8.139483725244645e-06}, {"id": 678, "seek": 324726, "start": 3247.26, "end": 3250.98, "text": " Now we're going to need a GPU based", "tokens": [823, 321, 434, 516, 281, 643, 257, 18407, 2361], "temperature": 0.0, "avg_logprob": -0.1978960625919295, "compression_ratio": 1.5175879396984924, "no_speech_prob": 7.4112149377469905e-06}, {"id": 679, "seek": 324726, "start": 3251.98, "end": 3253.98, "text": " instance", "tokens": [5197], "temperature": 0.0, "avg_logprob": -0.1978960625919295, "compression_ratio": 1.5175879396984924, "no_speech_prob": 7.4112149377469905e-06}, {"id": 680, "seek": 324726, "start": 3254.26, "end": 3258.38, "text": " Unfortunately when you first sign up for AWS they don't give you access to them", "tokens": [8590, 562, 291, 700, 1465, 493, 337, 17650, 436, 500, 380, 976, 291, 2105, 281, 552], "temperature": 0.0, "avg_logprob": -0.1978960625919295, "compression_ratio": 1.5175879396984924, "no_speech_prob": 7.4112149377469905e-06}, {"id": 681, "seek": 324726, "start": 3258.7000000000003, "end": 3265.42, "text": " So you have to request that access so go to limits up on the top left right and the main", "tokens": [407, 291, 362, 281, 5308, 300, 2105, 370, 352, 281, 10406, 493, 322, 264, 1192, 1411, 558, 293, 264, 2135], "temperature": 0.0, "avg_logprob": -0.1978960625919295, "compression_ratio": 1.5175879396984924, "no_speech_prob": 7.4112149377469905e-06}, {"id": 682, "seek": 324726, "start": 3266.3, "end": 3273.1000000000004, "text": " GPU instance will be using is called the P2 so scroll down to P2 and here P2 dot x large", "tokens": [18407, 5197, 486, 312, 1228, 307, 1219, 264, 430, 17, 370, 11369, 760, 281, 430, 17, 293, 510, 430, 17, 5893, 2031, 2416], "temperature": 0.0, "avg_logprob": -0.1978960625919295, "compression_ratio": 1.5175879396984924, "no_speech_prob": 7.4112149377469905e-06}, {"id": 683, "seek": 327310, "start": 3273.1, "end": 3277.58, "text": " You need to make sure that that number is not zero if you've just got a new account", "tokens": [509, 643, 281, 652, 988, 300, 300, 1230, 307, 406, 4018, 498, 291, 600, 445, 658, 257, 777, 2696], "temperature": 0.0, "avg_logprob": -0.14717852512252666, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.3405165140720783e-06}, {"id": 684, "seek": 327310, "start": 3277.58, "end": 3284.22, "text": " It probably is zero which means you won't be allowed to create one so you have to go request limit increase and the trick there is", "tokens": [467, 1391, 307, 4018, 597, 1355, 291, 1582, 380, 312, 4350, 281, 1884, 472, 370, 291, 362, 281, 352, 5308, 4948, 3488, 293, 264, 4282, 456, 307], "temperature": 0.0, "avg_logprob": -0.14717852512252666, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.3405165140720783e-06}, {"id": 685, "seek": 327310, "start": 3284.62, "end": 3288.66, "text": " When it asks you why do you want the limit increase type faster AI?", "tokens": [1133, 309, 8962, 291, 983, 360, 291, 528, 264, 4948, 3488, 2010, 4663, 7318, 30], "temperature": 0.0, "avg_logprob": -0.14717852512252666, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.3405165140720783e-06}, {"id": 686, "seek": 327310, "start": 3289.02, "end": 3295.8199999999997, "text": " Because AWS knows to look out and they know that faster AI people are good people so they'll do it quite quickly", "tokens": [1436, 17650, 3255, 281, 574, 484, 293, 436, 458, 300, 4663, 7318, 561, 366, 665, 561, 370, 436, 603, 360, 309, 1596, 2661], "temperature": 0.0, "avg_logprob": -0.14717852512252666, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.3405165140720783e-06}, {"id": 687, "seek": 327310, "start": 3296.58, "end": 3299.7599999999998, "text": " That takes a day or two generally speaking to go through", "tokens": [663, 2516, 257, 786, 420, 732, 5101, 4124, 281, 352, 807], "temperature": 0.0, "avg_logprob": -0.14717852512252666, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.3405165140720783e-06}, {"id": 688, "seek": 329976, "start": 3299.76, "end": 3304.2000000000003, "text": " So once you get the email saying you've been approved for P2 instances", "tokens": [407, 1564, 291, 483, 264, 3796, 1566, 291, 600, 668, 10826, 337, 430, 17, 14519], "temperature": 0.0, "avg_logprob": -0.2026736015497252, "compression_ratio": 1.5570776255707763, "no_speech_prob": 3.6119433843850857e-06}, {"id": 689, "seek": 329976, "start": 3304.84, "end": 3308.1200000000003, "text": " You can then go back here and say launch instance", "tokens": [509, 393, 550, 352, 646, 510, 293, 584, 4025, 5197], "temperature": 0.0, "avg_logprob": -0.2026736015497252, "compression_ratio": 1.5570776255707763, "no_speech_prob": 3.6119433843850857e-06}, {"id": 690, "seek": 329976, "start": 3309.48, "end": 3315.76, "text": " And so we've basically set up one that has everything you need so if you click on community AMI and", "tokens": [400, 370, 321, 600, 1936, 992, 493, 472, 300, 575, 1203, 291, 643, 370, 498, 291, 2052, 322, 1768, 6475, 40, 293], "temperature": 0.0, "avg_logprob": -0.2026736015497252, "compression_ratio": 1.5570776255707763, "no_speech_prob": 3.6119433843850857e-06}, {"id": 691, "seek": 329976, "start": 3316.0800000000004, "end": 3321.2400000000002, "text": " AMI is an Amazon machine image. It's basically a completely set up", "tokens": [6475, 40, 307, 364, 6795, 3479, 3256, 13, 467, 311, 1936, 257, 2584, 992, 493], "temperature": 0.0, "avg_logprob": -0.2026736015497252, "compression_ratio": 1.5570776255707763, "no_speech_prob": 3.6119433843850857e-06}, {"id": 692, "seek": 329976, "start": 3322.0, "end": 3326.0800000000004, "text": " Computer right and so if you type fast AI or one word", "tokens": [22289, 558, 293, 370, 498, 291, 2010, 2370, 7318, 420, 472, 1349], "temperature": 0.0, "avg_logprob": -0.2026736015497252, "compression_ratio": 1.5570776255707763, "no_speech_prob": 3.6119433843850857e-06}, {"id": 693, "seek": 332608, "start": 3326.08, "end": 3334.6, "text": " You'll find here fast AI deal part one version two for the P2 right so that's all set up ready to go", "tokens": [509, 603, 915, 510, 2370, 7318, 2028, 644, 472, 3037, 732, 337, 264, 430, 17, 558, 370, 300, 311, 439, 992, 493, 1919, 281, 352], "temperature": 0.0, "avg_logprob": -0.2379950056684778, "compression_ratio": 1.5467289719626167, "no_speech_prob": 3.089474375883583e-06}, {"id": 694, "seek": 332608, "start": 3334.6, "end": 3336.6, "text": " so if you click on", "tokens": [370, 498, 291, 2052, 322], "temperature": 0.0, "avg_logprob": -0.2379950056684778, "compression_ratio": 1.5467289719626167, "no_speech_prob": 3.089474375883583e-06}, {"id": 695, "seek": 332608, "start": 3336.92, "end": 3342.88, "text": " Select and it'll say okay. What kind of computer do you want right and so we have to say all right?", "tokens": [13638, 293, 309, 603, 584, 1392, 13, 708, 733, 295, 3820, 360, 291, 528, 558, 293, 370, 321, 362, 281, 584, 439, 558, 30], "temperature": 0.0, "avg_logprob": -0.2379950056684778, "compression_ratio": 1.5467289719626167, "no_speech_prob": 3.089474375883583e-06}, {"id": 696, "seek": 332608, "start": 3342.88, "end": 3346.16, "text": " I want a GPU compute type and", "tokens": [286, 528, 257, 18407, 14722, 2010, 293], "temperature": 0.0, "avg_logprob": -0.2379950056684778, "compression_ratio": 1.5467289719626167, "no_speech_prob": 3.089474375883583e-06}, {"id": 697, "seek": 332608, "start": 3348.04, "end": 3350.04, "text": " Specifically I want a P2", "tokens": [26058, 286, 528, 257, 430, 17], "temperature": 0.0, "avg_logprob": -0.2379950056684778, "compression_ratio": 1.5467289719626167, "no_speech_prob": 3.089474375883583e-06}, {"id": 698, "seek": 332608, "start": 3350.16, "end": 3351.48, "text": " extra large", "tokens": [2857, 2416], "temperature": 0.0, "avg_logprob": -0.2379950056684778, "compression_ratio": 1.5467289719626167, "no_speech_prob": 3.089474375883583e-06}, {"id": 699, "seek": 332608, "start": 3351.48, "end": 3353.68, "text": " Right and then you can say review and launch", "tokens": [1779, 293, 550, 291, 393, 584, 3131, 293, 4025], "temperature": 0.0, "avg_logprob": -0.2379950056684778, "compression_ratio": 1.5467289719626167, "no_speech_prob": 3.089474375883583e-06}, {"id": 700, "seek": 335368, "start": 3353.68, "end": 3360.6, "text": " I'm assuming you already know how to deal with SSH keys and all that kind of stuff if you don't check out the", "tokens": [286, 478, 11926, 291, 1217, 458, 577, 281, 2028, 365, 12238, 39, 9317, 293, 439, 300, 733, 295, 1507, 498, 291, 500, 380, 1520, 484, 264], "temperature": 0.0, "avg_logprob": -0.18462367441462374, "compression_ratio": 1.5569620253164558, "no_speech_prob": 5.3380981626105495e-06}, {"id": 701, "seek": 335368, "start": 3361.56, "end": 3367.8399999999997, "text": " introductory tutorials and workshop videos that we have online or Google around for SSH keys", "tokens": [39048, 17616, 293, 13541, 2145, 300, 321, 362, 2950, 420, 3329, 926, 337, 12238, 39, 9317], "temperature": 0.0, "avg_logprob": -0.18462367441462374, "compression_ratio": 1.5569620253164558, "no_speech_prob": 5.3380981626105495e-06}, {"id": 702, "seek": 335368, "start": 3369.2799999999997, "end": 3375.9199999999996, "text": " Very important skill to know anyway all right, so hopefully you get through all that you have", "tokens": [4372, 1021, 5389, 281, 458, 4033, 439, 558, 11, 370, 4696, 291, 483, 807, 439, 300, 291, 362], "temperature": 0.0, "avg_logprob": -0.18462367441462374, "compression_ratio": 1.5569620253164558, "no_speech_prob": 5.3380981626105495e-06}, {"id": 703, "seek": 335368, "start": 3377.3999999999996, "end": 3382.3599999999997, "text": " Something running on a GPU with the fast AI repo if you use Cressel just", "tokens": [6595, 2614, 322, 257, 18407, 365, 264, 2370, 7318, 49040, 498, 291, 764, 383, 735, 338, 445], "temperature": 0.0, "avg_logprob": -0.18462367441462374, "compression_ratio": 1.5569620253164558, "no_speech_prob": 5.3380981626105495e-06}, {"id": 704, "seek": 338236, "start": 3382.36, "end": 3386.7200000000003, "text": " See D fast AI to the repos already there get Paul", "tokens": [3008, 413, 2370, 7318, 281, 264, 1085, 329, 1217, 456, 483, 4552], "temperature": 0.0, "avg_logprob": -0.2421720542159735, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.565944436762948e-06}, {"id": 705, "seek": 338236, "start": 3387.88, "end": 3392.0, "text": " AWS CD fast AI the repo was already there get Paul", "tokens": [17650, 6743, 2370, 7318, 264, 49040, 390, 1217, 456, 483, 4552], "temperature": 0.0, "avg_logprob": -0.2421720542159735, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.565944436762948e-06}, {"id": 706, "seek": 338236, "start": 3393.0, "end": 3397.2400000000002, "text": " If it's your own computer, you'll just have to get clone and then away you go", "tokens": [759, 309, 311, 428, 1065, 3820, 11, 291, 603, 445, 362, 281, 483, 26506, 293, 550, 1314, 291, 352], "temperature": 0.0, "avg_logprob": -0.2421720542159735, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.565944436762948e-06}, {"id": 707, "seek": 338236, "start": 3398.2000000000003, "end": 3405.98, "text": " All right, so part of all of those is pytorch is pre-installed and so pytorch basically means we can write code", "tokens": [1057, 558, 11, 370, 644, 295, 439, 295, 729, 307, 25878, 284, 339, 307, 659, 12, 13911, 8907, 293, 370, 25878, 284, 339, 1936, 1355, 321, 393, 2464, 3089], "temperature": 0.0, "avg_logprob": -0.2421720542159735, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.565944436762948e-06}, {"id": 708, "seek": 338236, "start": 3405.98, "end": 3410.6400000000003, "text": " That looks a lot like numpy, but it's going to run really quickly on the GPU", "tokens": [663, 1542, 257, 688, 411, 1031, 8200, 11, 457, 309, 311, 516, 281, 1190, 534, 2661, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.2421720542159735, "compression_ratio": 1.6026200873362446, "no_speech_prob": 4.565944436762948e-06}, {"id": 709, "seek": 341064, "start": 3410.64, "end": 3412.64, "text": " secondly", "tokens": [26246], "temperature": 0.0, "avg_logprob": -0.15323949541364398, "compression_ratio": 1.7848101265822784, "no_speech_prob": 3.844918410322862e-06}, {"id": 710, "seek": 341064, "start": 3413.4, "end": 3419.98, "text": " Since we need to know like which direction and how much to move our parameters to improve our loss", "tokens": [4162, 321, 643, 281, 458, 411, 597, 3513, 293, 577, 709, 281, 1286, 527, 9834, 281, 3470, 527, 4470], "temperature": 0.0, "avg_logprob": -0.15323949541364398, "compression_ratio": 1.7848101265822784, "no_speech_prob": 3.844918410322862e-06}, {"id": 711, "seek": 341064, "start": 3420.24, "end": 3422.5, "text": " We need to know the derivative of functions", "tokens": [492, 643, 281, 458, 264, 13760, 295, 6828], "temperature": 0.0, "avg_logprob": -0.15323949541364398, "compression_ratio": 1.7848101265822784, "no_speech_prob": 3.844918410322862e-06}, {"id": 712, "seek": 341064, "start": 3423.12, "end": 3428.92, "text": " Pytorch has this amazing thing where any code you write using the pytorch library", "tokens": [430, 4328, 284, 339, 575, 341, 2243, 551, 689, 604, 3089, 291, 2464, 1228, 264, 25878, 284, 339, 6405], "temperature": 0.0, "avg_logprob": -0.15323949541364398, "compression_ratio": 1.7848101265822784, "no_speech_prob": 3.844918410322862e-06}, {"id": 713, "seek": 341064, "start": 3429.16, "end": 3431.8399999999997, "text": " It can automatically take the derivative of that for you", "tokens": [467, 393, 6772, 747, 264, 13760, 295, 300, 337, 291], "temperature": 0.0, "avg_logprob": -0.15323949541364398, "compression_ratio": 1.7848101265822784, "no_speech_prob": 3.844918410322862e-06}, {"id": 714, "seek": 341064, "start": 3432.08, "end": 3435.3199999999997, "text": " So we're not going to look at any calculus in this course", "tokens": [407, 321, 434, 406, 516, 281, 574, 412, 604, 33400, 294, 341, 1164], "temperature": 0.0, "avg_logprob": -0.15323949541364398, "compression_ratio": 1.7848101265822784, "no_speech_prob": 3.844918410322862e-06}, {"id": 715, "seek": 341064, "start": 3435.3199999999997, "end": 3439.08, "text": " And I don't look at any calculus in any of my courses or in any of my work", "tokens": [400, 286, 500, 380, 574, 412, 604, 33400, 294, 604, 295, 452, 7712, 420, 294, 604, 295, 452, 589], "temperature": 0.0, "avg_logprob": -0.15323949541364398, "compression_ratio": 1.7848101265822784, "no_speech_prob": 3.844918410322862e-06}, {"id": 716, "seek": 343908, "start": 3439.08, "end": 3445.44, "text": " Basically ever in terms of like actually calculating derivatives myself because I've never had to", "tokens": [8537, 1562, 294, 2115, 295, 411, 767, 28258, 33733, 2059, 570, 286, 600, 1128, 632, 281], "temperature": 0.0, "avg_logprob": -0.1731723986173931, "compression_ratio": 1.7043478260869565, "no_speech_prob": 5.338127266441006e-06}, {"id": 717, "seek": 343908, "start": 3446.08, "end": 3450.4, "text": " It's done for me by the library so as long as you write the Python code", "tokens": [467, 311, 1096, 337, 385, 538, 264, 6405, 370, 382, 938, 382, 291, 2464, 264, 15329, 3089], "temperature": 0.0, "avg_logprob": -0.1731723986173931, "compression_ratio": 1.7043478260869565, "no_speech_prob": 5.338127266441006e-06}, {"id": 718, "seek": 343908, "start": 3450.56, "end": 3452.08, "text": " It's the derivative is done", "tokens": [467, 311, 264, 13760, 307, 1096], "temperature": 0.0, "avg_logprob": -0.1731723986173931, "compression_ratio": 1.7043478260869565, "no_speech_prob": 5.338127266441006e-06}, {"id": 719, "seek": 343908, "start": 3452.08, "end": 3457.16, "text": " So the only calculus you really need to know to be an effective practitioner is like what is it?", "tokens": [407, 264, 787, 33400, 291, 534, 643, 281, 458, 281, 312, 364, 4942, 32125, 307, 411, 437, 307, 309, 30], "temperature": 0.0, "avg_logprob": -0.1731723986173931, "compression_ratio": 1.7043478260869565, "no_speech_prob": 5.338127266441006e-06}, {"id": 720, "seek": 343908, "start": 3457.16, "end": 3459.16, "text": " What does it mean to be a derivative?", "tokens": [708, 775, 309, 914, 281, 312, 257, 13760, 30], "temperature": 0.0, "avg_logprob": -0.1731723986173931, "compression_ratio": 1.7043478260869565, "no_speech_prob": 5.338127266441006e-06}, {"id": 721, "seek": 343908, "start": 3459.36, "end": 3462.24, "text": " And you also need to know the chain rule which will come to", "tokens": [400, 291, 611, 643, 281, 458, 264, 5021, 4978, 597, 486, 808, 281], "temperature": 0.0, "avg_logprob": -0.1731723986173931, "compression_ratio": 1.7043478260869565, "no_speech_prob": 5.338127266441006e-06}, {"id": 722, "seek": 346224, "start": 3462.24, "end": 3469.12, "text": " You all right, so we're going to start out kind of top-down", "tokens": [509, 439, 558, 11, 370, 321, 434, 516, 281, 722, 484, 733, 295, 1192, 12, 5093], "temperature": 0.0, "avg_logprob": -0.19580992530373967, "compression_ratio": 1.7342995169082125, "no_speech_prob": 2.5215592813765397e-06}, {"id": 723, "seek": 346224, "start": 3469.7599999999998, "end": 3475.04, "text": " Create a neural net and we're going to assume a whole bunch of stuff and gradually we're going to dig into each piece", "tokens": [20248, 257, 18161, 2533, 293, 321, 434, 516, 281, 6552, 257, 1379, 3840, 295, 1507, 293, 13145, 321, 434, 516, 281, 2528, 666, 1184, 2522], "temperature": 0.0, "avg_logprob": -0.19580992530373967, "compression_ratio": 1.7342995169082125, "no_speech_prob": 2.5215592813765397e-06}, {"id": 724, "seek": 346224, "start": 3475.2799999999997, "end": 3481.08, "text": " Right so to create neural nets we need to import the pytorch neural net library", "tokens": [1779, 370, 281, 1884, 18161, 36170, 321, 643, 281, 974, 264, 25878, 284, 339, 18161, 2533, 6405], "temperature": 0.0, "avg_logprob": -0.19580992530373967, "compression_ratio": 1.7342995169082125, "no_speech_prob": 2.5215592813765397e-06}, {"id": 725, "seek": 346224, "start": 3481.9599999999996, "end": 3489.24, "text": " Pytorch funnily enough is not called pytorch. It's called torch. Okay, so torch dot nn is the", "tokens": [430, 4328, 284, 339, 1019, 77, 953, 1547, 307, 406, 1219, 25878, 284, 339, 13, 467, 311, 1219, 27822, 13, 1033, 11, 370, 27822, 5893, 297, 77, 307, 264], "temperature": 0.0, "avg_logprob": -0.19580992530373967, "compression_ratio": 1.7342995169082125, "no_speech_prob": 2.5215592813765397e-06}, {"id": 726, "seek": 346224, "start": 3489.8799999999997, "end": 3491.6, "text": " pytorch", "tokens": [25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.19580992530373967, "compression_ratio": 1.7342995169082125, "no_speech_prob": 2.5215592813765397e-06}, {"id": 727, "seek": 349160, "start": 3491.6, "end": 3493.6, "text": " Fastsection that's responsible for neural nets", "tokens": [15968, 11963, 300, 311, 6250, 337, 18161, 36170], "temperature": 0.0, "avg_logprob": -0.16707353493602006, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.02943987865001e-06}, {"id": 728, "seek": 349160, "start": 3493.7599999999998, "end": 3499.24, "text": " Okay, so we'll call that nn and then we're going to import a few bits out of fast AI just to make life a bit easier", "tokens": [1033, 11, 370, 321, 603, 818, 300, 297, 77, 293, 550, 321, 434, 516, 281, 974, 257, 1326, 9239, 484, 295, 2370, 7318, 445, 281, 652, 993, 257, 857, 3571], "temperature": 0.0, "avg_logprob": -0.16707353493602006, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.02943987865001e-06}, {"id": 729, "seek": 349160, "start": 3499.24, "end": 3501.24, "text": " for us", "tokens": [337, 505], "temperature": 0.0, "avg_logprob": -0.16707353493602006, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.02943987865001e-06}, {"id": 730, "seek": 349160, "start": 3501.24, "end": 3504.52, "text": " So here is how you create a neural network in pytorch", "tokens": [407, 510, 307, 577, 291, 1884, 257, 18161, 3209, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.16707353493602006, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.02943987865001e-06}, {"id": 731, "seek": 349160, "start": 3505.68, "end": 3507.68, "text": " the simplest possible neural network", "tokens": [264, 22811, 1944, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.16707353493602006, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.02943987865001e-06}, {"id": 732, "seek": 349160, "start": 3508.48, "end": 3515.96, "text": " You say sequential and sequential means I am now going to give you a list of the layers that I want in my neural network", "tokens": [509, 584, 42881, 293, 42881, 1355, 286, 669, 586, 516, 281, 976, 291, 257, 1329, 295, 264, 7914, 300, 286, 528, 294, 452, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.16707353493602006, "compression_ratio": 1.6710526315789473, "no_speech_prob": 4.02943987865001e-06}, {"id": 733, "seek": 351596, "start": 3515.96, "end": 3522.32, "text": " Right so in this case my list has two things in it the first thing says", "tokens": [1779, 370, 294, 341, 1389, 452, 1329, 575, 732, 721, 294, 309, 264, 700, 551, 1619], "temperature": 0.0, "avg_logprob": -0.1709898959149371, "compression_ratio": 1.748792270531401, "no_speech_prob": 8.990946298581548e-07}, {"id": 734, "seek": 351596, "start": 3522.32, "end": 3529.38, "text": " I want a linear layer so a linear layer is something that's basically going to do y equals ax plus b right", "tokens": [286, 528, 257, 8213, 4583, 370, 257, 8213, 4583, 307, 746, 300, 311, 1936, 516, 281, 360, 288, 6915, 6360, 1804, 272, 558], "temperature": 0.0, "avg_logprob": -0.1709898959149371, "compression_ratio": 1.748792270531401, "no_speech_prob": 8.990946298581548e-07}, {"id": 735, "seek": 351596, "start": 3530.56, "end": 3532.08, "text": " but", "tokens": [457], "temperature": 0.0, "avg_logprob": -0.1709898959149371, "compression_ratio": 1.748792270531401, "no_speech_prob": 8.990946298581548e-07}, {"id": 736, "seek": 351596, "start": 3532.08, "end": 3535.44, "text": " Matrix matrix multiply not not univariate obviously", "tokens": [36274, 8141, 12972, 406, 406, 517, 592, 3504, 473, 2745], "temperature": 0.0, "avg_logprob": -0.1709898959149371, "compression_ratio": 1.748792270531401, "no_speech_prob": 8.990946298581548e-07}, {"id": 737, "seek": 351596, "start": 3536.56, "end": 3539.2, "text": " So it's going to do a matrix product basically", "tokens": [407, 309, 311, 516, 281, 360, 257, 8141, 1674, 1936], "temperature": 0.0, "avg_logprob": -0.1709898959149371, "compression_ratio": 1.748792270531401, "no_speech_prob": 8.990946298581548e-07}, {"id": 738, "seek": 353920, "start": 3539.2, "end": 3547.04, "text": " So the input to the matrix product is going to be a vector of length 28 times 28 because that's how many", "tokens": [407, 264, 4846, 281, 264, 8141, 1674, 307, 516, 281, 312, 257, 8062, 295, 4641, 7562, 1413, 7562, 570, 300, 311, 577, 867], "temperature": 0.0, "avg_logprob": -0.15218814073410708, "compression_ratio": 1.6755725190839694, "no_speech_prob": 8.714323485037312e-07}, {"id": 739, "seek": 353920, "start": 3547.8399999999997, "end": 3552.4399999999996, "text": " Pixels we have and the output needs to be of size 10", "tokens": [18652, 1625, 321, 362, 293, 264, 5598, 2203, 281, 312, 295, 2744, 1266], "temperature": 0.0, "avg_logprob": -0.15218814073410708, "compression_ratio": 1.6755725190839694, "no_speech_prob": 8.714323485037312e-07}, {"id": 740, "seek": 353920, "start": 3552.4399999999996, "end": 3557.0, "text": " We'll talk about why in a moment, but for now, you know, this is how we define a linear layer", "tokens": [492, 603, 751, 466, 983, 294, 257, 1623, 11, 457, 337, 586, 11, 291, 458, 11, 341, 307, 577, 321, 6964, 257, 8213, 4583], "temperature": 0.0, "avg_logprob": -0.15218814073410708, "compression_ratio": 1.6755725190839694, "no_speech_prob": 8.714323485037312e-07}, {"id": 741, "seek": 353920, "start": 3558.0, "end": 3560.2, "text": " And then again, we're going to dig into this in detail", "tokens": [400, 550, 797, 11, 321, 434, 516, 281, 2528, 666, 341, 294, 2607], "temperature": 0.0, "avg_logprob": -0.15218814073410708, "compression_ratio": 1.6755725190839694, "no_speech_prob": 8.714323485037312e-07}, {"id": 742, "seek": 353920, "start": 3560.2, "end": 3567.12, "text": " But every linear layer just about in neural nets has to have a non-linearity after it and we're going to learn about this particular", "tokens": [583, 633, 8213, 4583, 445, 466, 294, 18161, 36170, 575, 281, 362, 257, 2107, 12, 1889, 17409, 934, 309, 293, 321, 434, 516, 281, 1466, 466, 341, 1729], "temperature": 0.0, "avg_logprob": -0.15218814073410708, "compression_ratio": 1.6755725190839694, "no_speech_prob": 8.714323485037312e-07}, {"id": 743, "seek": 356712, "start": 3567.12, "end": 3571.96, "text": " Non-linearity in a moment. It's called the softmax and if you've done the DL course, you've already seen this", "tokens": [8774, 12, 1889, 17409, 294, 257, 1623, 13, 467, 311, 1219, 264, 2787, 41167, 293, 498, 291, 600, 1096, 264, 413, 43, 1164, 11, 291, 600, 1217, 1612, 341], "temperature": 0.0, "avg_logprob": -0.15426602279930784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.726621571331634e-06}, {"id": 744, "seek": 356712, "start": 3572.7999999999997, "end": 3576.72, "text": " So that's how we define a neural net. This is a two layer neural net", "tokens": [407, 300, 311, 577, 321, 6964, 257, 18161, 2533, 13, 639, 307, 257, 732, 4583, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.15426602279930784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.726621571331634e-06}, {"id": 745, "seek": 356712, "start": 3576.72, "end": 3581.64, "text": " There's also kind of an implicit additional first layer, which is the input", "tokens": [821, 311, 611, 733, 295, 364, 26947, 4497, 700, 4583, 11, 597, 307, 264, 4846], "temperature": 0.0, "avg_logprob": -0.15426602279930784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.726621571331634e-06}, {"id": 746, "seek": 356712, "start": 3582.16, "end": 3589.2, "text": " But with pytorch you don't have to explicitly mention the input that normally we think conceptually like the input image", "tokens": [583, 365, 25878, 284, 339, 291, 500, 380, 362, 281, 20803, 2152, 264, 4846, 300, 5646, 321, 519, 3410, 671, 411, 264, 4846, 3256], "temperature": 0.0, "avg_logprob": -0.15426602279930784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.726621571331634e-06}, {"id": 747, "seek": 356712, "start": 3589.48, "end": 3591.48, "text": " Is kind of also a layer", "tokens": [1119, 733, 295, 611, 257, 4583], "temperature": 0.0, "avg_logprob": -0.15426602279930784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.726621571331634e-06}, {"id": 748, "seek": 356712, "start": 3593.56, "end": 3595.72, "text": " Because we're kind of doing things pretty manually", "tokens": [1436, 321, 434, 733, 295, 884, 721, 1238, 16945], "temperature": 0.0, "avg_logprob": -0.15426602279930784, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.726621571331634e-06}, {"id": 749, "seek": 359572, "start": 3595.72, "end": 3602.12, "text": " With pytorch we're not taking advantage of any of the conveniences in fast AI for building this stuff", "tokens": [2022, 25878, 284, 339, 321, 434, 406, 1940, 5002, 295, 604, 295, 264, 7158, 14004, 294, 2370, 7318, 337, 2390, 341, 1507], "temperature": 0.0, "avg_logprob": -0.18791418255500072, "compression_ratio": 1.7, "no_speech_prob": 3.3931310099433176e-06}, {"id": 750, "seek": 359572, "start": 3602.12, "end": 3608.6, "text": " We have to then write dot CUDA which tells pytorch to copy this neural network across to the GPU", "tokens": [492, 362, 281, 550, 2464, 5893, 29777, 7509, 597, 5112, 25878, 284, 339, 281, 5055, 341, 18161, 3209, 2108, 281, 264, 18407], "temperature": 0.0, "avg_logprob": -0.18791418255500072, "compression_ratio": 1.7, "no_speech_prob": 3.3931310099433176e-06}, {"id": 751, "seek": 359572, "start": 3609.16, "end": 3613.6, "text": " So now from now on that network is going to be actually running on the GPU", "tokens": [407, 586, 490, 586, 322, 300, 3209, 307, 516, 281, 312, 767, 2614, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.18791418255500072, "compression_ratio": 1.7, "no_speech_prob": 3.3931310099433176e-06}, {"id": 752, "seek": 359572, "start": 3613.8799999999997, "end": 3616.4399999999996, "text": " If we didn't say that it would run on the CPU", "tokens": [759, 321, 994, 380, 584, 300, 309, 576, 1190, 322, 264, 13199], "temperature": 0.0, "avg_logprob": -0.18791418255500072, "compression_ratio": 1.7, "no_speech_prob": 3.3931310099433176e-06}, {"id": 753, "seek": 359572, "start": 3618.3199999999997, "end": 3622.2799999999997, "text": " So that gives us back a neural net very simple neural net", "tokens": [407, 300, 2709, 505, 646, 257, 18161, 2533, 588, 2199, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.18791418255500072, "compression_ratio": 1.7, "no_speech_prob": 3.3931310099433176e-06}, {"id": 754, "seek": 359572, "start": 3623.08, "end": 3625.08, "text": " so we're then going to try and", "tokens": [370, 321, 434, 550, 516, 281, 853, 293], "temperature": 0.0, "avg_logprob": -0.18791418255500072, "compression_ratio": 1.7, "no_speech_prob": 3.3931310099433176e-06}, {"id": 755, "seek": 362508, "start": 3625.08, "end": 3628.24, "text": " Fit the neural net to some data so we need some data", "tokens": [29263, 264, 18161, 2533, 281, 512, 1412, 370, 321, 643, 512, 1412], "temperature": 0.0, "avg_logprob": -0.1594803819378603, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.44655768458324e-06}, {"id": 756, "seek": 362508, "start": 3629.08, "end": 3636.72, "text": " So fast AI has this concept of a model data object, which is basically something that wraps up training data", "tokens": [407, 2370, 7318, 575, 341, 3410, 295, 257, 2316, 1412, 2657, 11, 597, 307, 1936, 746, 300, 25831, 493, 3097, 1412], "temperature": 0.0, "avg_logprob": -0.1594803819378603, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.44655768458324e-06}, {"id": 757, "seek": 362508, "start": 3637.44, "end": 3641.6, "text": " validation data and optionally test data and so to create a", "tokens": [24071, 1412, 293, 3614, 379, 1500, 1412, 293, 370, 281, 1884, 257], "temperature": 0.0, "avg_logprob": -0.1594803819378603, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.44655768458324e-06}, {"id": 758, "seek": 362508, "start": 3642.16, "end": 3648.2799999999997, "text": " Model data object you can just say I want to create some image classifier data. I'm going to grab it from some arrays", "tokens": [17105, 1412, 2657, 291, 393, 445, 584, 286, 528, 281, 1884, 512, 3256, 1508, 9902, 1412, 13, 286, 478, 516, 281, 4444, 309, 490, 512, 41011], "temperature": 0.0, "avg_logprob": -0.1594803819378603, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.44655768458324e-06}, {"id": 759, "seek": 362508, "start": 3648.92, "end": 3653.12, "text": " Right and you just say okay. This is the path that I'm going to save any temporary files", "tokens": [1779, 293, 291, 445, 584, 1392, 13, 639, 307, 264, 3100, 300, 286, 478, 516, 281, 3155, 604, 13413, 7098], "temperature": 0.0, "avg_logprob": -0.1594803819378603, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.44655768458324e-06}, {"id": 760, "seek": 365312, "start": 3653.12, "end": 3655.12, "text": " This is my training data", "tokens": [639, 307, 452, 3097, 1412], "temperature": 0.0, "avg_logprob": -0.15446032952824865, "compression_ratio": 1.8071748878923768, "no_speech_prob": 2.4060959731286857e-06}, {"id": 761, "seek": 365312, "start": 3656.3199999999997, "end": 3662.56, "text": " Arrays, and this is my validation data arrays, okay, and so that just returns an object", "tokens": [1587, 36212, 11, 293, 341, 307, 452, 24071, 1412, 41011, 11, 1392, 11, 293, 370, 300, 445, 11247, 364, 2657], "temperature": 0.0, "avg_logprob": -0.15446032952824865, "compression_ratio": 1.8071748878923768, "no_speech_prob": 2.4060959731286857e-06}, {"id": 762, "seek": 365312, "start": 3662.56, "end": 3666.7599999999998, "text": " That's going to wrap that all up and so we're going to be able to fit to that data", "tokens": [663, 311, 516, 281, 7019, 300, 439, 493, 293, 370, 321, 434, 516, 281, 312, 1075, 281, 3318, 281, 300, 1412], "temperature": 0.0, "avg_logprob": -0.15446032952824865, "compression_ratio": 1.8071748878923768, "no_speech_prob": 2.4060959731286857e-06}, {"id": 763, "seek": 365312, "start": 3667.04, "end": 3670.52, "text": " So now that we have a neural net and we have some data", "tokens": [407, 586, 300, 321, 362, 257, 18161, 2533, 293, 321, 362, 512, 1412], "temperature": 0.0, "avg_logprob": -0.15446032952824865, "compression_ratio": 1.8071748878923768, "no_speech_prob": 2.4060959731286857e-06}, {"id": 764, "seek": 365312, "start": 3670.88, "end": 3672.52, "text": " We're going to come back to this in a moment", "tokens": [492, 434, 516, 281, 808, 646, 281, 341, 294, 257, 1623], "temperature": 0.0, "avg_logprob": -0.15446032952824865, "compression_ratio": 1.8071748878923768, "no_speech_prob": 2.4060959731286857e-06}, {"id": 765, "seek": 365312, "start": 3672.52, "end": 3678.74, "text": " But we basically say what loss function do we want to use what optimizer do we want to use and then we say?", "tokens": [583, 321, 1936, 584, 437, 4470, 2445, 360, 321, 528, 281, 764, 437, 5028, 6545, 360, 321, 528, 281, 764, 293, 550, 321, 584, 30], "temperature": 0.0, "avg_logprob": -0.15446032952824865, "compression_ratio": 1.8071748878923768, "no_speech_prob": 2.4060959731286857e-06}, {"id": 766, "seek": 367874, "start": 3678.74, "end": 3683.52, "text": " Fit we say fit this network", "tokens": [29263, 321, 584, 3318, 341, 3209], "temperature": 0.0, "avg_logprob": -0.28994082132975263, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.2377477054978954e-06}, {"id": 767, "seek": 367874, "start": 3684.18, "end": 3687.64, "text": " To this data going over every image once", "tokens": [1407, 341, 1412, 516, 670, 633, 3256, 1564], "temperature": 0.0, "avg_logprob": -0.28994082132975263, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.2377477054978954e-06}, {"id": 768, "seek": 367874, "start": 3688.62, "end": 3693.18, "text": " Using this loss function and this optimizer and print out these metrics", "tokens": [11142, 341, 4470, 2445, 293, 341, 5028, 6545, 293, 4482, 484, 613, 16367], "temperature": 0.0, "avg_logprob": -0.28994082132975263, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.2377477054978954e-06}, {"id": 769, "seek": 367874, "start": 3694.2999999999997, "end": 3699.7, "text": " Bang okay, and this says here. This is 91 point eight percent accurate", "tokens": [11538, 1392, 11, 293, 341, 1619, 510, 13, 639, 307, 31064, 935, 3180, 3043, 8559], "temperature": 0.0, "avg_logprob": -0.28994082132975263, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.2377477054978954e-06}, {"id": 770, "seek": 367874, "start": 3700.3399999999997, "end": 3702.54, "text": " So that's like the simplest possible neuron it", "tokens": [407, 300, 311, 411, 264, 22811, 1944, 34090, 309], "temperature": 0.0, "avg_logprob": -0.28994082132975263, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.2377477054978954e-06}, {"id": 771, "seek": 367874, "start": 3703.3799999999997, "end": 3705.3799999999997, "text": " so what that's doing is", "tokens": [370, 437, 300, 311, 884, 307], "temperature": 0.0, "avg_logprob": -0.28994082132975263, "compression_ratio": 1.50253807106599, "no_speech_prob": 3.2377477054978954e-06}, {"id": 772, "seek": 370538, "start": 3705.38, "end": 3710.42, "text": " It's creating a matrix multiplication", "tokens": [467, 311, 4084, 257, 8141, 27290], "temperature": 0.0, "avg_logprob": -0.3081161452502739, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.2959136458666762e-06}, {"id": 773, "seek": 370538, "start": 3711.46, "end": 3715.7400000000002, "text": " Followed by a non-linearity, and then it's trying to find", "tokens": [9876, 292, 538, 257, 2107, 12, 1889, 17409, 11, 293, 550, 309, 311, 1382, 281, 915], "temperature": 0.0, "avg_logprob": -0.3081161452502739, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.2959136458666762e-06}, {"id": 774, "seek": 370538, "start": 3716.6600000000003, "end": 3718.82, "text": " the values for this matrix", "tokens": [264, 4190, 337, 341, 8141], "temperature": 0.0, "avg_logprob": -0.3081161452502739, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.2959136458666762e-06}, {"id": 775, "seek": 370538, "start": 3719.62, "end": 3723.94, "text": " Which cause which basically that fit the data as well as possible that?", "tokens": [3013, 3082, 597, 1936, 300, 3318, 264, 1412, 382, 731, 382, 1944, 300, 30], "temperature": 0.0, "avg_logprob": -0.3081161452502739, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.2959136458666762e-06}, {"id": 776, "seek": 370538, "start": 3724.38, "end": 3728.1, "text": " That end up predicting this is a 1 this is a 9 this is a 3 and", "tokens": [663, 917, 493, 32884, 341, 307, 257, 502, 341, 307, 257, 1722, 341, 307, 257, 805, 293], "temperature": 0.0, "avg_logprob": -0.3081161452502739, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.2959136458666762e-06}, {"id": 777, "seek": 370538, "start": 3728.7400000000002, "end": 3731.88, "text": " So we need some definition for as well as possible", "tokens": [407, 321, 643, 512, 7123, 337, 382, 731, 382, 1944], "temperature": 0.0, "avg_logprob": -0.3081161452502739, "compression_ratio": 1.5958549222797926, "no_speech_prob": 2.2959136458666762e-06}, {"id": 778, "seek": 373188, "start": 3731.88, "end": 3738.7400000000002, "text": " And so the general term for that thing is called the loss function so the loss function is the function that's going to be", "tokens": [400, 370, 264, 2674, 1433, 337, 300, 551, 307, 1219, 264, 4470, 2445, 370, 264, 4470, 2445, 307, 264, 2445, 300, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.16031894869017368, "compression_ratio": 1.7923076923076924, "no_speech_prob": 3.4465706448827405e-06}, {"id": 779, "seek": 373188, "start": 3739.02, "end": 3743.6600000000003, "text": " Lower if this is better right just like with random forests", "tokens": [25523, 498, 341, 307, 1101, 558, 445, 411, 365, 4974, 21700], "temperature": 0.0, "avg_logprob": -0.16031894869017368, "compression_ratio": 1.7923076923076924, "no_speech_prob": 3.4465706448827405e-06}, {"id": 780, "seek": 373188, "start": 3743.6600000000003, "end": 3748.08, "text": " We have this concept of information gain, and we got to like pick what function", "tokens": [492, 362, 341, 3410, 295, 1589, 6052, 11, 293, 321, 658, 281, 411, 1888, 437, 2445], "temperature": 0.0, "avg_logprob": -0.16031894869017368, "compression_ratio": 1.7923076923076924, "no_speech_prob": 3.4465706448827405e-06}, {"id": 781, "seek": 373188, "start": 3748.08, "end": 3753.36, "text": " Do you want to use to define information gain, and we were mainly looking at root mean squared error, right?", "tokens": [1144, 291, 528, 281, 764, 281, 6964, 1589, 6052, 11, 293, 321, 645, 8704, 1237, 412, 5593, 914, 8889, 6713, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16031894869017368, "compression_ratio": 1.7923076923076924, "no_speech_prob": 3.4465706448827405e-06}, {"id": 782, "seek": 373188, "start": 3754.78, "end": 3759.86, "text": " Most machine learning algorithms we call something very similar that loss right so the loss is", "tokens": [4534, 3479, 2539, 14642, 321, 818, 746, 588, 2531, 300, 4470, 558, 370, 264, 4470, 307], "temperature": 0.0, "avg_logprob": -0.16031894869017368, "compression_ratio": 1.7923076923076924, "no_speech_prob": 3.4465706448827405e-06}, {"id": 783, "seek": 375986, "start": 3759.86, "end": 3763.6600000000003, "text": " How do we score how good we are and so in the end?", "tokens": [1012, 360, 321, 6175, 577, 665, 321, 366, 293, 370, 294, 264, 917, 30], "temperature": 0.0, "avg_logprob": -0.183773890785549, "compression_ratio": 1.7136563876651982, "no_speech_prob": 1.628048380553082e-06}, {"id": 784, "seek": 375986, "start": 3763.6600000000003, "end": 3768.1600000000003, "text": " we're going to calculate the derivative of the loss with respect to the", "tokens": [321, 434, 516, 281, 8873, 264, 13760, 295, 264, 4470, 365, 3104, 281, 264], "temperature": 0.0, "avg_logprob": -0.183773890785549, "compression_ratio": 1.7136563876651982, "no_speech_prob": 1.628048380553082e-06}, {"id": 785, "seek": 375986, "start": 3768.6800000000003, "end": 3774.46, "text": " The weight matrix that we're multiplying by to figure out how to update it right so", "tokens": [440, 3364, 8141, 300, 321, 434, 30955, 538, 281, 2573, 484, 577, 281, 5623, 309, 558, 370], "temperature": 0.0, "avg_logprob": -0.183773890785549, "compression_ratio": 1.7136563876651982, "no_speech_prob": 1.628048380553082e-06}, {"id": 786, "seek": 375986, "start": 3774.94, "end": 3778.58, "text": " We're going to use something called negative log likelihood loss", "tokens": [492, 434, 516, 281, 764, 746, 1219, 3671, 3565, 22119, 4470], "temperature": 0.0, "avg_logprob": -0.183773890785549, "compression_ratio": 1.7136563876651982, "no_speech_prob": 1.628048380553082e-06}, {"id": 787, "seek": 375986, "start": 3779.7400000000002, "end": 3784.76, "text": " So negative log likelihood loss is also known as cross entropy", "tokens": [407, 3671, 3565, 22119, 4470, 307, 611, 2570, 382, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.183773890785549, "compression_ratio": 1.7136563876651982, "no_speech_prob": 1.628048380553082e-06}, {"id": 788, "seek": 375986, "start": 3785.34, "end": 3788.1800000000003, "text": " They're literally the same thing. There's two versions", "tokens": [814, 434, 3736, 264, 912, 551, 13, 821, 311, 732, 9606], "temperature": 0.0, "avg_logprob": -0.183773890785549, "compression_ratio": 1.7136563876651982, "no_speech_prob": 1.628048380553082e-06}, {"id": 789, "seek": 378818, "start": 3788.18, "end": 3791.5, "text": " one called binary cross entropy or", "tokens": [472, 1219, 17434, 3278, 30867, 420], "temperature": 0.0, "avg_logprob": -0.2016464619154341, "compression_ratio": 1.7641509433962264, "no_speech_prob": 8.315268473779724e-07}, {"id": 790, "seek": 378818, "start": 3792.22, "end": 3797.52, "text": " Binary negative log likelihood and another called categorical cross entropy now the same thing", "tokens": [363, 4066, 3671, 3565, 22119, 293, 1071, 1219, 19250, 804, 3278, 30867, 586, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.2016464619154341, "compression_ratio": 1.7641509433962264, "no_speech_prob": 8.315268473779724e-07}, {"id": 791, "seek": 378818, "start": 3797.94, "end": 3803.74, "text": " One is for when you've only got a zero or one dependent the other is if you've got like cat dog", "tokens": [1485, 307, 337, 562, 291, 600, 787, 658, 257, 4018, 420, 472, 12334, 264, 661, 307, 498, 291, 600, 658, 411, 3857, 3000], "temperature": 0.0, "avg_logprob": -0.2016464619154341, "compression_ratio": 1.7641509433962264, "no_speech_prob": 8.315268473779724e-07}, {"id": 792, "seek": 378818, "start": 3804.14, "end": 3811.96, "text": " Airplane or horse or zero one through nine or so forth so what we've got here is the binary version of", "tokens": [5774, 36390, 420, 6832, 420, 4018, 472, 807, 4949, 420, 370, 5220, 370, 437, 321, 600, 658, 510, 307, 264, 17434, 3037, 295], "temperature": 0.0, "avg_logprob": -0.2016464619154341, "compression_ratio": 1.7641509433962264, "no_speech_prob": 8.315268473779724e-07}, {"id": 793, "seek": 378818, "start": 3812.5, "end": 3815.74, "text": " Cross entropy and so here is the definition I", "tokens": [11623, 30867, 293, 370, 510, 307, 264, 7123, 286], "temperature": 0.0, "avg_logprob": -0.2016464619154341, "compression_ratio": 1.7641509433962264, "no_speech_prob": 8.315268473779724e-07}, {"id": 794, "seek": 381574, "start": 3815.74, "end": 3821.14, "text": " Think maybe the easiest way to understand this definition is to look at an example", "tokens": [6557, 1310, 264, 12889, 636, 281, 1223, 341, 7123, 307, 281, 574, 412, 364, 1365], "temperature": 0.0, "avg_logprob": -0.2041068333451466, "compression_ratio": 1.6978021978021978, "no_speech_prob": 4.5659326133318245e-06}, {"id": 795, "seek": 381574, "start": 3822.62, "end": 3824.74, "text": " So let's say we're trying to predict cat versus dog", "tokens": [407, 718, 311, 584, 321, 434, 1382, 281, 6069, 3857, 5717, 3000], "temperature": 0.0, "avg_logprob": -0.2041068333451466, "compression_ratio": 1.6978021978021978, "no_speech_prob": 4.5659326133318245e-06}, {"id": 796, "seek": 381574, "start": 3826.02, "end": 3828.4199999999996, "text": " One is cat zero is dog", "tokens": [1485, 307, 3857, 4018, 307, 3000], "temperature": 0.0, "avg_logprob": -0.2041068333451466, "compression_ratio": 1.6978021978021978, "no_speech_prob": 4.5659326133318245e-06}, {"id": 797, "seek": 381574, "start": 3829.7, "end": 3832.8599999999997, "text": " so here we've got cat dog dog cat and", "tokens": [370, 510, 321, 600, 658, 3857, 3000, 3000, 3857, 293], "temperature": 0.0, "avg_logprob": -0.2041068333451466, "compression_ratio": 1.6978021978021978, "no_speech_prob": 4.5659326133318245e-06}, {"id": 798, "seek": 381574, "start": 3833.7, "end": 3838.2999999999997, "text": " Here are our predictions. We said 90% sure it's a cat", "tokens": [1692, 366, 527, 21264, 13, 492, 848, 4289, 4, 988, 309, 311, 257, 3857], "temperature": 0.0, "avg_logprob": -0.2041068333451466, "compression_ratio": 1.6978021978021978, "no_speech_prob": 4.5659326133318245e-06}, {"id": 799, "seek": 381574, "start": 3839.7799999999997, "end": 3841.7799999999997, "text": " 90% sure it's a dog", "tokens": [4289, 4, 988, 309, 311, 257, 3000], "temperature": 0.0, "avg_logprob": -0.2041068333451466, "compression_ratio": 1.6978021978021978, "no_speech_prob": 4.5659326133318245e-06}, {"id": 800, "seek": 384178, "start": 3841.78, "end": 3845.5400000000004, "text": " 80% sure it's a dog 80% sure it's a cat", "tokens": [4688, 4, 988, 309, 311, 257, 3000, 4688, 4, 988, 309, 311, 257, 3857], "temperature": 0.0, "avg_logprob": -0.20608158111572267, "compression_ratio": 1.6235955056179776, "no_speech_prob": 8.059432730078697e-07}, {"id": 801, "seek": 384178, "start": 3846.1000000000004, "end": 3848.98, "text": " right, so we can then calculate the", "tokens": [558, 11, 370, 321, 393, 550, 8873, 264], "temperature": 0.0, "avg_logprob": -0.20608158111572267, "compression_ratio": 1.6235955056179776, "no_speech_prob": 8.059432730078697e-07}, {"id": 802, "seek": 384178, "start": 3850.3, "end": 3854.5400000000004, "text": " Binary cross entropy by calling our function, so it's going to say okay for the first one", "tokens": [363, 4066, 3278, 30867, 538, 5141, 527, 2445, 11, 370, 309, 311, 516, 281, 584, 1392, 337, 264, 700, 472], "temperature": 0.0, "avg_logprob": -0.20608158111572267, "compression_ratio": 1.6235955056179776, "no_speech_prob": 8.059432730078697e-07}, {"id": 803, "seek": 384178, "start": 3855.1000000000004, "end": 3858.86, "text": " We've got y equals 1 so it's going to be 1 times", "tokens": [492, 600, 658, 288, 6915, 502, 370, 309, 311, 516, 281, 312, 502, 1413], "temperature": 0.0, "avg_logprob": -0.20608158111572267, "compression_ratio": 1.6235955056179776, "no_speech_prob": 8.059432730078697e-07}, {"id": 804, "seek": 384178, "start": 3860.1800000000003, "end": 3862.1800000000003, "text": " log of point 9", "tokens": [3565, 295, 935, 1722], "temperature": 0.0, "avg_logprob": -0.20608158111572267, "compression_ratio": 1.6235955056179776, "no_speech_prob": 8.059432730078697e-07}, {"id": 805, "seek": 384178, "start": 3862.98, "end": 3869.34, "text": " Plus 1 minus y 1 minus 1 is 0 so that's going to be skipped", "tokens": [7721, 502, 3175, 288, 502, 3175, 502, 307, 1958, 370, 300, 311, 516, 281, 312, 30193], "temperature": 0.0, "avg_logprob": -0.20608158111572267, "compression_ratio": 1.6235955056179776, "no_speech_prob": 8.059432730078697e-07}, {"id": 806, "seek": 386934, "start": 3869.34, "end": 3876.7400000000002, "text": " Okay, and then the second one is going to be a 0 so it's going to be 0 times something so that's going to be skipped and", "tokens": [1033, 11, 293, 550, 264, 1150, 472, 307, 516, 281, 312, 257, 1958, 370, 309, 311, 516, 281, 312, 1958, 1413, 746, 370, 300, 311, 516, 281, 312, 30193, 293], "temperature": 0.0, "avg_logprob": -0.18940949440002441, "compression_ratio": 1.788888888888889, "no_speech_prob": 6.577919293704326e-07}, {"id": 807, "seek": 386934, "start": 3877.1800000000003, "end": 3881.9, "text": " The second part will be 1 minus 0 so this is 1 times", "tokens": [440, 1150, 644, 486, 312, 502, 3175, 1958, 370, 341, 307, 502, 1413], "temperature": 0.0, "avg_logprob": -0.18940949440002441, "compression_ratio": 1.788888888888889, "no_speech_prob": 6.577919293704326e-07}, {"id": 808, "seek": 386934, "start": 3882.54, "end": 3884.54, "text": " log of 1 minus p", "tokens": [3565, 295, 502, 3175, 280], "temperature": 0.0, "avg_logprob": -0.18940949440002441, "compression_ratio": 1.788888888888889, "no_speech_prob": 6.577919293704326e-07}, {"id": 809, "seek": 386934, "start": 3885.1400000000003, "end": 3887.42, "text": " 1 minus point 1 is point 9", "tokens": [502, 3175, 935, 502, 307, 935, 1722], "temperature": 0.0, "avg_logprob": -0.18940949440002441, "compression_ratio": 1.788888888888889, "no_speech_prob": 6.577919293704326e-07}, {"id": 810, "seek": 386934, "start": 3887.94, "end": 3894.32, "text": " So in other words the first piece and the second piece of this are going to give exactly the same number", "tokens": [407, 294, 661, 2283, 264, 700, 2522, 293, 264, 1150, 2522, 295, 341, 366, 516, 281, 976, 2293, 264, 912, 1230], "temperature": 0.0, "avg_logprob": -0.18940949440002441, "compression_ratio": 1.788888888888889, "no_speech_prob": 6.577919293704326e-07}, {"id": 811, "seek": 389432, "start": 3894.32, "end": 3898.96, "text": " Which makes sense because the first one we said we were 90% confident", "tokens": [3013, 1669, 2020, 570, 264, 700, 472, 321, 848, 321, 645, 4289, 4, 6679], "temperature": 0.0, "avg_logprob": -0.14473662330108938, "compression_ratio": 1.811926605504587, "no_speech_prob": 1.3496978681359906e-06}, {"id": 812, "seek": 389432, "start": 3898.96, "end": 3903.92, "text": " It was a cat and it was and the second we said we were 90% confident", "tokens": [467, 390, 257, 3857, 293, 309, 390, 293, 264, 1150, 321, 848, 321, 645, 4289, 4, 6679], "temperature": 0.0, "avg_logprob": -0.14473662330108938, "compression_ratio": 1.811926605504587, "no_speech_prob": 1.3496978681359906e-06}, {"id": 813, "seek": 389432, "start": 3903.92, "end": 3910.1200000000003, "text": " It was a dog and it was so in each case the loss is coming from the fact that you know", "tokens": [467, 390, 257, 3000, 293, 309, 390, 370, 294, 1184, 1389, 264, 4470, 307, 1348, 490, 264, 1186, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.14473662330108938, "compression_ratio": 1.811926605504587, "no_speech_prob": 1.3496978681359906e-06}, {"id": 814, "seek": 389432, "start": 3910.2400000000002, "end": 3915.8, "text": " We could have been more confident. Yeah, so if we said we're a hundred percent confident the loss would have been zero", "tokens": [492, 727, 362, 668, 544, 6679, 13, 865, 11, 370, 498, 321, 848, 321, 434, 257, 3262, 3043, 6679, 264, 4470, 576, 362, 668, 4018], "temperature": 0.0, "avg_logprob": -0.14473662330108938, "compression_ratio": 1.811926605504587, "no_speech_prob": 1.3496978681359906e-06}, {"id": 815, "seek": 389432, "start": 3916.7200000000003, "end": 3919.1200000000003, "text": " Right so let's look at that in Excel", "tokens": [1779, 370, 718, 311, 574, 412, 300, 294, 19060], "temperature": 0.0, "avg_logprob": -0.14473662330108938, "compression_ratio": 1.811926605504587, "no_speech_prob": 1.3496978681359906e-06}, {"id": 816, "seek": 391912, "start": 3919.12, "end": 3923.3399999999997, "text": " So here's our", "tokens": [407, 510, 311, 527], "temperature": 0.0, "avg_logprob": -0.3504827892969525, "compression_ratio": 1.8319327731092436, "no_speech_prob": 2.7693934043782065e-06}, {"id": 817, "seek": 391912, "start": 3927.6, "end": 3934.56, "text": " Point 9 point 1 point 2 point 8 right and here's our predictions 1001 so here's 1 minus the prediction", "tokens": [12387, 1722, 935, 502, 935, 568, 935, 1649, 558, 293, 510, 311, 527, 21264, 2319, 16, 370, 510, 311, 502, 3175, 264, 17630], "temperature": 0.0, "avg_logprob": -0.3504827892969525, "compression_ratio": 1.8319327731092436, "no_speech_prob": 2.7693934043782065e-06}, {"id": 818, "seek": 391912, "start": 3935.12, "end": 3938.3599999999997, "text": " Right here is log of our prediction", "tokens": [1779, 510, 307, 3565, 295, 527, 17630], "temperature": 0.0, "avg_logprob": -0.3504827892969525, "compression_ratio": 1.8319327731092436, "no_speech_prob": 2.7693934043782065e-06}, {"id": 819, "seek": 391912, "start": 3939.08, "end": 3942.24, "text": " Here is log of 1 minus our prediction", "tokens": [1692, 307, 3565, 295, 502, 3175, 527, 17630], "temperature": 0.0, "avg_logprob": -0.3504827892969525, "compression_ratio": 1.8319327731092436, "no_speech_prob": 2.7693934043782065e-06}, {"id": 820, "seek": 391912, "start": 3944.52, "end": 3946.52, "text": " And so then here is our", "tokens": [400, 370, 550, 510, 307, 527], "temperature": 0.0, "avg_logprob": -0.3504827892969525, "compression_ratio": 1.8319327731092436, "no_speech_prob": 2.7693934043782065e-06}, {"id": 821, "seek": 391912, "start": 3947.24, "end": 3948.6, "text": " sum", "tokens": [2408], "temperature": 0.0, "avg_logprob": -0.3504827892969525, "compression_ratio": 1.8319327731092436, "no_speech_prob": 2.7693934043782065e-06}, {"id": 822, "seek": 394860, "start": 3948.6, "end": 3949.92, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 823, "seek": 394860, "start": 3949.92, "end": 3951.92, "text": " So if you think about it", "tokens": [407, 498, 291, 519, 466, 309], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 824, "seek": 394860, "start": 3952.96, "end": 3957.68, "text": " And I want you to think about this during the week you could replace this", "tokens": [400, 286, 528, 291, 281, 519, 466, 341, 1830, 264, 1243, 291, 727, 7406, 341], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 825, "seek": 394860, "start": 3958.7599999999998, "end": 3960.7599999999998, "text": " with an if statement", "tokens": [365, 364, 498, 5629], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 826, "seek": 394860, "start": 3961.24, "end": 3964.3399999999997, "text": " Rather than why because why is always 1 or 0?", "tokens": [16571, 813, 983, 570, 983, 307, 1009, 502, 420, 1958, 30], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 827, "seek": 394860, "start": 3965.0, "end": 3970.7599999999998, "text": " Right then it's only ever going to use either this or this so you could replace this with an if statement", "tokens": [1779, 550, 309, 311, 787, 1562, 516, 281, 764, 2139, 341, 420, 341, 370, 291, 727, 7406, 341, 365, 364, 498, 5629], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 828, "seek": 394860, "start": 3970.7599999999998, "end": 3974.2, "text": " So I'd like you during the week to try to rewrite this", "tokens": [407, 286, 1116, 411, 291, 1830, 264, 1243, 281, 853, 281, 28132, 341], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 829, "seek": 394860, "start": 3975.04, "end": 3977.36, "text": " with an if statement okay, and", "tokens": [365, 364, 498, 5629, 1392, 11, 293], "temperature": 0.0, "avg_logprob": -0.20763994983791076, "compression_ratio": 1.8756476683937824, "no_speech_prob": 2.9023008210060652e-06}, {"id": 830, "seek": 397736, "start": 3977.36, "end": 3980.1600000000003, "text": " then see if you can then", "tokens": [550, 536, 498, 291, 393, 550], "temperature": 0.0, "avg_logprob": -0.22446229934692383, "compression_ratio": 2.0106382978723403, "no_speech_prob": 2.4060952910076594e-06}, {"id": 831, "seek": 397736, "start": 3981.2000000000003, "end": 3988.6, "text": " Scale it out to be a categorical cross entropy so categorical cross entropy works this way. Let's say we were trying to predict", "tokens": [42999, 309, 484, 281, 312, 257, 19250, 804, 3278, 30867, 370, 19250, 804, 3278, 30867, 1985, 341, 636, 13, 961, 311, 584, 321, 645, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.22446229934692383, "compression_ratio": 2.0106382978723403, "no_speech_prob": 2.4060952910076594e-06}, {"id": 832, "seek": 397736, "start": 3989.4, "end": 3991.88, "text": " 3 and then 6 and then 7 and then 2", "tokens": [805, 293, 550, 1386, 293, 550, 1614, 293, 550, 568], "temperature": 0.0, "avg_logprob": -0.22446229934692383, "compression_ratio": 2.0106382978723403, "no_speech_prob": 2.4060952910076594e-06}, {"id": 833, "seek": 397736, "start": 3993.28, "end": 3995.6800000000003, "text": " so if we were trying to predict 3 and", "tokens": [370, 498, 321, 645, 1382, 281, 6069, 805, 293], "temperature": 0.0, "avg_logprob": -0.22446229934692383, "compression_ratio": 2.0106382978723403, "no_speech_prob": 2.4060952910076594e-06}, {"id": 834, "seek": 397736, "start": 3996.48, "end": 3999.52, "text": " The actual thing that was predicted was like 4.7", "tokens": [440, 3539, 551, 300, 390, 19147, 390, 411, 1017, 13, 22], "temperature": 0.0, "avg_logprob": -0.22446229934692383, "compression_ratio": 2.0106382978723403, "no_speech_prob": 2.4060952910076594e-06}, {"id": 835, "seek": 397736, "start": 4000.28, "end": 4001.6, "text": " Right", "tokens": [1779], "temperature": 0.0, "avg_logprob": -0.22446229934692383, "compression_ratio": 2.0106382978723403, "no_speech_prob": 2.4060952910076594e-06}, {"id": 836, "seek": 397736, "start": 4001.6, "end": 4005.76, "text": " Versus like well actually think of this way we're trying to predict 3 and we actually predicted 5", "tokens": [12226, 301, 411, 731, 767, 519, 295, 341, 636, 321, 434, 1382, 281, 6069, 805, 293, 321, 767, 19147, 1025], "temperature": 0.0, "avg_logprob": -0.22446229934692383, "compression_ratio": 2.0106382978723403, "no_speech_prob": 2.4060952910076594e-06}, {"id": 837, "seek": 400576, "start": 4005.76, "end": 4009.2000000000003, "text": " Or we're trying to predict 3 and we accidentally predicted 9", "tokens": [1610, 321, 434, 1382, 281, 6069, 805, 293, 321, 15715, 19147, 1722], "temperature": 0.0, "avg_logprob": -0.14007414511914523, "compression_ratio": 1.9313304721030042, "no_speech_prob": 2.123371814377606e-06}, {"id": 838, "seek": 400576, "start": 4010.0400000000004, "end": 4015.6400000000003, "text": " Like being 5 instead of 3 is no better than being 9 instead of 3", "tokens": [1743, 885, 1025, 2602, 295, 805, 307, 572, 1101, 813, 885, 1722, 2602, 295, 805], "temperature": 0.0, "avg_logprob": -0.14007414511914523, "compression_ratio": 1.9313304721030042, "no_speech_prob": 2.123371814377606e-06}, {"id": 839, "seek": 400576, "start": 4015.76, "end": 4021.1600000000003, "text": " So we're not actually going to say like how far away is the actual number we're going to express it differently", "tokens": [407, 321, 434, 406, 767, 516, 281, 584, 411, 577, 1400, 1314, 307, 264, 3539, 1230, 321, 434, 516, 281, 5109, 309, 7614], "temperature": 0.0, "avg_logprob": -0.14007414511914523, "compression_ratio": 1.9313304721030042, "no_speech_prob": 2.123371814377606e-06}, {"id": 840, "seek": 400576, "start": 4021.6800000000003, "end": 4026.2400000000002, "text": " Or to put it another way what if we're trying to predict cats dogs horses and airplanes", "tokens": [1610, 281, 829, 309, 1071, 636, 437, 498, 321, 434, 1382, 281, 6069, 11111, 7197, 13112, 293, 32947], "temperature": 0.0, "avg_logprob": -0.14007414511914523, "compression_ratio": 1.9313304721030042, "no_speech_prob": 2.123371814377606e-06}, {"id": 841, "seek": 400576, "start": 4026.6400000000003, "end": 4033.4, "text": " You can't like how far away is cat from horse, so we're going to express these a little bit differently rather than thinking", "tokens": [509, 393, 380, 411, 577, 1400, 1314, 307, 3857, 490, 6832, 11, 370, 321, 434, 516, 281, 5109, 613, 257, 707, 857, 7614, 2831, 813, 1953], "temperature": 0.0, "avg_logprob": -0.14007414511914523, "compression_ratio": 1.9313304721030042, "no_speech_prob": 2.123371814377606e-06}, {"id": 842, "seek": 403340, "start": 4033.4, "end": 4036.04, "text": " What is a 3 let's think of it as a?", "tokens": [708, 307, 257, 805, 718, 311, 519, 295, 309, 382, 257, 30], "temperature": 0.0, "avg_logprob": -0.25084200944050705, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.1544593689905014e-06}, {"id": 843, "seek": 403340, "start": 4037.36, "end": 4038.76, "text": " vector", "tokens": [8062], "temperature": 0.0, "avg_logprob": -0.25084200944050705, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.1544593689905014e-06}, {"id": 844, "seek": 403340, "start": 4038.76, "end": 4040.92, "text": " With a 1 in the third location", "tokens": [2022, 257, 502, 294, 264, 2636, 4914], "temperature": 0.0, "avg_logprob": -0.25084200944050705, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.1544593689905014e-06}, {"id": 845, "seek": 403340, "start": 4041.64, "end": 4046.96, "text": " And rather than thinking it was a 6 let's think of it as a vector of zeros with a 1 in the 6th", "tokens": [400, 2831, 813, 1953, 309, 390, 257, 1386, 718, 311, 519, 295, 309, 382, 257, 8062, 295, 35193, 365, 257, 502, 294, 264, 1386, 392], "temperature": 0.0, "avg_logprob": -0.25084200944050705, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.1544593689905014e-06}, {"id": 846, "seek": 403340, "start": 4047.6800000000003, "end": 4053.48, "text": " Location so in other words one hot encoding right so let's one hot encode a dependent variable", "tokens": [12859, 399, 370, 294, 661, 2283, 472, 2368, 43430, 558, 370, 718, 311, 472, 2368, 2058, 1429, 257, 12334, 7006], "temperature": 0.0, "avg_logprob": -0.25084200944050705, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.1544593689905014e-06}, {"id": 847, "seek": 403340, "start": 4054.2400000000002, "end": 4060.44, "text": " And so that way now rather than predicting trying to predict a single number. Let's predict", "tokens": [400, 370, 300, 636, 586, 2831, 813, 32884, 1382, 281, 6069, 257, 2167, 1230, 13, 961, 311, 6069], "temperature": 0.0, "avg_logprob": -0.25084200944050705, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.1544593689905014e-06}, {"id": 848, "seek": 403340, "start": 4061.2000000000003, "end": 4062.92, "text": " 10 numbers", "tokens": [1266, 3547], "temperature": 0.0, "avg_logprob": -0.25084200944050705, "compression_ratio": 1.8208955223880596, "no_speech_prob": 1.1544593689905014e-06}, {"id": 849, "seek": 406292, "start": 4062.92, "end": 4067.28, "text": " Let's predict. What's the probability that it's a zero? What's the probability? It's a one", "tokens": [961, 311, 6069, 13, 708, 311, 264, 8482, 300, 309, 311, 257, 4018, 30, 708, 311, 264, 8482, 30, 467, 311, 257, 472], "temperature": 0.0, "avg_logprob": -0.17268522990118598, "compression_ratio": 1.816425120772947, "no_speech_prob": 6.748018677171785e-06}, {"id": 850, "seek": 406292, "start": 4067.44, "end": 4072.82, "text": " What's the probability is a two and so forth right and so let's say we're trying to predict", "tokens": [708, 311, 264, 8482, 307, 257, 732, 293, 370, 5220, 558, 293, 370, 718, 311, 584, 321, 434, 1382, 281, 6069], "temperature": 0.0, "avg_logprob": -0.17268522990118598, "compression_ratio": 1.816425120772947, "no_speech_prob": 6.748018677171785e-06}, {"id": 851, "seek": 406292, "start": 4073.52, "end": 4075.0, "text": " the two", "tokens": [264, 732], "temperature": 0.0, "avg_logprob": -0.17268522990118598, "compression_ratio": 1.816425120772947, "no_speech_prob": 6.748018677171785e-06}, {"id": 852, "seek": 406292, "start": 4075.0, "end": 4082.38, "text": " Right then here is our binary cross entropy sorry categorical cross entropy, so it's just saying okay", "tokens": [1779, 550, 510, 307, 527, 17434, 3278, 30867, 2597, 19250, 804, 3278, 30867, 11, 370, 309, 311, 445, 1566, 1392], "temperature": 0.0, "avg_logprob": -0.17268522990118598, "compression_ratio": 1.816425120772947, "no_speech_prob": 6.748018677171785e-06}, {"id": 853, "seek": 406292, "start": 4082.84, "end": 4087.96, "text": " Did this one predict correctly or not how far off was it and so forth for each one?", "tokens": [2589, 341, 472, 6069, 8944, 420, 406, 577, 1400, 766, 390, 309, 293, 370, 5220, 337, 1184, 472, 30], "temperature": 0.0, "avg_logprob": -0.17268522990118598, "compression_ratio": 1.816425120772947, "no_speech_prob": 6.748018677171785e-06}, {"id": 854, "seek": 408796, "start": 4087.96, "end": 4094.4, "text": " Right and so add them all up so categorical cross entropy is identical to binary cross entropy", "tokens": [1779, 293, 370, 909, 552, 439, 493, 370, 19250, 804, 3278, 30867, 307, 14800, 281, 17434, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.175580117760635, "compression_ratio": 1.9297297297297298, "no_speech_prob": 1.1544601647983654e-06}, {"id": 855, "seek": 408796, "start": 4094.4, "end": 4096.96, "text": " We just have to add it up across all of the categories", "tokens": [492, 445, 362, 281, 909, 309, 493, 2108, 439, 295, 264, 10479], "temperature": 0.0, "avg_logprob": -0.175580117760635, "compression_ratio": 1.9297297297297298, "no_speech_prob": 1.1544601647983654e-06}, {"id": 856, "seek": 408796, "start": 4099.68, "end": 4102.4800000000005, "text": " So try and turn the binary cross entropy", "tokens": [407, 853, 293, 1261, 264, 17434, 3278, 30867], "temperature": 0.0, "avg_logprob": -0.175580117760635, "compression_ratio": 1.9297297297297298, "no_speech_prob": 1.1544601647983654e-06}, {"id": 857, "seek": 408796, "start": 4102.84, "end": 4110.08, "text": " Function in Python into a categorical cross entropy Python and maybe create both the version with the if statement and the version with the sum", "tokens": [11166, 882, 294, 15329, 666, 257, 19250, 804, 3278, 30867, 15329, 293, 1310, 1884, 1293, 264, 3037, 365, 264, 498, 5629, 293, 264, 3037, 365, 264, 2408], "temperature": 0.0, "avg_logprob": -0.175580117760635, "compression_ratio": 1.9297297297297298, "no_speech_prob": 1.1544601647983654e-06}, {"id": 858, "seek": 408796, "start": 4110.08, "end": 4112.24, "text": " And the product right?", "tokens": [400, 264, 1674, 558, 30], "temperature": 0.0, "avg_logprob": -0.175580117760635, "compression_ratio": 1.9297297297297298, "no_speech_prob": 1.1544601647983654e-06}, {"id": 859, "seek": 411224, "start": 4112.24, "end": 4118.4, "text": " All right, so that's why in our pie torch", "tokens": [1057, 558, 11, 370, 300, 311, 983, 294, 527, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.21701348971014153, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6165170652348024e-07}, {"id": 860, "seek": 411224, "start": 4121.2, "end": 4126.5199999999995, "text": " We had 10 as the output as the output dimensionality for this matrix", "tokens": [492, 632, 1266, 382, 264, 5598, 382, 264, 5598, 10139, 1860, 337, 341, 8141], "temperature": 0.0, "avg_logprob": -0.21701348971014153, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6165170652348024e-07}, {"id": 861, "seek": 411224, "start": 4127.0, "end": 4131.08, "text": " Because when we multiply by a matrix with 10 columns", "tokens": [1436, 562, 321, 12972, 538, 257, 8141, 365, 1266, 13766], "temperature": 0.0, "avg_logprob": -0.21701348971014153, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6165170652348024e-07}, {"id": 862, "seek": 411224, "start": 4131.5599999999995, "end": 4136.0, "text": " We're going to end up with something of length 10, which is what we want we want to have", "tokens": [492, 434, 516, 281, 917, 493, 365, 746, 295, 4641, 1266, 11, 597, 307, 437, 321, 528, 321, 528, 281, 362], "temperature": 0.0, "avg_logprob": -0.21701348971014153, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6165170652348024e-07}, {"id": 863, "seek": 411224, "start": 4136.96, "end": 4138.96, "text": " 10 predictions", "tokens": [1266, 21264], "temperature": 0.0, "avg_logprob": -0.21701348971014153, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6165170652348024e-07}, {"id": 864, "seek": 411224, "start": 4139.12, "end": 4140.88, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.21701348971014153, "compression_ratio": 1.4782608695652173, "no_speech_prob": 2.6165170652348024e-07}, {"id": 865, "seek": 414088, "start": 4140.88, "end": 4142.88, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.1896557205024807, "compression_ratio": 1.61, "no_speech_prob": 1.4823541505393223e-06}, {"id": 866, "seek": 414088, "start": 4143.04, "end": 4145.04, "text": " So that's the loss function that we're using", "tokens": [407, 300, 311, 264, 4470, 2445, 300, 321, 434, 1228], "temperature": 0.0, "avg_logprob": -0.1896557205024807, "compression_ratio": 1.61, "no_speech_prob": 1.4823541505393223e-06}, {"id": 867, "seek": 414088, "start": 4148.36, "end": 4150.36, "text": " All right, so then we can fit the model", "tokens": [1057, 558, 11, 370, 550, 321, 393, 3318, 264, 2316], "temperature": 0.0, "avg_logprob": -0.1896557205024807, "compression_ratio": 1.61, "no_speech_prob": 1.4823541505393223e-06}, {"id": 868, "seek": 414088, "start": 4151.72, "end": 4155.4800000000005, "text": " And what it does is it goes through every image", "tokens": [400, 437, 309, 775, 307, 309, 1709, 807, 633, 3256], "temperature": 0.0, "avg_logprob": -0.1896557205024807, "compression_ratio": 1.61, "no_speech_prob": 1.4823541505393223e-06}, {"id": 869, "seek": 414088, "start": 4156.8, "end": 4160.16, "text": " this many times so in this case, it's just looking at every image once and", "tokens": [341, 867, 1413, 370, 294, 341, 1389, 11, 309, 311, 445, 1237, 412, 633, 3256, 1564, 293], "temperature": 0.0, "avg_logprob": -0.1896557205024807, "compression_ratio": 1.61, "no_speech_prob": 1.4823541505393223e-06}, {"id": 870, "seek": 414088, "start": 4160.88, "end": 4163.34, "text": " going to slightly update the", "tokens": [516, 281, 4748, 5623, 264], "temperature": 0.0, "avg_logprob": -0.1896557205024807, "compression_ratio": 1.61, "no_speech_prob": 1.4823541505393223e-06}, {"id": 871, "seek": 414088, "start": 4164.0, "end": 4168.0, "text": " values in that weight matrix based on those gradients and", "tokens": [4190, 294, 300, 3364, 8141, 2361, 322, 729, 2771, 2448, 293], "temperature": 0.0, "avg_logprob": -0.1896557205024807, "compression_ratio": 1.61, "no_speech_prob": 1.4823541505393223e-06}, {"id": 872, "seek": 416800, "start": 4168.0, "end": 4173.0, "text": " So once we've trained it we can then say predict", "tokens": [407, 1564, 321, 600, 8895, 309, 321, 393, 550, 584, 6069], "temperature": 0.0, "avg_logprob": -0.2225043223454402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.1875538297754247e-06}, {"id": 873, "seek": 416800, "start": 4174.4, "end": 4178.24, "text": " Using this model on the validation set", "tokens": [11142, 341, 2316, 322, 264, 24071, 992], "temperature": 0.0, "avg_logprob": -0.2225043223454402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.1875538297754247e-06}, {"id": 874, "seek": 416800, "start": 4178.92, "end": 4183.84, "text": " Right and now that spits out something of 10,000 by 10", "tokens": [1779, 293, 586, 300, 637, 1208, 484, 746, 295, 1266, 11, 1360, 538, 1266], "temperature": 0.0, "avg_logprob": -0.2225043223454402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.1875538297754247e-06}, {"id": 875, "seek": 416800, "start": 4184.8, "end": 4190.54, "text": " Can somebody tell me why is this of shape these predictions? Why are they of shape 10,000 by 10?", "tokens": [1664, 2618, 980, 385, 983, 307, 341, 295, 3909, 613, 21264, 30, 1545, 366, 436, 295, 3909, 1266, 11, 1360, 538, 1266, 30], "temperature": 0.0, "avg_logprob": -0.2225043223454402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.1875538297754247e-06}, {"id": 876, "seek": 416800, "start": 4193.52, "end": 4195.52, "text": " Go for it Chris. It's right next to you", "tokens": [1037, 337, 309, 6688, 13, 467, 311, 558, 958, 281, 291], "temperature": 0.0, "avg_logprob": -0.2225043223454402, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.1875538297754247e-06}, {"id": 877, "seek": 419552, "start": 4195.52, "end": 4199.8, "text": " Well, it's because we have 10,000 images", "tokens": [1042, 11, 309, 311, 570, 321, 362, 1266, 11, 1360, 5267], "temperature": 0.0, "avg_logprob": -0.24400847752888996, "compression_ratio": 1.9364406779661016, "no_speech_prob": 1.3211827535997145e-05}, {"id": 878, "seek": 419552, "start": 4200.6, "end": 4205.040000000001, "text": " What we're training on 10,000 images training on so that's what we're validating on", "tokens": [708, 321, 434, 3097, 322, 1266, 11, 1360, 5267, 3097, 322, 370, 300, 311, 437, 321, 434, 7363, 990, 322], "temperature": 0.0, "avg_logprob": -0.24400847752888996, "compression_ratio": 1.9364406779661016, "no_speech_prob": 1.3211827535997145e-05}, {"id": 879, "seek": 419552, "start": 4205.040000000001, "end": 4209.240000000001, "text": " Or this but the same thing so 10,000 we're validating on so that's the first axis", "tokens": [1610, 341, 457, 264, 912, 551, 370, 1266, 11, 1360, 321, 434, 7363, 990, 322, 370, 300, 311, 264, 700, 10298], "temperature": 0.0, "avg_logprob": -0.24400847752888996, "compression_ratio": 1.9364406779661016, "no_speech_prob": 1.3211827535997145e-05}, {"id": 880, "seek": 419552, "start": 4209.240000000001, "end": 4214.56, "text": " That's the first and then the second axis is because we actually make 10 predictions per image good good exactly", "tokens": [663, 311, 264, 700, 293, 550, 264, 1150, 10298, 307, 570, 321, 767, 652, 1266, 21264, 680, 3256, 665, 665, 2293], "temperature": 0.0, "avg_logprob": -0.24400847752888996, "compression_ratio": 1.9364406779661016, "no_speech_prob": 1.3211827535997145e-05}, {"id": 881, "seek": 419552, "start": 4214.56, "end": 4220.84, "text": " So each one of these rows is the probabilities that it's a naught that it's a one that is a two that's three and so forth", "tokens": [407, 1184, 472, 295, 613, 13241, 307, 264, 33783, 300, 309, 311, 257, 13138, 300, 309, 311, 257, 472, 300, 307, 257, 732, 300, 311, 1045, 293, 370, 5220], "temperature": 0.0, "avg_logprob": -0.24400847752888996, "compression_ratio": 1.9364406779661016, "no_speech_prob": 1.3211827535997145e-05}, {"id": 882, "seek": 419552, "start": 4220.84, "end": 4222.84, "text": " Okay, very good", "tokens": [1033, 11, 588, 665], "temperature": 0.0, "avg_logprob": -0.24400847752888996, "compression_ratio": 1.9364406779661016, "no_speech_prob": 1.3211827535997145e-05}, {"id": 883, "seek": 422284, "start": 4222.84, "end": 4226.9400000000005, "text": " So in math there's a really common", "tokens": [407, 294, 5221, 456, 311, 257, 534, 2689], "temperature": 0.0, "avg_logprob": -0.25012110615824606, "compression_ratio": 1.660633484162896, "no_speech_prob": 2.948006567748962e-06}, {"id": 884, "seek": 422284, "start": 4227.9400000000005, "end": 4231.360000000001, "text": " Operation we do called arg max and what I say it's common. It's funny like", "tokens": [27946, 321, 360, 1219, 3882, 11469, 293, 437, 286, 584, 309, 311, 2689, 13, 467, 311, 4074, 411], "temperature": 0.0, "avg_logprob": -0.25012110615824606, "compression_ratio": 1.660633484162896, "no_speech_prob": 2.948006567748962e-06}, {"id": 885, "seek": 422284, "start": 4232.84, "end": 4235.04, "text": " At high school I never saw arg max", "tokens": [1711, 1090, 1395, 286, 1128, 1866, 3882, 11469], "temperature": 0.0, "avg_logprob": -0.25012110615824606, "compression_ratio": 1.660633484162896, "no_speech_prob": 2.948006567748962e-06}, {"id": 886, "seek": 422284, "start": 4236.52, "end": 4241.02, "text": " First year undergrad I never saw arg max but somehow after university", "tokens": [2386, 1064, 14295, 286, 1128, 1866, 3882, 11469, 457, 6063, 934, 5454], "temperature": 0.0, "avg_logprob": -0.25012110615824606, "compression_ratio": 1.660633484162896, "no_speech_prob": 2.948006567748962e-06}, {"id": 887, "seek": 422284, "start": 4241.52, "end": 4246.12, "text": " Everything's about arg max so it's one of these things that's for some reason not really taught at school", "tokens": [5471, 311, 466, 3882, 11469, 370, 309, 311, 472, 295, 613, 721, 300, 311, 337, 512, 1778, 406, 534, 5928, 412, 1395], "temperature": 0.0, "avg_logprob": -0.25012110615824606, "compression_ratio": 1.660633484162896, "no_speech_prob": 2.948006567748962e-06}, {"id": 888, "seek": 422284, "start": 4246.12, "end": 4247.88, "text": " But it actually turns out to be super critical", "tokens": [583, 309, 767, 4523, 484, 281, 312, 1687, 4924], "temperature": 0.0, "avg_logprob": -0.25012110615824606, "compression_ratio": 1.660633484162896, "no_speech_prob": 2.948006567748962e-06}, {"id": 889, "seek": 424788, "start": 4247.88, "end": 4252.96, "text": " And so arg max is both something that you'll see in math, and it's just written out in full arg max", "tokens": [400, 370, 3882, 11469, 307, 1293, 746, 300, 291, 603, 536, 294, 5221, 11, 293, 309, 311, 445, 3720, 484, 294, 1577, 3882, 11469], "temperature": 0.0, "avg_logprob": -0.18758111251027962, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.4824728370731464e-06}, {"id": 890, "seek": 424788, "start": 4254.92, "end": 4257.38, "text": " It's in numpy it's in pytorch", "tokens": [467, 311, 294, 1031, 8200, 309, 311, 294, 25878, 284, 339], "temperature": 0.0, "avg_logprob": -0.18758111251027962, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.4824728370731464e-06}, {"id": 891, "seek": 424788, "start": 4257.38, "end": 4263.08, "text": " It's super important and what it does is it says let's take this array of preds", "tokens": [467, 311, 1687, 1021, 293, 437, 309, 775, 307, 309, 1619, 718, 311, 747, 341, 10225, 295, 3852, 82], "temperature": 0.0, "avg_logprob": -0.18758111251027962, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.4824728370731464e-06}, {"id": 892, "seek": 424788, "start": 4263.56, "end": 4271.42, "text": " Right and let's figure out on this axis remember axis one is columns right so across as Chris said the 10", "tokens": [1779, 293, 718, 311, 2573, 484, 322, 341, 10298, 1604, 10298, 472, 307, 13766, 558, 370, 2108, 382, 6688, 848, 264, 1266], "temperature": 0.0, "avg_logprob": -0.18758111251027962, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.4824728370731464e-06}, {"id": 893, "seek": 424788, "start": 4271.72, "end": 4273.72, "text": " predictions for each one for each row", "tokens": [21264, 337, 1184, 472, 337, 1184, 5386], "temperature": 0.0, "avg_logprob": -0.18758111251027962, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.4824728370731464e-06}, {"id": 894, "seek": 427372, "start": 4273.72, "end": 4280.320000000001, "text": " Let's find which prediction has the highest value and return not that if it just said max it would return the value", "tokens": [961, 311, 915, 597, 17630, 575, 264, 6343, 2158, 293, 2736, 406, 300, 498, 309, 445, 848, 11469, 309, 576, 2736, 264, 2158], "temperature": 0.0, "avg_logprob": -0.2575387676942696, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.7880603309095022e-06}, {"id": 895, "seek": 427372, "start": 4280.96, "end": 4282.96, "text": " arg max returns the index", "tokens": [3882, 11469, 11247, 264, 8186], "temperature": 0.0, "avg_logprob": -0.2575387676942696, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.7880603309095022e-06}, {"id": 896, "seek": 427372, "start": 4283.52, "end": 4289.3, "text": " Of the value right so by saying arg max axis equals one it's going to return", "tokens": [2720, 264, 2158, 558, 370, 538, 1566, 3882, 11469, 10298, 6915, 472, 309, 311, 516, 281, 2736], "temperature": 0.0, "avg_logprob": -0.2575387676942696, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.7880603309095022e-06}, {"id": 897, "seek": 427372, "start": 4290.52, "end": 4295.0, "text": " The index which is actually the number itself right so let's grab the first five", "tokens": [440, 8186, 597, 307, 767, 264, 1230, 2564, 558, 370, 718, 311, 4444, 264, 700, 1732], "temperature": 0.0, "avg_logprob": -0.2575387676942696, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.7880603309095022e-06}, {"id": 898, "seek": 427372, "start": 4295.68, "end": 4301.26, "text": " Okay, so for the first one it thinks as a three then it thinks next one's an eight next one's a six the next one's a", "tokens": [1033, 11, 370, 337, 264, 700, 472, 309, 7309, 382, 257, 1045, 550, 309, 7309, 958, 472, 311, 364, 3180, 958, 472, 311, 257, 2309, 264, 958, 472, 311, 257], "temperature": 0.0, "avg_logprob": -0.2575387676942696, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.7880603309095022e-06}, {"id": 899, "seek": 430126, "start": 4301.26, "end": 4305.6, "text": " Nine next one's a six again, okay, so that's how we can convert our", "tokens": [18939, 958, 472, 311, 257, 2309, 797, 11, 1392, 11, 370, 300, 311, 577, 321, 393, 7620, 527], "temperature": 0.0, "avg_logprob": -0.22013924916585287, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9033808484891779e-06}, {"id": 900, "seek": 430126, "start": 4306.96, "end": 4309.38, "text": " probabilities back into predictions", "tokens": [33783, 646, 666, 21264], "temperature": 0.0, "avg_logprob": -0.22013924916585287, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9033808484891779e-06}, {"id": 901, "seek": 430126, "start": 4310.4400000000005, "end": 4317.400000000001, "text": " All right, so if we save that away call it preds we can then say okay when does preds equal?", "tokens": [1057, 558, 11, 370, 498, 321, 3155, 300, 1314, 818, 309, 3852, 82, 321, 393, 550, 584, 1392, 562, 775, 3852, 82, 2681, 30], "temperature": 0.0, "avg_logprob": -0.22013924916585287, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9033808484891779e-06}, {"id": 902, "seek": 430126, "start": 4318.04, "end": 4322.46, "text": " The ground truth right so that's going to return an array of balls", "tokens": [440, 2727, 3494, 558, 370, 300, 311, 516, 281, 2736, 364, 10225, 295, 9803], "temperature": 0.0, "avg_logprob": -0.22013924916585287, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9033808484891779e-06}, {"id": 903, "seek": 430126, "start": 4323.24, "end": 4328.16, "text": " Which we can treat as ones and zeros and the mean of a bunch of ones and zeros", "tokens": [3013, 321, 393, 2387, 382, 2306, 293, 35193, 293, 264, 914, 295, 257, 3840, 295, 2306, 293, 35193], "temperature": 0.0, "avg_logprob": -0.22013924916585287, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.9033808484891779e-06}, {"id": 904, "seek": 432816, "start": 4328.16, "end": 4331.96, "text": " Is just the average so that gives us the accuracy", "tokens": [1119, 445, 264, 4274, 370, 300, 2709, 505, 264, 14170], "temperature": 0.0, "avg_logprob": -0.15575275242885697, "compression_ratio": 1.8558558558558558, "no_speech_prob": 1.505699401604943e-06}, {"id": 905, "seek": 432816, "start": 4332.8, "end": 4339.2, "text": " So there's our 91.8 percent and so you want to be able to like replicate the numbers you see and here it is", "tokens": [407, 456, 311, 527, 31064, 13, 23, 3043, 293, 370, 291, 528, 281, 312, 1075, 281, 411, 25356, 264, 3547, 291, 536, 293, 510, 309, 307], "temperature": 0.0, "avg_logprob": -0.15575275242885697, "compression_ratio": 1.8558558558558558, "no_speech_prob": 1.505699401604943e-06}, {"id": 906, "seek": 432816, "start": 4339.2, "end": 4341.2, "text": " There's our 91.8 percent", "tokens": [821, 311, 527, 31064, 13, 23, 3043], "temperature": 0.0, "avg_logprob": -0.15575275242885697, "compression_ratio": 1.8558558558558558, "no_speech_prob": 1.505699401604943e-06}, {"id": 907, "seek": 432816, "start": 4341.36, "end": 4348.5599999999995, "text": " All right, so when we train this it tells us the last thing it tells us is whatever metric we asked for and we asked for", "tokens": [1057, 558, 11, 370, 562, 321, 3847, 341, 309, 5112, 505, 264, 1036, 551, 309, 5112, 505, 307, 2035, 20678, 321, 2351, 337, 293, 321, 2351, 337], "temperature": 0.0, "avg_logprob": -0.15575275242885697, "compression_ratio": 1.8558558558558558, "no_speech_prob": 1.505699401604943e-06}, {"id": 908, "seek": 432816, "start": 4351.0, "end": 4356.68, "text": " Accuracy okay, so the last thing it tells us is our metric which is accuracy and then before that we get the", "tokens": [5725, 374, 2551, 1392, 11, 370, 264, 1036, 551, 309, 5112, 505, 307, 527, 20678, 597, 307, 14170, 293, 550, 949, 300, 321, 483, 264], "temperature": 0.0, "avg_logprob": -0.15575275242885697, "compression_ratio": 1.8558558558558558, "no_speech_prob": 1.505699401604943e-06}, {"id": 909, "seek": 435668, "start": 4356.68, "end": 4362.6, "text": " Training set loss and the loss is again whatever odds we asked for negative log likelihood", "tokens": [20620, 992, 4470, 293, 264, 4470, 307, 797, 2035, 17439, 321, 2351, 337, 3671, 3565, 22119], "temperature": 0.0, "avg_logprob": -0.22602448412167128, "compression_ratio": 1.7510917030567685, "no_speech_prob": 1.5056988331707544e-06}, {"id": 910, "seek": 435668, "start": 4362.6, "end": 4365.04, "text": " And the second thing is the validation set loss", "tokens": [400, 264, 1150, 551, 307, 264, 24071, 992, 4470], "temperature": 0.0, "avg_logprob": -0.22602448412167128, "compression_ratio": 1.7510917030567685, "no_speech_prob": 1.5056988331707544e-06}, {"id": 911, "seek": 435668, "start": 4366.200000000001, "end": 4372.08, "text": " Pi torch doesn't use the word loss they use the word criterion so you'll see here crit", "tokens": [17741, 27822, 1177, 380, 764, 264, 1349, 4470, 436, 764, 264, 1349, 46691, 370, 291, 603, 536, 510, 3113], "temperature": 0.0, "avg_logprob": -0.22602448412167128, "compression_ratio": 1.7510917030567685, "no_speech_prob": 1.5056988331707544e-06}, {"id": 912, "seek": 435668, "start": 4372.400000000001, "end": 4379.88, "text": " Okay, so that's criterion equals loss. This is what loss function do we want to use they call that the criterion same thing, okay?", "tokens": [1033, 11, 370, 300, 311, 46691, 6915, 4470, 13, 639, 307, 437, 4470, 2445, 360, 321, 528, 281, 764, 436, 818, 300, 264, 46691, 912, 551, 11, 1392, 30], "temperature": 0.0, "avg_logprob": -0.22602448412167128, "compression_ratio": 1.7510917030567685, "no_speech_prob": 1.5056988331707544e-06}, {"id": 913, "seek": 437988, "start": 4379.88, "end": 4386.08, "text": " So here is how we can recreate that accuracy", "tokens": [407, 510, 307, 577, 321, 393, 25833, 300, 14170], "temperature": 0.0, "avg_logprob": -0.19290775447696834, "compression_ratio": 1.5729166666666667, "no_speech_prob": 1.9033799389944761e-06}, {"id": 914, "seek": 437988, "start": 4388.96, "end": 4397.16, "text": " So now we can go ahead and plot eight of the images along with their predictions and we've got three eight six nine", "tokens": [407, 586, 321, 393, 352, 2286, 293, 7542, 3180, 295, 264, 5267, 2051, 365, 641, 21264, 293, 321, 600, 658, 1045, 3180, 2309, 4949], "temperature": 0.0, "avg_logprob": -0.19290775447696834, "compression_ratio": 1.5729166666666667, "no_speech_prob": 1.9033799389944761e-06}, {"id": 915, "seek": 437988, "start": 4397.88, "end": 4399.04, "text": " wrong", "tokens": [2085], "temperature": 0.0, "avg_logprob": -0.19290775447696834, "compression_ratio": 1.5729166666666667, "no_speech_prob": 1.9033799389944761e-06}, {"id": 916, "seek": 437988, "start": 4399.04, "end": 4404.900000000001, "text": " Five wrong okay, and you can see like why they're wrong like this is pretty close to a nine", "tokens": [9436, 2085, 1392, 11, 293, 291, 393, 536, 411, 983, 436, 434, 2085, 411, 341, 307, 1238, 1998, 281, 257, 4949], "temperature": 0.0, "avg_logprob": -0.19290775447696834, "compression_ratio": 1.5729166666666667, "no_speech_prob": 1.9033799389944761e-06}, {"id": 917, "seek": 437988, "start": 4405.16, "end": 4407.16, "text": " It's just missing a little cross at the top", "tokens": [467, 311, 445, 5361, 257, 707, 3278, 412, 264, 1192], "temperature": 0.0, "avg_logprob": -0.19290775447696834, "compression_ratio": 1.5729166666666667, "no_speech_prob": 1.9033799389944761e-06}, {"id": 918, "seek": 440716, "start": 4407.16, "end": 4413.88, "text": " This is pretty close to a five. It's got a little bit of the extra here right so we've made a start and", "tokens": [639, 307, 1238, 1998, 281, 257, 1732, 13, 467, 311, 658, 257, 707, 857, 295, 264, 2857, 510, 558, 370, 321, 600, 1027, 257, 722, 293], "temperature": 0.0, "avg_logprob": -0.18370532063604558, "compression_ratio": 1.759493670886076, "no_speech_prob": 9.132535865319369e-07}, {"id": 919, "seek": 440716, "start": 4414.599999999999, "end": 4419.38, "text": " And all we've done so far is we haven't actually created a deep neural net", "tokens": [400, 439, 321, 600, 1096, 370, 1400, 307, 321, 2378, 380, 767, 2942, 257, 2452, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.18370532063604558, "compression_ratio": 1.759493670886076, "no_speech_prob": 9.132535865319369e-07}, {"id": 920, "seek": 440716, "start": 4419.88, "end": 4421.88, "text": " We've actually got only one layer", "tokens": [492, 600, 767, 658, 787, 472, 4583], "temperature": 0.0, "avg_logprob": -0.18370532063604558, "compression_ratio": 1.759493670886076, "no_speech_prob": 9.132535865319369e-07}, {"id": 921, "seek": 440716, "start": 4422.04, "end": 4425.36, "text": " So what we've actually done is we've created a logistic regression", "tokens": [407, 437, 321, 600, 767, 1096, 307, 321, 600, 2942, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.18370532063604558, "compression_ratio": 1.759493670886076, "no_speech_prob": 9.132535865319369e-07}, {"id": 922, "seek": 440716, "start": 4425.72, "end": 4434.2, "text": " Okay, so a logistic regression is is literally what we just built and you could try and replicate this with SK learns logistic regression", "tokens": [1033, 11, 370, 257, 3565, 3142, 24590, 307, 307, 3736, 437, 321, 445, 3094, 293, 291, 727, 853, 293, 25356, 341, 365, 21483, 27152, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.18370532063604558, "compression_ratio": 1.759493670886076, "no_speech_prob": 9.132535865319369e-07}, {"id": 923, "seek": 443420, "start": 4434.2, "end": 4437.28, "text": " Package when I did it I got", "tokens": [18466, 609, 562, 286, 630, 309, 286, 658], "temperature": 0.0, "avg_logprob": -0.2357000945716776, "compression_ratio": 1.4811715481171548, "no_speech_prob": 3.9054675653460436e-06}, {"id": 924, "seek": 443420, "start": 4437.88, "end": 4443.22, "text": " Similar accuracy, but this version ran much faster because this is running on the GPU", "tokens": [10905, 14170, 11, 457, 341, 3037, 5872, 709, 4663, 570, 341, 307, 2614, 322, 264, 18407], "temperature": 0.0, "avg_logprob": -0.2357000945716776, "compression_ratio": 1.4811715481171548, "no_speech_prob": 3.9054675653460436e-06}, {"id": 925, "seek": 443420, "start": 4443.88, "end": 4446.4, "text": " Where else SK learn runs on the CPU?", "tokens": [2305, 1646, 21483, 1466, 6676, 322, 264, 13199, 30], "temperature": 0.0, "avg_logprob": -0.2357000945716776, "compression_ratio": 1.4811715481171548, "no_speech_prob": 3.9054675653460436e-06}, {"id": 926, "seek": 443420, "start": 4446.84, "end": 4454.34, "text": " Okay, so even for something like logistic regression we can you know implement it very quickly with pytorch. How can you pass that to him?", "tokens": [1033, 11, 370, 754, 337, 746, 411, 3565, 3142, 24590, 321, 393, 291, 458, 4445, 309, 588, 2661, 365, 25878, 284, 339, 13, 1012, 393, 291, 1320, 300, 281, 796, 30], "temperature": 0.0, "avg_logprob": -0.2357000945716776, "compression_ratio": 1.4811715481171548, "no_speech_prob": 3.9054675653460436e-06}, {"id": 927, "seek": 443420, "start": 4455.76, "end": 4460.76, "text": " So when we're when we're creating our net we have to do dot CUDA", "tokens": [407, 562, 321, 434, 562, 321, 434, 4084, 527, 2533, 321, 362, 281, 360, 5893, 29777, 7509], "temperature": 0.0, "avg_logprob": -0.2357000945716776, "compression_ratio": 1.4811715481171548, "no_speech_prob": 3.9054675653460436e-06}, {"id": 928, "seek": 446076, "start": 4460.76, "end": 4469.24, "text": " What would be the consequence of not doing that would it just not run it wouldn't run quickly? Yeah, it'll run on the CPU", "tokens": [708, 576, 312, 264, 18326, 295, 406, 884, 300, 576, 309, 445, 406, 1190, 309, 2759, 380, 1190, 2661, 30, 865, 11, 309, 603, 1190, 322, 264, 13199], "temperature": 0.0, "avg_logprob": -0.20380633217947824, "compression_ratio": 1.5288461538461537, "no_speech_prob": 7.690356369494111e-07}, {"id": 929, "seek": 446076, "start": 4471.0, "end": 4473.0, "text": " Can you pass it to Jay?", "tokens": [1664, 291, 1320, 309, 281, 11146, 30], "temperature": 0.0, "avg_logprob": -0.20380633217947824, "compression_ratio": 1.5288461538461537, "no_speech_prob": 7.690356369494111e-07}, {"id": 930, "seek": 446076, "start": 4474.88, "end": 4482.14, "text": " So maybe the neural network why is that we have to do linear and followed by a nonlinear", "tokens": [407, 1310, 264, 18161, 3209, 983, 307, 300, 321, 362, 281, 360, 8213, 293, 6263, 538, 257, 2107, 28263], "temperature": 0.0, "avg_logprob": -0.20380633217947824, "compression_ratio": 1.5288461538461537, "no_speech_prob": 7.690356369494111e-07}, {"id": 931, "seek": 448214, "start": 4482.14, "end": 4492.820000000001, "text": " So the short answer is because that's what the universal approximation theorem says is the structure which can give you", "tokens": [407, 264, 2099, 1867, 307, 570, 300, 311, 437, 264, 11455, 28023, 20904, 1619, 307, 264, 3877, 597, 393, 976, 291], "temperature": 0.0, "avg_logprob": -0.1701760618654016, "compression_ratio": 1.831578947368421, "no_speech_prob": 1.1189388260390842e-06}, {"id": 932, "seek": 448214, "start": 4494.14, "end": 4497.58, "text": " Arbitrarily accurate functions for any functional form, you know", "tokens": [1587, 5260, 81, 3289, 8559, 6828, 337, 604, 11745, 1254, 11, 291, 458], "temperature": 0.0, "avg_logprob": -0.1701760618654016, "compression_ratio": 1.831578947368421, "no_speech_prob": 1.1189388260390842e-06}, {"id": 933, "seek": 448214, "start": 4497.58, "end": 4502.92, "text": " So the long answer is the details of why the universal approximation theorem works", "tokens": [407, 264, 938, 1867, 307, 264, 4365, 295, 983, 264, 11455, 28023, 20904, 1985], "temperature": 0.0, "avg_logprob": -0.1701760618654016, "compression_ratio": 1.831578947368421, "no_speech_prob": 1.1189388260390842e-06}, {"id": 934, "seek": 448214, "start": 4504.02, "end": 4507.06, "text": " Another version of the short answer is that's the definition of a neural network", "tokens": [3996, 3037, 295, 264, 2099, 1867, 307, 300, 311, 264, 7123, 295, 257, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.1701760618654016, "compression_ratio": 1.831578947368421, "no_speech_prob": 1.1189388260390842e-06}, {"id": 935, "seek": 450706, "start": 4507.06, "end": 4512.620000000001, "text": " So the definition of a neural network is a linear layer followed by a", "tokens": [407, 264, 7123, 295, 257, 18161, 3209, 307, 257, 8213, 4583, 6263, 538, 257], "temperature": 0.0, "avg_logprob": -0.18747233381175032, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1189387123522465e-06}, {"id": 936, "seek": 450706, "start": 4513.26, "end": 4516.38, "text": " Activation function followed by a linear layer followed by an activation function", "tokens": [28550, 399, 2445, 6263, 538, 257, 8213, 4583, 6263, 538, 364, 24433, 2445], "temperature": 0.0, "avg_logprob": -0.18747233381175032, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1189387123522465e-06}, {"id": 937, "seek": 450706, "start": 4517.26, "end": 4518.620000000001, "text": " Etc", "tokens": [3790, 66], "temperature": 0.0, "avg_logprob": -0.18747233381175032, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1189387123522465e-06}, {"id": 938, "seek": 450706, "start": 4518.620000000001, "end": 4522.02, "text": " We go into a lot more detail of this in the deep learning course", "tokens": [492, 352, 666, 257, 688, 544, 2607, 295, 341, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.18747233381175032, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1189387123522465e-06}, {"id": 939, "seek": 450706, "start": 4522.900000000001, "end": 4528.22, "text": " But you know for this purpose. It's it's enough to know like that it works", "tokens": [583, 291, 458, 337, 341, 4334, 13, 467, 311, 309, 311, 1547, 281, 458, 411, 300, 309, 1985], "temperature": 0.0, "avg_logprob": -0.18747233381175032, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1189387123522465e-06}, {"id": 940, "seek": 450706, "start": 4529.14, "end": 4534.660000000001, "text": " So far of course we haven't actually built a deep neural net at all. We've just built a logistic regression", "tokens": [407, 1400, 295, 1164, 321, 2378, 380, 767, 3094, 257, 2452, 18161, 2533, 412, 439, 13, 492, 600, 445, 3094, 257, 3565, 3142, 24590], "temperature": 0.0, "avg_logprob": -0.18747233381175032, "compression_ratio": 1.7675438596491229, "no_speech_prob": 1.1189387123522465e-06}, {"id": 941, "seek": 453466, "start": 4534.66, "end": 4540.38, "text": " and so at this point if you think about it all we're doing is we're taking every input pixel and", "tokens": [293, 370, 412, 341, 935, 498, 291, 519, 466, 309, 439, 321, 434, 884, 307, 321, 434, 1940, 633, 4846, 19261, 293], "temperature": 0.0, "avg_logprob": -0.16075502123151506, "compression_ratio": 1.7450980392156863, "no_speech_prob": 5.17389526066836e-06}, {"id": 942, "seek": 453466, "start": 4541.18, "end": 4543.099999999999, "text": " Multiplying it by a weight", "tokens": [31150, 7310, 309, 538, 257, 3364], "temperature": 0.0, "avg_logprob": -0.16075502123151506, "compression_ratio": 1.7450980392156863, "no_speech_prob": 5.17389526066836e-06}, {"id": 943, "seek": 453466, "start": 4543.099999999999, "end": 4548.86, "text": " For each possible outcome, right? So we're basically saying, you know on average the number one", "tokens": [1171, 1184, 1944, 9700, 11, 558, 30, 407, 321, 434, 1936, 1566, 11, 291, 458, 322, 4274, 264, 1230, 472], "temperature": 0.0, "avg_logprob": -0.16075502123151506, "compression_ratio": 1.7450980392156863, "no_speech_prob": 5.17389526066836e-06}, {"id": 944, "seek": 453466, "start": 4549.139999999999, "end": 4555.82, "text": " You know has these pixels turned on the number two has these pixels turned on and that's why it's not terribly accurate, right?", "tokens": [509, 458, 575, 613, 18668, 3574, 322, 264, 1230, 732, 575, 613, 18668, 3574, 322, 293, 300, 311, 983, 309, 311, 406, 22903, 8559, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16075502123151506, "compression_ratio": 1.7450980392156863, "no_speech_prob": 5.17389526066836e-06}, {"id": 945, "seek": 453466, "start": 4555.82, "end": 4557.82, "text": " That's that's not how", "tokens": [663, 311, 300, 311, 406, 577], "temperature": 0.0, "avg_logprob": -0.16075502123151506, "compression_ratio": 1.7450980392156863, "no_speech_prob": 5.17389526066836e-06}, {"id": 946, "seek": 453466, "start": 4558.18, "end": 4563.7, "text": " Digit recognition works in real life, but that's that's always built so far", "tokens": [10976, 270, 11150, 1985, 294, 957, 993, 11, 457, 300, 311, 300, 311, 1009, 3094, 370, 1400], "temperature": 0.0, "avg_logprob": -0.16075502123151506, "compression_ratio": 1.7450980392156863, "no_speech_prob": 5.17389526066836e-06}, {"id": 947, "seek": 456370, "start": 4563.7, "end": 4566.179999999999, "text": " Okay, can you pass that to Devon?", "tokens": [1033, 11, 393, 291, 1320, 300, 281, 9096, 266, 30], "temperature": 0.0, "avg_logprob": -0.21574472126207853, "compression_ratio": 1.4585365853658536, "no_speech_prob": 5.771876203652937e-06}, {"id": 948, "seek": 456370, "start": 4567.42, "end": 4571.98, "text": " So you keep saying this universal approximation theorem. Yeah, did you define that?", "tokens": [407, 291, 1066, 1566, 341, 11455, 28023, 20904, 13, 865, 11, 630, 291, 6964, 300, 30], "temperature": 0.0, "avg_logprob": -0.21574472126207853, "compression_ratio": 1.4585365853658536, "no_speech_prob": 5.771876203652937e-06}, {"id": 949, "seek": 456370, "start": 4573.46, "end": 4578.22, "text": " Yeah, but let's cover it again because it's worth talking about so", "tokens": [865, 11, 457, 718, 311, 2060, 309, 797, 570, 309, 311, 3163, 1417, 466, 370], "temperature": 0.0, "avg_logprob": -0.21574472126207853, "compression_ratio": 1.4585365853658536, "no_speech_prob": 5.771876203652937e-06}, {"id": 950, "seek": 456370, "start": 4582.099999999999, "end": 4583.78, "text": " All right, so", "tokens": [1057, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.21574472126207853, "compression_ratio": 1.4585365853658536, "no_speech_prob": 5.771876203652937e-06}, {"id": 951, "seek": 456370, "start": 4583.78, "end": 4589.42, "text": " Michael Nielsen has this great website called neural networks and deep learning and his chapter 4 is", "tokens": [5116, 426, 1187, 6748, 575, 341, 869, 3144, 1219, 18161, 9590, 293, 2452, 2539, 293, 702, 7187, 1017, 307], "temperature": 0.0, "avg_logprob": -0.21574472126207853, "compression_ratio": 1.4585365853658536, "no_speech_prob": 5.771876203652937e-06}, {"id": 952, "seek": 458942, "start": 4589.42, "end": 4597.3, "text": " Is actually kind of famous now and in it he does this walkthrough of basically showing that a", "tokens": [1119, 767, 733, 295, 4618, 586, 293, 294, 309, 415, 775, 341, 1792, 11529, 295, 1936, 4099, 300, 257], "temperature": 0.0, "avg_logprob": -0.1967041643359993, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.710871507995762e-06}, {"id": 953, "seek": 458942, "start": 4597.7, "end": 4599.7, "text": " neural network can", "tokens": [18161, 3209, 393], "temperature": 0.0, "avg_logprob": -0.1967041643359993, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.710871507995762e-06}, {"id": 954, "seek": 458942, "start": 4602.1, "end": 4605.78, "text": " Can approximate any other function to arbitrarily", "tokens": [1664, 30874, 604, 661, 2445, 281, 19071, 3289], "temperature": 0.0, "avg_logprob": -0.1967041643359993, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.710871507995762e-06}, {"id": 955, "seek": 458942, "start": 4606.5, "end": 4607.74, "text": " close", "tokens": [1998], "temperature": 0.0, "avg_logprob": -0.1967041643359993, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.710871507995762e-06}, {"id": 956, "seek": 458942, "start": 4607.74, "end": 4614.62, "text": " Accuracy as long as it's big enough and we walk through this in a lot of detail in the deep learning course", "tokens": [5725, 374, 2551, 382, 938, 382, 309, 311, 955, 1547, 293, 321, 1792, 807, 341, 294, 257, 688, 295, 2607, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.1967041643359993, "compression_ratio": 1.5911330049261083, "no_speech_prob": 4.710871507995762e-06}, {"id": 957, "seek": 461462, "start": 4614.62, "end": 4619.18, "text": " but the basic trick is that he shows that with a", "tokens": [457, 264, 3875, 4282, 307, 300, 415, 3110, 300, 365, 257], "temperature": 0.0, "avg_logprob": -0.17800305015162418, "compression_ratio": 1.8311111111111111, "no_speech_prob": 5.0936737352458294e-06}, {"id": 958, "seek": 461462, "start": 4620.66, "end": 4626.42, "text": " Few different numbers you can basically kind of cause these things to kind of create little boxes", "tokens": [33468, 819, 3547, 291, 393, 1936, 733, 295, 3082, 613, 721, 281, 733, 295, 1884, 707, 9002], "temperature": 0.0, "avg_logprob": -0.17800305015162418, "compression_ratio": 1.8311111111111111, "no_speech_prob": 5.0936737352458294e-06}, {"id": 959, "seek": 461462, "start": 4626.42, "end": 4628.42, "text": " You can move the boxes up and down you can move them around", "tokens": [509, 393, 1286, 264, 9002, 493, 293, 760, 291, 393, 1286, 552, 926], "temperature": 0.0, "avg_logprob": -0.17800305015162418, "compression_ratio": 1.8311111111111111, "no_speech_prob": 5.0936737352458294e-06}, {"id": 960, "seek": 461462, "start": 4628.78, "end": 4633.58, "text": " You can join them together to eventually basically create like connections of towers", "tokens": [509, 393, 3917, 552, 1214, 281, 4728, 1936, 1884, 411, 9271, 295, 25045], "temperature": 0.0, "avg_logprob": -0.17800305015162418, "compression_ratio": 1.8311111111111111, "no_speech_prob": 5.0936737352458294e-06}, {"id": 961, "seek": 461462, "start": 4633.78, "end": 4638.18, "text": " Which you can like use to approximate any kind of surface, right?", "tokens": [3013, 291, 393, 411, 764, 281, 30874, 604, 733, 295, 3753, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17800305015162418, "compression_ratio": 1.8311111111111111, "no_speech_prob": 5.0936737352458294e-06}, {"id": 962, "seek": 461462, "start": 4639.099999999999, "end": 4643.26, "text": " so that's you know, that's basically the the trick and", "tokens": [370, 300, 311, 291, 458, 11, 300, 311, 1936, 264, 264, 4282, 293], "temperature": 0.0, "avg_logprob": -0.17800305015162418, "compression_ratio": 1.8311111111111111, "no_speech_prob": 5.0936737352458294e-06}, {"id": 963, "seek": 464326, "start": 4643.26, "end": 4646.04, "text": " And so all we need to do", "tokens": [400, 370, 439, 321, 643, 281, 360], "temperature": 0.0, "avg_logprob": -0.17600915370843348, "compression_ratio": 1.6647727272727273, "no_speech_prob": 2.0462413885979913e-05}, {"id": 964, "seek": 464326, "start": 4646.9800000000005, "end": 4651.780000000001, "text": " given given that is to kind of find the parameters for each of the", "tokens": [2212, 2212, 300, 307, 281, 733, 295, 915, 264, 9834, 337, 1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.17600915370843348, "compression_ratio": 1.6647727272727273, "no_speech_prob": 2.0462413885979913e-05}, {"id": 965, "seek": 464326, "start": 4652.46, "end": 4653.9800000000005, "text": " linear functions", "tokens": [8213, 6828], "temperature": 0.0, "avg_logprob": -0.17600915370843348, "compression_ratio": 1.6647727272727273, "no_speech_prob": 2.0462413885979913e-05}, {"id": 966, "seek": 464326, "start": 4653.9800000000005, "end": 4659.46, "text": " In that neural network so to find the weights in each of the in each of the matrices and so so far", "tokens": [682, 300, 18161, 3209, 370, 281, 915, 264, 17443, 294, 1184, 295, 264, 294, 1184, 295, 264, 32284, 293, 370, 370, 1400], "temperature": 0.0, "avg_logprob": -0.17600915370843348, "compression_ratio": 1.6647727272727273, "no_speech_prob": 2.0462413885979913e-05}, {"id": 967, "seek": 464326, "start": 4659.900000000001, "end": 4661.820000000001, "text": " We've got just one", "tokens": [492, 600, 658, 445, 472], "temperature": 0.0, "avg_logprob": -0.17600915370843348, "compression_ratio": 1.6647727272727273, "no_speech_prob": 2.0462413885979913e-05}, {"id": 968, "seek": 464326, "start": 4661.820000000001, "end": 4666.92, "text": " Matrix and so we've just built a simple logistic regression so far", "tokens": [36274, 293, 370, 321, 600, 445, 3094, 257, 2199, 3565, 3142, 24590, 370, 1400], "temperature": 0.0, "avg_logprob": -0.17600915370843348, "compression_ratio": 1.6647727272727273, "no_speech_prob": 2.0462413885979913e-05}, {"id": 969, "seek": 466692, "start": 4666.92, "end": 4671.76, "text": " I'm gonna do your question. Yeah, just a small doubt", "tokens": [286, 478, 799, 360, 428, 1168, 13, 865, 11, 445, 257, 1359, 6385], "temperature": 0.0, "avg_logprob": -0.3051019000191974, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.9310171157703735e-05}, {"id": 970, "seek": 466692, "start": 4671.76, "end": 4676.92, "text": " I just want to confirm that when you showed images of the examples of the images which were misclassified", "tokens": [286, 445, 528, 281, 9064, 300, 562, 291, 4712, 5267, 295, 264, 5110, 295, 264, 5267, 597, 645, 3346, 11665, 2587], "temperature": 0.0, "avg_logprob": -0.3051019000191974, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.9310171157703735e-05}, {"id": 971, "seek": 466692, "start": 4676.92, "end": 4681.72, "text": " Yeah, they look rectangular. So it's just that while rendering the pixels are being scaled differently", "tokens": [865, 11, 436, 574, 31167, 13, 407, 309, 311, 445, 300, 1339, 22407, 264, 18668, 366, 885, 36039, 7614], "temperature": 0.0, "avg_logprob": -0.3051019000191974, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.9310171157703735e-05}, {"id": 972, "seek": 466692, "start": 4682.16, "end": 4685.4, "text": " So are they still 28 by 28 square 28 by 28? I", "tokens": [407, 366, 436, 920, 7562, 538, 7562, 3732, 7562, 538, 7562, 30, 286], "temperature": 0.0, "avg_logprob": -0.3051019000191974, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.9310171157703735e-05}, {"id": 973, "seek": 466692, "start": 4686.04, "end": 4689.96, "text": " Think that's where I think they just look rectangular because they've got titles on the top. I'm not sure", "tokens": [6557, 300, 311, 689, 286, 519, 436, 445, 574, 31167, 570, 436, 600, 658, 12992, 322, 264, 1192, 13, 286, 478, 406, 988], "temperature": 0.0, "avg_logprob": -0.3051019000191974, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.9310171157703735e-05}, {"id": 974, "seek": 466692, "start": 4690.6, "end": 4692.6, "text": " I don't know. Anyway, they are square and", "tokens": [286, 500, 380, 458, 13, 5684, 11, 436, 366, 3732, 293], "temperature": 0.0, "avg_logprob": -0.3051019000191974, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.9310171157703735e-05}, {"id": 975, "seek": 466692, "start": 4693.4800000000005, "end": 4694.6, "text": " like", "tokens": [411], "temperature": 0.0, "avg_logprob": -0.3051019000191974, "compression_ratio": 1.6727272727272726, "no_speech_prob": 2.9310171157703735e-05}, {"id": 976, "seek": 469460, "start": 4694.6, "end": 4701.58, "text": " Matplotlib. Yeah, it does often fiddle around with you know what it considers black versus white and you know", "tokens": [6789, 564, 310, 38270, 13, 865, 11, 309, 775, 2049, 24553, 2285, 926, 365, 291, 458, 437, 309, 33095, 2211, 5717, 2418, 293, 291, 458], "temperature": 0.0, "avg_logprob": -0.18352940641803506, "compression_ratio": 1.4976958525345623, "no_speech_prob": 5.682371920556761e-06}, {"id": 977, "seek": 469460, "start": 4701.76, "end": 4705.42, "text": " Having different size axes and stuff. So yeah, you do have to be a bit careful there sometimes", "tokens": [10222, 819, 2744, 35387, 293, 1507, 13, 407, 1338, 11, 291, 360, 362, 281, 312, 257, 857, 5026, 456, 2171], "temperature": 0.0, "avg_logprob": -0.18352940641803506, "compression_ratio": 1.4976958525345623, "no_speech_prob": 5.682371920556761e-06}, {"id": 978, "seek": 469460, "start": 4709.56, "end": 4711.56, "text": " Okay, so", "tokens": [1033, 11, 370], "temperature": 0.0, "avg_logprob": -0.18352940641803506, "compression_ratio": 1.4976958525345623, "no_speech_prob": 5.682371920556761e-06}, {"id": 979, "seek": 469460, "start": 4712.280000000001, "end": 4717.8, "text": " Hopefully this will now make more sense because what we're going to do is like dig in a layer deeper and define", "tokens": [10429, 341, 486, 586, 652, 544, 2020, 570, 437, 321, 434, 516, 281, 360, 307, 411, 2528, 294, 257, 4583, 7731, 293, 6964], "temperature": 0.0, "avg_logprob": -0.18352940641803506, "compression_ratio": 1.4976958525345623, "no_speech_prob": 5.682371920556761e-06}, {"id": 980, "seek": 471780, "start": 4717.8, "end": 4725.12, "text": " Logistic regression without using an end dot sequential without using an end dot linear without using an end dot log softmax", "tokens": [10824, 3142, 24590, 1553, 1228, 364, 917, 5893, 42881, 1553, 1228, 364, 917, 5893, 8213, 1553, 1228, 364, 917, 5893, 3565, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.20719194412231445, "compression_ratio": 1.8450704225352113, "no_speech_prob": 5.255356882116757e-06}, {"id": 981, "seek": 471780, "start": 4725.28, "end": 4727.28, "text": " So we're going to do", "tokens": [407, 321, 434, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.20719194412231445, "compression_ratio": 1.8450704225352113, "no_speech_prob": 5.255356882116757e-06}, {"id": 982, "seek": 471780, "start": 4727.360000000001, "end": 4733.24, "text": " Nearly all of the layer definition from scratch. Okay, so to do that", "tokens": [38000, 439, 295, 264, 4583, 7123, 490, 8459, 13, 1033, 11, 370, 281, 360, 300], "temperature": 0.0, "avg_logprob": -0.20719194412231445, "compression_ratio": 1.8450704225352113, "no_speech_prob": 5.255356882116757e-06}, {"id": 983, "seek": 471780, "start": 4733.72, "end": 4741.68, "text": " We're going to have to define a pytorch module a pytorch module is basically either a neural net or a layer in a neural net", "tokens": [492, 434, 516, 281, 362, 281, 6964, 257, 25878, 284, 339, 10088, 257, 25878, 284, 339, 10088, 307, 1936, 2139, 257, 18161, 2533, 420, 257, 4583, 294, 257, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.20719194412231445, "compression_ratio": 1.8450704225352113, "no_speech_prob": 5.255356882116757e-06}, {"id": 984, "seek": 471780, "start": 4742.72, "end": 4745.2, "text": " Which is actually kind of a powerful concept of itself", "tokens": [3013, 307, 767, 733, 295, 257, 4005, 3410, 295, 2564], "temperature": 0.0, "avg_logprob": -0.20719194412231445, "compression_ratio": 1.8450704225352113, "no_speech_prob": 5.255356882116757e-06}, {"id": 985, "seek": 474520, "start": 4745.2, "end": 4750.599999999999, "text": " Basically anything that can kind of behave like a neural net can itself be part of another neural net", "tokens": [8537, 1340, 300, 393, 733, 295, 15158, 411, 257, 18161, 2533, 393, 2564, 312, 644, 295, 1071, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.17386524586737911, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.260313976876205e-06}, {"id": 986, "seek": 474520, "start": 4750.599999999999, "end": 4755.62, "text": " And so this is like how we can construct particularly powerful architectures", "tokens": [400, 370, 341, 307, 411, 577, 321, 393, 7690, 4098, 4005, 6331, 1303], "temperature": 0.0, "avg_logprob": -0.17386524586737911, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.260313976876205e-06}, {"id": 987, "seek": 474520, "start": 4756.12, "end": 4758.12, "text": " combining lots of other pieces", "tokens": [21928, 3195, 295, 661, 3755], "temperature": 0.0, "avg_logprob": -0.17386524586737911, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.260313976876205e-06}, {"id": 988, "seek": 474520, "start": 4759.08, "end": 4763.44, "text": " So to create a pytorch module just create a Python class", "tokens": [407, 281, 1884, 257, 25878, 284, 339, 10088, 445, 1884, 257, 15329, 1508], "temperature": 0.0, "avg_logprob": -0.17386524586737911, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.260313976876205e-06}, {"id": 989, "seek": 474520, "start": 4764.2, "end": 4768.76, "text": " But it has to inherit from an end module. So we haven't done inheritance before", "tokens": [583, 309, 575, 281, 21389, 490, 364, 917, 10088, 13, 407, 321, 2378, 380, 1096, 32122, 949], "temperature": 0.0, "avg_logprob": -0.17386524586737911, "compression_ratio": 1.6018518518518519, "no_speech_prob": 2.260313976876205e-06}, {"id": 990, "seek": 476876, "start": 4768.76, "end": 4775.320000000001, "text": " Other than that, this is all the same concepts we've seen in owo already", "tokens": [5358, 813, 300, 11, 341, 307, 439, 264, 912, 10392, 321, 600, 1612, 294, 277, 6120, 1217], "temperature": 0.0, "avg_logprob": -0.20530498150697687, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.948004521385883e-06}, {"id": 991, "seek": 476876, "start": 4776.4400000000005, "end": 4783.4800000000005, "text": " Basically, if you put something in parentheses here, what it means is that our class gets all of the functionality of this class", "tokens": [8537, 11, 498, 291, 829, 746, 294, 34153, 510, 11, 437, 309, 1355, 307, 300, 527, 1508, 2170, 439, 295, 264, 14980, 295, 341, 1508], "temperature": 0.0, "avg_logprob": -0.20530498150697687, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.948004521385883e-06}, {"id": 992, "seek": 476876, "start": 4783.96, "end": 4786.46, "text": " For free it's called subclassing it", "tokens": [1171, 1737, 309, 311, 1219, 1422, 11665, 278, 309], "temperature": 0.0, "avg_logprob": -0.20530498150697687, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.948004521385883e-06}, {"id": 993, "seek": 476876, "start": 4786.4800000000005, "end": 4793.88, "text": " So we're going to get all of the capabilities of a neural network module that the pytorch authors have provided and then we're going to add", "tokens": [407, 321, 434, 516, 281, 483, 439, 295, 264, 10862, 295, 257, 18161, 3209, 10088, 300, 264, 25878, 284, 339, 16552, 362, 5649, 293, 550, 321, 434, 516, 281, 909], "temperature": 0.0, "avg_logprob": -0.20530498150697687, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.948004521385883e-06}, {"id": 994, "seek": 476876, "start": 4794.84, "end": 4796.84, "text": " additional functionality to it", "tokens": [4497, 14980, 281, 309], "temperature": 0.0, "avg_logprob": -0.20530498150697687, "compression_ratio": 1.7215189873417722, "no_speech_prob": 2.948004521385883e-06}, {"id": 995, "seek": 479684, "start": 4796.84, "end": 4804.360000000001, "text": " When you create a subclass there is one key thing you need to remember to do which is when you initialize your class", "tokens": [1133, 291, 1884, 257, 1422, 11665, 456, 307, 472, 2141, 551, 291, 643, 281, 1604, 281, 360, 597, 307, 562, 291, 5883, 1125, 428, 1508], "temperature": 0.0, "avg_logprob": -0.16823017925297448, "compression_ratio": 1.8277310924369747, "no_speech_prob": 3.3931262350961333e-06}, {"id": 996, "seek": 479684, "start": 4804.68, "end": 4811.6, "text": " You have to first of all initialize the super class. That's how the super class is the nn dot module", "tokens": [509, 362, 281, 700, 295, 439, 5883, 1125, 264, 1687, 1508, 13, 663, 311, 577, 264, 1687, 1508, 307, 264, 297, 77, 5893, 10088], "temperature": 0.0, "avg_logprob": -0.16823017925297448, "compression_ratio": 1.8277310924369747, "no_speech_prob": 3.3931262350961333e-06}, {"id": 997, "seek": 479684, "start": 4811.6, "end": 4817.52, "text": " So the nn dot module has to be built before you can start adding your pieces to it", "tokens": [407, 264, 297, 77, 5893, 10088, 575, 281, 312, 3094, 949, 291, 393, 722, 5127, 428, 3755, 281, 309], "temperature": 0.0, "avg_logprob": -0.16823017925297448, "compression_ratio": 1.8277310924369747, "no_speech_prob": 3.3931262350961333e-06}, {"id": 998, "seek": 479684, "start": 4817.52, "end": 4822.96, "text": " And so this is just like something you can copy and paste into every one of your modules", "tokens": [400, 370, 341, 307, 445, 411, 746, 291, 393, 5055, 293, 9163, 666, 633, 472, 295, 428, 16679], "temperature": 0.0, "avg_logprob": -0.16823017925297448, "compression_ratio": 1.8277310924369747, "no_speech_prob": 3.3931262350961333e-06}, {"id": 999, "seek": 479684, "start": 4822.96, "end": 4825.78, "text": " You just say super dot in it. This just means", "tokens": [509, 445, 584, 1687, 5893, 294, 309, 13, 639, 445, 1355], "temperature": 0.0, "avg_logprob": -0.16823017925297448, "compression_ratio": 1.8277310924369747, "no_speech_prob": 3.3931262350961333e-06}, {"id": 1000, "seek": 482578, "start": 4825.78, "end": 4829.66, "text": " Construct the super class first. Okay", "tokens": [8574, 1757, 264, 1687, 1508, 700, 13, 1033], "temperature": 0.0, "avg_logprob": -0.22143028522359914, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340527655382175e-06}, {"id": 1001, "seek": 482578, "start": 4830.219999999999, "end": 4831.5, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.22143028522359914, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340527655382175e-06}, {"id": 1002, "seek": 482578, "start": 4831.5, "end": 4837.86, "text": " Having done that we can now go ahead and define our weights and our bias. So our weights is", "tokens": [10222, 1096, 300, 321, 393, 586, 352, 2286, 293, 6964, 527, 17443, 293, 527, 12577, 13, 407, 527, 17443, 307], "temperature": 0.0, "avg_logprob": -0.22143028522359914, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340527655382175e-06}, {"id": 1003, "seek": 482578, "start": 4838.62, "end": 4845.179999999999, "text": " The the weight matrix is the actual matrix that we're going to multiply our data by and as we discussed", "tokens": [440, 264, 3364, 8141, 307, 264, 3539, 8141, 300, 321, 434, 516, 281, 12972, 527, 1412, 538, 293, 382, 321, 7152], "temperature": 0.0, "avg_logprob": -0.22143028522359914, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340527655382175e-06}, {"id": 1004, "seek": 482578, "start": 4845.179999999999, "end": 4847.78, "text": " it's going to have 28 times 28 rows and", "tokens": [309, 311, 516, 281, 362, 7562, 1413, 7562, 13241, 293], "temperature": 0.0, "avg_logprob": -0.22143028522359914, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340527655382175e-06}, {"id": 1005, "seek": 482578, "start": 4848.9, "end": 4850.74, "text": " 10 columns", "tokens": [1266, 13766], "temperature": 0.0, "avg_logprob": -0.22143028522359914, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340527655382175e-06}, {"id": 1006, "seek": 482578, "start": 4850.74, "end": 4852.98, "text": " And that's because if we take an image", "tokens": [400, 300, 311, 570, 498, 321, 747, 364, 3256], "temperature": 0.0, "avg_logprob": -0.22143028522359914, "compression_ratio": 1.5673076923076923, "no_speech_prob": 3.340527655382175e-06}, {"id": 1007, "seek": 485298, "start": 4852.98, "end": 4858.58, "text": " Which we flattened out into a 28 by 28 length vector", "tokens": [3013, 321, 24183, 292, 484, 666, 257, 7562, 538, 7562, 4641, 8062], "temperature": 0.0, "avg_logprob": -0.23057396804230124, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.9944278594484786e-06}, {"id": 1008, "seek": 485298, "start": 4859.299999999999, "end": 4864.139999999999, "text": " right, then we can multiply it by this weight matrix to get back out a", "tokens": [558, 11, 550, 321, 393, 12972, 309, 538, 341, 3364, 8141, 281, 483, 646, 484, 257], "temperature": 0.0, "avg_logprob": -0.23057396804230124, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.9944278594484786e-06}, {"id": 1009, "seek": 485298, "start": 4864.66, "end": 4866.259999999999, "text": " length 10", "tokens": [4641, 1266], "temperature": 0.0, "avg_logprob": -0.23057396804230124, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.9944278594484786e-06}, {"id": 1010, "seek": 485298, "start": 4866.259999999999, "end": 4868.9, "text": " Vector which we can then use to", "tokens": [691, 20814, 597, 321, 393, 550, 764, 281], "temperature": 0.0, "avg_logprob": -0.23057396804230124, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.9944278594484786e-06}, {"id": 1011, "seek": 485298, "start": 4871.099999999999, "end": 4873.099999999999, "text": " Consider as a set of predictions", "tokens": [17416, 382, 257, 992, 295, 21264], "temperature": 0.0, "avg_logprob": -0.23057396804230124, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.9944278594484786e-06}, {"id": 1012, "seek": 485298, "start": 4875.179999999999, "end": 4877.62, "text": " So that's our weight matrix now", "tokens": [407, 300, 311, 527, 3364, 8141, 586], "temperature": 0.0, "avg_logprob": -0.23057396804230124, "compression_ratio": 1.5080213903743316, "no_speech_prob": 2.9944278594484786e-06}, {"id": 1013, "seek": 487762, "start": 4877.62, "end": 4885.34, "text": " The problem is that we don't just want y equals ax we want y equals ax plus b", "tokens": [440, 1154, 307, 300, 321, 500, 380, 445, 528, 288, 6915, 6360, 321, 528, 288, 6915, 6360, 1804, 272], "temperature": 0.0, "avg_logprob": -0.1691841761271159, "compression_ratio": 1.68, "no_speech_prob": 3.520908080645313e-07}, {"id": 1014, "seek": 487762, "start": 4885.82, "end": 4887.82, "text": " so the plus B in", "tokens": [370, 264, 1804, 363, 294], "temperature": 0.0, "avg_logprob": -0.1691841761271159, "compression_ratio": 1.68, "no_speech_prob": 3.520908080645313e-07}, {"id": 1015, "seek": 487762, "start": 4888.3, "end": 4891.74, "text": " Neural nets is called bias and so as well as defining weights", "tokens": [1734, 1807, 36170, 307, 1219, 12577, 293, 370, 382, 731, 382, 17827, 17443], "temperature": 0.0, "avg_logprob": -0.1691841761271159, "compression_ratio": 1.68, "no_speech_prob": 3.520908080645313e-07}, {"id": 1016, "seek": 487762, "start": 4891.74, "end": 4895.9, "text": " We're also going to find bias and so since this thing is going to spit out", "tokens": [492, 434, 611, 516, 281, 915, 12577, 293, 370, 1670, 341, 551, 307, 516, 281, 22127, 484], "temperature": 0.0, "avg_logprob": -0.1691841761271159, "compression_ratio": 1.68, "no_speech_prob": 3.520908080645313e-07}, {"id": 1017, "seek": 487762, "start": 4896.86, "end": 4899.4, "text": " For every image something of length 10", "tokens": [1171, 633, 3256, 746, 295, 4641, 1266], "temperature": 0.0, "avg_logprob": -0.1691841761271159, "compression_ratio": 1.68, "no_speech_prob": 3.520908080645313e-07}, {"id": 1018, "seek": 487762, "start": 4899.94, "end": 4905.3, "text": " That means that we need to create a vector of length 10 to be our", "tokens": [663, 1355, 300, 321, 643, 281, 1884, 257, 8062, 295, 4641, 1266, 281, 312, 527], "temperature": 0.0, "avg_logprob": -0.1691841761271159, "compression_ratio": 1.68, "no_speech_prob": 3.520908080645313e-07}, {"id": 1019, "seek": 490530, "start": 4905.3, "end": 4911.5, "text": " Biases in other words for everything nought one two three up to nine. We're going to have a different", "tokens": [363, 4609, 279, 294, 661, 2283, 337, 1203, 297, 930, 472, 732, 1045, 493, 281, 4949, 13, 492, 434, 516, 281, 362, 257, 819], "temperature": 0.0, "avg_logprob": -0.25528590819414926, "compression_ratio": 1.3167701863354038, "no_speech_prob": 5.989260216665571e-07}, {"id": 1020, "seek": 490530, "start": 4912.3, "end": 4914.3, "text": " Plus B that would be adding", "tokens": [7721, 363, 300, 576, 312, 5127], "temperature": 0.0, "avg_logprob": -0.25528590819414926, "compression_ratio": 1.3167701863354038, "no_speech_prob": 5.989260216665571e-07}, {"id": 1021, "seek": 490530, "start": 4914.9800000000005, "end": 4916.9800000000005, "text": " right, so", "tokens": [558, 11, 370], "temperature": 0.0, "avg_logprob": -0.25528590819414926, "compression_ratio": 1.3167701863354038, "no_speech_prob": 5.989260216665571e-07}, {"id": 1022, "seek": 490530, "start": 4919.62, "end": 4921.62, "text": " We've got our", "tokens": [492, 600, 658, 527], "temperature": 0.0, "avg_logprob": -0.25528590819414926, "compression_ratio": 1.3167701863354038, "no_speech_prob": 5.989260216665571e-07}, {"id": 1023, "seek": 490530, "start": 4923.9800000000005, "end": 4927.74, "text": " Data matrix here, which is of length 10,000", "tokens": [11888, 8141, 510, 11, 597, 307, 295, 4641, 1266, 11, 1360], "temperature": 0.0, "avg_logprob": -0.25528590819414926, "compression_ratio": 1.3167701863354038, "no_speech_prob": 5.989260216665571e-07}, {"id": 1024, "seek": 490530, "start": 4929.18, "end": 4930.9400000000005, "text": " by", "tokens": [538], "temperature": 0.0, "avg_logprob": -0.25528590819414926, "compression_ratio": 1.3167701863354038, "no_speech_prob": 5.989260216665571e-07}, {"id": 1025, "seek": 490530, "start": 4930.9400000000005, "end": 4932.9400000000005, "text": " 28 times 28", "tokens": [7562, 1413, 7562], "temperature": 0.0, "avg_logprob": -0.25528590819414926, "compression_ratio": 1.3167701863354038, "no_speech_prob": 5.989260216665571e-07}, {"id": 1026, "seek": 493294, "start": 4932.94, "end": 4936.74, "text": " All right, and then we've got our weight matrix", "tokens": [1057, 558, 11, 293, 550, 321, 600, 658, 527, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.24823449869624903, "compression_ratio": 1.3642857142857143, "no_speech_prob": 3.6119708965998143e-06}, {"id": 1027, "seek": 493294, "start": 4939.5, "end": 4941.58, "text": " Which is 28 by 28", "tokens": [3013, 307, 7562, 538, 7562], "temperature": 0.0, "avg_logprob": -0.24823449869624903, "compression_ratio": 1.3642857142857143, "no_speech_prob": 3.6119708965998143e-06}, {"id": 1028, "seek": 493294, "start": 4943.7, "end": 4947.339999999999, "text": " Rows by 10, so if we multiply those together", "tokens": [497, 1509, 538, 1266, 11, 370, 498, 321, 12972, 729, 1214], "temperature": 0.0, "avg_logprob": -0.24823449869624903, "compression_ratio": 1.3642857142857143, "no_speech_prob": 3.6119708965998143e-06}, {"id": 1029, "seek": 493294, "start": 4950.66, "end": 4953.0199999999995, "text": " We get something of size 10,000", "tokens": [492, 483, 746, 295, 2744, 1266, 11, 1360], "temperature": 0.0, "avg_logprob": -0.24823449869624903, "compression_ratio": 1.3642857142857143, "no_speech_prob": 3.6119708965998143e-06}, {"id": 1030, "seek": 493294, "start": 4954.339999999999, "end": 4956.219999999999, "text": " by 10", "tokens": [538, 1266], "temperature": 0.0, "avg_logprob": -0.24823449869624903, "compression_ratio": 1.3642857142857143, "no_speech_prob": 3.6119708965998143e-06}, {"id": 1031, "seek": 495622, "start": 4956.22, "end": 4963.5, "text": " Right and then we want to add on our bias", "tokens": [1779, 293, 550, 321, 528, 281, 909, 322, 527, 12577], "temperature": 0.0, "avg_logprob": -0.2482189987645005, "compression_ratio": 1.635135135135135, "no_speech_prob": 1.4144717397357454e-06}, {"id": 1032, "seek": 495622, "start": 4968.14, "end": 4970.46, "text": " Sorry wrong way around add on our bias", "tokens": [4919, 2085, 636, 926, 909, 322, 527, 12577], "temperature": 0.0, "avg_logprob": -0.2482189987645005, "compression_ratio": 1.635135135135135, "no_speech_prob": 1.4144717397357454e-06}, {"id": 1033, "seek": 495622, "start": 4973.54, "end": 4980.7, "text": " Okay like so and so when we add on and we're going to learn a lot more about this later, but when we add on a", "tokens": [1033, 411, 370, 293, 370, 562, 321, 909, 322, 293, 321, 434, 516, 281, 1466, 257, 688, 544, 466, 341, 1780, 11, 457, 562, 321, 909, 322, 257], "temperature": 0.0, "avg_logprob": -0.2482189987645005, "compression_ratio": 1.635135135135135, "no_speech_prob": 1.4144717397357454e-06}, {"id": 1034, "seek": 498070, "start": 4980.7, "end": 4987.34, "text": " Vector like this it basically is going to get added to every row", "tokens": [691, 20814, 411, 341, 309, 1936, 307, 516, 281, 483, 3869, 281, 633, 5386], "temperature": 0.0, "avg_logprob": -0.12378847444212282, "compression_ratio": 1.6875, "no_speech_prob": 3.393135330043151e-06}, {"id": 1035, "seek": 498070, "start": 4989.66, "end": 4992.34, "text": " Okay, so the bias is going to get added to every row", "tokens": [1033, 11, 370, 264, 12577, 307, 516, 281, 483, 3869, 281, 633, 5386], "temperature": 0.0, "avg_logprob": -0.12378847444212282, "compression_ratio": 1.6875, "no_speech_prob": 3.393135330043151e-06}, {"id": 1036, "seek": 498070, "start": 4993.42, "end": 4999.86, "text": " So we first of all define those and so to define them we've created a tiny little function called get weights", "tokens": [407, 321, 700, 295, 439, 6964, 729, 293, 370, 281, 6964, 552, 321, 600, 2942, 257, 5870, 707, 2445, 1219, 483, 17443], "temperature": 0.0, "avg_logprob": -0.12378847444212282, "compression_ratio": 1.6875, "no_speech_prob": 3.393135330043151e-06}, {"id": 1037, "seek": 498070, "start": 5000.26, "end": 5002.26, "text": " Which is over here, right?", "tokens": [3013, 307, 670, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12378847444212282, "compression_ratio": 1.6875, "no_speech_prob": 3.393135330043151e-06}, {"id": 1038, "seek": 498070, "start": 5002.42, "end": 5005.82, "text": " Which basically just creates some normally distributed random numbers", "tokens": [3013, 1936, 445, 7829, 512, 5646, 12631, 4974, 3547], "temperature": 0.0, "avg_logprob": -0.12378847444212282, "compression_ratio": 1.6875, "no_speech_prob": 3.393135330043151e-06}, {"id": 1039, "seek": 500582, "start": 5005.82, "end": 5011.82, "text": " So torch dot Rand n returns a tensor filled with random numbers from a normal distribution", "tokens": [407, 27822, 5893, 23614, 297, 11247, 257, 40863, 6412, 365, 4974, 3547, 490, 257, 2710, 7316], "temperature": 0.0, "avg_logprob": -0.1901586659749349, "compression_ratio": 1.53, "no_speech_prob": 2.8573097097250866e-06}, {"id": 1040, "seek": 500582, "start": 5013.299999999999, "end": 5015.299999999999, "text": " We have to be a bit careful though", "tokens": [492, 362, 281, 312, 257, 857, 5026, 1673], "temperature": 0.0, "avg_logprob": -0.1901586659749349, "compression_ratio": 1.53, "no_speech_prob": 2.8573097097250866e-06}, {"id": 1041, "seek": 500582, "start": 5015.54, "end": 5020.099999999999, "text": " when we do deep learning like when we add more linear layers later, I", "tokens": [562, 321, 360, 2452, 2539, 411, 562, 321, 909, 544, 8213, 7914, 1780, 11, 286], "temperature": 0.0, "avg_logprob": -0.1901586659749349, "compression_ratio": 1.53, "no_speech_prob": 2.8573097097250866e-06}, {"id": 1042, "seek": 500582, "start": 5021.179999999999, "end": 5023.179999999999, "text": " imagine if we have a", "tokens": [3811, 498, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.1901586659749349, "compression_ratio": 1.53, "no_speech_prob": 2.8573097097250866e-06}, {"id": 1043, "seek": 500582, "start": 5023.9, "end": 5030.599999999999, "text": " Matrix which on average tends to increase the size of the inputs we give to it if we then", "tokens": [36274, 597, 322, 4274, 12258, 281, 3488, 264, 2744, 295, 264, 15743, 321, 976, 281, 309, 498, 321, 550], "temperature": 0.0, "avg_logprob": -0.1901586659749349, "compression_ratio": 1.53, "no_speech_prob": 2.8573097097250866e-06}, {"id": 1044, "seek": 503060, "start": 5030.6, "end": 5038.120000000001, "text": " Multiply by lots of matrices of that size. It's going to make the numbers bigger and bigger and bigger like exponentially bigger", "tokens": [31150, 356, 538, 3195, 295, 32284, 295, 300, 2744, 13, 467, 311, 516, 281, 652, 264, 3547, 3801, 293, 3801, 293, 3801, 411, 37330, 3801], "temperature": 0.0, "avg_logprob": -0.1746723797856545, "compression_ratio": 2.0325581395348835, "no_speech_prob": 2.4439848402835196e-06}, {"id": 1045, "seek": 503060, "start": 5038.72, "end": 5040.72, "text": " Or what if it made them a bit smaller?", "tokens": [1610, 437, 498, 309, 1027, 552, 257, 857, 4356, 30], "temperature": 0.0, "avg_logprob": -0.1746723797856545, "compression_ratio": 2.0325581395348835, "no_speech_prob": 2.4439848402835196e-06}, {"id": 1046, "seek": 503060, "start": 5040.72, "end": 5044.120000000001, "text": " It's going to make them smaller and smaller and smaller exponentially smaller", "tokens": [467, 311, 516, 281, 652, 552, 4356, 293, 4356, 293, 4356, 37330, 4356], "temperature": 0.0, "avg_logprob": -0.1746723797856545, "compression_ratio": 2.0325581395348835, "no_speech_prob": 2.4439848402835196e-06}, {"id": 1047, "seek": 503060, "start": 5044.52, "end": 5048.08, "text": " So like because a deep network applies lots of linear layers", "tokens": [407, 411, 570, 257, 2452, 3209, 13165, 3195, 295, 8213, 7914], "temperature": 0.0, "avg_logprob": -0.1746723797856545, "compression_ratio": 2.0325581395348835, "no_speech_prob": 2.4439848402835196e-06}, {"id": 1048, "seek": 503060, "start": 5048.68, "end": 5056.8, "text": " If on average they result in things a bit bigger than they started with or a bit smaller than they started with it's going to like", "tokens": [759, 322, 4274, 436, 1874, 294, 721, 257, 857, 3801, 813, 436, 1409, 365, 420, 257, 857, 4356, 813, 436, 1409, 365, 309, 311, 516, 281, 411], "temperature": 0.0, "avg_logprob": -0.1746723797856545, "compression_ratio": 2.0325581395348835, "no_speech_prob": 2.4439848402835196e-06}, {"id": 1049, "seek": 505680, "start": 5056.8, "end": 5060.360000000001, "text": " exponentially multiply that difference", "tokens": [37330, 12972, 300, 2649], "temperature": 0.0, "avg_logprob": -0.18427288683154916, "compression_ratio": 1.6787564766839378, "no_speech_prob": 2.8736826607200783e-07}, {"id": 1050, "seek": 505680, "start": 5060.56, "end": 5065.2, "text": " So we need to make sure that the weight matrix is of an appropriate size", "tokens": [407, 321, 643, 281, 652, 988, 300, 264, 3364, 8141, 307, 295, 364, 6854, 2744], "temperature": 0.0, "avg_logprob": -0.18427288683154916, "compression_ratio": 1.6787564766839378, "no_speech_prob": 2.8736826607200783e-07}, {"id": 1051, "seek": 505680, "start": 5065.96, "end": 5067.56, "text": " that the", "tokens": [300, 264], "temperature": 0.0, "avg_logprob": -0.18427288683154916, "compression_ratio": 1.6787564766839378, "no_speech_prob": 2.8736826607200783e-07}, {"id": 1052, "seek": 505680, "start": 5067.56, "end": 5071.4400000000005, "text": " Inputs to it the kind of the mean of the inputs basically is not going to change", "tokens": [682, 2582, 82, 281, 309, 264, 733, 295, 264, 914, 295, 264, 15743, 1936, 307, 406, 516, 281, 1319], "temperature": 0.0, "avg_logprob": -0.18427288683154916, "compression_ratio": 1.6787564766839378, "no_speech_prob": 2.8736826607200783e-07}, {"id": 1053, "seek": 505680, "start": 5072.56, "end": 5077.52, "text": " so it turns out that if you use normally distributed random numbers and", "tokens": [370, 309, 4523, 484, 300, 498, 291, 764, 5646, 12631, 4974, 3547, 293], "temperature": 0.0, "avg_logprob": -0.18427288683154916, "compression_ratio": 1.6787564766839378, "no_speech_prob": 2.8736826607200783e-07}, {"id": 1054, "seek": 505680, "start": 5078.360000000001, "end": 5079.72, "text": " divided by", "tokens": [6666, 538], "temperature": 0.0, "avg_logprob": -0.18427288683154916, "compression_ratio": 1.6787564766839378, "no_speech_prob": 2.8736826607200783e-07}, {"id": 1055, "seek": 505680, "start": 5079.72, "end": 5082.360000000001, "text": " The number of rows in the weight matrix", "tokens": [440, 1230, 295, 13241, 294, 264, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.18427288683154916, "compression_ratio": 1.6787564766839378, "no_speech_prob": 2.8736826607200783e-07}, {"id": 1056, "seek": 508236, "start": 5082.36, "end": 5087.44, "text": " It turns out that particular random initialization", "tokens": [467, 4523, 484, 300, 1729, 4974, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.17070846931607114, "compression_ratio": 1.7755102040816326, "no_speech_prob": 3.2377508887293516e-06}, {"id": 1057, "seek": 508236, "start": 5087.799999999999, "end": 5091.0, "text": " Keeps your numbers at about the right scale, right?", "tokens": [5527, 82, 428, 3547, 412, 466, 264, 558, 4373, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17070846931607114, "compression_ratio": 1.7755102040816326, "no_speech_prob": 3.2377508887293516e-06}, {"id": 1058, "seek": 508236, "start": 5091.0, "end": 5094.5599999999995, "text": " So this idea that like if you've done linear algebra", "tokens": [407, 341, 1558, 300, 411, 498, 291, 600, 1096, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.17070846931607114, "compression_ratio": 1.7755102040816326, "no_speech_prob": 3.2377508887293516e-06}, {"id": 1059, "seek": 508236, "start": 5094.92, "end": 5100.719999999999, "text": " Basically if the eigenvalue the first eigenvalue is like bigger than one or smaller than one", "tokens": [8537, 498, 264, 10446, 29155, 264, 700, 10446, 29155, 307, 411, 3801, 813, 472, 420, 4356, 813, 472], "temperature": 0.0, "avg_logprob": -0.17070846931607114, "compression_ratio": 1.7755102040816326, "no_speech_prob": 3.2377508887293516e-06}, {"id": 1060, "seek": 508236, "start": 5100.799999999999, "end": 5108.099999999999, "text": " It's going to cause the gradients to like get bigger and bigger or smaller and smaller. That's called gradient explosion, right? So", "tokens": [467, 311, 516, 281, 3082, 264, 2771, 2448, 281, 411, 483, 3801, 293, 3801, 420, 4356, 293, 4356, 13, 663, 311, 1219, 16235, 15673, 11, 558, 30, 407], "temperature": 0.0, "avg_logprob": -0.17070846931607114, "compression_ratio": 1.7755102040816326, "no_speech_prob": 3.2377508887293516e-06}, {"id": 1061, "seek": 508236, "start": 5108.88, "end": 5110.799999999999, "text": " We'll talk more about this in the deep learning course", "tokens": [492, 603, 751, 544, 466, 341, 294, 264, 2452, 2539, 1164], "temperature": 0.0, "avg_logprob": -0.17070846931607114, "compression_ratio": 1.7755102040816326, "no_speech_prob": 3.2377508887293516e-06}, {"id": 1062, "seek": 511080, "start": 5110.8, "end": 5114.96, "text": " but if you're interested you can look up timing her initialization and", "tokens": [457, 498, 291, 434, 3102, 291, 393, 574, 493, 10822, 720, 5883, 2144, 293], "temperature": 0.0, "avg_logprob": -0.2029099918547131, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.57113014060451e-07}, {"id": 1063, "seek": 511080, "start": 5116.52, "end": 5118.320000000001, "text": " Read all about", "tokens": [17604, 439, 466], "temperature": 0.0, "avg_logprob": -0.2029099918547131, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.57113014060451e-07}, {"id": 1064, "seek": 511080, "start": 5118.320000000001, "end": 5124.400000000001, "text": " This concept right but for now, you know, it's probably just enough to know that if you use this", "tokens": [639, 3410, 558, 457, 337, 586, 11, 291, 458, 11, 309, 311, 1391, 445, 1547, 281, 458, 300, 498, 291, 764, 341], "temperature": 0.0, "avg_logprob": -0.2029099918547131, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.57113014060451e-07}, {"id": 1065, "seek": 511080, "start": 5125.24, "end": 5129.4400000000005, "text": " Type of random number generation you're going to get random numbers that are", "tokens": [15576, 295, 4974, 1230, 5125, 291, 434, 516, 281, 483, 4974, 3547, 300, 366], "temperature": 0.0, "avg_logprob": -0.2029099918547131, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.57113014060451e-07}, {"id": 1066, "seek": 511080, "start": 5130.0, "end": 5135.3, "text": " Nicely behaved you're going to start out with an input which is mean zero standard deviation one", "tokens": [14776, 736, 48249, 291, 434, 516, 281, 722, 484, 365, 364, 4846, 597, 307, 914, 4018, 3832, 25163, 472], "temperature": 0.0, "avg_logprob": -0.2029099918547131, "compression_ratio": 1.640552995391705, "no_speech_prob": 7.57113014060451e-07}, {"id": 1067, "seek": 513530, "start": 5135.3, "end": 5141.9800000000005, "text": " Once you put it through this set of random numbers, you'll still have something that's about mean zero standard deviation one", "tokens": [3443, 291, 829, 309, 807, 341, 992, 295, 4974, 3547, 11, 291, 603, 920, 362, 746, 300, 311, 466, 914, 4018, 3832, 25163, 472], "temperature": 0.0, "avg_logprob": -0.16047569801067485, "compression_ratio": 1.6776556776556777, "no_speech_prob": 2.482471700204769e-06}, {"id": 1068, "seek": 513530, "start": 5141.9800000000005, "end": 5143.9800000000005, "text": " That's basically the goal. Okay?", "tokens": [663, 311, 1936, 264, 3387, 13, 1033, 30], "temperature": 0.0, "avg_logprob": -0.16047569801067485, "compression_ratio": 1.6776556776556777, "no_speech_prob": 2.482471700204769e-06}, {"id": 1069, "seek": 513530, "start": 5144.78, "end": 5149.3, "text": " One nice thing about pie torch is that you can play with this stuff, right?", "tokens": [1485, 1481, 551, 466, 1730, 27822, 307, 300, 291, 393, 862, 365, 341, 1507, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16047569801067485, "compression_ratio": 1.6776556776556777, "no_speech_prob": 2.482471700204769e-06}, {"id": 1070, "seek": 513530, "start": 5149.3, "end": 5155.12, "text": " So torch dot random like try it out like every time you see a function being used run it", "tokens": [407, 27822, 5893, 4974, 411, 853, 309, 484, 411, 633, 565, 291, 536, 257, 2445, 885, 1143, 1190, 309], "temperature": 0.0, "avg_logprob": -0.16047569801067485, "compression_ratio": 1.6776556776556777, "no_speech_prob": 2.482471700204769e-06}, {"id": 1071, "seek": 513530, "start": 5155.5, "end": 5159.92, "text": " All right, and take a look and so you'll see it looks a lot like numpy", "tokens": [1057, 558, 11, 293, 747, 257, 574, 293, 370, 291, 603, 536, 309, 1542, 257, 688, 411, 1031, 8200], "temperature": 0.0, "avg_logprob": -0.16047569801067485, "compression_ratio": 1.6776556776556777, "no_speech_prob": 2.482471700204769e-06}, {"id": 1072, "seek": 515992, "start": 5159.92, "end": 5166.52, "text": " Right, but it doesn't return a numpy array. It returns a tensor and in fact", "tokens": [1779, 11, 457, 309, 1177, 380, 2736, 257, 1031, 8200, 10225, 13, 467, 11247, 257, 40863, 293, 294, 1186], "temperature": 0.0, "avg_logprob": -0.2196419872815096, "compression_ratio": 1.5025641025641026, "no_speech_prob": 4.6378440856642555e-06}, {"id": 1073, "seek": 515992, "start": 5168.56, "end": 5175.2, "text": " Now I'm GPU programming, okay, like put docuda and now it's doing it on the GPU so like I", "tokens": [823, 286, 478, 18407, 9410, 11, 1392, 11, 411, 829, 3211, 11152, 293, 586, 309, 311, 884, 309, 322, 264, 18407, 370, 411, 286], "temperature": 0.0, "avg_logprob": -0.2196419872815096, "compression_ratio": 1.5025641025641026, "no_speech_prob": 4.6378440856642555e-06}, {"id": 1074, "seek": 515992, "start": 5178.04, "end": 5186.26, "text": " Just multiplied that matrix by three very quickly on the GPU, right? So that's how we do GPU programming with pie torch, right?", "tokens": [1449, 17207, 300, 8141, 538, 1045, 588, 2661, 322, 264, 18407, 11, 558, 30, 407, 300, 311, 577, 321, 360, 18407, 9410, 365, 1730, 27822, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2196419872815096, "compression_ratio": 1.5025641025641026, "no_speech_prob": 4.6378440856642555e-06}, {"id": 1075, "seek": 518626, "start": 5186.26, "end": 5188.26, "text": " So", "tokens": [407], "temperature": 0.0, "avg_logprob": -0.22277034857334235, "compression_ratio": 1.5247524752475248, "no_speech_prob": 2.1568102965829894e-06}, {"id": 1076, "seek": 518626, "start": 5188.900000000001, "end": 5190.9800000000005, "text": " This this is our weight matrix", "tokens": [639, 341, 307, 527, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.22277034857334235, "compression_ratio": 1.5247524752475248, "no_speech_prob": 2.1568102965829894e-06}, {"id": 1077, "seek": 518626, "start": 5190.9800000000005, "end": 5197.56, "text": " So we create as I said we create 128 by 28 by 10 one is just rank one of 10 for the biases", "tokens": [407, 321, 1884, 382, 286, 848, 321, 1884, 29810, 538, 7562, 538, 1266, 472, 307, 445, 6181, 472, 295, 1266, 337, 264, 32152], "temperature": 0.0, "avg_logprob": -0.22277034857334235, "compression_ratio": 1.5247524752475248, "no_speech_prob": 2.1568102965829894e-06}, {"id": 1078, "seek": 518626, "start": 5198.7, "end": 5205.46, "text": " We have to make them a parameter. This is basically telling pie torch which things to update when it does SGD", "tokens": [492, 362, 281, 652, 552, 257, 13075, 13, 639, 307, 1936, 3585, 1730, 27822, 597, 721, 281, 5623, 562, 309, 775, 34520, 35], "temperature": 0.0, "avg_logprob": -0.22277034857334235, "compression_ratio": 1.5247524752475248, "no_speech_prob": 2.1568102965829894e-06}, {"id": 1079, "seek": 518626, "start": 5205.9400000000005, "end": 5211.280000000001, "text": " That's very minor technical detail. So having created the weight matrices", "tokens": [663, 311, 588, 6696, 6191, 2607, 13, 407, 1419, 2942, 264, 3364, 32284], "temperature": 0.0, "avg_logprob": -0.22277034857334235, "compression_ratio": 1.5247524752475248, "no_speech_prob": 2.1568102965829894e-06}, {"id": 1080, "seek": 521128, "start": 5211.28, "end": 5217.34, "text": " We then define a special method with the name forward. This is a special method", "tokens": [492, 550, 6964, 257, 2121, 3170, 365, 264, 1315, 2128, 13, 639, 307, 257, 2121, 3170], "temperature": 0.0, "avg_logprob": -0.16432058434737357, "compression_ratio": 1.8177570093457944, "no_speech_prob": 2.561271003287402e-06}, {"id": 1081, "seek": 521128, "start": 5218.0, "end": 5224.219999999999, "text": " The word the name forward has a special meaning in pie torch a method called forward in pie torch", "tokens": [440, 1349, 264, 1315, 2128, 575, 257, 2121, 3620, 294, 1730, 27822, 257, 3170, 1219, 2128, 294, 1730, 27822], "temperature": 0.0, "avg_logprob": -0.16432058434737357, "compression_ratio": 1.8177570093457944, "no_speech_prob": 2.561271003287402e-06}, {"id": 1082, "seek": 521128, "start": 5224.219999999999, "end": 5228.38, "text": " Is the name of the method that will get called when your layer is calculated?", "tokens": [1119, 264, 1315, 295, 264, 3170, 300, 486, 483, 1219, 562, 428, 4583, 307, 15598, 30], "temperature": 0.0, "avg_logprob": -0.16432058434737357, "compression_ratio": 1.8177570093457944, "no_speech_prob": 2.561271003287402e-06}, {"id": 1083, "seek": 521128, "start": 5229.099999999999, "end": 5234.0599999999995, "text": " Okay, so if you create a neural net or a layer you have to define", "tokens": [1033, 11, 370, 498, 291, 1884, 257, 18161, 2533, 420, 257, 4583, 291, 362, 281, 6964], "temperature": 0.0, "avg_logprob": -0.16432058434737357, "compression_ratio": 1.8177570093457944, "no_speech_prob": 2.561271003287402e-06}, {"id": 1084, "seek": 521128, "start": 5234.66, "end": 5236.66, "text": " Forward and it's going to get past", "tokens": [35524, 293, 309, 311, 516, 281, 483, 1791], "temperature": 0.0, "avg_logprob": -0.16432058434737357, "compression_ratio": 1.8177570093457944, "no_speech_prob": 2.561271003287402e-06}, {"id": 1085, "seek": 521128, "start": 5237.5, "end": 5239.5, "text": " the data from the previous layer", "tokens": [264, 1412, 490, 264, 3894, 4583], "temperature": 0.0, "avg_logprob": -0.16432058434737357, "compression_ratio": 1.8177570093457944, "no_speech_prob": 2.561271003287402e-06}, {"id": 1086, "seek": 523950, "start": 5239.5, "end": 5245.26, "text": " So our definition is to do a matrix multiplication of our input data", "tokens": [407, 527, 7123, 307, 281, 360, 257, 8141, 27290, 295, 527, 4846, 1412], "temperature": 0.0, "avg_logprob": -0.25756407437259204, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.7266131585056428e-06}, {"id": 1087, "seek": 523950, "start": 5246.1, "end": 5248.1, "text": " times our weights and", "tokens": [1413, 527, 17443, 293], "temperature": 0.0, "avg_logprob": -0.25756407437259204, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.7266131585056428e-06}, {"id": 1088, "seek": 523950, "start": 5248.1, "end": 5250.1, "text": " add on the biases", "tokens": [909, 322, 264, 32152], "temperature": 0.0, "avg_logprob": -0.25756407437259204, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.7266131585056428e-06}, {"id": 1089, "seek": 523950, "start": 5250.22, "end": 5256.42, "text": " So that's it. That's what happened earlier on when we said n n dot linear it created this", "tokens": [407, 300, 311, 309, 13, 663, 311, 437, 2011, 3071, 322, 562, 321, 848, 297, 297, 5893, 8213, 309, 2942, 341], "temperature": 0.0, "avg_logprob": -0.25756407437259204, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.7266131585056428e-06}, {"id": 1090, "seek": 523950, "start": 5257.34, "end": 5259.34, "text": " This thing for us", "tokens": [639, 551, 337, 505], "temperature": 0.0, "avg_logprob": -0.25756407437259204, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.7266131585056428e-06}, {"id": 1091, "seek": 523950, "start": 5259.46, "end": 5261.42, "text": " Okay", "tokens": [1033], "temperature": 0.0, "avg_logprob": -0.25756407437259204, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.7266131585056428e-06}, {"id": 1092, "seek": 523950, "start": 5261.42, "end": 5264.1, "text": " now unfortunately, though, we're not getting a", "tokens": [586, 7015, 11, 1673, 11, 321, 434, 406, 1242, 257], "temperature": 0.0, "avg_logprob": -0.25756407437259204, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.7266131585056428e-06}, {"id": 1093, "seek": 526410, "start": 5264.1, "end": 5270.26, "text": " 28 by 28 long vector we're getting a 28 row by 28 column matrix", "tokens": [7562, 538, 7562, 938, 8062, 321, 434, 1242, 257, 7562, 5386, 538, 7562, 7738, 8141], "temperature": 0.0, "avg_logprob": -0.19690264533547794, "compression_ratio": 1.5754716981132075, "no_speech_prob": 2.684188530110987e-06}, {"id": 1094, "seek": 526410, "start": 5270.54, "end": 5272.54, "text": " So we have to flatten it", "tokens": [407, 321, 362, 281, 24183, 309], "temperature": 0.0, "avg_logprob": -0.19690264533547794, "compression_ratio": 1.5754716981132075, "no_speech_prob": 2.684188530110987e-06}, {"id": 1095, "seek": 526410, "start": 5273.02, "end": 5276.72, "text": " Unfortunately in torch pie torch they tend to rename things", "tokens": [8590, 294, 27822, 1730, 27822, 436, 3928, 281, 36741, 721], "temperature": 0.0, "avg_logprob": -0.19690264533547794, "compression_ratio": 1.5754716981132075, "no_speech_prob": 2.684188530110987e-06}, {"id": 1096, "seek": 526410, "start": 5277.660000000001, "end": 5284.34, "text": " They they spell resize reshape. They spell it view. Okay, so view means reshape", "tokens": [814, 436, 9827, 50069, 725, 42406, 13, 814, 9827, 309, 1910, 13, 1033, 11, 370, 1910, 1355, 725, 42406], "temperature": 0.0, "avg_logprob": -0.19690264533547794, "compression_ratio": 1.5754716981132075, "no_speech_prob": 2.684188530110987e-06}, {"id": 1097, "seek": 526410, "start": 5284.5, "end": 5290.46, "text": " So you can see here we end up with something where the number of images we're going to leave the same and", "tokens": [407, 291, 393, 536, 510, 321, 917, 493, 365, 746, 689, 264, 1230, 295, 5267, 321, 434, 516, 281, 1856, 264, 912, 293], "temperature": 0.0, "avg_logprob": -0.19690264533547794, "compression_ratio": 1.5754716981132075, "no_speech_prob": 2.684188530110987e-06}, {"id": 1098, "seek": 529046, "start": 5290.46, "end": 5295.1, "text": " Then we're going to replace row by column with a single", "tokens": [1396, 321, 434, 516, 281, 7406, 5386, 538, 7738, 365, 257, 2167], "temperature": 0.0, "avg_logprob": -0.1711692174275716, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.067700964085816e-06}, {"id": 1099, "seek": 529046, "start": 5295.54, "end": 5300.74, "text": " Axis again negative one meaning as long as required. Okay, so this is how we", "tokens": [20118, 271, 797, 3671, 472, 3620, 382, 938, 382, 4739, 13, 1033, 11, 370, 341, 307, 577, 321], "temperature": 0.0, "avg_logprob": -0.1711692174275716, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.067700964085816e-06}, {"id": 1100, "seek": 529046, "start": 5301.66, "end": 5307.62, "text": " Flatten something using pie torch. So we flatten it do a matrix multiply and then finally", "tokens": [3235, 32733, 746, 1228, 1730, 27822, 13, 407, 321, 24183, 309, 360, 257, 8141, 12972, 293, 550, 2721], "temperature": 0.0, "avg_logprob": -0.1711692174275716, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.067700964085816e-06}, {"id": 1101, "seek": 529046, "start": 5308.22, "end": 5313.22, "text": " We do a soft max. So soft max is the activation function we use", "tokens": [492, 360, 257, 2787, 11469, 13, 407, 2787, 11469, 307, 264, 24433, 2445, 321, 764], "temperature": 0.0, "avg_logprob": -0.1711692174275716, "compression_ratio": 1.5416666666666667, "no_speech_prob": 1.067700964085816e-06}, {"id": 1102, "seek": 531322, "start": 5313.22, "end": 5319.740000000001, "text": " If you look in the deep learning repo, you'll find something called entropy example", "tokens": [759, 291, 574, 294, 264, 2452, 2539, 49040, 11, 291, 603, 915, 746, 1219, 30867, 1365], "temperature": 0.0, "avg_logprob": -0.18917959928512573, "compression_ratio": 1.6910112359550562, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1103, "seek": 531322, "start": 5320.820000000001, "end": 5327.06, "text": " Where you'll see an example of soft max, but a soft max simply takes the outputs from our", "tokens": [2305, 291, 603, 536, 364, 1365, 295, 2787, 11469, 11, 457, 257, 2787, 11469, 2935, 2516, 264, 23930, 490, 527], "temperature": 0.0, "avg_logprob": -0.18917959928512573, "compression_ratio": 1.6910112359550562, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1104, "seek": 531322, "start": 5327.66, "end": 5328.54, "text": " final layer", "tokens": [2572, 4583], "temperature": 0.0, "avg_logprob": -0.18917959928512573, "compression_ratio": 1.6910112359550562, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1105, "seek": 531322, "start": 5328.54, "end": 5331.9400000000005, "text": " so we get our outputs from our from our linear layer and", "tokens": [370, 321, 483, 527, 23930, 490, 527, 490, 527, 8213, 4583, 293], "temperature": 0.0, "avg_logprob": -0.18917959928512573, "compression_ratio": 1.6910112359550562, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1106, "seek": 531322, "start": 5332.38, "end": 5334.58, "text": " What we do is we go e to the power of?", "tokens": [708, 321, 360, 307, 321, 352, 308, 281, 264, 1347, 295, 30], "temperature": 0.0, "avg_logprob": -0.18917959928512573, "compression_ratio": 1.6910112359550562, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1107, "seek": 531322, "start": 5335.58, "end": 5337.58, "text": " for each output and", "tokens": [337, 1184, 5598, 293], "temperature": 0.0, "avg_logprob": -0.18917959928512573, "compression_ratio": 1.6910112359550562, "no_speech_prob": 3.1875533750280738e-06}, {"id": 1108, "seek": 533758, "start": 5337.58, "end": 5343.9, "text": " Then we take that number and we divide by the sum of the e to the power ofs", "tokens": [1396, 321, 747, 300, 1230, 293, 321, 9845, 538, 264, 2408, 295, 264, 308, 281, 264, 1347, 295, 82], "temperature": 0.0, "avg_logprob": -0.14936245055425734, "compression_ratio": 1.6424870466321244, "no_speech_prob": 4.0294403333973605e-06}, {"id": 1109, "seek": 533758, "start": 5344.5, "end": 5350.94, "text": " That's called soft max. Why do we do that? Well because we're dividing this by the sum", "tokens": [663, 311, 1219, 2787, 11469, 13, 1545, 360, 321, 360, 300, 30, 1042, 570, 321, 434, 26764, 341, 538, 264, 2408], "temperature": 0.0, "avg_logprob": -0.14936245055425734, "compression_ratio": 1.6424870466321244, "no_speech_prob": 4.0294403333973605e-06}, {"id": 1110, "seek": 533758, "start": 5351.7, "end": 5356.78, "text": " That means that the sum of those itself must add to one, right?", "tokens": [663, 1355, 300, 264, 2408, 295, 729, 2564, 1633, 909, 281, 472, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14936245055425734, "compression_ratio": 1.6424870466321244, "no_speech_prob": 4.0294403333973605e-06}, {"id": 1111, "seek": 533758, "start": 5356.78, "end": 5362.34, "text": " And that's what we want. We want the probabilities of all the possible outcomes add to one", "tokens": [400, 300, 311, 437, 321, 528, 13, 492, 528, 264, 33783, 295, 439, 264, 1944, 10070, 909, 281, 472], "temperature": 0.0, "avg_logprob": -0.14936245055425734, "compression_ratio": 1.6424870466321244, "no_speech_prob": 4.0294403333973605e-06}, {"id": 1112, "seek": 536234, "start": 5362.34, "end": 5369.62, "text": " Furthermore because we're using a to the power of that means we know that every one of these is between 0 and 1 and", "tokens": [23999, 570, 321, 434, 1228, 257, 281, 264, 1347, 295, 300, 1355, 321, 458, 300, 633, 472, 295, 613, 307, 1296, 1958, 293, 502, 293], "temperature": 0.0, "avg_logprob": -0.18402852915754223, "compression_ratio": 1.8247863247863247, "no_speech_prob": 2.260318069602363e-06}, {"id": 1113, "seek": 536234, "start": 5370.38, "end": 5372.9400000000005, "text": " Probabilities we know should be between 0 and 1", "tokens": [8736, 6167, 321, 458, 820, 312, 1296, 1958, 293, 502], "temperature": 0.0, "avg_logprob": -0.18402852915754223, "compression_ratio": 1.8247863247863247, "no_speech_prob": 2.260318069602363e-06}, {"id": 1114, "seek": 536234, "start": 5374.18, "end": 5381.82, "text": " And then finally because we're using a to the power of it tends to mean that slightly bigger values", "tokens": [400, 550, 2721, 570, 321, 434, 1228, 257, 281, 264, 1347, 295, 309, 12258, 281, 914, 300, 4748, 3801, 4190], "temperature": 0.0, "avg_logprob": -0.18402852915754223, "compression_ratio": 1.8247863247863247, "no_speech_prob": 2.260318069602363e-06}, {"id": 1115, "seek": 536234, "start": 5382.54, "end": 5386.42, "text": " In the input turn into much bigger values in the output", "tokens": [682, 264, 4846, 1261, 666, 709, 3801, 4190, 294, 264, 5598], "temperature": 0.0, "avg_logprob": -0.18402852915754223, "compression_ratio": 1.8247863247863247, "no_speech_prob": 2.260318069602363e-06}, {"id": 1116, "seek": 538642, "start": 5386.42, "end": 5393.7, "text": " So you'll see generally speaking my soft max there was going to be one big number and lots of small numbers and that's what we want", "tokens": [407, 291, 603, 536, 5101, 4124, 452, 2787, 11469, 456, 390, 516, 281, 312, 472, 955, 1230, 293, 3195, 295, 1359, 3547, 293, 300, 311, 437, 321, 528], "temperature": 0.0, "avg_logprob": -0.17107240970318133, "compression_ratio": 1.8015873015873016, "no_speech_prob": 6.893596946611069e-07}, {"id": 1117, "seek": 538642, "start": 5393.7, "end": 5396.62, "text": " Right because we know that the output is one hot encoded", "tokens": [1779, 570, 321, 458, 300, 264, 5598, 307, 472, 2368, 2058, 12340], "temperature": 0.0, "avg_logprob": -0.17107240970318133, "compression_ratio": 1.8015873015873016, "no_speech_prob": 6.893596946611069e-07}, {"id": 1118, "seek": 538642, "start": 5397.7, "end": 5399.82, "text": " So in other words a soft max", "tokens": [407, 294, 661, 2283, 257, 2787, 11469], "temperature": 0.0, "avg_logprob": -0.17107240970318133, "compression_ratio": 1.8015873015873016, "no_speech_prob": 6.893596946611069e-07}, {"id": 1119, "seek": 538642, "start": 5400.78, "end": 5408.1, "text": " Activation function the soft max non-linearity is something that returns things that behave like probabilities and", "tokens": [28550, 399, 2445, 264, 2787, 11469, 2107, 12, 1889, 17409, 307, 746, 300, 11247, 721, 300, 15158, 411, 33783, 293], "temperature": 0.0, "avg_logprob": -0.17107240970318133, "compression_ratio": 1.8015873015873016, "no_speech_prob": 6.893596946611069e-07}, {"id": 1120, "seek": 538642, "start": 5408.54, "end": 5414.42, "text": " Where one of those probabilities is more likely to be kind of high and the other ones that were more likely to be low and", "tokens": [2305, 472, 295, 729, 33783, 307, 544, 3700, 281, 312, 733, 295, 1090, 293, 264, 661, 2306, 300, 645, 544, 3700, 281, 312, 2295, 293], "temperature": 0.0, "avg_logprob": -0.17107240970318133, "compression_ratio": 1.8015873015873016, "no_speech_prob": 6.893596946611069e-07}, {"id": 1121, "seek": 541442, "start": 5414.42, "end": 5418.74, "text": " We know that's what we want for a to map to our one hot encoding", "tokens": [492, 458, 300, 311, 437, 321, 528, 337, 257, 281, 4471, 281, 527, 472, 2368, 43430], "temperature": 0.0, "avg_logprob": -0.17707297066661798, "compression_ratio": 1.8101265822784811, "no_speech_prob": 2.902303322116495e-06}, {"id": 1122, "seek": 541442, "start": 5419.06, "end": 5426.36, "text": " So a soft max is a great activation function to use to kind of help the neural net make it easier for the neural net", "tokens": [407, 257, 2787, 11469, 307, 257, 869, 24433, 2445, 281, 764, 281, 733, 295, 854, 264, 18161, 2533, 652, 309, 3571, 337, 264, 18161, 2533], "temperature": 0.0, "avg_logprob": -0.17707297066661798, "compression_ratio": 1.8101265822784811, "no_speech_prob": 2.902303322116495e-06}, {"id": 1123, "seek": 541442, "start": 5426.9, "end": 5428.9, "text": " to to map", "tokens": [281, 281, 4471], "temperature": 0.0, "avg_logprob": -0.17707297066661798, "compression_ratio": 1.8101265822784811, "no_speech_prob": 2.902303322116495e-06}, {"id": 1124, "seek": 541442, "start": 5428.9400000000005, "end": 5430.9400000000005, "text": " To the output that you wanted", "tokens": [1407, 264, 5598, 300, 291, 1415], "temperature": 0.0, "avg_logprob": -0.17707297066661798, "compression_ratio": 1.8101265822784811, "no_speech_prob": 2.902303322116495e-06}, {"id": 1125, "seek": 541442, "start": 5431.06, "end": 5434.06, "text": " This is what we generally want when we're kind of designing neural networks", "tokens": [639, 307, 437, 321, 5101, 528, 562, 321, 434, 733, 295, 14685, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.17707297066661798, "compression_ratio": 1.8101265822784811, "no_speech_prob": 2.902303322116495e-06}, {"id": 1126, "seek": 541442, "start": 5434.14, "end": 5439.42, "text": " We try to come up with little architectural tweaks that make it as easy for it as possible to", "tokens": [492, 853, 281, 808, 493, 365, 707, 26621, 46664, 300, 652, 309, 382, 1858, 337, 309, 382, 1944, 281], "temperature": 0.0, "avg_logprob": -0.17707297066661798, "compression_ratio": 1.8101265822784811, "no_speech_prob": 2.902303322116495e-06}, {"id": 1127, "seek": 541442, "start": 5440.34, "end": 5442.9, "text": " Match the output that we know we want", "tokens": [26178, 264, 5598, 300, 321, 458, 321, 528], "temperature": 0.0, "avg_logprob": -0.17707297066661798, "compression_ratio": 1.8101265822784811, "no_speech_prob": 2.902303322116495e-06}, {"id": 1128, "seek": 544290, "start": 5442.9, "end": 5444.9, "text": " so", "tokens": [370], "temperature": 0.0, "avg_logprob": -0.2524525451660156, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.0261343252059305e-06}, {"id": 1129, "seek": 544290, "start": 5445.139999999999, "end": 5449.139999999999, "text": " That's basically it right like rather than doing sequential", "tokens": [663, 311, 1936, 309, 558, 411, 2831, 813, 884, 42881], "temperature": 0.0, "avg_logprob": -0.2524525451660156, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.0261343252059305e-06}, {"id": 1130, "seek": 544290, "start": 5449.139999999999, "end": 5453.099999999999, "text": " You know and using an end dot linear and in dot soft max we have to find it from scratch", "tokens": [509, 458, 293, 1228, 364, 917, 5893, 8213, 293, 294, 5893, 2787, 11469, 321, 362, 281, 915, 309, 490, 8459], "temperature": 0.0, "avg_logprob": -0.2524525451660156, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.0261343252059305e-06}, {"id": 1131, "seek": 544290, "start": 5454.0199999999995, "end": 5458.9, "text": " We can now say just like before our net is equal to that class dot cuda", "tokens": [492, 393, 586, 584, 445, 411, 949, 527, 2533, 307, 2681, 281, 300, 1508, 5893, 269, 11152], "temperature": 0.0, "avg_logprob": -0.2524525451660156, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.0261343252059305e-06}, {"id": 1132, "seek": 544290, "start": 5458.9, "end": 5464.82, "text": " And we can say dot fit and we get to within a slight random deviation exactly the same output", "tokens": [400, 321, 393, 584, 5893, 3318, 293, 321, 483, 281, 1951, 257, 4036, 4974, 25163, 2293, 264, 912, 5598], "temperature": 0.0, "avg_logprob": -0.2524525451660156, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.0261343252059305e-06}, {"id": 1133, "seek": 544290, "start": 5465.98, "end": 5470.5, "text": " Okay, so what I'd like you to do during the week is to play around with like", "tokens": [1033, 11, 370, 437, 286, 1116, 411, 291, 281, 360, 1830, 264, 1243, 307, 281, 862, 926, 365, 411], "temperature": 0.0, "avg_logprob": -0.2524525451660156, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.0261343252059305e-06}, {"id": 1134, "seek": 547050, "start": 5470.5, "end": 5477.14, "text": " Torch dot random to generate some random tensors torch dot mat mole to start multiplying them together adding them up", "tokens": [7160, 339, 5893, 4974, 281, 8460, 512, 4974, 10688, 830, 27822, 5893, 3803, 6353, 281, 722, 30955, 552, 1214, 5127, 552, 493], "temperature": 0.0, "avg_logprob": -0.18088699270177772, "compression_ratio": 1.7346153846153847, "no_speech_prob": 4.092888048035093e-06}, {"id": 1135, "seek": 547050, "start": 5478.06, "end": 5481.58, "text": " Try to make sure that you can rewrite soft max yourself from scratch", "tokens": [6526, 281, 652, 988, 300, 291, 393, 28132, 2787, 11469, 1803, 490, 8459], "temperature": 0.0, "avg_logprob": -0.18088699270177772, "compression_ratio": 1.7346153846153847, "no_speech_prob": 4.092888048035093e-06}, {"id": 1136, "seek": 547050, "start": 5482.86, "end": 5489.42, "text": " You know like try to fiddle around a bit with you know reshaping view all that kind of stuff so that by the time you come", "tokens": [509, 458, 411, 853, 281, 24553, 2285, 926, 257, 857, 365, 291, 458, 725, 71, 569, 278, 1910, 439, 300, 733, 295, 1507, 370, 300, 538, 264, 565, 291, 808], "temperature": 0.0, "avg_logprob": -0.18088699270177772, "compression_ratio": 1.7346153846153847, "no_speech_prob": 4.092888048035093e-06}, {"id": 1137, "seek": 547050, "start": 5489.42, "end": 5495.06, "text": " Back next week you feel like pretty comfortable with pytorch and if you google for pytorch tutorial", "tokens": [5833, 958, 1243, 291, 841, 411, 1238, 4619, 365, 25878, 284, 339, 293, 498, 291, 20742, 337, 25878, 284, 339, 7073], "temperature": 0.0, "avg_logprob": -0.18088699270177772, "compression_ratio": 1.7346153846153847, "no_speech_prob": 4.092888048035093e-06}, {"id": 1138, "seek": 547050, "start": 5495.62, "end": 5497.62, "text": " You'll see there's a lot of great material", "tokens": [509, 603, 536, 456, 311, 257, 688, 295, 869, 2527], "temperature": 0.0, "avg_logprob": -0.18088699270177772, "compression_ratio": 1.7346153846153847, "no_speech_prob": 4.092888048035093e-06}, {"id": 1139, "seek": 549762, "start": 5497.62, "end": 5500.7, "text": " Actually on the pytorch website", "tokens": [5135, 322, 264, 25878, 284, 339, 3144], "temperature": 0.0, "avg_logprob": -0.2009785837597317, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.443974381094449e-06}, {"id": 1140, "seek": 549762, "start": 5501.9, "end": 5509.82, "text": " To help you along basically showing you how to create tensors and modify them and do operations on them", "tokens": [1407, 854, 291, 2051, 1936, 4099, 291, 577, 281, 1884, 10688, 830, 293, 16927, 552, 293, 360, 7705, 322, 552], "temperature": 0.0, "avg_logprob": -0.2009785837597317, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.443974381094449e-06}, {"id": 1141, "seek": 549762, "start": 5510.34, "end": 5511.78, "text": " Alright great", "tokens": [2798, 869], "temperature": 0.0, "avg_logprob": -0.2009785837597317, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.443974381094449e-06}, {"id": 1142, "seek": 549762, "start": 5511.78, "end": 5514.099999999999, "text": " Yes, you had a question. Can you pass it over?", "tokens": [1079, 11, 291, 632, 257, 1168, 13, 1664, 291, 1320, 309, 670, 30], "temperature": 0.0, "avg_logprob": -0.2009785837597317, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.443974381094449e-06}, {"id": 1143, "seek": 549762, "start": 5517.58, "end": 5522.7, "text": " So I see that the forward is the layer that gets applied after each of the linear layers", "tokens": [407, 286, 536, 300, 264, 2128, 307, 264, 4583, 300, 2170, 6456, 934, 1184, 295, 264, 8213, 7914], "temperature": 0.0, "avg_logprob": -0.2009785837597317, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.443974381094449e-06}, {"id": 1144, "seek": 552270, "start": 5522.7, "end": 5528.3, "text": " So not quite the forward is just the definition of the module", "tokens": [407, 406, 1596, 264, 2128, 307, 445, 264, 7123, 295, 264, 10088], "temperature": 0.0, "avg_logprob": -0.20975399017333984, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.3496920701072668e-06}, {"id": 1145, "seek": 552270, "start": 5528.3, "end": 5531.42, "text": " So this is like how we're this is how we're implementing linear", "tokens": [407, 341, 307, 411, 577, 321, 434, 341, 307, 577, 321, 434, 18114, 8213], "temperature": 0.0, "avg_logprob": -0.20975399017333984, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.3496920701072668e-06}, {"id": 1146, "seek": 552270, "start": 5531.42, "end": 5537.9, "text": " So does that mean after each linear layer we have to apply the same function. Let's say we can do a", "tokens": [407, 775, 300, 914, 934, 1184, 8213, 4583, 321, 362, 281, 3079, 264, 912, 2445, 13, 961, 311, 584, 321, 393, 360, 257], "temperature": 0.0, "avg_logprob": -0.20975399017333984, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.3496920701072668e-06}, {"id": 1147, "seek": 552270, "start": 5538.62, "end": 5544.58, "text": " Log soft max after layer one and then apply some other function after layer two if we have like a", "tokens": [10824, 2787, 11469, 934, 4583, 472, 293, 550, 3079, 512, 661, 2445, 934, 4583, 732, 498, 321, 362, 411, 257], "temperature": 0.0, "avg_logprob": -0.20975399017333984, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.3496920701072668e-06}, {"id": 1148, "seek": 552270, "start": 5545.42, "end": 5547.42, "text": " Multi-layer neural network", "tokens": [29238, 12, 8376, 260, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.20975399017333984, "compression_ratio": 1.7880184331797235, "no_speech_prob": 1.3496920701072668e-06}, {"id": 1149, "seek": 554742, "start": 5547.42, "end": 5552.82, "text": " So normally we define neural networks", "tokens": [407, 5646, 321, 6964, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.19336030541396723, "compression_ratio": 1.708994708994709, "no_speech_prob": 2.260311021018424e-06}, {"id": 1150, "seek": 554742, "start": 5556.58, "end": 5561.58, "text": " Normally we define neural networks like so we just say here is a list of the layers we want", "tokens": [17424, 321, 6964, 18161, 9590, 411, 370, 321, 445, 584, 510, 307, 257, 1329, 295, 264, 7914, 321, 528], "temperature": 0.0, "avg_logprob": -0.19336030541396723, "compression_ratio": 1.708994708994709, "no_speech_prob": 2.260311021018424e-06}, {"id": 1151, "seek": 554742, "start": 5562.22, "end": 5564.62, "text": " Right we don't you don't have to write", "tokens": [1779, 321, 500, 380, 291, 500, 380, 362, 281, 2464], "temperature": 0.0, "avg_logprob": -0.19336030541396723, "compression_ratio": 1.708994708994709, "no_speech_prob": 2.260311021018424e-06}, {"id": 1152, "seek": 554742, "start": 5565.18, "end": 5570.74, "text": " Your own forward right all we did just now was to say like okay instead of doing this", "tokens": [2260, 1065, 2128, 558, 439, 321, 630, 445, 586, 390, 281, 584, 411, 1392, 2602, 295, 884, 341], "temperature": 0.0, "avg_logprob": -0.19336030541396723, "compression_ratio": 1.708994708994709, "no_speech_prob": 2.260311021018424e-06}, {"id": 1153, "seek": 554742, "start": 5571.1, "end": 5574.82, "text": " Let's not use any of this at all, but write it all by hand ourselves", "tokens": [961, 311, 406, 764, 604, 295, 341, 412, 439, 11, 457, 2464, 309, 439, 538, 1011, 4175], "temperature": 0.0, "avg_logprob": -0.19336030541396723, "compression_ratio": 1.708994708994709, "no_speech_prob": 2.260311021018424e-06}, {"id": 1154, "seek": 557482, "start": 5574.82, "end": 5583.12, "text": " ourselves right so you can you can write as many layers as you like in what any order you like here the", "tokens": [4175, 558, 370, 291, 393, 291, 393, 2464, 382, 867, 7914, 382, 291, 411, 294, 437, 604, 1668, 291, 411, 510, 264], "temperature": 0.0, "avg_logprob": -0.28355358348173254, "compression_ratio": 1.694736842105263, "no_speech_prob": 2.857301751646446e-06}, {"id": 1155, "seek": 557482, "start": 5583.34, "end": 5584.98, "text": " Point was that?", "tokens": [12387, 390, 300, 30], "temperature": 0.0, "avg_logprob": -0.28355358348173254, "compression_ratio": 1.694736842105263, "no_speech_prob": 2.857301751646446e-06}, {"id": 1156, "seek": 557482, "start": 5584.98, "end": 5588.46, "text": " Here we're not using any of that. We've written our own", "tokens": [1692, 321, 434, 406, 1228, 604, 295, 300, 13, 492, 600, 3720, 527, 1065], "temperature": 0.0, "avg_logprob": -0.28355358348173254, "compression_ratio": 1.694736842105263, "no_speech_prob": 2.857301751646446e-06}, {"id": 1157, "seek": 557482, "start": 5589.54, "end": 5592.54, "text": " matmul plus bias our own", "tokens": [3803, 76, 425, 1804, 12577, 527, 1065], "temperature": 0.0, "avg_logprob": -0.28355358348173254, "compression_ratio": 1.694736842105263, "no_speech_prob": 2.857301751646446e-06}, {"id": 1158, "seek": 557482, "start": 5593.38, "end": 5599.46, "text": " Soft max so this is like this is this is just Python code you can write whatever Python code", "tokens": [16985, 11469, 370, 341, 307, 411, 341, 307, 341, 307, 445, 15329, 3089, 291, 393, 2464, 2035, 15329, 3089], "temperature": 0.0, "avg_logprob": -0.28355358348173254, "compression_ratio": 1.694736842105263, "no_speech_prob": 2.857301751646446e-06}, {"id": 1159, "seek": 557482, "start": 5599.98, "end": 5601.98, "text": " Inside forward that you like", "tokens": [15123, 2128, 300, 291, 411], "temperature": 0.0, "avg_logprob": -0.28355358348173254, "compression_ratio": 1.694736842105263, "no_speech_prob": 2.857301751646446e-06}, {"id": 1160, "seek": 560198, "start": 5601.98, "end": 5608.0599999999995, "text": " To define your own neural net so like you won't normally do this yourself", "tokens": [1407, 6964, 428, 1065, 18161, 2533, 370, 411, 291, 1582, 380, 5646, 360, 341, 1803], "temperature": 0.0, "avg_logprob": -0.1902829785890217, "compression_ratio": 1.5874439461883407, "no_speech_prob": 8.800882824289147e-06}, {"id": 1161, "seek": 560198, "start": 5608.0599999999995, "end": 5613.58, "text": " Normally you'll just use the layers that pytorch provides and you're used dot sequential to put them together or even more likely", "tokens": [17424, 291, 603, 445, 764, 264, 7914, 300, 25878, 284, 339, 6417, 293, 291, 434, 1143, 5893, 42881, 281, 829, 552, 1214, 420, 754, 544, 3700], "temperature": 0.0, "avg_logprob": -0.1902829785890217, "compression_ratio": 1.5874439461883407, "no_speech_prob": 8.800882824289147e-06}, {"id": 1162, "seek": 560198, "start": 5613.86, "end": 5621.12, "text": " You'll download a predefined architecture and use that we're just doing this to learn how it works behind the scenes", "tokens": [509, 603, 5484, 257, 659, 37716, 9482, 293, 764, 300, 321, 434, 445, 884, 341, 281, 1466, 577, 309, 1985, 2261, 264, 8026], "temperature": 0.0, "avg_logprob": -0.1902829785890217, "compression_ratio": 1.5874439461883407, "no_speech_prob": 8.800882824289147e-06}, {"id": 1163, "seek": 562112, "start": 5621.12, "end": 5630.68, "text": " All right great. Thanks everybody", "tokens": [50364, 1057, 558, 869, 13, 2561, 2201, 50842], "temperature": 0.0, "avg_logprob": -0.6705342398749458, "compression_ratio": 0.8048780487804879, "no_speech_prob": 4.130415618419647e-05}], "language": "en"}