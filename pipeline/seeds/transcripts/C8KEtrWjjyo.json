{"text": " Okay, I'm going to go ahead and get started. I wanted to announce that Friday I'm going to be out of town, and that's normally when I have office hours, but I'll be around the other day, so feel free to contact me if you want to set up another time. Yes. I wanted to start with a little bit of review today, just because I know we covered a lot of material last week. So I want to start with a matrix vector product, and kind of a different perspective on them. So this, I've got a matrix A times a vector X. You can think of the matrix in terms of its columns. Okay, so here, the columns are A1 through AN, we're multiplying by X, and this could be written as, and actually, let me write out X as a vector. So it's got entries X1 through XN. This could be written as X1 times A1, the column plus X2 times A2, and so on up to the scalar XN times the column AN. So this is a linear combination of the columns of A. This is a kind of different perspective on matrix vector multiplication than you see usually in a first linear algebra class because there you often talk about A being this transformation of X. Whereas here, you can kind of think of X is acting on A. X is saying how we're going to take a linear combination of the columns of A. So this is a pretty helpful perspective for numerical linear algebra. Any questions? I think this might seem simple, but it's surprisingly powerful, this idea of kind of taking linear combinations of the columns of A. So if you move on to thinking about matrix-matrix multiplication, again, you can think about A as being a collection of columns. B is also a collection of columns. Then here, we can write the product as the first column is B1. Actually, let me keep the order the same. The first column is the matrix A times B1. So you're still taking a linear combination of the columns of A, and you're using the coefficients in the column B1, as your coefficients, and that gives you one column of your result. And then so on. A times B2 is the second column of the result. On up to A times BK is the last column. So matrix-matrix multiplication could be thought of as just doing several kind of linear combinations of columns of A with different coefficients. And we'll see this. So we're going to cover a lot of material last week, so we're going to start with a bit of a review. But we'll kind of see SBD and NMF from a different perspective. I think this will be helpful to keep in mind there. No. So here, the capital A is the whole matrix, and the lowercase a is a single column. And so, and we could even actually, this might be nice to see, kind of write this out like a single column here is still, so this might get too long, but it's basically V11 times the column A1 plus V12, or actually put the row first. V21 times the column A2. And you've got this whole sum. Just to give you a single column, AB1, the first column of your results. And then so this, yeah, this perspective of thinking of matrices in terms of kind of collections of columns will come up a lot. Do you have anything in particular about linear combinations, Jeremy, that you wanted to say? Oh, I was just thinking this. This is something I only came to kind of recently, like the idea of rather than going across the rows and down the columns, it's like a linear combination of columns. But in data science, the way you're showing comes up much more, like it's kind of statistical, you know, like linear regression is like just a weighted sum of columns. I think this is super helpful. Thank you. Yeah, and to note, like in data science, often the A1, each column would stand for a different one of your variables or a different one of your features. And that's one reason it's nice to think of them as a unit. Yeah, we'll come back to linear combinations again. I'm going to show the three blue, one brown video, but probably I was thinking a different week. Next week, we'll come to that. It's also kind of this notation of being able to switch between matrices and columns and individual entry entries gives you a lot of flexibility because it can be easier and kind of more concise to talk about like, okay, we're taking a linear combination of the columns as opposed to having to think of it as all the separate entries. Okay, and so I remember we talked about putting matrices together, which is mostly through matrix multiplication, also through matrix addition, matrix vector multiplication. I thought pulling matrices apart, matrix decomposition, and what makes matrix decompositions meaningful are kind of what properties are in the decomposed matrices. It's typically kind of having it decomposed into something that's orthonormal or non-negative, then it's giving you kind of a new perspective or more information. And now, I'm going to ask you some questions. Do you remember what the four considerations we talked about for algorithms are? And you can just say one at a time. So, do you remember any one of the four? That's a good one here. We tossed the catch box up. Memory, this one? Yes. Actually, I asked before you threw back too quickly. I was going to ask if you wanted to say any more detail about memory. So, like with algorithms, you worry about efficiency, but you also take into consideration how much memory you take up when you write efficient algorithms. So, there's a tradeoff at times. So, you have to consider how much space you have to work with. Yes. And do you remember? Oh. I'll ask this to someone else. Does anyone remember any particular consideration with storing matrices in memory? Connor? So, we discussed how we want to keep our matrices in a cache, potentially like when we're performing operations. So, it doesn't, so like our computation doesn't read from, have to read from disk and perform our computations slow. So, our algorithms should favor like saving towards like reading from like an L3 cache or something like that. Yes. That's a very important one. That's about both memory and speed. This idea of something that's really slow is when we're having to take matrices in and out of our fast types of memory. Does anyone remember kind of the fancy vocabulary word for that issue that Connor described? Okay. I heard it. Does anyone want to say it? Is it a microphone? Is it locality? That's it. Yes. Locality. Yeah. Yeah. So, in here we've kind of gone from memory to speed because locality relates to both, but again, classifying this under a speed issue. Does anyone want to say any of the other questions to finish up memory, something else that comes up? Is the idea of sparse versus dense matrices? And do you want to store all your entries or just kind of store, okay, these are the non-zero ones? So, what else comes up under speed as a consideration? Okay. Parallelization. Yes. Yeah. Do you want to say more about that? So, this is the idea that you can have a process where it doesn't have to be serial. You could like compute parts of the result all at the same time and combine them together. Yes. Exactly. Anything else under speed? Okay. You want to avoid redundant computation when you go through? Yes. So, I guess it falls under the efficiency umbrella. That's true. Yeah. You want to avoid that kind of computation? Although, something that came up in the Halide video is that sometimes you have these trade-offs of like redundant computation could allow you to, I don't know, better parallelize or have to do fewer memory transfers. But yeah, in general, avoiding it is good. Okay. I'll go ahead and say one. Vectorization is another one I was thinking of, and that's when you can have a single instruction acting on multiple data. So, SIMD. And that is typically handled for you by lower-level libraries. So, we briefly learned about BLAST and LawPak, but they would be handling that. And so, that's different than parallelization. Often, parallelization, you have different cores doing the same work on different parts of your data. Let me check. I have a list. Okay. So, I just kind of mentioned there, with parallelization, this idea of scaling to multiple cores. There's one other issue I mentioned with speed that's actually a pretty classic consideration around speed. Is this runtime complexity? Yes. Yeah. Let me give you the microphone for that. Runtime complexity? Exactly. Yes. Also known as Big O. Okay. So, I think we've hit speed pretty thoroughly. So, we talked about memory use, speed, scalability and parallelization, and then there's a fourth big area that's important to us with our algorithms. Accuracy? Exactly. And can you say any of the subcategories of your accuracy? I cannot. Okay. That's fine. Accuracy is important. Approximate algorithms. Oh, sure. Approximate algorithms versus algorithms that take a very long time but give you the exact answer. Exactly. Yeah. That's an important one. Yeah. Doing something that might be slightly less accurate but much faster. And then what's another reason why sometimes approximate algorithms are appealing? Any ideas? If problems are NP-hard. Oh, yeah. So, if we can't solve it. NP-hard problems are ones where no, kind of for large problems, that's no reasonable speed, no solution with a reasonable speed has been found. Something else I was thinking of is often your input data may be inaccurate or not that precise or have errors in it. And so, having a highly accurate algorithm is kind of a waste then because it's not going to be that precise given the quality of your data. Okay. Also, approximate algorithms, the randomness, they tend to require randomness and the randomness often results in a bit of generalizability. Oh, yes. Yeah. So, it's a way to avoid overfitting. You can kind of think of it as an automatic regularization technique. Any other kind of categories underneath accuracy? Talk about approximate algorithms. Any ideas? Setting up the tolerance. Sorry. By setting up the tolerance, not too tiny, very small. So, set up a stopping point or the tolerance level. Yeah, so, now we're just going to do that. And a kind of a related issue where this often comes up is floating point arithmetic. And so, that's this idea or kind of the system that computers have for storing numbers. And so, math is infinite, it's continuous, and computers are finite and discrete kind of by nature. And so, floating point arithmetic is the standard of how computers store numbers. And it kind of means that numbers aren't continuous. There are gaps between them. Does anyone remember what the gap that is half of the distance between one and the next closest number is? Machine epsilon, 2 to the negative 53, I think. Yeah, so, machine epsilon, and that's the kind of term that comes up a fair amount. So, you can't get more accurate than that since that's this gap that computers can't capture. Jeremy? Did you want to mention why it's not necessarily 2 to the negative 53? Oh, so, the implementation may vary by computer, although IEEE does have standards that it kind of recommends for most, and most computers are following those standards. But yeah, a lot of these things do end up becoming, yeah, kind of more implementation dependent to the specific computer. And then one other big area of accuracy that we'll be talking about more later in the course. I'll give you a hint. We saw the eigenvalues. We saw a specific problem where we just changed the input a little bit, and the eigenvalues went from being, I think, 1 and 1 to 0 and 2, which is a huge change in the solution of the eigenvalue problem. Does anyone remember what that's called? Yeah. Stability? Stability. Yeah, there's actually conditioning and stability, and those terms are sometimes used interchangeably. One refers to a problem and one refers to the algorithm. And that's an issue that we'll return to more as the course goes on. So, yeah, I think that was a good review. Definitely, kind of stay mindful of those concepts, so we'll see them show up in different places. Okay, so I wanted to return to NMF and SPD, but from a very different perspective. So I have an Excel notebook, and I've uploaded this to GitHub. It's called BritLit. And here are all the calculations. I actually did them in a Jupyter notebook with Python, but I want to use this as a way to really visualize the matrices to kind of get a different perspective on what's going on. Let me make this larger. Oh, okay, great. Thank you. And also at any time, if you're having trouble hearing me or having trouble seeing something on the screen, please let me know. So I can adjust. So here on the left are 27 works of kind of classic British literature. They're all written by the author's last name, followed by the beginning of the title. So some of these, I recognize the second is Jane Austen's Pride and Prejudice, Sense and Sensibility, Vanity Fair. And then along the top are different vocabulary words that showed up in the books. These are, so last time we talked some about a count matrix, which would have the counts of how many times the words showed up in each work. And this is a TF-IDF. Does anyone remember the concept behind TF-IDF? We kind of went over it pretty quickly. Is that a hand? Or no, sorry. Okay, so that stands for Term Frequency Inverse Document Frequency. And it's basically a way to normalize a term document matrix. It takes into account that some words are super common and show up all the time. It also takes into account the length of the documents themselves. And so this, if we hadn't been using it, these numbers here would be integers. Since we are, this is just kind of a normalized equivalent, which is taking those things into account. And actually I did this both ways. And when I did it on the counts, I ended up with a lot more of words like, or his kind of taking more prominence. So I thought it was a little bit more interesting here to kind of give more attention to, really this kind of gave more attention to the proper names. But to take a kind of look at, actually here's a good one, the name Phineas. Is zero for most of these. It's the largest for a book titled Phineas. So that fits with what we would expect. Another one we can check is Kathy Linton is the protagonist of Wuthering Heights. And so you'll see Linton shows up in a few books, but it's much, actually make this even bigger. Are you all able to see the numbers okay? So Linton is 0.4 for Wuthering Heights and then zero for most, 0.001, 0.0004 and some. So this seems reasonable of what we would get. And so this is kind of when we were talking about representing a class of documents as a matrix. This is what we were doing. And we don't have any information about the syntax. It's really just about kind of the word frequencies in these different documents. So I thought this was nice to be able to visualize it. Any questions just about this representation? Okay, so let's go to SVD. So here with SVD, we've gotten the U matrix back, which is, so again, we have 27 documents. It's 27 by 10 in this case. And can anyone tell me what properties this matrix U has? Orthonormal, at least we're talking about orthonormal. Yeah, the columns are orthonormal. Exactly. Thank you. And so we can check that in Excel. So I come down here. And so I looked at the correlations between the columns of U. And so here, if you can see this, this is doing the sum product, which is basically what Excel calls the dot product of B2 to B28, which is this column in blue with itself. And we got one. And then here we're doing it with that column to the column next to it. And we get something 10 to the negative 16th, which is practically zero. And kind of ditto as we scroll over. And when you click on the formula in Excel, it highlights which terms are being used. So the dot product of these two arrays, 10 to the negative 16th, close to zero. So this fits with what we would expect for you having orthonormal columns. And then, and Kelsey said it was orthonormal. This is true if we were to have 27 columns, then the rows would also be orthonormal with each other. Here they're not since I've kind of chopped it off at 10 just to save space. So that's U. Next, we have a matrix S. And what's special about S? You can just shout it out. It's diagonal, yes. So S is not the most exciting, but it's nice. It's simple. It's diagonal. It's also ordered such that the largest value is first and they're in descending order. And S kind of intuitively gives us a notion of importance. Here it's also what allows us to kind of get, since orthonormal matrices, you know, we're getting these dot products of zero or one, since our original matrix could have any values. And we would see this even more if we had done the one of the raw counts. In order to kind of get some magnitude back in there, we need to be able to multiply by numbers bigger than one. And S is letting us do that. So this is S. And then this is V. And V might remind you of U in that it's rows or orthonormal here. And so you can see kind of the dimensions of these. So U was the titles of the works by topics. And note that we're kind of assigning the meaning topics. You know, that wasn't anywhere in our input. And that's a kind of notion that makes sense for this dimension. Then S is topics by topics and V is topics by words. And so here it's I think we should look at a few examples. We can kind of go backwards. So if we look at Darcy, well, that shows up in several. It kind of shows up most in topic four and topic seven. So we might expect Pride and Prejudice to have a lot of topic four and topic seven. You can check that hypothesis. Well, Pride and Prejudice has a lot of topic two as well. It does have a lot of topic four. What? Oh, negative of topic two. OK, so yeah, the largest values are for topic four and topic seven. So we saw the word Darcy was very noticeable in topics four and seven. Pride and Prejudice has a lot of topic four and topic seven. Yeah, as Jeremy pointed out, I misread this. Topic two is negatively present in Pride and Prejudice, which it's kind of hard to think about what that means. This is one of the downsides of SPD. There's probably less intuitive meaning there to talk about a book having a negative topic. And let's do it. And actually, I should check. Do any of you have any favorite British novels that are showing up on here that you want to suggest a word from? Let me check. I do not think so. Yeah, this is kind of a bit older. Yeah, I did have to Google a few of these as I was kind of sanity checking my data to see. Let's take, oh. I know a lot of people say negatives don't make sense in topic modeling. Let's take a look at that. If you've got a really miserable book and you've got a topic which is like happy and joy, then there would probably be a negative correlation. Sometimes it maybe makes sense. So let's look at topic two and see if it reminds us of the opposite of Pride and Prejudice. Let me see if anything stands out as being a particularly large number here. I bet some Dickens book would have like industry type topics. Probably Pride and Prejudice would have a lot about factories. That's true. Yeah. Yeah, this is harder because it's a lot of names. I don't know. Finneas is, oh, well, although, so you can also get these double negatives. So topic two is negative 0.15 Finneas. So I would actually expect something with Finneas in it to then have, I guess, negative of topic two to get negative of a negative. Toby is big. Okay. So yeah, I don't remember a Toby in Pride and Prejudice, so that's fitting that it's negative. And let's look at, let's look at Kathy Linton again for Weathering Heights. So if we come over here to Linton, that shows up most in which what is line nine, I guess, topic eight. Yeah, topic eight. So let's go over here. See if Weathering Heights has. Yeah, so a lot of topic eight is in Weathering Heights, so that's fitting. Any other questions about kind of how to look at this? Yes. Do you want to throw the microphone, Jeremy? Good catch. So here we have a lot of words, right? But we finally settled down into top 10 topic. How does this 10 come around? Oh, so and actually I cheated or I didn't tell you. So I cheated a bit. There were originally 55,000 words in these novels, and I used all of those for the Python part of this, but I didn't want to put that in Excel. So I just chose the top 64 words. But really how this worked was so for a full SVD, we were getting 27 novels. We got 27 topics, and each could involve all 55,000 words. So I just chose the top eight words from the top 10 topics and put that in this Excel workbook. So each of the 27 words could have components from all 55,000 words. So I looked for what had the greatest magnitude. And remember, because the topics are because S is ordered in terms of the values, taking the top 10 makes sense because these have larger values. And so what that means is they make up a bigger component of the original matrix. So remember, the goal here is that we want U times S times V to give us our original matrix back. And since I've only used 10 of the topics, it's not going to give us the original matrix, but hopefully it's close. And that's why this is used in data compression. Just a general question. Would this be a block of our original matrix? Would this be an actual block in the original full matrix if you multiply this out? Oh, yes. That's a good question. Let me write that down. This is a great question. Tim brought up block matrices, which are a pretty important concept in numerical linear algebra. So let me go back to... Oh, I see. Thanks. So the idea of a block matrix is to kind of think about a matrix as having smaller components. So here... I guess if U is 27 by 55,000, we've just taken kind of this section that was... Let me make it a little bit wider just so I can write on it. Taking this section that was 27 by 64, but we could still think of the rest of the matrix being there, and it's 27 by... whatever 55,000 minus 64 is. So I'll round that to 54,000, but it's actually 54,900. Then S could be written as... So here S was 27 by 27, but we were only interested in the top kind of 10 by 10. And that leaves us with like four other matrices. We've got something that's 10 by 7, 7 by 10, and 7 by 7. And then V... Oh, that's not the right... Oh, sorry, I miswrote, guys. U is not 27 by 55,000. U is 27 by 27. Sorry about that. Going back, U is the works by the topics. I was confused. This... So that was actually 27 by 10 and then 27 by 7. I was writing V there, which is 27 by 55,000. Yes. No. Oh, yes. 17. V is the one that's 27 by 55,000. And V is topics by words. And then here, because we've just picked off the top 10, this is where we get that 10 by 64 matrix. And this would be 10 by 54,000, 17 by 64, and 17 by 54,000. Anyway, so block matrix is kind of breaking these matrices down into smaller matrices. And you'll notice that by how matrix multiplication works, that the result can be written as block matrices, kind of products of these within them. So here, the product would be, if I call this U1 and U2, kind of the top square would be U1 times S1 times V1, and so on. I actually really should have done this with an example with just two matrices to start. And actually, maybe let me do that. This is getting to be a complicated example. But this can be a very efficient way of doing matrix multiplication or matrix operations. And it takes into account locality of the idea of you can bring in this matrix that's stored near the part that's stored together in memory, bring it into your cache, multiply it by other block matrices, kind of use it, and then when you're done, put it back. Okay, so scratch that. We're going to do a much simpler example if we have A is A1 and A2. And here, A1 and A2 are both matrices. And say we multiply that by V1 and V2. The result is A1.B1, A2.B1. Oh, sorry. Okay, this is getting too convoluted. I will come back to this next time with a pre-worked example for you. So yeah, we'll revisit this next time. I'm not doing this well on the fly. But that was a good question. We'll go back to it. So back to kind of our perspective of SVD from within Excel. Any other questions about SVD or kind of this way of looking at it? Oh, and then I guess kind of one fine point is that the words I've picked out aren't all clumped together. So I was kind of having to re-arranging your indices. All right, let's switch to NMF. So NMF stands for non-negative matrix factorization, which kind of gives away that the key property of the matrices is that they're non-negative. So here, or with NMF, you get just two matrices typically called W and H that you're factoring into. Zoom in. Yeah, so again, we have the 27 works. And I've chosen 10 topics with NMF. That's a parameter. You can say how many topics you want to calculate. Has anyone noticed something that's kind of distinctive about this matrix? So that's true. There are no negatives. It's sparse. Yeah, I heard several people say sparse, which means there are a lot of zeros. And this is typically an additional constraint that's put on NMF matrices. And it also makes sense because you don't want to value everywhere because you can't get negatives. So if you think of kind of like building up your original matrix, you don't want to have positive place. You can't have it get too large because you're never going to have a negative to cancel that out. So here, let's take a look. And then I'll show you H is over here. And again, this is also sparse and also non-negative. And so this we can look at. So Cathy shows up a lot in topic six. If we go back here, we would expect Wuthering Heights to have a lot of topic six. And it does. It has 0.79, which is kind of on the larger side. So this is a on the surface, at least, this seems more interpretable, I think, of kind of finding large numbers, seeing what topics they line up with. What are some are there any downsides to NMF? Something as we oh, Kelsey. Backs, so it doesn't have a unique solution. That's true. It doesn't have a unique solution. The way SBD does. It's a good one. Any others? This is kind of closely related. Yes. Isn't it difficult to solve? Kind of an ambiguous solution. Yeah, the solution is ambiguous. It's also typically you have to add additional constraints. Difficult to solve, you mean like a little bit slow to calculate or? Yeah, it is. Yeah. Our original approach. Although SBD can also be slow to calculate depending on what you're doing. But yeah, we did run into speed speed problems on on Thursday. Jeremy, I was going to say the lack of a diagonal is a problem because it's not as easy to see which topics are important. But then I was wondering, can you just look at the norm of each row, the norm of each column, get the same kind of idea? That's a really good point. I actually hadn't thought about that. I would say not having it ordered for you of what's most important. And I actually don't know if the norm gives you equivalent information. The only reason we needed the diagonal before is because of the orthonormal constraint. This doesn't have the constraint. It has the concept of how big each row and each column is. Maybe I was wrong and that's not a problem. I think it would not be straightforward just in that you've got the topic represented both in terms of how it intersects with the works and W and how it intersects with the vocabulary words and H. So you'd have to do some sort of normalization on the norm. So I would say it's not as straightforward to find the topic importance. Can you toss it to Tim? Did you say that singular value composition was unique? It's the singular values are unique. Oh, yes. Thank you. Thank you. Yes. It's that. Yeah, that's a great clarification. It's the singular values are unique. U and V are not unique. I think some of it you can also so definitely you can multiply by negatives. This will show up like if you multiply U and V by negatives and then particularly when you're doing the full version for the kind of fill in columns and whichever has the larger dimension. Those part are not unique. OK, so another another kind of downside to NMF I was thinking about is that it's an exact. So SVD if you do the full SVD, you can get something that fully reconstructs your matrix when you multiply back through. NMF is an exact though because you're not even guaranteed that a non negative W and H exist that perfectly multiply to get your original matrix. Yeah. And the idea is you would multiply W and H together and here you can see in blue going row by column. Yes. I also wonder if the fact that they're not orthogonal is a downside like an SVD having orthogonal topics maybe becomes more interpretable because like there's no overlap between them. Where else in this case there might be some overlap and that might be kind of weird. That's true. You could have overlap here or you will have overlap since they're not not orthonormal. All right. I have a general question. So in practice I just see how the process goes right. So suppose we do these SVD decomposition and then say we is the satellite we carry out every unique single value. For example here you only choose 10 out of maybe 30 topic. So for example we we have 30 eigenvalues we construct the X matrix and then we decide whether we want a top 10 or top 20. And then once we finalize the number of topics then we reconstruct the U and V. Is that how it works. And so that this is a great question. So I think you're asking kind of do you have to calculate the full SVD of getting all the topics and then just throw away the ones you don't want. And you do not. And we'll be talking about that in more detail with the new material today. It is possible to just calculate the topics you want for SVD. Although that's kind of a newer approach and there's still a surprising number of materials that recommend you are like found a lot of algorithms online that will kind of be calculating the full SVD and then just throwing information away even though that's much slower. But yeah you don't have to do that. Oh wait grab the microphone. So I was just wondering like in PCA do you have to like still because in that sense I guess like you do have to calculate all the eigenvalues and vectors so that you can pick like the top you know eigenvalues that corresponds to like the eigenvectors which are going to be your like principal components. Is that the case. It actually depends. So some algorithms with and we'll talk in a later lesson about algorithms for calculating eigenvalues but some algorithms kind of pick out the largest eigenvalues first particularly like iterative algorithms for finding them. Cool. And could you also please talk about like the purpose of doing NFMF because like I remember in the advanced machine learning class we kind of talk about like recommendation system where this thing can be helpful. But like in this sort of like top modeling area why it's helpful. I explained to you we want to do this. I think the main reason people argue for NFMF is interpretability and I think I tend to think interpretability is sometimes overblown just in that you can get interpretability from any algorithm by altering your inputs that you put into it. So a lot of kind of supposedly black box algorithms are still interpretable. But that's I would say the main argument I hear for it. But it is definitely something that you that shows up kind of a fair amount. Jeremy I don't know if it's misinterpreting but I thought I heard something else in your question which was about like just given that you have an algorithm that you can select how many columns topics you want. Exists. Do you have to run the whole thing ahead of time before knowing how many to pick off. Like I know later on you're going to show us the little pictures like here's the graph of how the single values decrease. Do you have to like run the whole thing to know to draw that picture and then run it again to pick off. The number of columns. OK. And so this this is kind of the question of do you know how your singular singular values are decreasing as you go because that could let you know what a good stopping point is. And yes you can kind of look at your singular values to have a sense of if you want to calculate more. So you would have to calculate the whole thing first in that case to draw the picture to then go back and decide how many to keep. That's true but you I mean you can be calculating kind of calculate it for a set number see if you want to calculate more. Yeah. Yeah. As opposed to doing the whole thing. When we do robust PCA will actually see an approach that works that way. Yeah. OK. Any other questions about kind of this this other view of Adam F. and SVD. Talk about like the pros and cons of like storing a sparse matrix like why sometimes we would rather just for sparse matrix and sometimes we would have like a dense one. And so yeah the pros and cons briefly and we'll go into more detail of this later because we'll see kind of how sci pi handles this and sci pi actually gives you three different ways to store sparse matrices all of which have their own tradeoffs. But if you don't have that many non zero entries it's a kind of it's like wasting all this memory to just be storing zero and lots of spaces and can take less memory to only store the sparse version. And this also comes up in algorithms of if you do a lot of computations with just zero you might be doing wasted computations because you know anything time zero is zero. So that can be a way to save time as well. Yeah. I was actually thinking with this question about the usefulness of MF actually first just a few pictures I wanted to show to kind of revisit what we talked about. I'll start here on Thursday. And I've kind of updated the notebook from Thursday just a minor additions if you want to grab that again. But I showed that you know if you actually I guess first I should show kind of using this perspective of matrix matrix multiplication being about taking linear combinations of columns. In the case of MF with reconstructing someone's face. Let me make this bigger. You can see that here we've got facial features and so this is kind of like the bridge of someone's nose and underneath their eyes. And another feature might be the tip of somebody's nose. And here a feature is somebody's brows and you can think about taking these different features and wanting to add up a combination of them to make somebody's face. And so here kind of each each face is taken by or made by doing a linear combination of these facial feature columns. So this is kind of a good example of this linear combination perspective. And so here the coefficients are coming from this column they're telling you the importance of each different facial feature and you take your linear combination. And then over here you have faces. So this is kind of an example of that. And going back to kind of the motivation for MF if you were doing something like SBD you would be able to have negative values in the faces which has perhaps less meaning. I guess that's like canceling out some other facial feature but kind of coming up with things that are only non-negative is more immediately interpretable. I think we should probably stop for our break soon. So let's meet back in seven minutes and we will be continuing with some new material later in class. So I just wanted to briefly show a tweet that I saw this morning that references one of the concepts we talked about last week. Do you remember what temporaries are? It's like you're keeping the memory A plus B but when you add C then you have to forget the result of A plus B. So it is that you're having to allocate memory to store A plus B. And if you had a longer computation like we saw an example last week that I forget it was like A times B squared plus the natural log of C. NumPy was storing or the old version of NumPy was storing the natural log of C in one place was having to store A squared, A squared times B and allocate all this temporary memory that kind of takes up time and memory to do. And so I saw this, this is an announcement just from two weeks ago that the newest release of NumPy is allowing the reuse of temporaries. So it is still allocating temporaries but it's allowing to reuse them. So I thought this was kind of a neat example of how this field is kind of always changing and it's interesting to keep up on it and kind of to see it in the news. So kind of before we move on I wanted to revisit PyTorch and last time and I just kind of found some material from a PyTorch autograd introduction but does anyone remember what autograd is for PyTorch? Actually okay even before that let me ask, do you remember why I decided to use PyTorch last time? Yeah so I hear a lot of people saying GPU. I wanted to speed things up by running them on the GPU and PyTorch, one of its purposes is it's a deep learning framework, but another purpose is that it's a alternative to NumPy that runs on the GPU. Does anybody remember the method that we use to tell PyTorch to put things on the GPU? Yes I heard a lot of people say.cuda and so that actually let me go to that. There was a note that if you're not using a GPU you want to delete the.cuda's that show up here, but that's where we're explicitly telling PyTorch to put things on the GPU. Okay so autograd, does anyone remember what that is? Oh yeah? I don't think I remember completely but isn't it like an optimization? So it is very useful for optimization, yes. And so, no that was good, definitely in the very right ballpark. So what do we need, so we talked about stochastic gradient descent last time, what do we need to know to be able to do gradient descent or stochastic gradient descent? You need to know the derivatives. Exactly, we need derivatives and so autograd is automatic differentiation because if you don't know the derivative and don't want to have to calculate it, in some cases you may not even be able to calculate it, PyTorch's autograd calculates it for you by letting variables kind of keep track of how they were made. Jeremy? When I tell people this, they always assume that it must be doing it really slowly by like calculating the function and then the function plus a little bit, finite difference. Like even Terrence here at USF, I was telling him and he was like oh too slow, and I had to keep saying to him like no it actually calculates the derivative properly, like it's so amazing that it's possible and I don't believe you. Yeah, thank you. Yeah, so this is kind of calculating the derivative very fastly. So I just wanted to kind of show some basics of PyTorch again because I think I know what we did last time was very quick. So here creating a variable, this is just a two by two matrix and we're saying requires grad equals true and so that's letting it know, hey we're going to want to be able to get derivatives with respect to this variable later on. And then I'm printing X and it tells me this is just, oh and I've initialized it to torch.ones. NumPy has a very similar method, np.ones that initializes a variable all to ones. There's also zeros that initializes all to zeros. So here we have a two by two tensor where the values are all one. And a tensor is just a generalization of a matrix. So you can think of it as a matrix, but it can have higher dimensions. And then X has its data, which in this case is all ones, and then it also has a grad attribute, which is going to store the gradient, which right now is zero because we haven't done anything. So then we do Y equals X plus two, we print Y, that's all threes. And we'll come back to this idea in lesson kind of notebook three, but there's something called broadcasting. You'll notice here we're adding a scalar to a matrix and what it's done is it's basically just kind of multiplied the scalar to be the right dimensions. So basically we added our matrix one one one one two two two two two to get all these threes. But this idea of broadcasting is pretty important and we'll see more of it. Then we do Z equals Y times Y times three. The out equals the sum of Z. So if we print out Z and out, Z is twenty seven, twenty seven, twenty seven, twenty seven, out is one oh eight. And then we do out dot backwards, which is kind of back propagation and it's calculating the gradient. And let's take a moment. Let's see if this worked now. Oh, yes, it did. OK. So remember, right. Z is. So Y was. X plus two squared. Times three. And we want to take and then Z is the sum of that and really kind of if we think of X being the individual entries now, the sum of that is going to be equal to four kind of times an individual entry since they're all the same. So that's twelve X plus two. Squared. And if we take the derivative, so we're interested in out. The acts. Say, hold on a moment, maybe. Come back. OK. So, yeah, I'm not going to sum them yet. So you take the derivative and you should remember kind of with an exponent, you pull it down, subtract one off. So that would be six times X plus two to the one power, which is just itself, times the derivative of what's inside, which is just one. And so we're getting that the derivative is six times X plus two. And remember, we're interested in where X is equal to one. So. The derivative is six times three equals eighteen. Which is what it's telling us. So pie torch has done this for us. Any questions about that? Remind us about the difference between a tensor and a variable. So tensors and variables have the same API. These are both kind of pie torch notions, but variables have the auto grad option and that they keep track of how they were created. And so you have if you want to use auto grad, you need to use variables. Other questions? So you can see it's really handy to have it calculated for us. OK, so I want to return to kind of where we left off now. Let's start back with comparing approaches. So Scikit learns NMF. It was fast. We didn't have to tune parameters. The people that created it used decades of academic research and it was pretty specific to NMF. And then this is a list of kind of some relatively recent research in NMF. So you can see it's kind of an active field. And this comes from a Python library called NIMFA that's specifically about NMF. So then we decided we wanted something that we could use or build ourselves. So we used pie torch and stochastic gradient descent. Stochastic gradient descent is a very general purpose optimization algorithm. So we were just choosing to apply it to NMF, but it works for many, many different optimization problems. So it's a very good general purpose tool to have. It didn't take us that long to implement. We did have to do more fiddling with the parameters, though. So remember we had a learning rate, kind of keeping track of what size steps we wanted to take. And it was also not as fast. We initially tried it in NumPy and because it was so slow, we had to switch to pie torch. Jeremy? And we also found one benefit of it, another benefit of it, which was because we made the non-negativity a penalty rather than a constraint. We had just a few small negatives and that allowed us actually to have a more accurate decomposition, which maybe created some better topics. That's true, yes. And that did give us, though, more parameters that we were having to tune of how much weight to give, wanting it to be non-negative versus wanting it to multiply to give us the right answer. So any questions about NMF or these different approaches? Okay, so we're going to return to SPD. And we've kind of talked about this sum, this idea of truncated SPD. So for full SPD would be calculating kind of the full dimension of topics. But we've already seen that it's handy to just have kind of a limited number of topics because those are the most important ones. This is also how SPD is used in data compression, where you are kind of choosing a smaller value because you want to kind of save space with your data. And so, yeah, this is truncated SPD. This is the picture from the Facebook blog post again here. And I'm also, I realized after class Thursday that some of the examples, I think, were documents by words and some were words by documents. So I think I may have misspoke a few times on Thursday about which order they were because we do see both in here. Here, this is saying the words are rows, the hashtags are columns. So words, this would basically be words by topics then, topics by topics, and then topics by hashtags. And so we're going to be looking at using a randomized algorithm to calculate the truncated SPD. And so this is going to be a quick method. And this kind of gets back to April's question earlier about, do you have to calculate the full SPD and then throw away information? We're going to use a randomized approach to just calculate, well, what we want plus a little bit more of buffer, but to not have to calculate the full thing. And so we talked about, you know, shortcomings of classical algorithms, just matrices are so large and often data is missing or inaccurate. So why spend the extra computation when your result is not going to be perfectly accurate? Another key theme of the course that data transfer is a major role in the time of algorithms. So it's not just about computational complexity. And then so we're going to be referencing this paper by HALCO. Let me pull it up. It's called Finding Structure with Randomness, and I think it's very well written. I particularly like the introduction, so you might want to check it out. But this is where a lot of material in this lecture, as well as I think some in the next lecture comes from. And they give different examples. So just know that that's out there. So Scikit-learn has a randomized SVD built in, and this is from the decomposition module. Actually, I should probably not start running things now because I don't know what's been run. You'll see here we can request how many components we want or how many singular values. So we're saying five, and it's quite quick. So what we get back, and so just to remember the data set that we were using here was from newsgroups. So these are kind of discussion boards on the internet on different topics. We had 2,000 posts to newsgroups. We're saying that we just want to get five topics, and then there were 26,000 different vocabulary words used. And so the top five topics, you'll remember we just requested things from four categories, and those categories were space, graphics, religion, and atheism. And the top five topics that we see are JPEG, image, EDU, file, graphics, images, GIF, data. So these all have to do with graphics. So does the second topic. This next one seems like a mix of space and religion. You do have graphics showing up. So this, I don't know, there's some problems with these not being kind of fully separated out, but overall they're pretty good. So I'm going to talk about the approach of kind of how this randomized SVD was calculated, because it is really handy to not have to calculate the full thing. And so the basic approach is that we want to find an approximation to the range of A. And does anyone remember what the range of a matrix is? This is kind of a linear algebra vocabulary question. Anyone? Okay, yeah, go for it. Is it the space covered by the column basis? Exactly, yeah. Yeah, the space covered by the column basis. And that's actually, that's really great language because that is kind of getting back to this idea of thinking of the, you know, the taking linear combinations of the columns. But so one way to write it would be that the range of a matrix A is all Y such that AX equals Y for some X. And so if you think about a lot of times, so this is going back now to the image of A being some transformation, but if A is kind of transforming, you know, taking vectors X and transforming them to Y, the range is everything that you can hit over here by multiplying by A. But yeah, I really like Kelsey's definition of thinking of the columns as a basis and saying, you know, what are the columns span? I have a good one off the top of my head. Well, okay, so Jeremy's asking what a basis is. Can anyone answer what a basis is? You think of your basis as like the coordinate axes for your space? You could, so, yes. Yeah, so kind of, yeah, depending on the space. So if we're talking about all the numbers in R2, then the coordinate axes are the standard basis. Yeah. Yeah. And so really kind of depending on your space, the basis are vectors that you can take linear combinations of to get any value in the space that you're talking about. I think it's like, just for R2, you can interpret it as like, what area can you tile over with parallelograms where the waves are, what's your basis? Yes. Yeah. Yes. Okay. I actually wasn't going to show this till later, but I really think this is like the perfect time probably to watch the 3Blue1Brown video about bases. So if you're not familiar with 3Blue1Brown, these are really fantastic videos about linear algebra. Well, he makes them about many topics. And you're going to switch over. Let me just get to the video. But I highly recommend these. And this I've kind of specifically chosen. Actually, okay. So I was going to show the change of basis video, which I particularly like. But do you feel like you need to see the linear combination span and basis intro video before you see the one on change of basis? How? Okay. So no shame about your answers. Raise your hand if you feel really comfortable with the idea of bases and span. Okay. And then raise your hand if you feel like you need a refresher on the ideas of bases and span. Okay. So it was about half and half. So I'm going to go with the review. Hopefully, even for those of you that feel really comfortable with these ideas, I find 3Blue1Brown to just be a really new perspective that you don't see in a lot of linear algebra classes. So I think that you'll still kind of gain something from this. Jeremy, do I need to do anything else? Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. definition of a basis of a space is a set of linearly independent vectors that span that space. Now given how I described a basis earlier and given your current understanding of the words span and linearly independent, think about why this definition would make sense. In the next video I'll get into matrices and transforming space. See you then. Okay great. Is there anything that you found particularly noteworthy or interesting about this? It's okay if not. It can also just be a good review of the terminology. I also just want to highlight that he has some videos not related to linear algebra that are just really interesting and beautiful. I particularly recommend his one on the towers of Hanoi which is kind of this puzzle combined with binary and with Sierpinski triangles. And so there's some really surprising connections between those three topics that I did not know about and also some really nice visualizations. I don't know this person at all but I did just start supporting him through Patreon because I really love his videos. I just mentioned it because I mean hopefully people see this video in the future and if they do then yeah but I think it's yeah very worthwhile thing to to support because they're really beautiful. Okay so kind of going back to and so we'll return another day to kind of talk about change of basis but going back to this idea of the range of A. So as Kelsey said that's the space spanned by the columns of A and so hopefully this kind of helps you get up to speed on those concepts of columns of A. We're taking all possible linear algebra and we're taking all possible linear combinations of them and that's the range of A. And so our goal and just to kind of to remind you what we're doing our goal is to come up with an algorithm for the randomized SVD and so to be or for a truncated SVD using randomized values. So we want to be able to just calculate as many topics as we're interested in not have to calculate them all we're going to use randomization and this is kind of outlining a path for us. So the first step is we want to find a matrix Q that has r orthonormal columns such that A is approximately Q times Q transpose times A and so the thing to note here let me write this down maybe is that you know suppose A is m by n and Q is just going to be m by r so there could be a lot of space savings there and so Q times Q transpose actually this is a question for you is Q by n times Q transpose times A is equal to Q transpose times A. So that's a question for you. Is Q by Q transpose going to be the identity? You can just shout out your guesses. Okay I see both nodding and shaking heads. Okay who wants to vote for yes it will be the identity? Who wants to vote no it will not be the identity? Okay does anyone want to say why they voted the way they did? Okay I'll say so if oh what because it's columns but the row actually the multiplication of the rows might not be orthographic. Exactly yes yeah so this was kind of a tricky question if both the columns and rows of Q had been orthonormal then it would be the identity and even yeah so if both the columns and the rows were orthonormal we would get the identity however it's just the columns and so we kind of have this tall skinny matrix so Q kind of looks like this and we multiply Q by Q transpose. We're getting something we want something that acts kind of like the identity for A but it's not actually going to be the identity because we didn't have enough inputs kind of going into it but we're hoping to kind of find something so that Q by Q transpose at least acts similar to the identity for A. And so we'll come back to the question of how do we actually find such a Q but for now just know that that's what that it is possible. So then we want to construct B equals Q transpose times A and actually probably should okay I guess I have to pull this up here I'll sorry I'll write that again but I will not erase it this time so remember A is m by n Q is m by r so then when we do Q transpose times A that's something that's r by m times m by n and we get that the product is r by n so B is a lot smaller than A. So we can compute the SVD of B by standard methods and this will be much quicker than it would have been to compute the SVD of A and then plugging back in so kind of here that's the formula for S oh that's actually a typo that B is U times sigma times V transpose typical formula for the SVD oh no it's not a typo I called it S because it's a different one uh we're going to have U later on then remember A is approximately Q times Q transpose times A so we plug in for Q transpose times A plug in this SVD for B and we get A is approximately Q times S times sigma times B transpose and we can set U equal to QS and now we have a SVD for A a low rank SVD so I have a question why uh why is it okay to say U is QS because we want you to be or or have orthonormal columns Kelsey I think U is just a rotation of S yes um but if you change S is orthonormal and Q is orthonormal yeah so that's yeah that's the key yeah since S and Q are orthonormal we'll get something else that's orthonormal exactly yeah and I guess um yeah we'll talk about the topic of rotations later but yeah S and Q are orthonormal so that works so now we're going to kind of return to these questions of okay how did we find Q but are there any questions just about kind of this plan of what we're going to do so you happen to remember the computational complexity of SVD so when we reduced the size but are we reducing its speed by I do not remember let me write that down I'll talk about that next time okay it might be squared in the number of columns that is might be a C-value yeah I would I would believe that but I will check yeah so general idea is kind of we're finding this special Q then we find the SVD on a smaller matrix Q transpose times A and we're able to plug that back in to have our our truncated SVD for A okay so first question how do we find Q and so it turns out we can just take a bunch of random vectors W and look at the subspace formed by A W for the these different random vectors we'll form a matrix W with the W's as its columns and we take the QR decomposition of A W equals QR and I'll so we will be talking about the QR decomposition a lot for now all you need to know is that the QR decomposition exists for any matrix and it's an orthonormal matrix times a upper upper triangular matrix and so this is something I think nice about linear algebra is you kind of have these standard naming conventions and so pretty much anytime you see my matrix named Q you can assume it's orthonormal because that's just a commonly used convention but the QR decomposition is all about getting an orthonormal matrix times an upper triangular matrix and we will we'll learn how to do that in a later lesson so we'll take A W W is random get the QR and the Q this is kind of a property of from the QR decomposition the Q forms an orthonormal basis for A W and A W is giving us the range of A since it's kind of A times these different values and since A W has far more rows than columns it turns out that it works in practice that these columns are approximately orthonormal it's just really unlikely that you would get columns that were linearly dependent when you're choosing random values any questions about this and we'll go ahead so it was talking about why A W is the range of a roughly we'll come back to that under how we should choose R I think and yeah this idea of the Q or really the QR decomposition will show up in almost every lesson so you will get to see a lot of the QR decomposition in this class it's pretty pretty foundational to numerical linear algebra and so yeah for now you just need to know that Q consists of orthonormal columns R is upper triangular when I say upper triangular that means that everything below the diagonal is zero trevathan says the QR decomposition is the most important idea in numerical linear algebra so that's in pretty high praise and then this question so remember we chose Q to have R orthonormal columns and then R is giving us what the dimension of B is going to be so how do we want to choose R so if we wanted suppose we had a matrix with 100 columns and we wanted to get five so in the question of going back to our like our literary example we just want five topics to be safe we're going to choose something larger than five so you could just kind of as a rule of thumb say let's add 10 to what we're doing so let's do it with 15 so you don't want to calculate exactly five because we've got this randomized component so it's kind of safer to give yourself some buffer but we don't need to calculate the full 100 topics and so our projection was only approximate so we're making it a little bit bigger than we need bigger than we need now let me show you what this looks like in code so we above we used scikit learns implementation now i'm going to write my own which is based off of the scikit learn source code only it takes into account fewer special cases so this is a less robust version but i think it's kind of clearer what's happening first we want a randomized range finder and so this is what just finds the q that we saw above in step one when we said like okay we want q such that a is approximately q times q transpose times or yeah q times q transpose times a okay so what we'll do in here is just randomly initialize a matrix to our size and here yeah and here size we're kind of telling it how many columns we want and then for now let's imagine that the number of iterations were zero so we can ignore this inner for loop then we could just call the qr decomposition on a times q and we get back kind of q and r and we'll return our q and so that's giving us the q that we want to approximate questions about the randomized range finder so this is kind of just finding finding that q we want to kind of approximate the range of a and then in the next lesson we'll be covering lu decomposition but lu decomposition decomposes a matrix into a lower triangular matrix times an upper triangular matrix and we can add some iterations of that here in the middle that this will make our result more more accurate and it's basically kind of gives us this chance so we kind of want to imprint a like we're really interested in the range of a so we want to imprint a again and again and so it would be nice to just kind of keep multiplying by a you know because if you multiply by a a bunch of times you're getting something that's like super in the range of a the issue with just doing that directly is that you know it could shrink to zero or it could explode that's very unstable to kind of multiply by the same number again and again unless that number happens to be one and so taking the lu decomposition is a way to kind of normalize it and take into account like okay we want something that's normalized so that it doesn't explode or vanish so it's just kind of like a very kind of high level intuitive idea of what this is doing and then how that's how that's used inside the a randomized svd is first the number of random columns we're going to create is the number of components we want plus the number of over samples and so i mentioned since this is random we're going to give ourselves a buffer and kind of over sample so that's defaulting to 10 but you could choose another another number if you wanted then we'll call the randomized range finder with our matrix and with the sum of the number of components plus the number of over samples that's saying how many columns we want to find then we do q transpose times m another way to think about that is projecting m to this k plus p dimensional space using the basis vectors space i'm using the basis vectors so q is the yeah the k plus or kind of giving this giving us this basis of a k plus p dimensional space then we calculate our svd on b remember b is a lot smaller than our original matrix and we get that back here we delete b to free up the memory and then we say u is equal to our q times the the u that came back for b and we'll just return the number of components and so remember we've calculated the number of components plus over samples so we don't want to return everything we'll kind of chop off the the last 10 last 10 columns the last 10 singular values and just return what else is there and so we can try this out see that we get stuff back and this is um and this is i let me think i think this is faster let me check just marginally um and we're still getting topics that seem reasonable so this is this is working um that even though we had 2000 posts by 25 000 words if we were going to do a full svd that would be finding 2000 topics we've said we just want five we're going to calculate 15 and we get back pretty good topics and we get back pretty good topics oh yes i looked up the computational complexity and for an m by n matrix svd's form computational complexity is m squared n plus n cubed oh wow so this is um massive massive massive improvement okay thank you jeremy yeah so what jeremy was saying is it's m squared n plus n cubed so both both of those terms are cubic which is really slow so it is awesome to be able to slice stuff off of there because you know before we would have been and we are still i guess our m is staying the same for this smaller matrix b but n is changing yeah and then we are almost out of time maybe we'll start here at the end i just had a quick exercise for you um but we'll do that next time yeah so next time we'll kind of finish up um talk about this a little bit more and i think that'll be good to kind of return to it after after two days and then we'll be getting into background removal after that which is exciting i also wanted to remind you there's homework one is available on github and that is due thursday yes tim um that's right i i think either is fine yeah so whatever um maybe print it but yeah maybe print a pdf would be best yeah so print a pdf i'll say that in the slack channel as well but yeah print a pdf for the homework um do on thursday um to email us yeah that's good all right great thank you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.44, "text": " Okay, I'm going to go ahead and get started.", "tokens": [1033, 11, 286, 478, 516, 281, 352, 2286, 293, 483, 1409, 13], "temperature": 0.0, "avg_logprob": -0.20670738587012658, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.030194377526640892}, {"id": 1, "seek": 0, "start": 7.44, "end": 11.0, "text": " I wanted to announce that Friday I'm going to be out of town,", "tokens": [286, 1415, 281, 7478, 300, 6984, 286, 478, 516, 281, 312, 484, 295, 3954, 11], "temperature": 0.0, "avg_logprob": -0.20670738587012658, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.030194377526640892}, {"id": 2, "seek": 0, "start": 11.0, "end": 13.08, "text": " and that's normally when I have office hours,", "tokens": [293, 300, 311, 5646, 562, 286, 362, 3398, 2496, 11], "temperature": 0.0, "avg_logprob": -0.20670738587012658, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.030194377526640892}, {"id": 3, "seek": 0, "start": 13.08, "end": 14.16, "text": " but I'll be around the other day,", "tokens": [457, 286, 603, 312, 926, 264, 661, 786, 11], "temperature": 0.0, "avg_logprob": -0.20670738587012658, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.030194377526640892}, {"id": 4, "seek": 0, "start": 14.16, "end": 17.6, "text": " so feel free to contact me if you want to set up another time.", "tokens": [370, 841, 1737, 281, 3385, 385, 498, 291, 528, 281, 992, 493, 1071, 565, 13], "temperature": 0.0, "avg_logprob": -0.20670738587012658, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.030194377526640892}, {"id": 5, "seek": 0, "start": 17.6, "end": 22.44, "text": " Yes. I wanted to start with a little bit of review today,", "tokens": [1079, 13, 286, 1415, 281, 722, 365, 257, 707, 857, 295, 3131, 965, 11], "temperature": 0.0, "avg_logprob": -0.20670738587012658, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.030194377526640892}, {"id": 6, "seek": 0, "start": 22.44, "end": 25.44, "text": " just because I know we covered a lot of material last week.", "tokens": [445, 570, 286, 458, 321, 5343, 257, 688, 295, 2527, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.20670738587012658, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.030194377526640892}, {"id": 7, "seek": 2544, "start": 25.44, "end": 30.080000000000002, "text": " So I want to start with a matrix vector product,", "tokens": [407, 286, 528, 281, 722, 365, 257, 8141, 8062, 1674, 11], "temperature": 0.0, "avg_logprob": -0.3818163267323669, "compression_ratio": 1.4311377245508983, "no_speech_prob": 2.5462059056735598e-05}, {"id": 8, "seek": 2544, "start": 30.080000000000002, "end": 33.08, "text": " and kind of a different perspective on them.", "tokens": [293, 733, 295, 257, 819, 4585, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.3818163267323669, "compression_ratio": 1.4311377245508983, "no_speech_prob": 2.5462059056735598e-05}, {"id": 9, "seek": 2544, "start": 33.44, "end": 38.96, "text": " So this, I've got a matrix A times a vector X.", "tokens": [407, 341, 11, 286, 600, 658, 257, 8141, 316, 1413, 257, 8062, 1783, 13], "temperature": 0.0, "avg_logprob": -0.3818163267323669, "compression_ratio": 1.4311377245508983, "no_speech_prob": 2.5462059056735598e-05}, {"id": 10, "seek": 2544, "start": 38.96, "end": 43.52, "text": " You can think of the matrix in terms of its columns.", "tokens": [509, 393, 519, 295, 264, 8141, 294, 2115, 295, 1080, 13766, 13], "temperature": 0.0, "avg_logprob": -0.3818163267323669, "compression_ratio": 1.4311377245508983, "no_speech_prob": 2.5462059056735598e-05}, {"id": 11, "seek": 2544, "start": 49.44, "end": 54.36, "text": " Okay, so here, the columns are A1 through AN,", "tokens": [1033, 11, 370, 510, 11, 264, 13766, 366, 316, 16, 807, 5252, 11], "temperature": 0.0, "avg_logprob": -0.3818163267323669, "compression_ratio": 1.4311377245508983, "no_speech_prob": 2.5462059056735598e-05}, {"id": 12, "seek": 5436, "start": 54.36, "end": 57.519999999999996, "text": " we're multiplying by X,", "tokens": [321, 434, 30955, 538, 1783, 11], "temperature": 0.0, "avg_logprob": -0.23543188127420717, "compression_ratio": 1.3650793650793651, "no_speech_prob": 4.469077248359099e-05}, {"id": 13, "seek": 5436, "start": 57.519999999999996, "end": 60.72, "text": " and this could be written as,", "tokens": [293, 341, 727, 312, 3720, 382, 11], "temperature": 0.0, "avg_logprob": -0.23543188127420717, "compression_ratio": 1.3650793650793651, "no_speech_prob": 4.469077248359099e-05}, {"id": 14, "seek": 5436, "start": 60.72, "end": 65.6, "text": " and actually, let me write out X as a vector.", "tokens": [293, 767, 11, 718, 385, 2464, 484, 1783, 382, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.23543188127420717, "compression_ratio": 1.3650793650793651, "no_speech_prob": 4.469077248359099e-05}, {"id": 15, "seek": 5436, "start": 65.6, "end": 70.64, "text": " So it's got entries X1 through XN.", "tokens": [407, 309, 311, 658, 23041, 1783, 16, 807, 1783, 45, 13], "temperature": 0.0, "avg_logprob": -0.23543188127420717, "compression_ratio": 1.3650793650793651, "no_speech_prob": 4.469077248359099e-05}, {"id": 16, "seek": 5436, "start": 70.64, "end": 77.56, "text": " This could be written as X1 times A1,", "tokens": [639, 727, 312, 3720, 382, 1783, 16, 1413, 316, 16, 11], "temperature": 0.0, "avg_logprob": -0.23543188127420717, "compression_ratio": 1.3650793650793651, "no_speech_prob": 4.469077248359099e-05}, {"id": 17, "seek": 7756, "start": 77.56, "end": 84.56, "text": " the column plus X2 times A2,", "tokens": [264, 7738, 1804, 1783, 17, 1413, 316, 17, 11], "temperature": 0.0, "avg_logprob": -0.1773747163660386, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.9637720924947644e-06}, {"id": 18, "seek": 7756, "start": 84.56, "end": 90.60000000000001, "text": " and so on up to the scalar XN times the column AN.", "tokens": [293, 370, 322, 493, 281, 264, 39684, 1783, 45, 1413, 264, 7738, 5252, 13], "temperature": 0.0, "avg_logprob": -0.1773747163660386, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.9637720924947644e-06}, {"id": 19, "seek": 7756, "start": 90.60000000000001, "end": 95.88, "text": " So this is a linear combination of the columns of A.", "tokens": [407, 341, 307, 257, 8213, 6562, 295, 264, 13766, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.1773747163660386, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.9637720924947644e-06}, {"id": 20, "seek": 7756, "start": 95.88, "end": 98.0, "text": " This is a kind of different perspective", "tokens": [639, 307, 257, 733, 295, 819, 4585], "temperature": 0.0, "avg_logprob": -0.1773747163660386, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.9637720924947644e-06}, {"id": 21, "seek": 7756, "start": 98.0, "end": 101.52000000000001, "text": " on matrix vector multiplication than you see usually in", "tokens": [322, 8141, 8062, 27290, 813, 291, 536, 2673, 294], "temperature": 0.0, "avg_logprob": -0.1773747163660386, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.9637720924947644e-06}, {"id": 22, "seek": 7756, "start": 101.52000000000001, "end": 104.48, "text": " a first linear algebra class because there you often talk about", "tokens": [257, 700, 8213, 21989, 1508, 570, 456, 291, 2049, 751, 466], "temperature": 0.0, "avg_logprob": -0.1773747163660386, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.9637720924947644e-06}, {"id": 23, "seek": 7756, "start": 104.48, "end": 106.92, "text": " A being this transformation of X.", "tokens": [316, 885, 341, 9887, 295, 1783, 13], "temperature": 0.0, "avg_logprob": -0.1773747163660386, "compression_ratio": 1.5748792270531402, "no_speech_prob": 1.9637720924947644e-06}, {"id": 24, "seek": 10692, "start": 106.92, "end": 110.60000000000001, "text": " Whereas here, you can kind of think of X is acting on A.", "tokens": [13813, 510, 11, 291, 393, 733, 295, 519, 295, 1783, 307, 6577, 322, 316, 13], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 25, "seek": 10692, "start": 110.60000000000001, "end": 112.2, "text": " X is saying how we're going to take", "tokens": [1783, 307, 1566, 577, 321, 434, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 26, "seek": 10692, "start": 112.2, "end": 114.68, "text": " a linear combination of the columns of A.", "tokens": [257, 8213, 6562, 295, 264, 13766, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 27, "seek": 10692, "start": 114.68, "end": 117.84, "text": " So this is a pretty helpful perspective", "tokens": [407, 341, 307, 257, 1238, 4961, 4585], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 28, "seek": 10692, "start": 117.84, "end": 120.28, "text": " for numerical linear algebra.", "tokens": [337, 29054, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 29, "seek": 10692, "start": 120.28, "end": 123.16, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 30, "seek": 10692, "start": 124.88, "end": 127.64, "text": " I think this might seem simple,", "tokens": [286, 519, 341, 1062, 1643, 2199, 11], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 31, "seek": 10692, "start": 127.64, "end": 129.04, "text": " but it's surprisingly powerful,", "tokens": [457, 309, 311, 17600, 4005, 11], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 32, "seek": 10692, "start": 129.04, "end": 130.2, "text": " this idea of kind of taking", "tokens": [341, 1558, 295, 733, 295, 1940], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 33, "seek": 10692, "start": 130.2, "end": 133.0, "text": " linear combinations of the columns of A.", "tokens": [8213, 21267, 295, 264, 13766, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.1822073435046009, "compression_ratio": 1.6525821596244132, "no_speech_prob": 1.7603274500288535e-06}, {"id": 34, "seek": 13300, "start": 133.0, "end": 140.76, "text": " So if you move on to thinking about matrix-matrix multiplication,", "tokens": [407, 498, 291, 1286, 322, 281, 1953, 466, 8141, 12, 15677, 6579, 27290, 11], "temperature": 0.0, "avg_logprob": -0.27654860236428, "compression_ratio": 1.481818181818182, "no_speech_prob": 1.1842946150864009e-05}, {"id": 35, "seek": 13300, "start": 141.44, "end": 146.76, "text": " again, you can think about A as being a collection of columns.", "tokens": [797, 11, 291, 393, 519, 466, 316, 382, 885, 257, 5765, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.27654860236428, "compression_ratio": 1.481818181818182, "no_speech_prob": 1.1842946150864009e-05}, {"id": 36, "seek": 13300, "start": 150.92000000000002, "end": 156.04, "text": " B is also a collection of columns.", "tokens": [363, 307, 611, 257, 5765, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.27654860236428, "compression_ratio": 1.481818181818182, "no_speech_prob": 1.1842946150864009e-05}, {"id": 37, "seek": 15604, "start": 156.04, "end": 168.35999999999999, "text": " Then here, we can write the product as the first column is B1.", "tokens": [1396, 510, 11, 321, 393, 2464, 264, 1674, 382, 264, 700, 7738, 307, 363, 16, 13], "temperature": 0.0, "avg_logprob": -0.25772707621256513, "compression_ratio": 1.5808383233532934, "no_speech_prob": 5.2552923079929315e-06}, {"id": 38, "seek": 15604, "start": 168.35999999999999, "end": 171.32, "text": " Actually, let me keep the order the same.", "tokens": [5135, 11, 718, 385, 1066, 264, 1668, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.25772707621256513, "compression_ratio": 1.5808383233532934, "no_speech_prob": 5.2552923079929315e-06}, {"id": 39, "seek": 15604, "start": 174.12, "end": 179.0, "text": " The first column is the matrix A times B1.", "tokens": [440, 700, 7738, 307, 264, 8141, 316, 1413, 363, 16, 13], "temperature": 0.0, "avg_logprob": -0.25772707621256513, "compression_ratio": 1.5808383233532934, "no_speech_prob": 5.2552923079929315e-06}, {"id": 40, "seek": 15604, "start": 179.0, "end": 182.64, "text": " So you're still taking a linear combination of the columns of A,", "tokens": [407, 291, 434, 920, 1940, 257, 8213, 6562, 295, 264, 13766, 295, 316, 11], "temperature": 0.0, "avg_logprob": -0.25772707621256513, "compression_ratio": 1.5808383233532934, "no_speech_prob": 5.2552923079929315e-06}, {"id": 41, "seek": 15604, "start": 182.64, "end": 185.84, "text": " and you're using the coefficients in the column B1,", "tokens": [293, 291, 434, 1228, 264, 31994, 294, 264, 7738, 363, 16, 11], "temperature": 0.0, "avg_logprob": -0.25772707621256513, "compression_ratio": 1.5808383233532934, "no_speech_prob": 5.2552923079929315e-06}, {"id": 42, "seek": 18584, "start": 185.84, "end": 187.12, "text": " as your coefficients,", "tokens": [382, 428, 31994, 11], "temperature": 0.0, "avg_logprob": -0.2243417403277229, "compression_ratio": 1.634020618556701, "no_speech_prob": 2.3552065613330342e-05}, {"id": 43, "seek": 18584, "start": 187.12, "end": 189.96, "text": " and that gives you one column of your result.", "tokens": [293, 300, 2709, 291, 472, 7738, 295, 428, 1874, 13], "temperature": 0.0, "avg_logprob": -0.2243417403277229, "compression_ratio": 1.634020618556701, "no_speech_prob": 2.3552065613330342e-05}, {"id": 44, "seek": 18584, "start": 189.96, "end": 192.4, "text": " And then so on.", "tokens": [400, 550, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.2243417403277229, "compression_ratio": 1.634020618556701, "no_speech_prob": 2.3552065613330342e-05}, {"id": 45, "seek": 18584, "start": 192.4, "end": 197.08, "text": " A times B2 is the second column of the result.", "tokens": [316, 1413, 363, 17, 307, 264, 1150, 7738, 295, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.2243417403277229, "compression_ratio": 1.634020618556701, "no_speech_prob": 2.3552065613330342e-05}, {"id": 46, "seek": 18584, "start": 197.32, "end": 202.68, "text": " On up to A times BK is the last column.", "tokens": [1282, 493, 281, 316, 1413, 363, 42, 307, 264, 1036, 7738, 13], "temperature": 0.0, "avg_logprob": -0.2243417403277229, "compression_ratio": 1.634020618556701, "no_speech_prob": 2.3552065613330342e-05}, {"id": 47, "seek": 18584, "start": 202.68, "end": 208.08, "text": " So matrix-matrix multiplication could be thought of as just doing", "tokens": [407, 8141, 12, 15677, 6579, 27290, 727, 312, 1194, 295, 382, 445, 884], "temperature": 0.0, "avg_logprob": -0.2243417403277229, "compression_ratio": 1.634020618556701, "no_speech_prob": 2.3552065613330342e-05}, {"id": 48, "seek": 18584, "start": 208.08, "end": 214.24, "text": " several kind of linear combinations of columns of A with different coefficients.", "tokens": [2940, 733, 295, 8213, 21267, 295, 13766, 295, 316, 365, 819, 31994, 13], "temperature": 0.0, "avg_logprob": -0.2243417403277229, "compression_ratio": 1.634020618556701, "no_speech_prob": 2.3552065613330342e-05}, {"id": 49, "seek": 21424, "start": 214.24, "end": 216.84, "text": " And we'll see this.", "tokens": [400, 321, 603, 536, 341, 13], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 50, "seek": 21424, "start": 216.84, "end": 219.64000000000001, "text": " So we're going to cover a lot of material last week,", "tokens": [407, 321, 434, 516, 281, 2060, 257, 688, 295, 2527, 1036, 1243, 11], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 51, "seek": 21424, "start": 219.64000000000001, "end": 221.32000000000002, "text": " so we're going to start with a bit of a review.", "tokens": [370, 321, 434, 516, 281, 722, 365, 257, 857, 295, 257, 3131, 13], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 52, "seek": 21424, "start": 221.32000000000002, "end": 224.36, "text": " But we'll kind of see SBD and NMF from a different perspective.", "tokens": [583, 321, 603, 733, 295, 536, 26944, 35, 293, 426, 44, 37, 490, 257, 819, 4585, 13], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 53, "seek": 21424, "start": 224.36, "end": 227.48000000000002, "text": " I think this will be helpful to keep in mind there.", "tokens": [286, 519, 341, 486, 312, 4961, 281, 1066, 294, 1575, 456, 13], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 54, "seek": 21424, "start": 230.64000000000001, "end": 234.44, "text": " No. So here, the capital A is the whole matrix,", "tokens": [883, 13, 407, 510, 11, 264, 4238, 316, 307, 264, 1379, 8141, 11], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 55, "seek": 21424, "start": 234.44, "end": 237.20000000000002, "text": " and the lowercase a is a single column.", "tokens": [293, 264, 3126, 9765, 257, 307, 257, 2167, 7738, 13], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 56, "seek": 21424, "start": 237.20000000000002, "end": 240.04000000000002, "text": " And so, and we could even actually,", "tokens": [400, 370, 11, 293, 321, 727, 754, 767, 11], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 57, "seek": 21424, "start": 240.04000000000002, "end": 241.88, "text": " this might be nice to see,", "tokens": [341, 1062, 312, 1481, 281, 536, 11], "temperature": 0.0, "avg_logprob": -0.22302385965983074, "compression_ratio": 1.579591836734694, "no_speech_prob": 2.318642509635538e-05}, {"id": 58, "seek": 24188, "start": 241.88, "end": 249.68, "text": " kind of write this out like a single column here is still,", "tokens": [733, 295, 2464, 341, 484, 411, 257, 2167, 7738, 510, 307, 920, 11], "temperature": 0.0, "avg_logprob": -0.33725591806265026, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.8058024579659104e-05}, {"id": 59, "seek": 24188, "start": 256.0, "end": 258.0, "text": " so this might get too long,", "tokens": [370, 341, 1062, 483, 886, 938, 11], "temperature": 0.0, "avg_logprob": -0.33725591806265026, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.8058024579659104e-05}, {"id": 60, "seek": 24188, "start": 258.0, "end": 268.64, "text": " but it's basically V11 times the column A1 plus V12,", "tokens": [457, 309, 311, 1936, 691, 5348, 1413, 264, 7738, 316, 16, 1804, 691, 4762, 11], "temperature": 0.0, "avg_logprob": -0.33725591806265026, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.8058024579659104e-05}, {"id": 61, "seek": 24188, "start": 268.64, "end": 270.48, "text": " or actually put the row first.", "tokens": [420, 767, 829, 264, 5386, 700, 13], "temperature": 0.0, "avg_logprob": -0.33725591806265026, "compression_ratio": 1.317829457364341, "no_speech_prob": 1.8058024579659104e-05}, {"id": 62, "seek": 27048, "start": 270.48, "end": 274.8, "text": " V21 times the column A2.", "tokens": [691, 4436, 1413, 264, 7738, 316, 17, 13], "temperature": 0.0, "avg_logprob": -0.29120826721191406, "compression_ratio": 1.4970760233918128, "no_speech_prob": 2.355145443289075e-05}, {"id": 63, "seek": 27048, "start": 277.52000000000004, "end": 280.6, "text": " And you've got this whole sum.", "tokens": [400, 291, 600, 658, 341, 1379, 2408, 13], "temperature": 0.0, "avg_logprob": -0.29120826721191406, "compression_ratio": 1.4970760233918128, "no_speech_prob": 2.355145443289075e-05}, {"id": 64, "seek": 27048, "start": 280.6, "end": 283.52000000000004, "text": " Just to give you a single column,", "tokens": [1449, 281, 976, 291, 257, 2167, 7738, 11], "temperature": 0.0, "avg_logprob": -0.29120826721191406, "compression_ratio": 1.4970760233918128, "no_speech_prob": 2.355145443289075e-05}, {"id": 65, "seek": 27048, "start": 283.52000000000004, "end": 287.20000000000005, "text": " AB1, the first column of your results.", "tokens": [13838, 16, 11, 264, 700, 7738, 295, 428, 3542, 13], "temperature": 0.0, "avg_logprob": -0.29120826721191406, "compression_ratio": 1.4970760233918128, "no_speech_prob": 2.355145443289075e-05}, {"id": 66, "seek": 27048, "start": 288.36, "end": 292.40000000000003, "text": " And then so this, yeah, this perspective of thinking of matrices", "tokens": [400, 550, 370, 341, 11, 1338, 11, 341, 4585, 295, 1953, 295, 32284], "temperature": 0.0, "avg_logprob": -0.29120826721191406, "compression_ratio": 1.4970760233918128, "no_speech_prob": 2.355145443289075e-05}, {"id": 67, "seek": 29240, "start": 292.4, "end": 301.2, "text": " in terms of kind of collections of columns will come up a lot.", "tokens": [294, 2115, 295, 733, 295, 16641, 295, 13766, 486, 808, 493, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.2782622782389323, "compression_ratio": 1.4845360824742269, "no_speech_prob": 4.683656879933551e-05}, {"id": 68, "seek": 29240, "start": 309.03999999999996, "end": 312.12, "text": " Do you have anything in particular about linear combinations, Jeremy,", "tokens": [1144, 291, 362, 1340, 294, 1729, 466, 8213, 21267, 11, 17809, 11], "temperature": 0.0, "avg_logprob": -0.2782622782389323, "compression_ratio": 1.4845360824742269, "no_speech_prob": 4.683656879933551e-05}, {"id": 69, "seek": 29240, "start": 312.12, "end": 313.0, "text": " that you wanted to say?", "tokens": [300, 291, 1415, 281, 584, 30], "temperature": 0.0, "avg_logprob": -0.2782622782389323, "compression_ratio": 1.4845360824742269, "no_speech_prob": 4.683656879933551e-05}, {"id": 70, "seek": 29240, "start": 313.0, "end": 316.47999999999996, "text": " Oh, I was just thinking this.", "tokens": [876, 11, 286, 390, 445, 1953, 341, 13], "temperature": 0.0, "avg_logprob": -0.2782622782389323, "compression_ratio": 1.4845360824742269, "no_speech_prob": 4.683656879933551e-05}, {"id": 71, "seek": 29240, "start": 316.47999999999996, "end": 318.44, "text": " This is something I only came to kind of recently,", "tokens": [639, 307, 746, 286, 787, 1361, 281, 733, 295, 3938, 11], "temperature": 0.0, "avg_logprob": -0.2782622782389323, "compression_ratio": 1.4845360824742269, "no_speech_prob": 4.683656879933551e-05}, {"id": 72, "seek": 29240, "start": 318.44, "end": 321.08, "text": " like the idea of rather than going across the rows", "tokens": [411, 264, 1558, 295, 2831, 813, 516, 2108, 264, 13241], "temperature": 0.0, "avg_logprob": -0.2782622782389323, "compression_ratio": 1.4845360824742269, "no_speech_prob": 4.683656879933551e-05}, {"id": 73, "seek": 32108, "start": 321.08, "end": 322.47999999999996, "text": " and down the columns,", "tokens": [293, 760, 264, 13766, 11], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 74, "seek": 32108, "start": 322.47999999999996, "end": 324.15999999999997, "text": " it's like a linear combination of columns.", "tokens": [309, 311, 411, 257, 8213, 6562, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 75, "seek": 32108, "start": 324.15999999999997, "end": 328.64, "text": " But in data science, the way you're showing comes up much more,", "tokens": [583, 294, 1412, 3497, 11, 264, 636, 291, 434, 4099, 1487, 493, 709, 544, 11], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 76, "seek": 32108, "start": 328.64, "end": 331.52, "text": " like it's kind of statistical, you know,", "tokens": [411, 309, 311, 733, 295, 22820, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 77, "seek": 32108, "start": 331.52, "end": 335.12, "text": " like linear regression is like just a weighted sum of columns.", "tokens": [411, 8213, 24590, 307, 411, 445, 257, 32807, 2408, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 78, "seek": 32108, "start": 335.12, "end": 337.15999999999997, "text": " I think this is super helpful.", "tokens": [286, 519, 341, 307, 1687, 4961, 13], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 79, "seek": 32108, "start": 337.15999999999997, "end": 339.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 80, "seek": 32108, "start": 339.0, "end": 340.47999999999996, "text": " Yeah, and to note, like in data science,", "tokens": [865, 11, 293, 281, 3637, 11, 411, 294, 1412, 3497, 11], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 81, "seek": 32108, "start": 340.47999999999996, "end": 344.4, "text": " often the A1, each column would stand for a different one", "tokens": [2049, 264, 316, 16, 11, 1184, 7738, 576, 1463, 337, 257, 819, 472], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 82, "seek": 32108, "start": 344.4, "end": 347.36, "text": " of your variables or a different one of your features.", "tokens": [295, 428, 9102, 420, 257, 819, 472, 295, 428, 4122, 13], "temperature": 0.0, "avg_logprob": -0.2116837259066307, "compression_ratio": 1.7258064516129032, "no_speech_prob": 5.224205233389512e-05}, {"id": 83, "seek": 34736, "start": 347.36, "end": 353.12, "text": " And that's one reason it's nice to think of them as a unit.", "tokens": [400, 300, 311, 472, 1778, 309, 311, 1481, 281, 519, 295, 552, 382, 257, 4985, 13], "temperature": 0.0, "avg_logprob": -0.1596175479888916, "compression_ratio": 1.606694560669456, "no_speech_prob": 3.0237739338190295e-05}, {"id": 84, "seek": 34736, "start": 353.12, "end": 358.56, "text": " Yeah, we'll come back to linear combinations again.", "tokens": [865, 11, 321, 603, 808, 646, 281, 8213, 21267, 797, 13], "temperature": 0.0, "avg_logprob": -0.1596175479888916, "compression_ratio": 1.606694560669456, "no_speech_prob": 3.0237739338190295e-05}, {"id": 85, "seek": 34736, "start": 358.56, "end": 360.52000000000004, "text": " I'm going to show the three blue, one brown video,", "tokens": [286, 478, 516, 281, 855, 264, 1045, 3344, 11, 472, 6292, 960, 11], "temperature": 0.0, "avg_logprob": -0.1596175479888916, "compression_ratio": 1.606694560669456, "no_speech_prob": 3.0237739338190295e-05}, {"id": 86, "seek": 34736, "start": 360.52000000000004, "end": 363.88, "text": " but probably I was thinking a different week.", "tokens": [457, 1391, 286, 390, 1953, 257, 819, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1596175479888916, "compression_ratio": 1.606694560669456, "no_speech_prob": 3.0237739338190295e-05}, {"id": 87, "seek": 34736, "start": 363.88, "end": 367.92, "text": " Next week, we'll come to that.", "tokens": [3087, 1243, 11, 321, 603, 808, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.1596175479888916, "compression_ratio": 1.606694560669456, "no_speech_prob": 3.0237739338190295e-05}, {"id": 88, "seek": 34736, "start": 367.92, "end": 372.8, "text": " It's also kind of this notation of being able to switch between matrices", "tokens": [467, 311, 611, 733, 295, 341, 24657, 295, 885, 1075, 281, 3679, 1296, 32284], "temperature": 0.0, "avg_logprob": -0.1596175479888916, "compression_ratio": 1.606694560669456, "no_speech_prob": 3.0237739338190295e-05}, {"id": 89, "seek": 34736, "start": 372.8, "end": 376.6, "text": " and columns and individual entry entries gives you a lot of flexibility", "tokens": [293, 13766, 293, 2609, 8729, 23041, 2709, 291, 257, 688, 295, 12635], "temperature": 0.0, "avg_logprob": -0.1596175479888916, "compression_ratio": 1.606694560669456, "no_speech_prob": 3.0237739338190295e-05}, {"id": 90, "seek": 37660, "start": 376.6, "end": 379.72, "text": " because it can be easier and kind of more concise to talk about like,", "tokens": [570, 309, 393, 312, 3571, 293, 733, 295, 544, 44882, 281, 751, 466, 411, 11], "temperature": 0.0, "avg_logprob": -0.1768181467630777, "compression_ratio": 1.672811059907834, "no_speech_prob": 2.0781850253115408e-05}, {"id": 91, "seek": 37660, "start": 379.72, "end": 382.32000000000005, "text": " okay, we're taking a linear combination of the columns as opposed to having", "tokens": [1392, 11, 321, 434, 1940, 257, 8213, 6562, 295, 264, 13766, 382, 8851, 281, 1419], "temperature": 0.0, "avg_logprob": -0.1768181467630777, "compression_ratio": 1.672811059907834, "no_speech_prob": 2.0781850253115408e-05}, {"id": 92, "seek": 37660, "start": 382.32000000000005, "end": 385.12, "text": " to think of it as all the separate entries.", "tokens": [281, 519, 295, 309, 382, 439, 264, 4994, 23041, 13], "temperature": 0.0, "avg_logprob": -0.1768181467630777, "compression_ratio": 1.672811059907834, "no_speech_prob": 2.0781850253115408e-05}, {"id": 93, "seek": 37660, "start": 393.12, "end": 398.12, "text": " Okay, and so I remember we talked about putting matrices together,", "tokens": [1033, 11, 293, 370, 286, 1604, 321, 2825, 466, 3372, 32284, 1214, 11], "temperature": 0.0, "avg_logprob": -0.1768181467630777, "compression_ratio": 1.672811059907834, "no_speech_prob": 2.0781850253115408e-05}, {"id": 94, "seek": 37660, "start": 398.12, "end": 401.76000000000005, "text": " which is mostly through matrix multiplication,", "tokens": [597, 307, 5240, 807, 8141, 27290, 11], "temperature": 0.0, "avg_logprob": -0.1768181467630777, "compression_ratio": 1.672811059907834, "no_speech_prob": 2.0781850253115408e-05}, {"id": 95, "seek": 37660, "start": 401.76000000000005, "end": 405.92, "text": " also through matrix addition, matrix vector multiplication.", "tokens": [611, 807, 8141, 4500, 11, 8141, 8062, 27290, 13], "temperature": 0.0, "avg_logprob": -0.1768181467630777, "compression_ratio": 1.672811059907834, "no_speech_prob": 2.0781850253115408e-05}, {"id": 96, "seek": 40592, "start": 405.92, "end": 410.6, "text": " I thought pulling matrices apart, matrix decomposition,", "tokens": [286, 1194, 8407, 32284, 4936, 11, 8141, 48356, 11], "temperature": 0.0, "avg_logprob": -0.2026922369516024, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.644142139411997e-05}, {"id": 97, "seek": 40592, "start": 410.6, "end": 414.2, "text": " and what makes matrix decompositions meaningful are kind", "tokens": [293, 437, 1669, 8141, 22867, 329, 2451, 10995, 366, 733], "temperature": 0.0, "avg_logprob": -0.2026922369516024, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.644142139411997e-05}, {"id": 98, "seek": 40592, "start": 414.2, "end": 417.48, "text": " of what properties are in the decomposed matrices.", "tokens": [295, 437, 7221, 366, 294, 264, 22867, 1744, 32284, 13], "temperature": 0.0, "avg_logprob": -0.2026922369516024, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.644142139411997e-05}, {"id": 99, "seek": 40592, "start": 417.48, "end": 420.76, "text": " It's typically kind of having it decomposed into something that's", "tokens": [467, 311, 5850, 733, 295, 1419, 309, 22867, 1744, 666, 746, 300, 311], "temperature": 0.0, "avg_logprob": -0.2026922369516024, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.644142139411997e-05}, {"id": 100, "seek": 40592, "start": 420.76, "end": 425.84000000000003, "text": " orthonormal or non-negative, then it's giving you kind", "tokens": [420, 11943, 24440, 420, 2107, 12, 28561, 1166, 11, 550, 309, 311, 2902, 291, 733], "temperature": 0.0, "avg_logprob": -0.2026922369516024, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.644142139411997e-05}, {"id": 101, "seek": 40592, "start": 425.84000000000003, "end": 428.24, "text": " of a new perspective or more information.", "tokens": [295, 257, 777, 4585, 420, 544, 1589, 13], "temperature": 0.0, "avg_logprob": -0.2026922369516024, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.644142139411997e-05}, {"id": 102, "seek": 40592, "start": 428.24, "end": 432.92, "text": " And now, I'm going to ask you some questions.", "tokens": [400, 586, 11, 286, 478, 516, 281, 1029, 291, 512, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2026922369516024, "compression_ratio": 1.7222222222222223, "no_speech_prob": 1.644142139411997e-05}, {"id": 103, "seek": 43292, "start": 432.92, "end": 436.28000000000003, "text": " Do you remember what the four considerations we talked", "tokens": [1144, 291, 1604, 437, 264, 1451, 24070, 321, 2825], "temperature": 0.0, "avg_logprob": -0.32168809194413445, "compression_ratio": 1.4026845637583893, "no_speech_prob": 2.354932075832039e-05}, {"id": 104, "seek": 43292, "start": 436.28000000000003, "end": 438.68, "text": " about for algorithms are?", "tokens": [466, 337, 14642, 366, 30], "temperature": 0.0, "avg_logprob": -0.32168809194413445, "compression_ratio": 1.4026845637583893, "no_speech_prob": 2.354932075832039e-05}, {"id": 105, "seek": 43292, "start": 438.68, "end": 441.2, "text": " And you can just say one at a time.", "tokens": [400, 291, 393, 445, 584, 472, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.32168809194413445, "compression_ratio": 1.4026845637583893, "no_speech_prob": 2.354932075832039e-05}, {"id": 106, "seek": 43292, "start": 441.2, "end": 444.44, "text": " So, do you remember any one of the four?", "tokens": [407, 11, 360, 291, 1604, 604, 472, 295, 264, 1451, 30], "temperature": 0.0, "avg_logprob": -0.32168809194413445, "compression_ratio": 1.4026845637583893, "no_speech_prob": 2.354932075832039e-05}, {"id": 107, "seek": 43292, "start": 444.44, "end": 449.76, "text": " That's a good one here.", "tokens": [663, 311, 257, 665, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.32168809194413445, "compression_ratio": 1.4026845637583893, "no_speech_prob": 2.354932075832039e-05}, {"id": 108, "seek": 43292, "start": 449.76, "end": 452.84000000000003, "text": " We tossed the catch box up.", "tokens": [492, 42768, 264, 3745, 2424, 493, 13], "temperature": 0.0, "avg_logprob": -0.32168809194413445, "compression_ratio": 1.4026845637583893, "no_speech_prob": 2.354932075832039e-05}, {"id": 109, "seek": 45284, "start": 452.84, "end": 458.84, "text": " Memory, this one?", "tokens": [38203, 11, 341, 472, 30], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 110, "seek": 45284, "start": 458.84, "end": 463.84, "text": " Yes. Actually, I asked before you threw back too quickly.", "tokens": [1079, 13, 5135, 11, 286, 2351, 949, 291, 11918, 646, 886, 2661, 13], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 111, "seek": 45284, "start": 463.84, "end": 469.32, "text": " I was going to ask if you wanted to say any more detail about memory.", "tokens": [286, 390, 516, 281, 1029, 498, 291, 1415, 281, 584, 604, 544, 2607, 466, 4675, 13], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 112, "seek": 45284, "start": 469.32, "end": 471.71999999999997, "text": " So, like with algorithms, you worry about efficiency,", "tokens": [407, 11, 411, 365, 14642, 11, 291, 3292, 466, 10493, 11], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 113, "seek": 45284, "start": 471.71999999999997, "end": 474.47999999999996, "text": " but you also take into consideration how much memory you take", "tokens": [457, 291, 611, 747, 666, 12381, 577, 709, 4675, 291, 747], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 114, "seek": 45284, "start": 474.47999999999996, "end": 476.4, "text": " up when you write efficient algorithms.", "tokens": [493, 562, 291, 2464, 7148, 14642, 13], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 115, "seek": 45284, "start": 476.4, "end": 478.44, "text": " So, there's a tradeoff at times.", "tokens": [407, 11, 456, 311, 257, 4923, 4506, 412, 1413, 13], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 116, "seek": 45284, "start": 478.44, "end": 481.0, "text": " So, you have to consider how much space you have to work with.", "tokens": [407, 11, 291, 362, 281, 1949, 577, 709, 1901, 291, 362, 281, 589, 365, 13], "temperature": 0.0, "avg_logprob": -0.23663007988120024, "compression_ratio": 1.7112068965517242, "no_speech_prob": 0.00012888801575172693}, {"id": 117, "seek": 48100, "start": 481.0, "end": 483.08, "text": " Yes. And do you remember?", "tokens": [1079, 13, 400, 360, 291, 1604, 30], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 118, "seek": 48100, "start": 483.08, "end": 485.72, "text": " Oh. I'll ask this to someone else.", "tokens": [876, 13, 286, 603, 1029, 341, 281, 1580, 1646, 13], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 119, "seek": 48100, "start": 485.72, "end": 488.92, "text": " Does anyone remember any particular consideration", "tokens": [4402, 2878, 1604, 604, 1729, 12381], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 120, "seek": 48100, "start": 488.92, "end": 493.48, "text": " with storing matrices in memory?", "tokens": [365, 26085, 32284, 294, 4675, 30], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 121, "seek": 48100, "start": 493.48, "end": 495.08, "text": " Connor?", "tokens": [33133, 30], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 122, "seek": 48100, "start": 495.08, "end": 498.4, "text": " So, we discussed how we want to keep our matrices", "tokens": [407, 11, 321, 7152, 577, 321, 528, 281, 1066, 527, 32284], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 123, "seek": 48100, "start": 498.4, "end": 501.76, "text": " in a cache, potentially like when we're performing operations.", "tokens": [294, 257, 19459, 11, 7263, 411, 562, 321, 434, 10205, 7705, 13], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 124, "seek": 48100, "start": 501.76, "end": 506.08, "text": " So, it doesn't, so like our computation doesn't read from,", "tokens": [407, 11, 309, 1177, 380, 11, 370, 411, 527, 24903, 1177, 380, 1401, 490, 11], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 125, "seek": 48100, "start": 506.08, "end": 510.48, "text": " have to read from disk and perform our computations slow.", "tokens": [362, 281, 1401, 490, 12355, 293, 2042, 527, 2807, 763, 2964, 13], "temperature": 0.0, "avg_logprob": -0.21829189640460628, "compression_ratio": 1.6351931330472103, "no_speech_prob": 3.81851386919152e-05}, {"id": 126, "seek": 51048, "start": 510.48, "end": 517.8000000000001, "text": " So, our algorithms should favor like saving towards like reading", "tokens": [407, 11, 527, 14642, 820, 2294, 411, 6816, 3030, 411, 3760], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 127, "seek": 51048, "start": 517.8000000000001, "end": 520.44, "text": " from like an L3 cache or something like that.", "tokens": [490, 411, 364, 441, 18, 19459, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 128, "seek": 51048, "start": 520.44, "end": 522.84, "text": " Yes. That's a very important one.", "tokens": [1079, 13, 663, 311, 257, 588, 1021, 472, 13], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 129, "seek": 51048, "start": 522.84, "end": 526.0, "text": " That's about both memory and speed.", "tokens": [663, 311, 466, 1293, 4675, 293, 3073, 13], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 130, "seek": 51048, "start": 526.0, "end": 528.5600000000001, "text": " This idea of something that's really slow is when we're having", "tokens": [639, 1558, 295, 746, 300, 311, 534, 2964, 307, 562, 321, 434, 1419], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 131, "seek": 51048, "start": 528.5600000000001, "end": 532.12, "text": " to take matrices in and out of our fast types of memory.", "tokens": [281, 747, 32284, 294, 293, 484, 295, 527, 2370, 3467, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 132, "seek": 51048, "start": 532.12, "end": 534.44, "text": " Does anyone remember kind of the fancy vocabulary word", "tokens": [4402, 2878, 1604, 733, 295, 264, 10247, 19864, 1349], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 133, "seek": 51048, "start": 534.44, "end": 537.08, "text": " for that issue that Connor described?", "tokens": [337, 300, 2734, 300, 33133, 7619, 30], "temperature": 0.0, "avg_logprob": -0.1777309841579861, "compression_ratio": 1.5975609756097562, "no_speech_prob": 2.667879562068265e-05}, {"id": 134, "seek": 53708, "start": 537.08, "end": 541.36, "text": " Okay. I heard it. Does anyone want to say it?", "tokens": [1033, 13, 286, 2198, 309, 13, 4402, 2878, 528, 281, 584, 309, 30], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 135, "seek": 53708, "start": 541.36, "end": 543.2, "text": " Is it a microphone?", "tokens": [1119, 309, 257, 10952, 30], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 136, "seek": 53708, "start": 543.2, "end": 547.4000000000001, "text": " Is it locality?", "tokens": [1119, 309, 1628, 1860, 30], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 137, "seek": 53708, "start": 547.4000000000001, "end": 549.8000000000001, "text": " That's it. Yes. Locality.", "tokens": [663, 311, 309, 13, 1079, 13, 12859, 1860, 13], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 138, "seek": 53708, "start": 549.8000000000001, "end": 553.6, "text": " Yeah. Yeah.", "tokens": [865, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 139, "seek": 53708, "start": 553.6, "end": 556.32, "text": " So, in here we've kind of gone from memory to speed", "tokens": [407, 11, 294, 510, 321, 600, 733, 295, 2780, 490, 4675, 281, 3073], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 140, "seek": 53708, "start": 556.32, "end": 559.24, "text": " because locality relates to both, but again,", "tokens": [570, 1628, 1860, 16155, 281, 1293, 11, 457, 797, 11], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 141, "seek": 53708, "start": 559.24, "end": 562.0, "text": " classifying this under a speed issue.", "tokens": [1508, 5489, 341, 833, 257, 3073, 2734, 13], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 142, "seek": 53708, "start": 562.0, "end": 564.1600000000001, "text": " Does anyone want to say any of the other questions", "tokens": [4402, 2878, 528, 281, 584, 604, 295, 264, 661, 1651], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 143, "seek": 53708, "start": 564.1600000000001, "end": 567.0400000000001, "text": " to finish up memory, something else that comes up?", "tokens": [281, 2413, 493, 4675, 11, 746, 1646, 300, 1487, 493, 30], "temperature": 0.0, "avg_logprob": -0.362527812610973, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.425357863306999e-05}, {"id": 144, "seek": 56704, "start": 567.04, "end": 570.04, "text": " Is the idea of sparse versus dense matrices?", "tokens": [1119, 264, 1558, 295, 637, 11668, 5717, 18011, 32284, 30], "temperature": 0.0, "avg_logprob": -0.23343214931258235, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00020019058138132095}, {"id": 145, "seek": 56704, "start": 570.04, "end": 576.4399999999999, "text": " And do you want to store all your entries or just kind of store,", "tokens": [400, 360, 291, 528, 281, 3531, 439, 428, 23041, 420, 445, 733, 295, 3531, 11], "temperature": 0.0, "avg_logprob": -0.23343214931258235, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00020019058138132095}, {"id": 146, "seek": 56704, "start": 576.4399999999999, "end": 579.24, "text": " okay, these are the non-zero ones?", "tokens": [1392, 11, 613, 366, 264, 2107, 12, 32226, 2306, 30], "temperature": 0.0, "avg_logprob": -0.23343214931258235, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00020019058138132095}, {"id": 147, "seek": 56704, "start": 579.24, "end": 583.1999999999999, "text": " So, what else comes up under speed as a consideration?", "tokens": [407, 11, 437, 1646, 1487, 493, 833, 3073, 382, 257, 12381, 30], "temperature": 0.0, "avg_logprob": -0.23343214931258235, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00020019058138132095}, {"id": 148, "seek": 56704, "start": 586.7199999999999, "end": 591.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23343214931258235, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00020019058138132095}, {"id": 149, "seek": 56704, "start": 591.4, "end": 592.5999999999999, "text": " Parallelization.", "tokens": [3457, 336, 338, 2144, 13], "temperature": 0.0, "avg_logprob": -0.23343214931258235, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00020019058138132095}, {"id": 150, "seek": 56704, "start": 592.5999999999999, "end": 594.4399999999999, "text": " Yes. Yeah. Do you want to say more about that?", "tokens": [1079, 13, 865, 13, 1144, 291, 528, 281, 584, 544, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.23343214931258235, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.00020019058138132095}, {"id": 151, "seek": 59444, "start": 594.44, "end": 597.5200000000001, "text": " So, this is the idea that you can have a process", "tokens": [407, 11, 341, 307, 264, 1558, 300, 291, 393, 362, 257, 1399], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 152, "seek": 59444, "start": 597.5200000000001, "end": 598.72, "text": " where it doesn't have to be serial.", "tokens": [689, 309, 1177, 380, 362, 281, 312, 17436, 13], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 153, "seek": 59444, "start": 598.72, "end": 601.4000000000001, "text": " You could like compute parts of the result all at the same time", "tokens": [509, 727, 411, 14722, 3166, 295, 264, 1874, 439, 412, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 154, "seek": 59444, "start": 601.4000000000001, "end": 606.5200000000001, "text": " and combine them together. Yes. Exactly.", "tokens": [293, 10432, 552, 1214, 13, 1079, 13, 7587, 13], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 155, "seek": 59444, "start": 606.5200000000001, "end": 608.5200000000001, "text": " Anything else under speed?", "tokens": [11998, 1646, 833, 3073, 30], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 156, "seek": 59444, "start": 616.0, "end": 618.8000000000001, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 157, "seek": 59444, "start": 618.8000000000001, "end": 621.2, "text": " You want to avoid redundant computation when you go through?", "tokens": [509, 528, 281, 5042, 40997, 24903, 562, 291, 352, 807, 30], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 158, "seek": 59444, "start": 621.2, "end": 624.24, "text": " Yes. So, I guess it falls under the efficiency", "tokens": [1079, 13, 407, 11, 286, 2041, 309, 8804, 833, 264, 10493], "temperature": 0.0, "avg_logprob": -0.28488585154215496, "compression_ratio": 1.5, "no_speech_prob": 5.2224040700821206e-05}, {"id": 159, "seek": 62424, "start": 624.24, "end": 625.36, "text": " umbrella. That's true.", "tokens": [21925, 13, 663, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 160, "seek": 62424, "start": 625.36, "end": 629.12, "text": " Yeah. You want to avoid that kind of computation?", "tokens": [865, 13, 509, 528, 281, 5042, 300, 733, 295, 24903, 30], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 161, "seek": 62424, "start": 629.12, "end": 631.0, "text": " Although, something that came up in the Halide video", "tokens": [5780, 11, 746, 300, 1361, 493, 294, 264, 13896, 482, 960], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 162, "seek": 62424, "start": 631.0, "end": 632.72, "text": " is that sometimes you have these trade-offs", "tokens": [307, 300, 2171, 291, 362, 613, 4923, 12, 19231], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 163, "seek": 62424, "start": 632.72, "end": 636.64, "text": " of like redundant computation could allow you to,", "tokens": [295, 411, 40997, 24903, 727, 2089, 291, 281, 11], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 164, "seek": 62424, "start": 636.64, "end": 640.52, "text": " I don't know, better parallelize or have to do fewer memory transfers.", "tokens": [286, 500, 380, 458, 11, 1101, 8952, 1125, 420, 362, 281, 360, 13366, 4675, 29137, 13], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 165, "seek": 62424, "start": 640.52, "end": 643.52, "text": " But yeah, in general, avoiding it is good.", "tokens": [583, 1338, 11, 294, 2674, 11, 20220, 309, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 166, "seek": 62424, "start": 649.44, "end": 651.08, "text": " Okay. I'll go ahead and say one.", "tokens": [1033, 13, 286, 603, 352, 2286, 293, 584, 472, 13], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 167, "seek": 62424, "start": 651.08, "end": 653.4, "text": " Vectorization is another one I was thinking of,", "tokens": [691, 20814, 2144, 307, 1071, 472, 286, 390, 1953, 295, 11], "temperature": 0.0, "avg_logprob": -0.26727166092186644, "compression_ratio": 1.544776119402985, "no_speech_prob": 6.601690256502479e-05}, {"id": 168, "seek": 65340, "start": 653.4, "end": 656.04, "text": " and that's when you can have a single instruction", "tokens": [293, 300, 311, 562, 291, 393, 362, 257, 2167, 10951], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 169, "seek": 65340, "start": 656.04, "end": 657.84, "text": " acting on multiple data.", "tokens": [6577, 322, 3866, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 170, "seek": 65340, "start": 657.84, "end": 659.84, "text": " So, SIMD.", "tokens": [407, 11, 24738, 35, 13], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 171, "seek": 65340, "start": 659.84, "end": 663.68, "text": " And that is typically handled for you by lower-level libraries.", "tokens": [400, 300, 307, 5850, 18033, 337, 291, 538, 3126, 12, 12418, 15148, 13], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 172, "seek": 65340, "start": 663.68, "end": 666.48, "text": " So, we briefly learned about BLAST and LawPak,", "tokens": [407, 11, 321, 10515, 3264, 466, 15132, 20398, 293, 7744, 47, 514, 11], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 173, "seek": 65340, "start": 666.48, "end": 668.48, "text": " but they would be handling that.", "tokens": [457, 436, 576, 312, 13175, 300, 13], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 174, "seek": 65340, "start": 670.8, "end": 674.04, "text": " And so, that's different than parallelization.", "tokens": [400, 370, 11, 300, 311, 819, 813, 8952, 2144, 13], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 175, "seek": 65340, "start": 674.04, "end": 679.52, "text": " Often, parallelization, you have different cores doing the same work", "tokens": [20043, 11, 8952, 2144, 11, 291, 362, 819, 24826, 884, 264, 912, 589], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 176, "seek": 65340, "start": 679.52, "end": 681.92, "text": " on different parts of your data.", "tokens": [322, 819, 3166, 295, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2421650932830514, "compression_ratio": 1.584033613445378, "no_speech_prob": 9.309577581007034e-05}, {"id": 177, "seek": 68192, "start": 681.92, "end": 684.92, "text": " Let me check. I have a list.", "tokens": [961, 385, 1520, 13, 286, 362, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.3770863701315487, "compression_ratio": 1.5023474178403755, "no_speech_prob": 8.880960376700386e-05}, {"id": 178, "seek": 68192, "start": 691.92, "end": 693.68, "text": " Okay. So, I just kind of mentioned there,", "tokens": [1033, 13, 407, 11, 286, 445, 733, 295, 2835, 456, 11], "temperature": 0.0, "avg_logprob": -0.3770863701315487, "compression_ratio": 1.5023474178403755, "no_speech_prob": 8.880960376700386e-05}, {"id": 179, "seek": 68192, "start": 693.68, "end": 697.52, "text": " with parallelization, this idea of scaling to multiple cores.", "tokens": [365, 8952, 2144, 11, 341, 1558, 295, 21589, 281, 3866, 24826, 13], "temperature": 0.0, "avg_logprob": -0.3770863701315487, "compression_ratio": 1.5023474178403755, "no_speech_prob": 8.880960376700386e-05}, {"id": 180, "seek": 68192, "start": 697.52, "end": 701.12, "text": " There's one other issue I mentioned with speed", "tokens": [821, 311, 472, 661, 2734, 286, 2835, 365, 3073], "temperature": 0.0, "avg_logprob": -0.3770863701315487, "compression_ratio": 1.5023474178403755, "no_speech_prob": 8.880960376700386e-05}, {"id": 181, "seek": 68192, "start": 701.12, "end": 704.64, "text": " that's actually a pretty classic consideration around speed.", "tokens": [300, 311, 767, 257, 1238, 7230, 12381, 926, 3073, 13], "temperature": 0.0, "avg_logprob": -0.3770863701315487, "compression_ratio": 1.5023474178403755, "no_speech_prob": 8.880960376700386e-05}, {"id": 182, "seek": 68192, "start": 704.64, "end": 707.04, "text": " Is this runtime complexity?", "tokens": [1119, 341, 34474, 14024, 30], "temperature": 0.0, "avg_logprob": -0.3770863701315487, "compression_ratio": 1.5023474178403755, "no_speech_prob": 8.880960376700386e-05}, {"id": 183, "seek": 68192, "start": 707.04, "end": 709.24, "text": " Yes. Yeah. Let me give you the microphone for that.", "tokens": [1079, 13, 865, 13, 961, 385, 976, 291, 264, 10952, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.3770863701315487, "compression_ratio": 1.5023474178403755, "no_speech_prob": 8.880960376700386e-05}, {"id": 184, "seek": 70924, "start": 709.24, "end": 712.84, "text": " Runtime complexity? Exactly. Yes.", "tokens": [497, 2760, 1312, 14024, 30, 7587, 13, 1079, 13], "temperature": 0.0, "avg_logprob": -0.3073186050226659, "compression_ratio": 1.398989898989899, "no_speech_prob": 8.475888898828998e-05}, {"id": 185, "seek": 70924, "start": 712.84, "end": 715.84, "text": " Also known as Big O.", "tokens": [2743, 2570, 382, 5429, 422, 13], "temperature": 0.0, "avg_logprob": -0.3073186050226659, "compression_ratio": 1.398989898989899, "no_speech_prob": 8.475888898828998e-05}, {"id": 186, "seek": 70924, "start": 715.84, "end": 720.64, "text": " Okay. So, I think we've hit speed pretty thoroughly.", "tokens": [1033, 13, 407, 11, 286, 519, 321, 600, 2045, 3073, 1238, 17987, 13], "temperature": 0.0, "avg_logprob": -0.3073186050226659, "compression_ratio": 1.398989898989899, "no_speech_prob": 8.475888898828998e-05}, {"id": 187, "seek": 70924, "start": 720.64, "end": 725.44, "text": " So, we talked about memory use, speed, scalability and parallelization,", "tokens": [407, 11, 321, 2825, 466, 4675, 764, 11, 3073, 11, 15664, 2310, 293, 8952, 2144, 11], "temperature": 0.0, "avg_logprob": -0.3073186050226659, "compression_ratio": 1.398989898989899, "no_speech_prob": 8.475888898828998e-05}, {"id": 188, "seek": 70924, "start": 725.44, "end": 729.84, "text": " and then there's a fourth big area that's important to us with our algorithms.", "tokens": [293, 550, 456, 311, 257, 6409, 955, 1859, 300, 311, 1021, 281, 505, 365, 527, 14642, 13], "temperature": 0.0, "avg_logprob": -0.3073186050226659, "compression_ratio": 1.398989898989899, "no_speech_prob": 8.475888898828998e-05}, {"id": 189, "seek": 70924, "start": 735.84, "end": 738.44, "text": " Accuracy? Exactly.", "tokens": [5725, 374, 2551, 30, 7587, 13], "temperature": 0.0, "avg_logprob": -0.3073186050226659, "compression_ratio": 1.398989898989899, "no_speech_prob": 8.475888898828998e-05}, {"id": 190, "seek": 73844, "start": 738.44, "end": 742.6400000000001, "text": " And can you say any of the subcategories of your accuracy?", "tokens": [400, 393, 291, 584, 604, 295, 264, 1422, 66, 2968, 2083, 295, 428, 14170, 30], "temperature": 0.0, "avg_logprob": -0.2565565887762576, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.00025297008687630296}, {"id": 191, "seek": 73844, "start": 742.6400000000001, "end": 744.6400000000001, "text": " I cannot. Okay. That's fine.", "tokens": [286, 2644, 13, 1033, 13, 663, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.2565565887762576, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.00025297008687630296}, {"id": 192, "seek": 73844, "start": 744.6400000000001, "end": 747.6400000000001, "text": " Accuracy is important.", "tokens": [5725, 374, 2551, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.2565565887762576, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.00025297008687630296}, {"id": 193, "seek": 73844, "start": 747.6400000000001, "end": 749.6400000000001, "text": " Approximate algorithms. Oh, sure.", "tokens": [29551, 87, 2905, 14642, 13, 876, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.2565565887762576, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.00025297008687630296}, {"id": 194, "seek": 73844, "start": 749.6400000000001, "end": 754.6400000000001, "text": " Approximate algorithms versus algorithms that take a very long time", "tokens": [29551, 87, 2905, 14642, 5717, 14642, 300, 747, 257, 588, 938, 565], "temperature": 0.0, "avg_logprob": -0.2565565887762576, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.00025297008687630296}, {"id": 195, "seek": 73844, "start": 754.6400000000001, "end": 758.6400000000001, "text": " but give you the exact answer. Exactly. Yeah. That's an important one.", "tokens": [457, 976, 291, 264, 1900, 1867, 13, 7587, 13, 865, 13, 663, 311, 364, 1021, 472, 13], "temperature": 0.0, "avg_logprob": -0.2565565887762576, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.00025297008687630296}, {"id": 196, "seek": 73844, "start": 758.6400000000001, "end": 764.6400000000001, "text": " Yeah. Doing something that might be slightly less accurate but much faster.", "tokens": [865, 13, 18496, 746, 300, 1062, 312, 4748, 1570, 8559, 457, 709, 4663, 13], "temperature": 0.0, "avg_logprob": -0.2565565887762576, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.00025297008687630296}, {"id": 197, "seek": 76464, "start": 764.64, "end": 769.64, "text": " And then what's another reason why sometimes approximate algorithms are appealing?", "tokens": [400, 550, 437, 311, 1071, 1778, 983, 2171, 30874, 14642, 366, 23842, 30], "temperature": 0.0, "avg_logprob": -0.2652292705717541, "compression_ratio": 1.4064516129032258, "no_speech_prob": 5.1416209316812456e-05}, {"id": 198, "seek": 76464, "start": 774.64, "end": 777.64, "text": " Any ideas?", "tokens": [2639, 3487, 30], "temperature": 0.0, "avg_logprob": -0.2652292705717541, "compression_ratio": 1.4064516129032258, "no_speech_prob": 5.1416209316812456e-05}, {"id": 199, "seek": 76464, "start": 779.64, "end": 781.64, "text": " If problems are NP-hard.", "tokens": [759, 2740, 366, 38611, 12, 21491, 13], "temperature": 0.0, "avg_logprob": -0.2652292705717541, "compression_ratio": 1.4064516129032258, "no_speech_prob": 5.1416209316812456e-05}, {"id": 200, "seek": 76464, "start": 781.64, "end": 785.64, "text": " Oh, yeah. So, if we can't solve it.", "tokens": [876, 11, 1338, 13, 407, 11, 498, 321, 393, 380, 5039, 309, 13], "temperature": 0.0, "avg_logprob": -0.2652292705717541, "compression_ratio": 1.4064516129032258, "no_speech_prob": 5.1416209316812456e-05}, {"id": 201, "seek": 76464, "start": 785.64, "end": 790.64, "text": " NP-hard problems are ones where no, kind of for large problems,", "tokens": [38611, 12, 21491, 2740, 366, 2306, 689, 572, 11, 733, 295, 337, 2416, 2740, 11], "temperature": 0.0, "avg_logprob": -0.2652292705717541, "compression_ratio": 1.4064516129032258, "no_speech_prob": 5.1416209316812456e-05}, {"id": 202, "seek": 79064, "start": 790.64, "end": 796.64, "text": " that's no reasonable speed, no solution with a reasonable speed has been found.", "tokens": [300, 311, 572, 10585, 3073, 11, 572, 3827, 365, 257, 10585, 3073, 575, 668, 1352, 13], "temperature": 0.0, "avg_logprob": -0.1006249050761378, "compression_ratio": 1.6, "no_speech_prob": 7.03024779795669e-05}, {"id": 203, "seek": 79064, "start": 796.64, "end": 800.64, "text": " Something else I was thinking of is often your input data may be inaccurate", "tokens": [6595, 1646, 286, 390, 1953, 295, 307, 2049, 428, 4846, 1412, 815, 312, 46443], "temperature": 0.0, "avg_logprob": -0.1006249050761378, "compression_ratio": 1.6, "no_speech_prob": 7.03024779795669e-05}, {"id": 204, "seek": 79064, "start": 800.64, "end": 803.64, "text": " or not that precise or have errors in it.", "tokens": [420, 406, 300, 13600, 420, 362, 13603, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.1006249050761378, "compression_ratio": 1.6, "no_speech_prob": 7.03024779795669e-05}, {"id": 205, "seek": 79064, "start": 803.64, "end": 808.64, "text": " And so, having a highly accurate algorithm is kind of a waste then", "tokens": [400, 370, 11, 1419, 257, 5405, 8559, 9284, 307, 733, 295, 257, 5964, 550], "temperature": 0.0, "avg_logprob": -0.1006249050761378, "compression_ratio": 1.6, "no_speech_prob": 7.03024779795669e-05}, {"id": 206, "seek": 79064, "start": 808.64, "end": 813.64, "text": " because it's not going to be that precise given the quality of your data.", "tokens": [570, 309, 311, 406, 516, 281, 312, 300, 13600, 2212, 264, 3125, 295, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1006249050761378, "compression_ratio": 1.6, "no_speech_prob": 7.03024779795669e-05}, {"id": 207, "seek": 79064, "start": 813.64, "end": 815.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1006249050761378, "compression_ratio": 1.6, "no_speech_prob": 7.03024779795669e-05}, {"id": 208, "seek": 81564, "start": 815.64, "end": 821.64, "text": " Also, approximate algorithms, the randomness, they tend to require randomness", "tokens": [2743, 11, 30874, 14642, 11, 264, 4974, 1287, 11, 436, 3928, 281, 3651, 4974, 1287], "temperature": 0.0, "avg_logprob": -0.10861809687180952, "compression_ratio": 1.5855855855855856, "no_speech_prob": 2.2825473934062757e-05}, {"id": 209, "seek": 81564, "start": 821.64, "end": 825.64, "text": " and the randomness often results in a bit of generalizability.", "tokens": [293, 264, 4974, 1287, 2049, 3542, 294, 257, 857, 295, 2674, 590, 2310, 13], "temperature": 0.0, "avg_logprob": -0.10861809687180952, "compression_ratio": 1.5855855855855856, "no_speech_prob": 2.2825473934062757e-05}, {"id": 210, "seek": 81564, "start": 825.64, "end": 828.64, "text": " Oh, yes. Yeah. So, it's a way to avoid overfitting.", "tokens": [876, 11, 2086, 13, 865, 13, 407, 11, 309, 311, 257, 636, 281, 5042, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.10861809687180952, "compression_ratio": 1.5855855855855856, "no_speech_prob": 2.2825473934062757e-05}, {"id": 211, "seek": 81564, "start": 828.64, "end": 833.64, "text": " You can kind of think of it as an automatic regularization technique.", "tokens": [509, 393, 733, 295, 519, 295, 309, 382, 364, 12509, 3890, 2144, 6532, 13], "temperature": 0.0, "avg_logprob": -0.10861809687180952, "compression_ratio": 1.5855855855855856, "no_speech_prob": 2.2825473934062757e-05}, {"id": 212, "seek": 81564, "start": 833.64, "end": 838.64, "text": " Any other kind of categories underneath accuracy?", "tokens": [2639, 661, 733, 295, 10479, 7223, 14170, 30], "temperature": 0.0, "avg_logprob": -0.10861809687180952, "compression_ratio": 1.5855855855855856, "no_speech_prob": 2.2825473934062757e-05}, {"id": 213, "seek": 83864, "start": 838.64, "end": 845.64, "text": " Talk about approximate algorithms.", "tokens": [8780, 466, 30874, 14642, 13], "temperature": 0.0, "avg_logprob": -0.3322769546508789, "compression_ratio": 1.0129870129870129, "no_speech_prob": 0.00010062522778753191}, {"id": 214, "seek": 83864, "start": 852.64, "end": 855.64, "text": " Any ideas?", "tokens": [2639, 3487, 30], "temperature": 0.0, "avg_logprob": -0.3322769546508789, "compression_ratio": 1.0129870129870129, "no_speech_prob": 0.00010062522778753191}, {"id": 215, "seek": 83864, "start": 858.64, "end": 862.64, "text": " Setting up the tolerance.", "tokens": [21063, 493, 264, 23368, 13], "temperature": 0.0, "avg_logprob": -0.3322769546508789, "compression_ratio": 1.0129870129870129, "no_speech_prob": 0.00010062522778753191}, {"id": 216, "seek": 83864, "start": 862.64, "end": 864.64, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.3322769546508789, "compression_ratio": 1.0129870129870129, "no_speech_prob": 0.00010062522778753191}, {"id": 217, "seek": 86464, "start": 864.64, "end": 868.64, "text": " By setting up the tolerance, not too tiny, very small.", "tokens": [3146, 3287, 493, 264, 23368, 11, 406, 886, 5870, 11, 588, 1359, 13], "temperature": 0.0, "avg_logprob": -0.20980447711366595, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.000105489649286028}, {"id": 218, "seek": 86464, "start": 868.64, "end": 872.64, "text": " So, set up a stopping point or the tolerance level.", "tokens": [407, 11, 992, 493, 257, 12767, 935, 420, 264, 23368, 1496, 13], "temperature": 0.0, "avg_logprob": -0.20980447711366595, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.000105489649286028}, {"id": 219, "seek": 86464, "start": 872.64, "end": 876.64, "text": " Yeah, so, now we're just going to do that.", "tokens": [865, 11, 370, 11, 586, 321, 434, 445, 516, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.20980447711366595, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.000105489649286028}, {"id": 220, "seek": 86464, "start": 876.64, "end": 881.64, "text": " And a kind of a related issue where this often comes up is floating point arithmetic.", "tokens": [400, 257, 733, 295, 257, 4077, 2734, 689, 341, 2049, 1487, 493, 307, 12607, 935, 42973, 13], "temperature": 0.0, "avg_logprob": -0.20980447711366595, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.000105489649286028}, {"id": 221, "seek": 86464, "start": 881.64, "end": 886.64, "text": " And so, that's this idea or kind of the system that computers have for storing numbers.", "tokens": [400, 370, 11, 300, 311, 341, 1558, 420, 733, 295, 264, 1185, 300, 10807, 362, 337, 26085, 3547, 13], "temperature": 0.0, "avg_logprob": -0.20980447711366595, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.000105489649286028}, {"id": 222, "seek": 86464, "start": 886.64, "end": 891.64, "text": " And so, math is infinite, it's continuous,", "tokens": [400, 370, 11, 5221, 307, 13785, 11, 309, 311, 10957, 11], "temperature": 0.0, "avg_logprob": -0.20980447711366595, "compression_ratio": 1.641255605381166, "no_speech_prob": 0.000105489649286028}, {"id": 223, "seek": 89164, "start": 891.64, "end": 895.64, "text": " and computers are finite and discrete kind of by nature.", "tokens": [293, 10807, 366, 19362, 293, 27706, 733, 295, 538, 3687, 13], "temperature": 0.0, "avg_logprob": -0.10028832209737677, "compression_ratio": 1.64321608040201, "no_speech_prob": 5.646971476380713e-05}, {"id": 224, "seek": 89164, "start": 895.64, "end": 900.64, "text": " And so, floating point arithmetic is the standard of how computers store numbers.", "tokens": [400, 370, 11, 12607, 935, 42973, 307, 264, 3832, 295, 577, 10807, 3531, 3547, 13], "temperature": 0.0, "avg_logprob": -0.10028832209737677, "compression_ratio": 1.64321608040201, "no_speech_prob": 5.646971476380713e-05}, {"id": 225, "seek": 89164, "start": 900.64, "end": 903.64, "text": " And it kind of means that numbers aren't continuous.", "tokens": [400, 309, 733, 295, 1355, 300, 3547, 3212, 380, 10957, 13], "temperature": 0.0, "avg_logprob": -0.10028832209737677, "compression_ratio": 1.64321608040201, "no_speech_prob": 5.646971476380713e-05}, {"id": 226, "seek": 89164, "start": 903.64, "end": 905.64, "text": " There are gaps between them.", "tokens": [821, 366, 15031, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.10028832209737677, "compression_ratio": 1.64321608040201, "no_speech_prob": 5.646971476380713e-05}, {"id": 227, "seek": 89164, "start": 905.64, "end": 916.64, "text": " Does anyone remember what the gap that is half of the distance between one and the next closest number is?", "tokens": [4402, 2878, 1604, 437, 264, 7417, 300, 307, 1922, 295, 264, 4560, 1296, 472, 293, 264, 958, 13699, 1230, 307, 30], "temperature": 0.0, "avg_logprob": -0.10028832209737677, "compression_ratio": 1.64321608040201, "no_speech_prob": 5.646971476380713e-05}, {"id": 228, "seek": 91664, "start": 916.64, "end": 921.64, "text": " Machine epsilon, 2 to the negative 53, I think.", "tokens": [22155, 17889, 11, 568, 281, 264, 3671, 21860, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.141932589667184, "compression_ratio": 1.5751295336787565, "no_speech_prob": 7.24987403373234e-05}, {"id": 229, "seek": 91664, "start": 921.64, "end": 926.64, "text": " Yeah, so, machine epsilon, and that's the kind of term that comes up a fair amount.", "tokens": [865, 11, 370, 11, 3479, 17889, 11, 293, 300, 311, 264, 733, 295, 1433, 300, 1487, 493, 257, 3143, 2372, 13], "temperature": 0.0, "avg_logprob": -0.141932589667184, "compression_ratio": 1.5751295336787565, "no_speech_prob": 7.24987403373234e-05}, {"id": 230, "seek": 91664, "start": 926.64, "end": 935.64, "text": " So, you can't get more accurate than that since that's this gap that computers can't capture.", "tokens": [407, 11, 291, 393, 380, 483, 544, 8559, 813, 300, 1670, 300, 311, 341, 7417, 300, 10807, 393, 380, 7983, 13], "temperature": 0.0, "avg_logprob": -0.141932589667184, "compression_ratio": 1.5751295336787565, "no_speech_prob": 7.24987403373234e-05}, {"id": 231, "seek": 91664, "start": 935.64, "end": 937.64, "text": " Jeremy?", "tokens": [17809, 30], "temperature": 0.0, "avg_logprob": -0.141932589667184, "compression_ratio": 1.5751295336787565, "no_speech_prob": 7.24987403373234e-05}, {"id": 232, "seek": 91664, "start": 937.64, "end": 940.64, "text": " Did you want to mention why it's not necessarily 2 to the negative 53?", "tokens": [2589, 291, 528, 281, 2152, 983, 309, 311, 406, 4725, 568, 281, 264, 3671, 21860, 30], "temperature": 0.0, "avg_logprob": -0.141932589667184, "compression_ratio": 1.5751295336787565, "no_speech_prob": 7.24987403373234e-05}, {"id": 233, "seek": 94064, "start": 940.64, "end": 947.64, "text": " Oh, so, the implementation may vary by computer,", "tokens": [876, 11, 370, 11, 264, 11420, 815, 10559, 538, 3820, 11], "temperature": 0.0, "avg_logprob": -0.08563421083533246, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.0128237590834033e-05}, {"id": 234, "seek": 94064, "start": 947.64, "end": 953.64, "text": " although IEEE does have standards that it kind of recommends for most,", "tokens": [4878, 286, 7258, 36, 775, 362, 7787, 300, 309, 733, 295, 34556, 337, 881, 11], "temperature": 0.0, "avg_logprob": -0.08563421083533246, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.0128237590834033e-05}, {"id": 235, "seek": 94064, "start": 953.64, "end": 955.64, "text": " and most computers are following those standards.", "tokens": [293, 881, 10807, 366, 3480, 729, 7787, 13], "temperature": 0.0, "avg_logprob": -0.08563421083533246, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.0128237590834033e-05}, {"id": 236, "seek": 94064, "start": 955.64, "end": 967.64, "text": " But yeah, a lot of these things do end up becoming, yeah, kind of more implementation dependent to the specific computer.", "tokens": [583, 1338, 11, 257, 688, 295, 613, 721, 360, 917, 493, 5617, 11, 1338, 11, 733, 295, 544, 11420, 12334, 281, 264, 2685, 3820, 13], "temperature": 0.0, "avg_logprob": -0.08563421083533246, "compression_ratio": 1.598901098901099, "no_speech_prob": 1.0128237590834033e-05}, {"id": 237, "seek": 96764, "start": 967.64, "end": 973.64, "text": " And then one other big area of accuracy that we'll be talking about more later in the course.", "tokens": [400, 550, 472, 661, 955, 1859, 295, 14170, 300, 321, 603, 312, 1417, 466, 544, 1780, 294, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.058058402651832215, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.8851434763055295e-05}, {"id": 238, "seek": 96764, "start": 973.64, "end": 974.64, "text": " I'll give you a hint.", "tokens": [286, 603, 976, 291, 257, 12075, 13], "temperature": 0.0, "avg_logprob": -0.058058402651832215, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.8851434763055295e-05}, {"id": 239, "seek": 96764, "start": 974.64, "end": 976.64, "text": " We saw the eigenvalues.", "tokens": [492, 1866, 264, 10446, 46033, 13], "temperature": 0.0, "avg_logprob": -0.058058402651832215, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.8851434763055295e-05}, {"id": 240, "seek": 96764, "start": 976.64, "end": 980.64, "text": " We saw a specific problem where we just changed the input a little bit,", "tokens": [492, 1866, 257, 2685, 1154, 689, 321, 445, 3105, 264, 4846, 257, 707, 857, 11], "temperature": 0.0, "avg_logprob": -0.058058402651832215, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.8851434763055295e-05}, {"id": 241, "seek": 96764, "start": 980.64, "end": 985.64, "text": " and the eigenvalues went from being, I think, 1 and 1 to 0 and 2,", "tokens": [293, 264, 10446, 46033, 1437, 490, 885, 11, 286, 519, 11, 502, 293, 502, 281, 1958, 293, 568, 11], "temperature": 0.0, "avg_logprob": -0.058058402651832215, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.8851434763055295e-05}, {"id": 242, "seek": 96764, "start": 985.64, "end": 992.64, "text": " which is a huge change in the solution of the eigenvalue problem.", "tokens": [597, 307, 257, 2603, 1319, 294, 264, 3827, 295, 264, 10446, 29155, 1154, 13], "temperature": 0.0, "avg_logprob": -0.058058402651832215, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.8851434763055295e-05}, {"id": 243, "seek": 96764, "start": 992.64, "end": 996.64, "text": " Does anyone remember what that's called?", "tokens": [4402, 2878, 1604, 437, 300, 311, 1219, 30], "temperature": 0.0, "avg_logprob": -0.058058402651832215, "compression_ratio": 1.606694560669456, "no_speech_prob": 2.8851434763055295e-05}, {"id": 244, "seek": 99664, "start": 996.64, "end": 999.64, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 245, "seek": 99664, "start": 999.64, "end": 1000.64, "text": " Stability?", "tokens": [745, 2310, 30], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 246, "seek": 99664, "start": 1000.64, "end": 1002.64, "text": " Stability.", "tokens": [745, 2310, 13], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 247, "seek": 99664, "start": 1002.64, "end": 1004.64, "text": " Yeah, there's actually conditioning and stability,", "tokens": [865, 11, 456, 311, 767, 21901, 293, 11826, 11], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 248, "seek": 99664, "start": 1004.64, "end": 1007.64, "text": " and those terms are sometimes used interchangeably.", "tokens": [293, 729, 2115, 366, 2171, 1143, 30358, 1188, 13], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 249, "seek": 99664, "start": 1007.64, "end": 1010.64, "text": " One refers to a problem and one refers to the algorithm.", "tokens": [1485, 14942, 281, 257, 1154, 293, 472, 14942, 281, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 250, "seek": 99664, "start": 1010.64, "end": 1016.64, "text": " And that's an issue that we'll return to more as the course goes on.", "tokens": [400, 300, 311, 364, 2734, 300, 321, 603, 2736, 281, 544, 382, 264, 1164, 1709, 322, 13], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 251, "seek": 99664, "start": 1016.64, "end": 1018.64, "text": " So, yeah, I think that was a good review.", "tokens": [407, 11, 1338, 11, 286, 519, 300, 390, 257, 665, 3131, 13], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 252, "seek": 99664, "start": 1018.64, "end": 1023.64, "text": " Definitely, kind of stay mindful of those concepts,", "tokens": [12151, 11, 733, 295, 1754, 14618, 295, 729, 10392, 11], "temperature": 0.0, "avg_logprob": -0.15390808066141973, "compression_ratio": 1.5625, "no_speech_prob": 2.840777233359404e-05}, {"id": 253, "seek": 102364, "start": 1023.64, "end": 1049.6399999999999, "text": " so we'll see them show up in different places.", "tokens": [370, 321, 603, 536, 552, 855, 493, 294, 819, 3190, 13], "temperature": 0.0, "avg_logprob": -0.1733822504679362, "compression_ratio": 0.8679245283018868, "no_speech_prob": 2.2826581698609516e-05}, {"id": 254, "seek": 104964, "start": 1049.64, "end": 1055.64, "text": " Okay, so I wanted to return to NMF and SPD, but from a very different perspective.", "tokens": [1033, 11, 370, 286, 1415, 281, 2736, 281, 426, 44, 37, 293, 19572, 11, 457, 490, 257, 588, 819, 4585, 13], "temperature": 0.0, "avg_logprob": -0.06650782081316102, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.045998007815797e-05}, {"id": 255, "seek": 104964, "start": 1055.64, "end": 1059.64, "text": " So I have an Excel notebook, and I've uploaded this to GitHub.", "tokens": [407, 286, 362, 364, 19060, 21060, 11, 293, 286, 600, 17135, 341, 281, 23331, 13], "temperature": 0.0, "avg_logprob": -0.06650782081316102, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.045998007815797e-05}, {"id": 256, "seek": 104964, "start": 1059.64, "end": 1062.64, "text": " It's called BritLit.", "tokens": [467, 311, 1219, 4760, 43, 270, 13], "temperature": 0.0, "avg_logprob": -0.06650782081316102, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.045998007815797e-05}, {"id": 257, "seek": 104964, "start": 1062.64, "end": 1065.64, "text": " And here are all the calculations.", "tokens": [400, 510, 366, 439, 264, 20448, 13], "temperature": 0.0, "avg_logprob": -0.06650782081316102, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.045998007815797e-05}, {"id": 258, "seek": 104964, "start": 1065.64, "end": 1068.64, "text": " I actually did them in a Jupyter notebook with Python,", "tokens": [286, 767, 630, 552, 294, 257, 22125, 88, 391, 21060, 365, 15329, 11], "temperature": 0.0, "avg_logprob": -0.06650782081316102, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.045998007815797e-05}, {"id": 259, "seek": 104964, "start": 1068.64, "end": 1071.64, "text": " but I want to use this as a way to really visualize the matrices", "tokens": [457, 286, 528, 281, 764, 341, 382, 257, 636, 281, 534, 23273, 264, 32284], "temperature": 0.0, "avg_logprob": -0.06650782081316102, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.045998007815797e-05}, {"id": 260, "seek": 104964, "start": 1071.64, "end": 1075.64, "text": " to kind of get a different perspective on what's going on.", "tokens": [281, 733, 295, 483, 257, 819, 4585, 322, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.06650782081316102, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.045998007815797e-05}, {"id": 261, "seek": 107564, "start": 1075.64, "end": 1082.64, "text": " Let me make this larger.", "tokens": [961, 385, 652, 341, 4833, 13], "temperature": 0.0, "avg_logprob": -0.12201518761484247, "compression_ratio": 1.446236559139785, "no_speech_prob": 9.311822213931009e-05}, {"id": 262, "seek": 107564, "start": 1082.64, "end": 1086.64, "text": " Oh, okay, great. Thank you.", "tokens": [876, 11, 1392, 11, 869, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.12201518761484247, "compression_ratio": 1.446236559139785, "no_speech_prob": 9.311822213931009e-05}, {"id": 263, "seek": 107564, "start": 1086.64, "end": 1088.64, "text": " And also at any time, if you're having trouble hearing me", "tokens": [400, 611, 412, 604, 565, 11, 498, 291, 434, 1419, 5253, 4763, 385], "temperature": 0.0, "avg_logprob": -0.12201518761484247, "compression_ratio": 1.446236559139785, "no_speech_prob": 9.311822213931009e-05}, {"id": 264, "seek": 107564, "start": 1088.64, "end": 1092.64, "text": " or having trouble seeing something on the screen, please let me know.", "tokens": [420, 1419, 5253, 2577, 746, 322, 264, 2568, 11, 1767, 718, 385, 458, 13], "temperature": 0.0, "avg_logprob": -0.12201518761484247, "compression_ratio": 1.446236559139785, "no_speech_prob": 9.311822213931009e-05}, {"id": 265, "seek": 107564, "start": 1092.64, "end": 1095.64, "text": " So I can adjust.", "tokens": [407, 286, 393, 4369, 13], "temperature": 0.0, "avg_logprob": -0.12201518761484247, "compression_ratio": 1.446236559139785, "no_speech_prob": 9.311822213931009e-05}, {"id": 266, "seek": 107564, "start": 1095.64, "end": 1101.64, "text": " So here on the left are 27 works of kind of classic British literature.", "tokens": [407, 510, 322, 264, 1411, 366, 7634, 1985, 295, 733, 295, 7230, 6221, 10394, 13], "temperature": 0.0, "avg_logprob": -0.12201518761484247, "compression_ratio": 1.446236559139785, "no_speech_prob": 9.311822213931009e-05}, {"id": 267, "seek": 110164, "start": 1101.64, "end": 1107.64, "text": " They're all written by the author's last name, followed by the beginning of the title.", "tokens": [814, 434, 439, 3720, 538, 264, 3793, 311, 1036, 1315, 11, 6263, 538, 264, 2863, 295, 264, 4876, 13], "temperature": 0.0, "avg_logprob": -0.06677503843565245, "compression_ratio": 1.4489795918367347, "no_speech_prob": 8.138322300510481e-06}, {"id": 268, "seek": 110164, "start": 1107.64, "end": 1113.64, "text": " So some of these, I recognize the second is Jane Austen's Pride and Prejudice,", "tokens": [407, 512, 295, 613, 11, 286, 5521, 264, 1150, 307, 13048, 4126, 268, 311, 30319, 293, 6001, 9218, 573, 11], "temperature": 0.0, "avg_logprob": -0.06677503843565245, "compression_ratio": 1.4489795918367347, "no_speech_prob": 8.138322300510481e-06}, {"id": 269, "seek": 110164, "start": 1113.64, "end": 1117.64, "text": " Sense and Sensibility, Vanity Fair.", "tokens": [33123, 293, 40926, 2841, 11, 8979, 507, 12157, 13], "temperature": 0.0, "avg_logprob": -0.06677503843565245, "compression_ratio": 1.4489795918367347, "no_speech_prob": 8.138322300510481e-06}, {"id": 270, "seek": 110164, "start": 1117.64, "end": 1125.64, "text": " And then along the top are different vocabulary words that showed up in the books.", "tokens": [400, 550, 2051, 264, 1192, 366, 819, 19864, 2283, 300, 4712, 493, 294, 264, 3642, 13], "temperature": 0.0, "avg_logprob": -0.06677503843565245, "compression_ratio": 1.4489795918367347, "no_speech_prob": 8.138322300510481e-06}, {"id": 271, "seek": 112564, "start": 1125.64, "end": 1131.64, "text": " These are, so last time we talked some about a count matrix,", "tokens": [1981, 366, 11, 370, 1036, 565, 321, 2825, 512, 466, 257, 1207, 8141, 11], "temperature": 0.0, "avg_logprob": -0.08499121098291307, "compression_ratio": 1.4213197969543148, "no_speech_prob": 9.817876161832828e-06}, {"id": 272, "seek": 112564, "start": 1131.64, "end": 1136.64, "text": " which would have the counts of how many times the words showed up in each work.", "tokens": [597, 576, 362, 264, 14893, 295, 577, 867, 1413, 264, 2283, 4712, 493, 294, 1184, 589, 13], "temperature": 0.0, "avg_logprob": -0.08499121098291307, "compression_ratio": 1.4213197969543148, "no_speech_prob": 9.817876161832828e-06}, {"id": 273, "seek": 112564, "start": 1136.64, "end": 1140.64, "text": " And this is a TF-IDF.", "tokens": [400, 341, 307, 257, 40964, 12, 2777, 37, 13], "temperature": 0.0, "avg_logprob": -0.08499121098291307, "compression_ratio": 1.4213197969543148, "no_speech_prob": 9.817876161832828e-06}, {"id": 274, "seek": 112564, "start": 1140.64, "end": 1146.64, "text": " Does anyone remember the concept behind TF-IDF?", "tokens": [4402, 2878, 1604, 264, 3410, 2261, 40964, 12, 2777, 37, 30], "temperature": 0.0, "avg_logprob": -0.08499121098291307, "compression_ratio": 1.4213197969543148, "no_speech_prob": 9.817876161832828e-06}, {"id": 275, "seek": 112564, "start": 1146.64, "end": 1150.64, "text": " We kind of went over it pretty quickly.", "tokens": [492, 733, 295, 1437, 670, 309, 1238, 2661, 13], "temperature": 0.0, "avg_logprob": -0.08499121098291307, "compression_ratio": 1.4213197969543148, "no_speech_prob": 9.817876161832828e-06}, {"id": 276, "seek": 112564, "start": 1150.64, "end": 1153.64, "text": " Is that a hand? Or no, sorry.", "tokens": [1119, 300, 257, 1011, 30, 1610, 572, 11, 2597, 13], "temperature": 0.0, "avg_logprob": -0.08499121098291307, "compression_ratio": 1.4213197969543148, "no_speech_prob": 9.817876161832828e-06}, {"id": 277, "seek": 115364, "start": 1153.64, "end": 1158.64, "text": " Okay, so that stands for Term Frequency Inverse Document Frequency.", "tokens": [1033, 11, 370, 300, 7382, 337, 19835, 6142, 48154, 682, 4308, 37684, 6142, 48154, 13], "temperature": 0.0, "avg_logprob": -0.055442097481716884, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.11975454678759e-05}, {"id": 278, "seek": 115364, "start": 1158.64, "end": 1163.64, "text": " And it's basically a way to normalize a term document matrix.", "tokens": [400, 309, 311, 1936, 257, 636, 281, 2710, 1125, 257, 1433, 4166, 8141, 13], "temperature": 0.0, "avg_logprob": -0.055442097481716884, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.11975454678759e-05}, {"id": 279, "seek": 115364, "start": 1163.64, "end": 1168.64, "text": " It takes into account that some words are super common and show up all the time.", "tokens": [467, 2516, 666, 2696, 300, 512, 2283, 366, 1687, 2689, 293, 855, 493, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.055442097481716884, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.11975454678759e-05}, {"id": 280, "seek": 115364, "start": 1168.64, "end": 1175.64, "text": " It also takes into account the length of the documents themselves.", "tokens": [467, 611, 2516, 666, 2696, 264, 4641, 295, 264, 8512, 2969, 13], "temperature": 0.0, "avg_logprob": -0.055442097481716884, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.11975454678759e-05}, {"id": 281, "seek": 115364, "start": 1175.64, "end": 1180.64, "text": " And so this, if we hadn't been using it, these numbers here would be integers.", "tokens": [400, 370, 341, 11, 498, 321, 8782, 380, 668, 1228, 309, 11, 613, 3547, 510, 576, 312, 41674, 13], "temperature": 0.0, "avg_logprob": -0.055442097481716884, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.11975454678759e-05}, {"id": 282, "seek": 118064, "start": 1180.64, "end": 1184.64, "text": " Since we are, this is just kind of a normalized equivalent,", "tokens": [4162, 321, 366, 11, 341, 307, 445, 733, 295, 257, 48704, 10344, 11], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 283, "seek": 118064, "start": 1184.64, "end": 1186.64, "text": " which is taking those things into account.", "tokens": [597, 307, 1940, 729, 721, 666, 2696, 13], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 284, "seek": 118064, "start": 1186.64, "end": 1188.64, "text": " And actually I did this both ways.", "tokens": [400, 767, 286, 630, 341, 1293, 2098, 13], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 285, "seek": 118064, "start": 1188.64, "end": 1194.64, "text": " And when I did it on the counts, I ended up with a lot more of words like,", "tokens": [400, 562, 286, 630, 309, 322, 264, 14893, 11, 286, 4590, 493, 365, 257, 688, 544, 295, 2283, 411, 11], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 286, "seek": 118064, "start": 1194.64, "end": 1197.64, "text": " or his kind of taking more prominence.", "tokens": [420, 702, 733, 295, 1940, 544, 39225, 655, 13], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 287, "seek": 118064, "start": 1197.64, "end": 1201.64, "text": " So I thought it was a little bit more interesting here to kind of give more attention to,", "tokens": [407, 286, 1194, 309, 390, 257, 707, 857, 544, 1880, 510, 281, 733, 295, 976, 544, 3202, 281, 11], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 288, "seek": 118064, "start": 1201.64, "end": 1204.64, "text": " really this kind of gave more attention to the proper names.", "tokens": [534, 341, 733, 295, 2729, 544, 3202, 281, 264, 2296, 5288, 13], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 289, "seek": 118064, "start": 1204.64, "end": 1209.64, "text": " But to take a kind of look at, actually here's a good one, the name Phineas.", "tokens": [583, 281, 747, 257, 733, 295, 574, 412, 11, 767, 510, 311, 257, 665, 472, 11, 264, 1315, 2623, 533, 296, 13], "temperature": 0.0, "avg_logprob": -0.08936957425849382, "compression_ratio": 1.7545787545787546, "no_speech_prob": 1.5206142961687874e-05}, {"id": 290, "seek": 120964, "start": 1209.64, "end": 1212.64, "text": " Is zero for most of these.", "tokens": [1119, 4018, 337, 881, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.07441879573621248, "compression_ratio": 1.471111111111111, "no_speech_prob": 1.6700627384125255e-05}, {"id": 291, "seek": 120964, "start": 1212.64, "end": 1215.64, "text": " It's the largest for a book titled Phineas.", "tokens": [467, 311, 264, 6443, 337, 257, 1446, 19841, 2623, 533, 296, 13], "temperature": 0.0, "avg_logprob": -0.07441879573621248, "compression_ratio": 1.471111111111111, "no_speech_prob": 1.6700627384125255e-05}, {"id": 292, "seek": 120964, "start": 1215.64, "end": 1218.64, "text": " So that fits with what we would expect.", "tokens": [407, 300, 9001, 365, 437, 321, 576, 2066, 13], "temperature": 0.0, "avg_logprob": -0.07441879573621248, "compression_ratio": 1.471111111111111, "no_speech_prob": 1.6700627384125255e-05}, {"id": 293, "seek": 120964, "start": 1218.64, "end": 1227.64, "text": " Another one we can check is Kathy Linton is the protagonist of Wuthering Heights.", "tokens": [3996, 472, 321, 393, 1520, 307, 30740, 441, 12442, 307, 264, 24506, 295, 343, 17696, 278, 44039, 13], "temperature": 0.0, "avg_logprob": -0.07441879573621248, "compression_ratio": 1.471111111111111, "no_speech_prob": 1.6700627384125255e-05}, {"id": 294, "seek": 120964, "start": 1227.64, "end": 1232.64, "text": " And so you'll see Linton shows up in a few books, but it's much, actually make this even bigger.", "tokens": [400, 370, 291, 603, 536, 441, 12442, 3110, 493, 294, 257, 1326, 3642, 11, 457, 309, 311, 709, 11, 767, 652, 341, 754, 3801, 13], "temperature": 0.0, "avg_logprob": -0.07441879573621248, "compression_ratio": 1.471111111111111, "no_speech_prob": 1.6700627384125255e-05}, {"id": 295, "seek": 120964, "start": 1232.64, "end": 1237.64, "text": " Are you all able to see the numbers okay?", "tokens": [2014, 291, 439, 1075, 281, 536, 264, 3547, 1392, 30], "temperature": 0.0, "avg_logprob": -0.07441879573621248, "compression_ratio": 1.471111111111111, "no_speech_prob": 1.6700627384125255e-05}, {"id": 296, "seek": 123764, "start": 1237.64, "end": 1247.64, "text": " So Linton is 0.4 for Wuthering Heights and then zero for most, 0.001, 0.0004 and some.", "tokens": [407, 441, 12442, 307, 1958, 13, 19, 337, 343, 17696, 278, 44039, 293, 550, 4018, 337, 881, 11, 1958, 13, 628, 16, 11, 1958, 13, 1360, 19, 293, 512, 13], "temperature": 0.0, "avg_logprob": -0.07229944033043406, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.8625264500733465e-06}, {"id": 297, "seek": 123764, "start": 1247.64, "end": 1250.64, "text": " So this seems reasonable of what we would get.", "tokens": [407, 341, 2544, 10585, 295, 437, 321, 576, 483, 13], "temperature": 0.0, "avg_logprob": -0.07229944033043406, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.8625264500733465e-06}, {"id": 298, "seek": 123764, "start": 1250.64, "end": 1256.64, "text": " And so this is kind of when we were talking about representing a class of documents as a matrix.", "tokens": [400, 370, 341, 307, 733, 295, 562, 321, 645, 1417, 466, 13460, 257, 1508, 295, 8512, 382, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07229944033043406, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.8625264500733465e-06}, {"id": 299, "seek": 123764, "start": 1256.64, "end": 1257.64, "text": " This is what we were doing.", "tokens": [639, 307, 437, 321, 645, 884, 13], "temperature": 0.0, "avg_logprob": -0.07229944033043406, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.8625264500733465e-06}, {"id": 300, "seek": 123764, "start": 1257.64, "end": 1259.64, "text": " And we don't have any information about the syntax.", "tokens": [400, 321, 500, 380, 362, 604, 1589, 466, 264, 28431, 13], "temperature": 0.0, "avg_logprob": -0.07229944033043406, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.8625264500733465e-06}, {"id": 301, "seek": 123764, "start": 1259.64, "end": 1264.64, "text": " It's really just about kind of the word frequencies in these different documents.", "tokens": [467, 311, 534, 445, 466, 733, 295, 264, 1349, 20250, 294, 613, 819, 8512, 13], "temperature": 0.0, "avg_logprob": -0.07229944033043406, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.8625264500733465e-06}, {"id": 302, "seek": 126464, "start": 1264.64, "end": 1268.64, "text": " So I thought this was nice to be able to visualize it.", "tokens": [407, 286, 1194, 341, 390, 1481, 281, 312, 1075, 281, 23273, 309, 13], "temperature": 0.0, "avg_logprob": -0.09479842060490658, "compression_ratio": 1.3442622950819672, "no_speech_prob": 1.644182157178875e-05}, {"id": 303, "seek": 126464, "start": 1268.64, "end": 1274.64, "text": " Any questions just about this representation?", "tokens": [2639, 1651, 445, 466, 341, 10290, 30], "temperature": 0.0, "avg_logprob": -0.09479842060490658, "compression_ratio": 1.3442622950819672, "no_speech_prob": 1.644182157178875e-05}, {"id": 304, "seek": 126464, "start": 1274.64, "end": 1281.64, "text": " Okay, so let's go to SVD.", "tokens": [1033, 11, 370, 718, 311, 352, 281, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.09479842060490658, "compression_ratio": 1.3442622950819672, "no_speech_prob": 1.644182157178875e-05}, {"id": 305, "seek": 126464, "start": 1281.64, "end": 1287.64, "text": " So here with SVD, we've gotten the U matrix back, which is, so again, we have 27 documents.", "tokens": [407, 510, 365, 31910, 35, 11, 321, 600, 5768, 264, 624, 8141, 646, 11, 597, 307, 11, 370, 797, 11, 321, 362, 7634, 8512, 13], "temperature": 0.0, "avg_logprob": -0.09479842060490658, "compression_ratio": 1.3442622950819672, "no_speech_prob": 1.644182157178875e-05}, {"id": 306, "seek": 126464, "start": 1287.64, "end": 1291.64, "text": " It's 27 by 10 in this case.", "tokens": [467, 311, 7634, 538, 1266, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.09479842060490658, "compression_ratio": 1.3442622950819672, "no_speech_prob": 1.644182157178875e-05}, {"id": 307, "seek": 129164, "start": 1291.64, "end": 1301.64, "text": " And can anyone tell me what properties this matrix U has?", "tokens": [400, 393, 2878, 980, 385, 437, 7221, 341, 8141, 624, 575, 30], "temperature": 0.0, "avg_logprob": -0.1311626434326172, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.42201314601698e-06}, {"id": 308, "seek": 129164, "start": 1301.64, "end": 1304.64, "text": " Orthonormal, at least we're talking about orthonormal.", "tokens": [1610, 11943, 24440, 11, 412, 1935, 321, 434, 1417, 466, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.1311626434326172, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.42201314601698e-06}, {"id": 309, "seek": 129164, "start": 1304.64, "end": 1306.64, "text": " Yeah, the columns are orthonormal.", "tokens": [865, 11, 264, 13766, 366, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.1311626434326172, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.42201314601698e-06}, {"id": 310, "seek": 129164, "start": 1306.64, "end": 1308.64, "text": " Exactly. Thank you.", "tokens": [7587, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.1311626434326172, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.42201314601698e-06}, {"id": 311, "seek": 129164, "start": 1308.64, "end": 1311.64, "text": " And so we can check that in Excel.", "tokens": [400, 370, 321, 393, 1520, 300, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.1311626434326172, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.42201314601698e-06}, {"id": 312, "seek": 129164, "start": 1311.64, "end": 1313.64, "text": " So I come down here.", "tokens": [407, 286, 808, 760, 510, 13], "temperature": 0.0, "avg_logprob": -0.1311626434326172, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.42201314601698e-06}, {"id": 313, "seek": 129164, "start": 1313.64, "end": 1318.64, "text": " And so I looked at the correlations between the columns of U.", "tokens": [400, 370, 286, 2956, 412, 264, 13983, 763, 1296, 264, 13766, 295, 624, 13], "temperature": 0.0, "avg_logprob": -0.1311626434326172, "compression_ratio": 1.507936507936508, "no_speech_prob": 5.42201314601698e-06}, {"id": 314, "seek": 131864, "start": 1318.64, "end": 1330.64, "text": " And so here, if you can see this, this is doing the sum product, which is basically what Excel calls the dot product of B2 to B28,", "tokens": [400, 370, 510, 11, 498, 291, 393, 536, 341, 11, 341, 307, 884, 264, 2408, 1674, 11, 597, 307, 1936, 437, 19060, 5498, 264, 5893, 1674, 295, 363, 17, 281, 363, 11205, 11], "temperature": 0.0, "avg_logprob": -0.08095481603041939, "compression_ratio": 1.6485148514851484, "no_speech_prob": 2.9772056223009713e-05}, {"id": 315, "seek": 131864, "start": 1330.64, "end": 1334.64, "text": " which is this column in blue with itself.", "tokens": [597, 307, 341, 7738, 294, 3344, 365, 2564, 13], "temperature": 0.0, "avg_logprob": -0.08095481603041939, "compression_ratio": 1.6485148514851484, "no_speech_prob": 2.9772056223009713e-05}, {"id": 316, "seek": 131864, "start": 1334.64, "end": 1335.64, "text": " And we got one.", "tokens": [400, 321, 658, 472, 13], "temperature": 0.0, "avg_logprob": -0.08095481603041939, "compression_ratio": 1.6485148514851484, "no_speech_prob": 2.9772056223009713e-05}, {"id": 317, "seek": 131864, "start": 1335.64, "end": 1341.64, "text": " And then here we're doing it with that column to the column next to it.", "tokens": [400, 550, 510, 321, 434, 884, 309, 365, 300, 7738, 281, 264, 7738, 958, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.08095481603041939, "compression_ratio": 1.6485148514851484, "no_speech_prob": 2.9772056223009713e-05}, {"id": 318, "seek": 131864, "start": 1341.64, "end": 1346.64, "text": " And we get something 10 to the negative 16th, which is practically zero.", "tokens": [400, 321, 483, 746, 1266, 281, 264, 3671, 3165, 392, 11, 597, 307, 15667, 4018, 13], "temperature": 0.0, "avg_logprob": -0.08095481603041939, "compression_ratio": 1.6485148514851484, "no_speech_prob": 2.9772056223009713e-05}, {"id": 319, "seek": 134664, "start": 1346.64, "end": 1349.64, "text": " And kind of ditto as we scroll over.", "tokens": [400, 733, 295, 274, 34924, 382, 321, 11369, 670, 13], "temperature": 0.0, "avg_logprob": -0.06422006000172008, "compression_ratio": 1.4748858447488584, "no_speech_prob": 7.888971595093608e-06}, {"id": 320, "seek": 134664, "start": 1349.64, "end": 1354.64, "text": " And when you click on the formula in Excel, it highlights which terms are being used.", "tokens": [400, 562, 291, 2052, 322, 264, 8513, 294, 19060, 11, 309, 14254, 597, 2115, 366, 885, 1143, 13], "temperature": 0.0, "avg_logprob": -0.06422006000172008, "compression_ratio": 1.4748858447488584, "no_speech_prob": 7.888971595093608e-06}, {"id": 321, "seek": 134664, "start": 1354.64, "end": 1361.64, "text": " So the dot product of these two arrays, 10 to the negative 16th, close to zero.", "tokens": [407, 264, 5893, 1674, 295, 613, 732, 41011, 11, 1266, 281, 264, 3671, 3165, 392, 11, 1998, 281, 4018, 13], "temperature": 0.0, "avg_logprob": -0.06422006000172008, "compression_ratio": 1.4748858447488584, "no_speech_prob": 7.888971595093608e-06}, {"id": 322, "seek": 134664, "start": 1361.64, "end": 1368.64, "text": " So this fits with what we would expect for you having orthonormal columns.", "tokens": [407, 341, 9001, 365, 437, 321, 576, 2066, 337, 291, 1419, 420, 11943, 24440, 13766, 13], "temperature": 0.0, "avg_logprob": -0.06422006000172008, "compression_ratio": 1.4748858447488584, "no_speech_prob": 7.888971595093608e-06}, {"id": 323, "seek": 134664, "start": 1368.64, "end": 1371.64, "text": " And then, and Kelsey said it was orthonormal.", "tokens": [400, 550, 11, 293, 44714, 848, 309, 390, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.06422006000172008, "compression_ratio": 1.4748858447488584, "no_speech_prob": 7.888971595093608e-06}, {"id": 324, "seek": 137164, "start": 1371.64, "end": 1379.64, "text": " This is true if we were to have 27 columns, then the rows would also be orthonormal with each other.", "tokens": [639, 307, 2074, 498, 321, 645, 281, 362, 7634, 13766, 11, 550, 264, 13241, 576, 611, 312, 420, 11943, 24440, 365, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.06666239296517722, "compression_ratio": 1.3826530612244898, "no_speech_prob": 1.5779468185428414e-06}, {"id": 325, "seek": 137164, "start": 1379.64, "end": 1387.64, "text": " Here they're not since I've kind of chopped it off at 10 just to save space.", "tokens": [1692, 436, 434, 406, 1670, 286, 600, 733, 295, 16497, 309, 766, 412, 1266, 445, 281, 3155, 1901, 13], "temperature": 0.0, "avg_logprob": -0.06666239296517722, "compression_ratio": 1.3826530612244898, "no_speech_prob": 1.5779468185428414e-06}, {"id": 326, "seek": 137164, "start": 1387.64, "end": 1390.64, "text": " So that's U.", "tokens": [407, 300, 311, 624, 13], "temperature": 0.0, "avg_logprob": -0.06666239296517722, "compression_ratio": 1.3826530612244898, "no_speech_prob": 1.5779468185428414e-06}, {"id": 327, "seek": 137164, "start": 1390.64, "end": 1396.64, "text": " Next, we have a matrix S. And what's special about S?", "tokens": [3087, 11, 321, 362, 257, 8141, 318, 13, 400, 437, 311, 2121, 466, 318, 30], "temperature": 0.0, "avg_logprob": -0.06666239296517722, "compression_ratio": 1.3826530612244898, "no_speech_prob": 1.5779468185428414e-06}, {"id": 328, "seek": 137164, "start": 1396.64, "end": 1399.64, "text": " You can just shout it out.", "tokens": [509, 393, 445, 8043, 309, 484, 13], "temperature": 0.0, "avg_logprob": -0.06666239296517722, "compression_ratio": 1.3826530612244898, "no_speech_prob": 1.5779468185428414e-06}, {"id": 329, "seek": 139964, "start": 1399.64, "end": 1402.64, "text": " It's diagonal, yes.", "tokens": [467, 311, 21539, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.06718590411734074, "compression_ratio": 1.5804878048780489, "no_speech_prob": 7.646309313713573e-06}, {"id": 330, "seek": 139964, "start": 1402.64, "end": 1405.64, "text": " So S is not the most exciting, but it's nice.", "tokens": [407, 318, 307, 406, 264, 881, 4670, 11, 457, 309, 311, 1481, 13], "temperature": 0.0, "avg_logprob": -0.06718590411734074, "compression_ratio": 1.5804878048780489, "no_speech_prob": 7.646309313713573e-06}, {"id": 331, "seek": 139964, "start": 1405.64, "end": 1406.64, "text": " It's simple.", "tokens": [467, 311, 2199, 13], "temperature": 0.0, "avg_logprob": -0.06718590411734074, "compression_ratio": 1.5804878048780489, "no_speech_prob": 7.646309313713573e-06}, {"id": 332, "seek": 139964, "start": 1406.64, "end": 1407.64, "text": " It's diagonal.", "tokens": [467, 311, 21539, 13], "temperature": 0.0, "avg_logprob": -0.06718590411734074, "compression_ratio": 1.5804878048780489, "no_speech_prob": 7.646309313713573e-06}, {"id": 333, "seek": 139964, "start": 1407.64, "end": 1413.64, "text": " It's also ordered such that the largest value is first and they're in descending order.", "tokens": [467, 311, 611, 8866, 1270, 300, 264, 6443, 2158, 307, 700, 293, 436, 434, 294, 40182, 1668, 13], "temperature": 0.0, "avg_logprob": -0.06718590411734074, "compression_ratio": 1.5804878048780489, "no_speech_prob": 7.646309313713573e-06}, {"id": 334, "seek": 139964, "start": 1413.64, "end": 1419.64, "text": " And S kind of intuitively gives us a notion of importance.", "tokens": [400, 318, 733, 295, 46506, 2709, 505, 257, 10710, 295, 7379, 13], "temperature": 0.0, "avg_logprob": -0.06718590411734074, "compression_ratio": 1.5804878048780489, "no_speech_prob": 7.646309313713573e-06}, {"id": 335, "seek": 139964, "start": 1419.64, "end": 1424.64, "text": " Here it's also what allows us to kind of get, since orthonormal matrices, you know,", "tokens": [1692, 309, 311, 611, 437, 4045, 505, 281, 733, 295, 483, 11, 1670, 420, 11943, 24440, 32284, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.06718590411734074, "compression_ratio": 1.5804878048780489, "no_speech_prob": 7.646309313713573e-06}, {"id": 336, "seek": 142464, "start": 1424.64, "end": 1430.64, "text": " we're getting these dot products of zero or one, since our original matrix could have any values.", "tokens": [321, 434, 1242, 613, 5893, 3383, 295, 4018, 420, 472, 11, 1670, 527, 3380, 8141, 727, 362, 604, 4190, 13], "temperature": 0.0, "avg_logprob": -0.06791058339570698, "compression_ratio": 1.6108949416342413, "no_speech_prob": 2.3687480279477313e-06}, {"id": 337, "seek": 142464, "start": 1430.64, "end": 1434.64, "text": " And we would see this even more if we had done the one of the raw counts.", "tokens": [400, 321, 576, 536, 341, 754, 544, 498, 321, 632, 1096, 264, 472, 295, 264, 8936, 14893, 13], "temperature": 0.0, "avg_logprob": -0.06791058339570698, "compression_ratio": 1.6108949416342413, "no_speech_prob": 2.3687480279477313e-06}, {"id": 338, "seek": 142464, "start": 1434.64, "end": 1439.64, "text": " In order to kind of get some magnitude back in there, we need to be able to multiply by numbers bigger than one.", "tokens": [682, 1668, 281, 733, 295, 483, 512, 15668, 646, 294, 456, 11, 321, 643, 281, 312, 1075, 281, 12972, 538, 3547, 3801, 813, 472, 13], "temperature": 0.0, "avg_logprob": -0.06791058339570698, "compression_ratio": 1.6108949416342413, "no_speech_prob": 2.3687480279477313e-06}, {"id": 339, "seek": 142464, "start": 1439.64, "end": 1443.64, "text": " And S is letting us do that.", "tokens": [400, 318, 307, 8295, 505, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.06791058339570698, "compression_ratio": 1.6108949416342413, "no_speech_prob": 2.3687480279477313e-06}, {"id": 340, "seek": 142464, "start": 1443.64, "end": 1445.64, "text": " So this is S.", "tokens": [407, 341, 307, 318, 13], "temperature": 0.0, "avg_logprob": -0.06791058339570698, "compression_ratio": 1.6108949416342413, "no_speech_prob": 2.3687480279477313e-06}, {"id": 341, "seek": 142464, "start": 1445.64, "end": 1453.64, "text": " And then this is V. And V might remind you of U in that it's rows or orthonormal here.", "tokens": [400, 550, 341, 307, 691, 13, 400, 691, 1062, 4160, 291, 295, 624, 294, 300, 309, 311, 13241, 420, 420, 11943, 24440, 510, 13], "temperature": 0.0, "avg_logprob": -0.06791058339570698, "compression_ratio": 1.6108949416342413, "no_speech_prob": 2.3687480279477313e-06}, {"id": 342, "seek": 145364, "start": 1453.64, "end": 1457.64, "text": " And so you can see kind of the dimensions of these.", "tokens": [400, 370, 291, 393, 536, 733, 295, 264, 12819, 295, 613, 13], "temperature": 0.0, "avg_logprob": -0.0642443233066135, "compression_ratio": 1.6335403726708075, "no_speech_prob": 2.947914254036732e-06}, {"id": 343, "seek": 145364, "start": 1457.64, "end": 1464.64, "text": " So U was the titles of the works by topics.", "tokens": [407, 624, 390, 264, 12992, 295, 264, 1985, 538, 8378, 13], "temperature": 0.0, "avg_logprob": -0.0642443233066135, "compression_ratio": 1.6335403726708075, "no_speech_prob": 2.947914254036732e-06}, {"id": 344, "seek": 145364, "start": 1464.64, "end": 1467.64, "text": " And note that we're kind of assigning the meaning topics.", "tokens": [400, 3637, 300, 321, 434, 733, 295, 49602, 264, 3620, 8378, 13], "temperature": 0.0, "avg_logprob": -0.0642443233066135, "compression_ratio": 1.6335403726708075, "no_speech_prob": 2.947914254036732e-06}, {"id": 345, "seek": 145364, "start": 1467.64, "end": 1469.64, "text": " You know, that wasn't anywhere in our input.", "tokens": [509, 458, 11, 300, 2067, 380, 4992, 294, 527, 4846, 13], "temperature": 0.0, "avg_logprob": -0.0642443233066135, "compression_ratio": 1.6335403726708075, "no_speech_prob": 2.947914254036732e-06}, {"id": 346, "seek": 145364, "start": 1469.64, "end": 1475.64, "text": " And that's a kind of notion that makes sense for this dimension.", "tokens": [400, 300, 311, 257, 733, 295, 10710, 300, 1669, 2020, 337, 341, 10139, 13], "temperature": 0.0, "avg_logprob": -0.0642443233066135, "compression_ratio": 1.6335403726708075, "no_speech_prob": 2.947914254036732e-06}, {"id": 347, "seek": 147564, "start": 1475.64, "end": 1483.64, "text": " Then S is topics by topics and V is topics by words.", "tokens": [1396, 318, 307, 8378, 538, 8378, 293, 691, 307, 8378, 538, 2283, 13], "temperature": 0.0, "avg_logprob": -0.0677187631004735, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.340462399137323e-06}, {"id": 348, "seek": 147564, "start": 1483.64, "end": 1488.64, "text": " And so here it's I think we should look at a few examples.", "tokens": [400, 370, 510, 309, 311, 286, 519, 321, 820, 574, 412, 257, 1326, 5110, 13], "temperature": 0.0, "avg_logprob": -0.0677187631004735, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.340462399137323e-06}, {"id": 349, "seek": 147564, "start": 1488.64, "end": 1490.64, "text": " We can kind of go backwards.", "tokens": [492, 393, 733, 295, 352, 12204, 13], "temperature": 0.0, "avg_logprob": -0.0677187631004735, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.340462399137323e-06}, {"id": 350, "seek": 147564, "start": 1490.64, "end": 1497.64, "text": " So if we look at Darcy, well, that shows up in several.", "tokens": [407, 498, 321, 574, 412, 7803, 1344, 11, 731, 11, 300, 3110, 493, 294, 2940, 13], "temperature": 0.0, "avg_logprob": -0.0677187631004735, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.340462399137323e-06}, {"id": 351, "seek": 147564, "start": 1497.64, "end": 1502.64, "text": " It kind of shows up most in topic four and topic seven.", "tokens": [467, 733, 295, 3110, 493, 881, 294, 4829, 1451, 293, 4829, 3407, 13], "temperature": 0.0, "avg_logprob": -0.0677187631004735, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.340462399137323e-06}, {"id": 352, "seek": 150264, "start": 1502.64, "end": 1509.64, "text": " So we might expect Pride and Prejudice to have a lot of topic four and topic seven.", "tokens": [407, 321, 1062, 2066, 30319, 293, 6001, 9218, 573, 281, 362, 257, 688, 295, 4829, 1451, 293, 4829, 3407, 13], "temperature": 0.0, "avg_logprob": -0.08533755055180302, "compression_ratio": 1.8173076923076923, "no_speech_prob": 9.817930731514934e-06}, {"id": 353, "seek": 150264, "start": 1509.64, "end": 1513.64, "text": " You can check that hypothesis.", "tokens": [509, 393, 1520, 300, 17291, 13], "temperature": 0.0, "avg_logprob": -0.08533755055180302, "compression_ratio": 1.8173076923076923, "no_speech_prob": 9.817930731514934e-06}, {"id": 354, "seek": 150264, "start": 1513.64, "end": 1515.64, "text": " Well, Pride and Prejudice has a lot of topic two as well.", "tokens": [1042, 11, 30319, 293, 6001, 9218, 573, 575, 257, 688, 295, 4829, 732, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.08533755055180302, "compression_ratio": 1.8173076923076923, "no_speech_prob": 9.817930731514934e-06}, {"id": 355, "seek": 150264, "start": 1515.64, "end": 1518.64, "text": " It does have a lot of topic four.", "tokens": [467, 775, 362, 257, 688, 295, 4829, 1451, 13], "temperature": 0.0, "avg_logprob": -0.08533755055180302, "compression_ratio": 1.8173076923076923, "no_speech_prob": 9.817930731514934e-06}, {"id": 356, "seek": 150264, "start": 1518.64, "end": 1521.64, "text": " What? Oh, negative of topic two.", "tokens": [708, 30, 876, 11, 3671, 295, 4829, 732, 13], "temperature": 0.0, "avg_logprob": -0.08533755055180302, "compression_ratio": 1.8173076923076923, "no_speech_prob": 9.817930731514934e-06}, {"id": 357, "seek": 150264, "start": 1521.64, "end": 1524.64, "text": " OK, so yeah, the largest values are for topic four and topic seven.", "tokens": [2264, 11, 370, 1338, 11, 264, 6443, 4190, 366, 337, 4829, 1451, 293, 4829, 3407, 13], "temperature": 0.0, "avg_logprob": -0.08533755055180302, "compression_ratio": 1.8173076923076923, "no_speech_prob": 9.817930731514934e-06}, {"id": 358, "seek": 150264, "start": 1524.64, "end": 1528.64, "text": " So we saw the word Darcy was very noticeable in topics four and seven.", "tokens": [407, 321, 1866, 264, 1349, 7803, 1344, 390, 588, 26041, 294, 8378, 1451, 293, 3407, 13], "temperature": 0.0, "avg_logprob": -0.08533755055180302, "compression_ratio": 1.8173076923076923, "no_speech_prob": 9.817930731514934e-06}, {"id": 359, "seek": 152864, "start": 1528.64, "end": 1532.64, "text": " Pride and Prejudice has a lot of topic four and topic seven.", "tokens": [30319, 293, 6001, 9218, 573, 575, 257, 688, 295, 4829, 1451, 293, 4829, 3407, 13], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 360, "seek": 152864, "start": 1532.64, "end": 1535.64, "text": " Yeah, as Jeremy pointed out, I misread this.", "tokens": [865, 11, 382, 17809, 10932, 484, 11, 286, 3346, 2538, 341, 13], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 361, "seek": 152864, "start": 1535.64, "end": 1540.64, "text": " Topic two is negatively present in Pride and Prejudice,", "tokens": [8840, 299, 732, 307, 29519, 1974, 294, 30319, 293, 6001, 9218, 573, 11], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 362, "seek": 152864, "start": 1540.64, "end": 1543.64, "text": " which it's kind of hard to think about what that means.", "tokens": [597, 309, 311, 733, 295, 1152, 281, 519, 466, 437, 300, 1355, 13], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 363, "seek": 152864, "start": 1543.64, "end": 1545.64, "text": " This is one of the downsides of SPD.", "tokens": [639, 307, 472, 295, 264, 21554, 1875, 295, 19572, 13], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 364, "seek": 152864, "start": 1545.64, "end": 1551.64, "text": " There's probably less intuitive meaning there to talk about a book having a negative topic.", "tokens": [821, 311, 1391, 1570, 21769, 3620, 456, 281, 751, 466, 257, 1446, 1419, 257, 3671, 4829, 13], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 365, "seek": 152864, "start": 1551.64, "end": 1553.64, "text": " And let's do it. And actually, I should check.", "tokens": [400, 718, 311, 360, 309, 13, 400, 767, 11, 286, 820, 1520, 13], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 366, "seek": 152864, "start": 1553.64, "end": 1557.64, "text": " Do any of you have any favorite British novels that are showing up on here", "tokens": [1144, 604, 295, 291, 362, 604, 2954, 6221, 24574, 300, 366, 4099, 493, 322, 510], "temperature": 0.0, "avg_logprob": -0.07649274099440802, "compression_ratio": 1.6537102473498233, "no_speech_prob": 4.425334736879449e-06}, {"id": 367, "seek": 155764, "start": 1557.64, "end": 1562.64, "text": " that you want to suggest a word from?", "tokens": [300, 291, 528, 281, 3402, 257, 1349, 490, 30], "temperature": 0.0, "avg_logprob": -0.12489046833731911, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.00010228515020571649}, {"id": 368, "seek": 155764, "start": 1562.64, "end": 1566.64, "text": " Let me check. I do not think so.", "tokens": [961, 385, 1520, 13, 286, 360, 406, 519, 370, 13], "temperature": 0.0, "avg_logprob": -0.12489046833731911, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.00010228515020571649}, {"id": 369, "seek": 155764, "start": 1566.64, "end": 1570.64, "text": " Yeah, this is kind of a bit older.", "tokens": [865, 11, 341, 307, 733, 295, 257, 857, 4906, 13], "temperature": 0.0, "avg_logprob": -0.12489046833731911, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.00010228515020571649}, {"id": 370, "seek": 155764, "start": 1570.64, "end": 1578.64, "text": " Yeah, I did have to Google a few of these as I was kind of sanity checking my data to see.", "tokens": [865, 11, 286, 630, 362, 281, 3329, 257, 1326, 295, 613, 382, 286, 390, 733, 295, 47892, 8568, 452, 1412, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.12489046833731911, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.00010228515020571649}, {"id": 371, "seek": 155764, "start": 1578.64, "end": 1580.64, "text": " Let's take, oh.", "tokens": [961, 311, 747, 11, 1954, 13], "temperature": 0.0, "avg_logprob": -0.12489046833731911, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.00010228515020571649}, {"id": 372, "seek": 155764, "start": 1580.64, "end": 1585.64, "text": " I know a lot of people say negatives don't make sense in topic modeling.", "tokens": [286, 458, 257, 688, 295, 561, 584, 40019, 500, 380, 652, 2020, 294, 4829, 15983, 13], "temperature": 0.0, "avg_logprob": -0.12489046833731911, "compression_ratio": 1.4690721649484537, "no_speech_prob": 0.00010228515020571649}, {"id": 373, "seek": 158564, "start": 1585.64, "end": 1587.64, "text": " Let's take a look at that.", "tokens": [961, 311, 747, 257, 574, 412, 300, 13], "temperature": 0.0, "avg_logprob": -0.17152039210001627, "compression_ratio": 1.5879828326180256, "no_speech_prob": 5.338045411917847e-06}, {"id": 374, "seek": 158564, "start": 1587.64, "end": 1594.64, "text": " If you've got a really miserable book and you've got a topic which is like happy and joy,", "tokens": [759, 291, 600, 658, 257, 534, 22321, 1446, 293, 291, 600, 658, 257, 4829, 597, 307, 411, 2055, 293, 6258, 11], "temperature": 0.0, "avg_logprob": -0.17152039210001627, "compression_ratio": 1.5879828326180256, "no_speech_prob": 5.338045411917847e-06}, {"id": 375, "seek": 158564, "start": 1594.64, "end": 1597.64, "text": " then there would probably be a negative correlation.", "tokens": [550, 456, 576, 1391, 312, 257, 3671, 20009, 13], "temperature": 0.0, "avg_logprob": -0.17152039210001627, "compression_ratio": 1.5879828326180256, "no_speech_prob": 5.338045411917847e-06}, {"id": 376, "seek": 158564, "start": 1597.64, "end": 1598.64, "text": " Sometimes it maybe makes sense.", "tokens": [4803, 309, 1310, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.17152039210001627, "compression_ratio": 1.5879828326180256, "no_speech_prob": 5.338045411917847e-06}, {"id": 377, "seek": 158564, "start": 1598.64, "end": 1606.64, "text": " So let's look at topic two and see if it reminds us of the opposite of Pride and Prejudice.", "tokens": [407, 718, 311, 574, 412, 4829, 732, 293, 536, 498, 309, 12025, 505, 295, 264, 6182, 295, 30319, 293, 6001, 9218, 573, 13], "temperature": 0.0, "avg_logprob": -0.17152039210001627, "compression_ratio": 1.5879828326180256, "no_speech_prob": 5.338045411917847e-06}, {"id": 378, "seek": 158564, "start": 1606.64, "end": 1613.64, "text": " Let me see if anything stands out as being a particularly large number here.", "tokens": [961, 385, 536, 498, 1340, 7382, 484, 382, 885, 257, 4098, 2416, 1230, 510, 13], "temperature": 0.0, "avg_logprob": -0.17152039210001627, "compression_ratio": 1.5879828326180256, "no_speech_prob": 5.338045411917847e-06}, {"id": 379, "seek": 161364, "start": 1613.64, "end": 1617.64, "text": " I bet some Dickens book would have like industry type topics.", "tokens": [286, 778, 512, 18754, 694, 1446, 576, 362, 411, 3518, 2010, 8378, 13], "temperature": 0.0, "avg_logprob": -0.16619313756624857, "compression_ratio": 1.4838709677419355, "no_speech_prob": 1.4970659321988933e-05}, {"id": 380, "seek": 161364, "start": 1617.64, "end": 1619.64, "text": " Probably Pride and Prejudice would have a lot about factories.", "tokens": [9210, 30319, 293, 6001, 9218, 573, 576, 362, 257, 688, 466, 24813, 13], "temperature": 0.0, "avg_logprob": -0.16619313756624857, "compression_ratio": 1.4838709677419355, "no_speech_prob": 1.4970659321988933e-05}, {"id": 381, "seek": 161364, "start": 1619.64, "end": 1625.64, "text": " That's true. Yeah.", "tokens": [663, 311, 2074, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.16619313756624857, "compression_ratio": 1.4838709677419355, "no_speech_prob": 1.4970659321988933e-05}, {"id": 382, "seek": 161364, "start": 1625.64, "end": 1629.64, "text": " Yeah, this is harder because it's a lot of names.", "tokens": [865, 11, 341, 307, 6081, 570, 309, 311, 257, 688, 295, 5288, 13], "temperature": 0.0, "avg_logprob": -0.16619313756624857, "compression_ratio": 1.4838709677419355, "no_speech_prob": 1.4970659321988933e-05}, {"id": 383, "seek": 161364, "start": 1629.64, "end": 1633.64, "text": " I don't know. Finneas is, oh, well, although, so you can also get these double negatives.", "tokens": [286, 500, 380, 458, 13, 3773, 716, 296, 307, 11, 1954, 11, 731, 11, 4878, 11, 370, 291, 393, 611, 483, 613, 3834, 40019, 13], "temperature": 0.0, "avg_logprob": -0.16619313756624857, "compression_ratio": 1.4838709677419355, "no_speech_prob": 1.4970659321988933e-05}, {"id": 384, "seek": 161364, "start": 1633.64, "end": 1641.64, "text": " So topic two is negative 0.15 Finneas.", "tokens": [407, 4829, 732, 307, 3671, 1958, 13, 5211, 3773, 716, 296, 13], "temperature": 0.0, "avg_logprob": -0.16619313756624857, "compression_ratio": 1.4838709677419355, "no_speech_prob": 1.4970659321988933e-05}, {"id": 385, "seek": 164164, "start": 1641.64, "end": 1650.64, "text": " So I would actually expect something with Finneas in it to then have, I guess, negative of topic two to get negative of a negative.", "tokens": [407, 286, 576, 767, 2066, 746, 365, 3773, 716, 296, 294, 309, 281, 550, 362, 11, 286, 2041, 11, 3671, 295, 4829, 732, 281, 483, 3671, 295, 257, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1474473580070164, "compression_ratio": 1.5432692307692308, "no_speech_prob": 3.089395931965555e-06}, {"id": 386, "seek": 164164, "start": 1650.64, "end": 1654.64, "text": " Toby is big.", "tokens": [40223, 307, 955, 13], "temperature": 0.0, "avg_logprob": -0.1474473580070164, "compression_ratio": 1.5432692307692308, "no_speech_prob": 3.089395931965555e-06}, {"id": 387, "seek": 164164, "start": 1654.64, "end": 1656.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1474473580070164, "compression_ratio": 1.5432692307692308, "no_speech_prob": 3.089395931965555e-06}, {"id": 388, "seek": 164164, "start": 1656.64, "end": 1663.64, "text": " So yeah, I don't remember a Toby in Pride and Prejudice, so that's fitting that it's negative.", "tokens": [407, 1338, 11, 286, 500, 380, 1604, 257, 40223, 294, 30319, 293, 6001, 9218, 573, 11, 370, 300, 311, 15669, 300, 309, 311, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1474473580070164, "compression_ratio": 1.5432692307692308, "no_speech_prob": 3.089395931965555e-06}, {"id": 389, "seek": 164164, "start": 1663.64, "end": 1666.64, "text": " And let's look at, let's look at Kathy Linton again for Weathering Heights.", "tokens": [400, 718, 311, 574, 412, 11, 718, 311, 574, 412, 30740, 441, 12442, 797, 337, 34441, 278, 44039, 13], "temperature": 0.0, "avg_logprob": -0.1474473580070164, "compression_ratio": 1.5432692307692308, "no_speech_prob": 3.089395931965555e-06}, {"id": 390, "seek": 166664, "start": 1666.64, "end": 1680.64, "text": " So if we come over here to Linton, that shows up most in which what is line nine, I guess, topic eight.", "tokens": [407, 498, 321, 808, 670, 510, 281, 441, 12442, 11, 300, 3110, 493, 881, 294, 597, 437, 307, 1622, 4949, 11, 286, 2041, 11, 4829, 3180, 13], "temperature": 0.0, "avg_logprob": -0.15850143432617186, "compression_ratio": 1.6513157894736843, "no_speech_prob": 1.9033609532925766e-06}, {"id": 391, "seek": 166664, "start": 1680.64, "end": 1683.64, "text": " Yeah, topic eight. So let's go over here.", "tokens": [865, 11, 4829, 3180, 13, 407, 718, 311, 352, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.15850143432617186, "compression_ratio": 1.6513157894736843, "no_speech_prob": 1.9033609532925766e-06}, {"id": 392, "seek": 166664, "start": 1683.64, "end": 1688.64, "text": " See if Weathering Heights has.", "tokens": [3008, 498, 34441, 278, 44039, 575, 13], "temperature": 0.0, "avg_logprob": -0.15850143432617186, "compression_ratio": 1.6513157894736843, "no_speech_prob": 1.9033609532925766e-06}, {"id": 393, "seek": 166664, "start": 1688.64, "end": 1694.64, "text": " Yeah, so a lot of topic eight is in Weathering Heights, so that's fitting.", "tokens": [865, 11, 370, 257, 688, 295, 4829, 3180, 307, 294, 34441, 278, 44039, 11, 370, 300, 311, 15669, 13], "temperature": 0.0, "avg_logprob": -0.15850143432617186, "compression_ratio": 1.6513157894736843, "no_speech_prob": 1.9033609532925766e-06}, {"id": 394, "seek": 169464, "start": 1694.64, "end": 1697.64, "text": " Any other questions about kind of how to look at this? Yes.", "tokens": [2639, 661, 1651, 466, 733, 295, 577, 281, 574, 412, 341, 30, 1079, 13], "temperature": 0.0, "avg_logprob": -0.20028334944995482, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.2246294217184186e-05}, {"id": 395, "seek": 169464, "start": 1697.64, "end": 1701.64, "text": " Do you want to throw the microphone, Jeremy?", "tokens": [1144, 291, 528, 281, 3507, 264, 10952, 11, 17809, 30], "temperature": 0.0, "avg_logprob": -0.20028334944995482, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.2246294217184186e-05}, {"id": 396, "seek": 169464, "start": 1701.64, "end": 1705.64, "text": " Good catch.", "tokens": [2205, 3745, 13], "temperature": 0.0, "avg_logprob": -0.20028334944995482, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.2246294217184186e-05}, {"id": 397, "seek": 169464, "start": 1705.64, "end": 1711.64, "text": " So here we have a lot of words, right? But we finally settled down into top 10 topic.", "tokens": [407, 510, 321, 362, 257, 688, 295, 2283, 11, 558, 30, 583, 321, 2721, 14819, 760, 666, 1192, 1266, 4829, 13], "temperature": 0.0, "avg_logprob": -0.20028334944995482, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.2246294217184186e-05}, {"id": 398, "seek": 169464, "start": 1711.64, "end": 1714.64, "text": " How does this 10 come around?", "tokens": [1012, 775, 341, 1266, 808, 926, 30], "temperature": 0.0, "avg_logprob": -0.20028334944995482, "compression_ratio": 1.3333333333333333, "no_speech_prob": 5.2246294217184186e-05}, {"id": 399, "seek": 171464, "start": 1714.64, "end": 1724.64, "text": " Oh, so and actually I cheated or I didn't tell you. So I cheated a bit.", "tokens": [876, 11, 370, 293, 767, 286, 28079, 420, 286, 994, 380, 980, 291, 13, 407, 286, 28079, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.11015611081509977, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.8052316590910777e-05}, {"id": 400, "seek": 171464, "start": 1724.64, "end": 1731.64, "text": " There were originally 55,000 words in these novels, and I used all of those for the Python part of this,", "tokens": [821, 645, 7993, 12330, 11, 1360, 2283, 294, 613, 24574, 11, 293, 286, 1143, 439, 295, 729, 337, 264, 15329, 644, 295, 341, 11], "temperature": 0.0, "avg_logprob": -0.11015611081509977, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.8052316590910777e-05}, {"id": 401, "seek": 171464, "start": 1731.64, "end": 1734.64, "text": " but I didn't want to put that in Excel.", "tokens": [457, 286, 994, 380, 528, 281, 829, 300, 294, 19060, 13], "temperature": 0.0, "avg_logprob": -0.11015611081509977, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.8052316590910777e-05}, {"id": 402, "seek": 171464, "start": 1734.64, "end": 1738.64, "text": " So I just chose the top 64 words.", "tokens": [407, 286, 445, 5111, 264, 1192, 12145, 2283, 13], "temperature": 0.0, "avg_logprob": -0.11015611081509977, "compression_ratio": 1.4450867052023122, "no_speech_prob": 1.8052316590910777e-05}, {"id": 403, "seek": 173864, "start": 1738.64, "end": 1745.64, "text": " But really how this worked was so for a full SVD, we were getting 27 novels.", "tokens": [583, 534, 577, 341, 2732, 390, 370, 337, 257, 1577, 31910, 35, 11, 321, 645, 1242, 7634, 24574, 13], "temperature": 0.0, "avg_logprob": -0.089308244426076, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.287669526092941e-06}, {"id": 404, "seek": 173864, "start": 1745.64, "end": 1750.64, "text": " We got 27 topics, and each could involve all 55,000 words.", "tokens": [492, 658, 7634, 8378, 11, 293, 1184, 727, 9494, 439, 12330, 11, 1360, 2283, 13], "temperature": 0.0, "avg_logprob": -0.089308244426076, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.287669526092941e-06}, {"id": 405, "seek": 173864, "start": 1750.64, "end": 1759.64, "text": " So I just chose the top eight words from the top 10 topics and put that in this Excel workbook.", "tokens": [407, 286, 445, 5111, 264, 1192, 3180, 2283, 490, 264, 1192, 1266, 8378, 293, 829, 300, 294, 341, 19060, 589, 2939, 13], "temperature": 0.0, "avg_logprob": -0.089308244426076, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.287669526092941e-06}, {"id": 406, "seek": 173864, "start": 1759.64, "end": 1765.64, "text": " So each of the 27 words could have components from all 55,000 words.", "tokens": [407, 1184, 295, 264, 7634, 2283, 727, 362, 6677, 490, 439, 12330, 11, 1360, 2283, 13], "temperature": 0.0, "avg_logprob": -0.089308244426076, "compression_ratio": 1.530612244897959, "no_speech_prob": 4.287669526092941e-06}, {"id": 407, "seek": 176564, "start": 1765.64, "end": 1769.64, "text": " So I looked for what had the greatest magnitude.", "tokens": [407, 286, 2956, 337, 437, 632, 264, 6636, 15668, 13], "temperature": 0.0, "avg_logprob": -0.07948475895505963, "compression_ratio": 1.550561797752809, "no_speech_prob": 2.046133522526361e-05}, {"id": 408, "seek": 176564, "start": 1769.64, "end": 1777.64, "text": " And remember, because the topics are because S is ordered in terms of the values,", "tokens": [400, 1604, 11, 570, 264, 8378, 366, 570, 318, 307, 8866, 294, 2115, 295, 264, 4190, 11], "temperature": 0.0, "avg_logprob": -0.07948475895505963, "compression_ratio": 1.550561797752809, "no_speech_prob": 2.046133522526361e-05}, {"id": 409, "seek": 176564, "start": 1777.64, "end": 1783.64, "text": " taking the top 10 makes sense because these have larger values.", "tokens": [1940, 264, 1192, 1266, 1669, 2020, 570, 613, 362, 4833, 4190, 13], "temperature": 0.0, "avg_logprob": -0.07948475895505963, "compression_ratio": 1.550561797752809, "no_speech_prob": 2.046133522526361e-05}, {"id": 410, "seek": 176564, "start": 1783.64, "end": 1789.64, "text": " And so what that means is they make up a bigger component of the original matrix.", "tokens": [400, 370, 437, 300, 1355, 307, 436, 652, 493, 257, 3801, 6542, 295, 264, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.07948475895505963, "compression_ratio": 1.550561797752809, "no_speech_prob": 2.046133522526361e-05}, {"id": 411, "seek": 178964, "start": 1789.64, "end": 1796.64, "text": " So remember, the goal here is that we want U times S times V to give us our original matrix back.", "tokens": [407, 1604, 11, 264, 3387, 510, 307, 300, 321, 528, 624, 1413, 318, 1413, 691, 281, 976, 505, 527, 3380, 8141, 646, 13], "temperature": 0.0, "avg_logprob": -0.07190747644709444, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.6700330888852477e-05}, {"id": 412, "seek": 178964, "start": 1796.64, "end": 1802.64, "text": " And since I've only used 10 of the topics, it's not going to give us the original matrix, but hopefully it's close.", "tokens": [400, 1670, 286, 600, 787, 1143, 1266, 295, 264, 8378, 11, 309, 311, 406, 516, 281, 976, 505, 264, 3380, 8141, 11, 457, 4696, 309, 311, 1998, 13], "temperature": 0.0, "avg_logprob": -0.07190747644709444, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.6700330888852477e-05}, {"id": 413, "seek": 178964, "start": 1802.64, "end": 1810.64, "text": " And that's why this is used in data compression.", "tokens": [400, 300, 311, 983, 341, 307, 1143, 294, 1412, 19355, 13], "temperature": 0.0, "avg_logprob": -0.07190747644709444, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.6700330888852477e-05}, {"id": 414, "seek": 178964, "start": 1810.64, "end": 1814.64, "text": " Just a general question. Would this be a block of our original matrix?", "tokens": [1449, 257, 2674, 1168, 13, 6068, 341, 312, 257, 3461, 295, 527, 3380, 8141, 30], "temperature": 0.0, "avg_logprob": -0.07190747644709444, "compression_ratio": 1.5933014354066986, "no_speech_prob": 1.6700330888852477e-05}, {"id": 415, "seek": 181464, "start": 1814.64, "end": 1820.64, "text": " Would this be an actual block in the original full matrix if you multiply this out?", "tokens": [6068, 341, 312, 364, 3539, 3461, 294, 264, 3380, 1577, 8141, 498, 291, 12972, 341, 484, 30], "temperature": 0.0, "avg_logprob": -0.15528048891009708, "compression_ratio": 1.4216216216216215, "no_speech_prob": 2.9308823286555707e-05}, {"id": 416, "seek": 181464, "start": 1820.64, "end": 1825.64, "text": " Oh, yes. That's a good question. Let me write that down.", "tokens": [876, 11, 2086, 13, 663, 311, 257, 665, 1168, 13, 961, 385, 2464, 300, 760, 13], "temperature": 0.0, "avg_logprob": -0.15528048891009708, "compression_ratio": 1.4216216216216215, "no_speech_prob": 2.9308823286555707e-05}, {"id": 417, "seek": 181464, "start": 1825.64, "end": 1832.64, "text": " This is a great question. Tim brought up block matrices,", "tokens": [639, 307, 257, 869, 1168, 13, 7172, 3038, 493, 3461, 32284, 11], "temperature": 0.0, "avg_logprob": -0.15528048891009708, "compression_ratio": 1.4216216216216215, "no_speech_prob": 2.9308823286555707e-05}, {"id": 418, "seek": 181464, "start": 1832.64, "end": 1837.64, "text": " which are a pretty important concept in numerical linear algebra.", "tokens": [597, 366, 257, 1238, 1021, 3410, 294, 29054, 8213, 21989, 13], "temperature": 0.0, "avg_logprob": -0.15528048891009708, "compression_ratio": 1.4216216216216215, "no_speech_prob": 2.9308823286555707e-05}, {"id": 419, "seek": 183764, "start": 1837.64, "end": 1855.64, "text": " So let me go back to...", "tokens": [407, 718, 385, 352, 646, 281, 485], "temperature": 0.0, "avg_logprob": -0.1186801138378325, "compression_ratio": 1.2342342342342343, "no_speech_prob": 1.34193605845212e-05}, {"id": 420, "seek": 183764, "start": 1855.64, "end": 1858.64, "text": " Oh, I see. Thanks.", "tokens": [876, 11, 286, 536, 13, 2561, 13], "temperature": 0.0, "avg_logprob": -0.1186801138378325, "compression_ratio": 1.2342342342342343, "no_speech_prob": 1.34193605845212e-05}, {"id": 421, "seek": 183764, "start": 1858.64, "end": 1864.64, "text": " So the idea of a block matrix is to kind of think about a matrix as having smaller components.", "tokens": [407, 264, 1558, 295, 257, 3461, 8141, 307, 281, 733, 295, 519, 466, 257, 8141, 382, 1419, 4356, 6677, 13], "temperature": 0.0, "avg_logprob": -0.1186801138378325, "compression_ratio": 1.2342342342342343, "no_speech_prob": 1.34193605845212e-05}, {"id": 422, "seek": 186464, "start": 1864.64, "end": 1871.64, "text": " So here...", "tokens": [407, 510, 485], "temperature": 0.0, "avg_logprob": -0.1572578001995476, "compression_ratio": 1.208, "no_speech_prob": 1.8631046259542927e-05}, {"id": 423, "seek": 186464, "start": 1871.64, "end": 1886.64, "text": " I guess if U is 27 by 55,000, we've just taken kind of this section that was...", "tokens": [286, 2041, 498, 624, 307, 7634, 538, 12330, 11, 1360, 11, 321, 600, 445, 2726, 733, 295, 341, 3541, 300, 390, 485], "temperature": 0.0, "avg_logprob": -0.1572578001995476, "compression_ratio": 1.208, "no_speech_prob": 1.8631046259542927e-05}, {"id": 424, "seek": 186464, "start": 1886.64, "end": 1890.64, "text": " Let me make it a little bit wider just so I can write on it.", "tokens": [961, 385, 652, 309, 257, 707, 857, 11842, 445, 370, 286, 393, 2464, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.1572578001995476, "compression_ratio": 1.208, "no_speech_prob": 1.8631046259542927e-05}, {"id": 425, "seek": 189064, "start": 1890.64, "end": 1895.64, "text": " Taking this section that was 27 by 64,", "tokens": [17837, 341, 3541, 300, 390, 7634, 538, 12145, 11], "temperature": 0.0, "avg_logprob": -0.14474992903452072, "compression_ratio": 1.3486842105263157, "no_speech_prob": 4.356796580395894e-06}, {"id": 426, "seek": 189064, "start": 1895.64, "end": 1902.64, "text": " but we could still think of the rest of the matrix being there, and it's 27 by...", "tokens": [457, 321, 727, 920, 519, 295, 264, 1472, 295, 264, 8141, 885, 456, 11, 293, 309, 311, 7634, 538, 485], "temperature": 0.0, "avg_logprob": -0.14474992903452072, "compression_ratio": 1.3486842105263157, "no_speech_prob": 4.356796580395894e-06}, {"id": 427, "seek": 189064, "start": 1902.64, "end": 1914.64, "text": " whatever 55,000 minus 64 is. So I'll round that to 54,000, but it's actually 54,900.", "tokens": [2035, 12330, 11, 1360, 3175, 12145, 307, 13, 407, 286, 603, 3098, 300, 281, 20793, 11, 1360, 11, 457, 309, 311, 767, 20793, 11, 23943, 13], "temperature": 0.0, "avg_logprob": -0.14474992903452072, "compression_ratio": 1.3486842105263157, "no_speech_prob": 4.356796580395894e-06}, {"id": 428, "seek": 191464, "start": 1914.64, "end": 1926.64, "text": " Then S could be written as...", "tokens": [1396, 318, 727, 312, 3720, 382, 485], "temperature": 0.0, "avg_logprob": -0.1367283139910017, "compression_ratio": 1.134020618556701, "no_speech_prob": 3.288712150606443e-06}, {"id": 429, "seek": 191464, "start": 1926.64, "end": 1938.64, "text": " So here S was 27 by 27, but we were only interested in the top kind of 10 by 10.", "tokens": [407, 510, 318, 390, 7634, 538, 7634, 11, 457, 321, 645, 787, 3102, 294, 264, 1192, 733, 295, 1266, 538, 1266, 13], "temperature": 0.0, "avg_logprob": -0.1367283139910017, "compression_ratio": 1.134020618556701, "no_speech_prob": 3.288712150606443e-06}, {"id": 430, "seek": 193864, "start": 1938.64, "end": 1947.64, "text": " And that leaves us with like four other matrices. We've got something that's 10 by 7,", "tokens": [400, 300, 5510, 505, 365, 411, 1451, 661, 32284, 13, 492, 600, 658, 746, 300, 311, 1266, 538, 1614, 11], "temperature": 0.0, "avg_logprob": -0.12756955866910974, "compression_ratio": 1.2231404958677685, "no_speech_prob": 7.64644664741354e-06}, {"id": 431, "seek": 193864, "start": 1947.64, "end": 1957.64, "text": " 7 by 10, and 7 by 7.", "tokens": [1614, 538, 1266, 11, 293, 1614, 538, 1614, 13], "temperature": 0.0, "avg_logprob": -0.12756955866910974, "compression_ratio": 1.2231404958677685, "no_speech_prob": 7.64644664741354e-06}, {"id": 432, "seek": 193864, "start": 1957.64, "end": 1966.64, "text": " And then V... Oh, that's not the right...", "tokens": [400, 550, 691, 485, 876, 11, 300, 311, 406, 264, 558, 485], "temperature": 0.0, "avg_logprob": -0.12756955866910974, "compression_ratio": 1.2231404958677685, "no_speech_prob": 7.64644664741354e-06}, {"id": 433, "seek": 196664, "start": 1966.64, "end": 1981.64, "text": " Oh, sorry, I miswrote, guys. U is not 27 by 55,000. U is 27 by 27.", "tokens": [876, 11, 2597, 11, 286, 3346, 7449, 1370, 11, 1074, 13, 624, 307, 406, 7634, 538, 12330, 11, 1360, 13, 624, 307, 7634, 538, 7634, 13], "temperature": 0.0, "avg_logprob": -0.0797549511523957, "compression_ratio": 1.2, "no_speech_prob": 6.854249022580916e-06}, {"id": 434, "seek": 196664, "start": 1981.64, "end": 1987.64, "text": " Sorry about that. Going back, U is the works by the topics.", "tokens": [4919, 466, 300, 13, 10963, 646, 11, 624, 307, 264, 1985, 538, 264, 8378, 13], "temperature": 0.0, "avg_logprob": -0.0797549511523957, "compression_ratio": 1.2, "no_speech_prob": 6.854249022580916e-06}, {"id": 435, "seek": 198764, "start": 1987.64, "end": 1996.64, "text": " I was confused. This... So that was actually 27 by 10 and then 27 by 7.", "tokens": [286, 390, 9019, 13, 639, 485, 407, 300, 390, 767, 7634, 538, 1266, 293, 550, 7634, 538, 1614, 13], "temperature": 0.0, "avg_logprob": -0.12362119479057117, "compression_ratio": 1.1818181818181819, "no_speech_prob": 3.705096241901629e-05}, {"id": 436, "seek": 198764, "start": 1996.64, "end": 2008.64, "text": " I was writing V there, which is 27 by 55,000.", "tokens": [286, 390, 3579, 691, 456, 11, 597, 307, 7634, 538, 12330, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.12362119479057117, "compression_ratio": 1.1818181818181819, "no_speech_prob": 3.705096241901629e-05}, {"id": 437, "seek": 200864, "start": 2008.64, "end": 2020.64, "text": " Yes. No. Oh, yes. 17. V is the one that's 27 by 55,000.", "tokens": [1079, 13, 883, 13, 876, 11, 2086, 13, 3282, 13, 691, 307, 264, 472, 300, 311, 7634, 538, 12330, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.11409356858995226, "compression_ratio": 1.135135135135135, "no_speech_prob": 1.4593672403862001e-06}, {"id": 438, "seek": 200864, "start": 2020.64, "end": 2031.64, "text": " And V is topics by words. And then here, because we've just picked off", "tokens": [400, 691, 307, 8378, 538, 2283, 13, 400, 550, 510, 11, 570, 321, 600, 445, 6183, 766], "temperature": 0.0, "avg_logprob": -0.11409356858995226, "compression_ratio": 1.135135135135135, "no_speech_prob": 1.4593672403862001e-06}, {"id": 439, "seek": 203164, "start": 2031.64, "end": 2046.64, "text": " the top 10, this is where we get that 10 by 64 matrix. And this would be 10 by 54,000,", "tokens": [264, 1192, 1266, 11, 341, 307, 689, 321, 483, 300, 1266, 538, 12145, 8141, 13, 400, 341, 576, 312, 1266, 538, 20793, 11, 1360, 11], "temperature": 0.0, "avg_logprob": -0.07601354916890463, "compression_ratio": 1.4565217391304348, "no_speech_prob": 1.0289365491189528e-05}, {"id": 440, "seek": 203164, "start": 2046.64, "end": 2054.6400000000003, "text": " 17 by 64, and 17 by 54,000.", "tokens": [3282, 538, 12145, 11, 293, 3282, 538, 20793, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.07601354916890463, "compression_ratio": 1.4565217391304348, "no_speech_prob": 1.0289365491189528e-05}, {"id": 441, "seek": 203164, "start": 2054.6400000000003, "end": 2059.6400000000003, "text": " Anyway, so block matrix is kind of breaking these matrices down into smaller matrices.", "tokens": [5684, 11, 370, 3461, 8141, 307, 733, 295, 7697, 613, 32284, 760, 666, 4356, 32284, 13], "temperature": 0.0, "avg_logprob": -0.07601354916890463, "compression_ratio": 1.4565217391304348, "no_speech_prob": 1.0289365491189528e-05}, {"id": 442, "seek": 205964, "start": 2059.64, "end": 2067.64, "text": " And you'll notice that by how matrix multiplication works, that the result can be written as block matrices,", "tokens": [400, 291, 603, 3449, 300, 538, 577, 8141, 27290, 1985, 11, 300, 264, 1874, 393, 312, 3720, 382, 3461, 32284, 11], "temperature": 0.0, "avg_logprob": -0.1363713246471477, "compression_ratio": 1.3509933774834437, "no_speech_prob": 5.173856607143534e-06}, {"id": 443, "seek": 205964, "start": 2067.64, "end": 2079.64, "text": " kind of products of these within them. So here, the product would be, if I call this U1 and U2,", "tokens": [733, 295, 3383, 295, 613, 1951, 552, 13, 407, 510, 11, 264, 1674, 576, 312, 11, 498, 286, 818, 341, 624, 16, 293, 624, 17, 11], "temperature": 0.0, "avg_logprob": -0.1363713246471477, "compression_ratio": 1.3509933774834437, "no_speech_prob": 5.173856607143534e-06}, {"id": 444, "seek": 207964, "start": 2079.64, "end": 2092.64, "text": " kind of the top square would be U1 times S1 times V1, and so on.", "tokens": [733, 295, 264, 1192, 3732, 576, 312, 624, 16, 1413, 318, 16, 1413, 691, 16, 11, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.06951198880634611, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.3287614137880155e-06}, {"id": 445, "seek": 207964, "start": 2092.64, "end": 2097.64, "text": " I actually really should have done this with an example with just two matrices to start.", "tokens": [286, 767, 534, 820, 362, 1096, 341, 365, 364, 1365, 365, 445, 732, 32284, 281, 722, 13], "temperature": 0.0, "avg_logprob": -0.06951198880634611, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.3287614137880155e-06}, {"id": 446, "seek": 207964, "start": 2097.64, "end": 2103.64, "text": " And actually, maybe let me do that. This is getting to be a complicated example.", "tokens": [400, 767, 11, 1310, 718, 385, 360, 300, 13, 639, 307, 1242, 281, 312, 257, 6179, 1365, 13], "temperature": 0.0, "avg_logprob": -0.06951198880634611, "compression_ratio": 1.4444444444444444, "no_speech_prob": 1.3287614137880155e-06}, {"id": 447, "seek": 210364, "start": 2103.64, "end": 2109.64, "text": " But this can be a very efficient way of doing matrix multiplication or matrix operations.", "tokens": [583, 341, 393, 312, 257, 588, 7148, 636, 295, 884, 8141, 27290, 420, 8141, 7705, 13], "temperature": 0.0, "avg_logprob": -0.08938492045683019, "compression_ratio": 1.661904761904762, "no_speech_prob": 3.844853381451685e-06}, {"id": 448, "seek": 210364, "start": 2109.64, "end": 2115.64, "text": " And it takes into account locality of the idea of you can bring in this matrix that's stored near", "tokens": [400, 309, 2516, 666, 2696, 1628, 1860, 295, 264, 1558, 295, 291, 393, 1565, 294, 341, 8141, 300, 311, 12187, 2651], "temperature": 0.0, "avg_logprob": -0.08938492045683019, "compression_ratio": 1.661904761904762, "no_speech_prob": 3.844853381451685e-06}, {"id": 449, "seek": 210364, "start": 2115.64, "end": 2123.64, "text": " the part that's stored together in memory, bring it into your cache, multiply it by other block matrices,", "tokens": [264, 644, 300, 311, 12187, 1214, 294, 4675, 11, 1565, 309, 666, 428, 19459, 11, 12972, 309, 538, 661, 3461, 32284, 11], "temperature": 0.0, "avg_logprob": -0.08938492045683019, "compression_ratio": 1.661904761904762, "no_speech_prob": 3.844853381451685e-06}, {"id": 450, "seek": 210364, "start": 2123.64, "end": 2128.64, "text": " kind of use it, and then when you're done, put it back.", "tokens": [733, 295, 764, 309, 11, 293, 550, 562, 291, 434, 1096, 11, 829, 309, 646, 13], "temperature": 0.0, "avg_logprob": -0.08938492045683019, "compression_ratio": 1.661904761904762, "no_speech_prob": 3.844853381451685e-06}, {"id": 451, "seek": 212864, "start": 2128.64, "end": 2136.64, "text": " Okay, so scratch that. We're going to do a much simpler example if we have A is A1 and A2.", "tokens": [1033, 11, 370, 8459, 300, 13, 492, 434, 516, 281, 360, 257, 709, 18587, 1365, 498, 321, 362, 316, 307, 316, 16, 293, 316, 17, 13], "temperature": 0.0, "avg_logprob": -0.13629101003919328, "compression_ratio": 1.2923076923076924, "no_speech_prob": 4.637745860236464e-06}, {"id": 452, "seek": 212864, "start": 2136.64, "end": 2154.64, "text": " And here, A1 and A2 are both matrices. And say we multiply that by V1 and V2.", "tokens": [400, 510, 11, 316, 16, 293, 316, 17, 366, 1293, 32284, 13, 400, 584, 321, 12972, 300, 538, 691, 16, 293, 691, 17, 13], "temperature": 0.0, "avg_logprob": -0.13629101003919328, "compression_ratio": 1.2923076923076924, "no_speech_prob": 4.637745860236464e-06}, {"id": 453, "seek": 215464, "start": 2154.64, "end": 2173.64, "text": " The result is A1.B1, A2.B1.", "tokens": [440, 1874, 307, 316, 16, 13, 33, 16, 11, 316, 17, 13, 33, 16, 13], "temperature": 0.0, "avg_logprob": -0.11195805387676887, "compression_ratio": 1.168, "no_speech_prob": 4.029407591588097e-06}, {"id": 454, "seek": 215464, "start": 2173.64, "end": 2180.64, "text": " Oh, sorry. Okay, this is getting too convoluted. I will come back to this next time with a pre-worked example for you.", "tokens": [876, 11, 2597, 13, 1033, 11, 341, 307, 1242, 886, 3754, 2308, 292, 13, 286, 486, 808, 646, 281, 341, 958, 565, 365, 257, 659, 12, 1902, 292, 1365, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.11195805387676887, "compression_ratio": 1.168, "no_speech_prob": 4.029407591588097e-06}, {"id": 455, "seek": 218064, "start": 2180.64, "end": 2185.64, "text": " So yeah, we'll revisit this next time. I'm not doing this well on the fly.", "tokens": [407, 1338, 11, 321, 603, 32676, 341, 958, 565, 13, 286, 478, 406, 884, 341, 731, 322, 264, 3603, 13], "temperature": 0.0, "avg_logprob": -0.08518874967420423, "compression_ratio": 1.4431818181818181, "no_speech_prob": 7.64635933592217e-06}, {"id": 456, "seek": 218064, "start": 2185.64, "end": 2189.64, "text": " But that was a good question. We'll go back to it.", "tokens": [583, 300, 390, 257, 665, 1168, 13, 492, 603, 352, 646, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.08518874967420423, "compression_ratio": 1.4431818181818181, "no_speech_prob": 7.64635933592217e-06}, {"id": 457, "seek": 218064, "start": 2189.64, "end": 2195.64, "text": " So back to kind of our perspective of SVD from within Excel.", "tokens": [407, 646, 281, 733, 295, 527, 4585, 295, 31910, 35, 490, 1951, 19060, 13], "temperature": 0.0, "avg_logprob": -0.08518874967420423, "compression_ratio": 1.4431818181818181, "no_speech_prob": 7.64635933592217e-06}, {"id": 458, "seek": 218064, "start": 2195.64, "end": 2203.64, "text": " Any other questions about SVD or kind of this way of looking at it?", "tokens": [2639, 661, 1651, 466, 31910, 35, 420, 733, 295, 341, 636, 295, 1237, 412, 309, 30], "temperature": 0.0, "avg_logprob": -0.08518874967420423, "compression_ratio": 1.4431818181818181, "no_speech_prob": 7.64635933592217e-06}, {"id": 459, "seek": 220364, "start": 2203.64, "end": 2210.64, "text": " Oh, and then I guess kind of one fine point is that the words I've picked out aren't all clumped together.", "tokens": [876, 11, 293, 550, 286, 2041, 733, 295, 472, 2489, 935, 307, 300, 264, 2283, 286, 600, 6183, 484, 3212, 380, 439, 596, 1420, 292, 1214, 13], "temperature": 0.0, "avg_logprob": -0.13977378209431965, "compression_ratio": 1.3061224489795917, "no_speech_prob": 2.078449324471876e-05}, {"id": 460, "seek": 220364, "start": 2210.64, "end": 2218.64, "text": " So I was kind of having to re-arranging your indices.", "tokens": [407, 286, 390, 733, 295, 1419, 281, 319, 12, 2284, 9741, 428, 43840, 13], "temperature": 0.0, "avg_logprob": -0.13977378209431965, "compression_ratio": 1.3061224489795917, "no_speech_prob": 2.078449324471876e-05}, {"id": 461, "seek": 220364, "start": 2218.64, "end": 2225.64, "text": " All right, let's switch to NMF.", "tokens": [1057, 558, 11, 718, 311, 3679, 281, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.13977378209431965, "compression_ratio": 1.3061224489795917, "no_speech_prob": 2.078449324471876e-05}, {"id": 462, "seek": 222564, "start": 2225.64, "end": 2234.64, "text": " So NMF stands for non-negative matrix factorization, which kind of gives away that the key property of the matrices is that they're non-negative.", "tokens": [407, 426, 44, 37, 7382, 337, 2107, 12, 28561, 1166, 8141, 5952, 2144, 11, 597, 733, 295, 2709, 1314, 300, 264, 2141, 4707, 295, 264, 32284, 307, 300, 436, 434, 2107, 12, 28561, 1166, 13], "temperature": 0.0, "avg_logprob": -0.07295015059321759, "compression_ratio": 1.4873096446700507, "no_speech_prob": 1.3419148672255687e-05}, {"id": 463, "seek": 222564, "start": 2234.64, "end": 2245.64, "text": " So here, or with NMF, you get just two matrices typically called W and H that you're factoring into.", "tokens": [407, 510, 11, 420, 365, 426, 44, 37, 11, 291, 483, 445, 732, 32284, 5850, 1219, 343, 293, 389, 300, 291, 434, 1186, 3662, 666, 13], "temperature": 0.0, "avg_logprob": -0.07295015059321759, "compression_ratio": 1.4873096446700507, "no_speech_prob": 1.3419148672255687e-05}, {"id": 464, "seek": 222564, "start": 2245.64, "end": 2254.64, "text": " Zoom in. Yeah, so again, we have the 27 works.", "tokens": [13453, 294, 13, 865, 11, 370, 797, 11, 321, 362, 264, 7634, 1985, 13], "temperature": 0.0, "avg_logprob": -0.07295015059321759, "compression_ratio": 1.4873096446700507, "no_speech_prob": 1.3419148672255687e-05}, {"id": 465, "seek": 225464, "start": 2254.64, "end": 2262.64, "text": " And I've chosen 10 topics with NMF. That's a parameter. You can say how many topics you want to calculate.", "tokens": [400, 286, 600, 8614, 1266, 8378, 365, 426, 44, 37, 13, 663, 311, 257, 13075, 13, 509, 393, 584, 577, 867, 8378, 291, 528, 281, 8873, 13], "temperature": 0.0, "avg_logprob": -0.0791182972135998, "compression_ratio": 1.4626168224299065, "no_speech_prob": 3.8448074519692454e-06}, {"id": 466, "seek": 225464, "start": 2262.64, "end": 2272.64, "text": " Has anyone noticed something that's kind of distinctive about this matrix?", "tokens": [8646, 2878, 5694, 746, 300, 311, 733, 295, 27766, 466, 341, 8141, 30], "temperature": 0.0, "avg_logprob": -0.0791182972135998, "compression_ratio": 1.4626168224299065, "no_speech_prob": 3.8448074519692454e-06}, {"id": 467, "seek": 225464, "start": 2272.64, "end": 2276.64, "text": " So that's true. There are no negatives.", "tokens": [407, 300, 311, 2074, 13, 821, 366, 572, 40019, 13], "temperature": 0.0, "avg_logprob": -0.0791182972135998, "compression_ratio": 1.4626168224299065, "no_speech_prob": 3.8448074519692454e-06}, {"id": 468, "seek": 225464, "start": 2276.64, "end": 2281.64, "text": " It's sparse. Yeah, I heard several people say sparse, which means there are a lot of zeros.", "tokens": [467, 311, 637, 11668, 13, 865, 11, 286, 2198, 2940, 561, 584, 637, 11668, 11, 597, 1355, 456, 366, 257, 688, 295, 35193, 13], "temperature": 0.0, "avg_logprob": -0.0791182972135998, "compression_ratio": 1.4626168224299065, "no_speech_prob": 3.8448074519692454e-06}, {"id": 469, "seek": 228164, "start": 2281.64, "end": 2287.64, "text": " And this is typically an additional constraint that's put on NMF matrices.", "tokens": [400, 341, 307, 5850, 364, 4497, 25534, 300, 311, 829, 322, 426, 44, 37, 32284, 13], "temperature": 0.0, "avg_logprob": -0.08443171113401979, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809540613438e-06}, {"id": 470, "seek": 228164, "start": 2287.64, "end": 2293.64, "text": " And it also makes sense because you don't want to value everywhere because you can't get negatives.", "tokens": [400, 309, 611, 1669, 2020, 570, 291, 500, 380, 528, 281, 2158, 5315, 570, 291, 393, 380, 483, 40019, 13], "temperature": 0.0, "avg_logprob": -0.08443171113401979, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809540613438e-06}, {"id": 471, "seek": 228164, "start": 2293.64, "end": 2300.64, "text": " So if you think of kind of like building up your original matrix, you don't want to have positive place.", "tokens": [407, 498, 291, 519, 295, 733, 295, 411, 2390, 493, 428, 3380, 8141, 11, 291, 500, 380, 528, 281, 362, 3353, 1081, 13], "temperature": 0.0, "avg_logprob": -0.08443171113401979, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809540613438e-06}, {"id": 472, "seek": 228164, "start": 2300.64, "end": 2306.64, "text": " You can't have it get too large because you're never going to have a negative to cancel that out.", "tokens": [509, 393, 380, 362, 309, 483, 886, 2416, 570, 291, 434, 1128, 516, 281, 362, 257, 3671, 281, 10373, 300, 484, 13], "temperature": 0.0, "avg_logprob": -0.08443171113401979, "compression_ratio": 1.6830357142857142, "no_speech_prob": 3.668809540613438e-06}, {"id": 473, "seek": 230664, "start": 2306.64, "end": 2313.64, "text": " So here, let's take a look. And then I'll show you H is over here.", "tokens": [407, 510, 11, 718, 311, 747, 257, 574, 13, 400, 550, 286, 603, 855, 291, 389, 307, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.1090515795208159, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.3404867281205952e-06}, {"id": 474, "seek": 230664, "start": 2313.64, "end": 2318.64, "text": " And again, this is also sparse and also non-negative.", "tokens": [400, 797, 11, 341, 307, 611, 637, 11668, 293, 611, 2107, 12, 28561, 1166, 13], "temperature": 0.0, "avg_logprob": -0.1090515795208159, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.3404867281205952e-06}, {"id": 475, "seek": 230664, "start": 2318.64, "end": 2325.64, "text": " And so this we can look at. So Cathy shows up a lot in topic six.", "tokens": [400, 370, 341, 321, 393, 574, 412, 13, 407, 39799, 3110, 493, 257, 688, 294, 4829, 2309, 13], "temperature": 0.0, "avg_logprob": -0.1090515795208159, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.3404867281205952e-06}, {"id": 476, "seek": 230664, "start": 2325.64, "end": 2331.64, "text": " If we go back here, we would expect Wuthering Heights to have a lot of topic six.", "tokens": [759, 321, 352, 646, 510, 11, 321, 576, 2066, 343, 17696, 278, 44039, 281, 362, 257, 688, 295, 4829, 2309, 13], "temperature": 0.0, "avg_logprob": -0.1090515795208159, "compression_ratio": 1.5141242937853108, "no_speech_prob": 3.3404867281205952e-06}, {"id": 477, "seek": 233164, "start": 2331.64, "end": 2338.64, "text": " And it does. It has 0.79, which is kind of on the larger side.", "tokens": [400, 309, 775, 13, 467, 575, 1958, 13, 32042, 11, 597, 307, 733, 295, 322, 264, 4833, 1252, 13], "temperature": 0.0, "avg_logprob": -0.13768047332763672, "compression_ratio": 1.4153005464480874, "no_speech_prob": 4.936859113513492e-06}, {"id": 478, "seek": 233164, "start": 2338.64, "end": 2349.64, "text": " So this is a on the surface, at least, this seems more interpretable, I think, of kind of finding large numbers, seeing what topics they line up with.", "tokens": [407, 341, 307, 257, 322, 264, 3753, 11, 412, 1935, 11, 341, 2544, 544, 7302, 712, 11, 286, 519, 11, 295, 733, 295, 5006, 2416, 3547, 11, 2577, 437, 8378, 436, 1622, 493, 365, 13], "temperature": 0.0, "avg_logprob": -0.13768047332763672, "compression_ratio": 1.4153005464480874, "no_speech_prob": 4.936859113513492e-06}, {"id": 479, "seek": 233164, "start": 2349.64, "end": 2359.64, "text": " What are some are there any downsides to NMF?", "tokens": [708, 366, 512, 366, 456, 604, 21554, 1875, 281, 426, 44, 37, 30], "temperature": 0.0, "avg_logprob": -0.13768047332763672, "compression_ratio": 1.4153005464480874, "no_speech_prob": 4.936859113513492e-06}, {"id": 480, "seek": 235964, "start": 2359.64, "end": 2365.64, "text": " Something as we oh, Kelsey.", "tokens": [6595, 382, 321, 1954, 11, 44714, 13], "temperature": 0.0, "avg_logprob": -0.3158122774154421, "compression_ratio": 1.4598540145985401, "no_speech_prob": 6.604221562156454e-05}, {"id": 481, "seek": 235964, "start": 2365.64, "end": 2368.64, "text": " Backs, so it doesn't have a unique solution.", "tokens": [5833, 82, 11, 370, 309, 1177, 380, 362, 257, 3845, 3827, 13], "temperature": 0.0, "avg_logprob": -0.3158122774154421, "compression_ratio": 1.4598540145985401, "no_speech_prob": 6.604221562156454e-05}, {"id": 482, "seek": 235964, "start": 2368.64, "end": 2374.64, "text": " That's true. It doesn't have a unique solution. The way SBD does. It's a good one.", "tokens": [663, 311, 2074, 13, 467, 1177, 380, 362, 257, 3845, 3827, 13, 440, 636, 26944, 35, 775, 13, 467, 311, 257, 665, 472, 13], "temperature": 0.0, "avg_logprob": -0.3158122774154421, "compression_ratio": 1.4598540145985401, "no_speech_prob": 6.604221562156454e-05}, {"id": 483, "seek": 235964, "start": 2374.64, "end": 2382.64, "text": " Any others? This is kind of closely related.", "tokens": [2639, 2357, 30, 639, 307, 733, 295, 8185, 4077, 13], "temperature": 0.0, "avg_logprob": -0.3158122774154421, "compression_ratio": 1.4598540145985401, "no_speech_prob": 6.604221562156454e-05}, {"id": 484, "seek": 238264, "start": 2382.64, "end": 2390.64, "text": " Yes. Isn't it difficult to solve? Kind of an ambiguous solution.", "tokens": [1079, 13, 6998, 380, 309, 2252, 281, 5039, 30, 9242, 295, 364, 39465, 3827, 13], "temperature": 0.0, "avg_logprob": -0.3197825257207306, "compression_ratio": 1.5056179775280898, "no_speech_prob": 9.755574865266681e-05}, {"id": 485, "seek": 238264, "start": 2390.64, "end": 2397.64, "text": " Yeah, the solution is ambiguous. It's also typically you have to add additional constraints.", "tokens": [865, 11, 264, 3827, 307, 39465, 13, 467, 311, 611, 5850, 291, 362, 281, 909, 4497, 18491, 13], "temperature": 0.0, "avg_logprob": -0.3197825257207306, "compression_ratio": 1.5056179775280898, "no_speech_prob": 9.755574865266681e-05}, {"id": 486, "seek": 238264, "start": 2397.64, "end": 2401.64, "text": " Difficult to solve, you mean like a little bit slow to calculate or?", "tokens": [35940, 1786, 723, 281, 5039, 11, 291, 914, 411, 257, 707, 857, 2964, 281, 8873, 420, 30], "temperature": 0.0, "avg_logprob": -0.3197825257207306, "compression_ratio": 1.5056179775280898, "no_speech_prob": 9.755574865266681e-05}, {"id": 487, "seek": 238264, "start": 2401.64, "end": 2405.64, "text": " Yeah, it is. Yeah. Our original approach.", "tokens": [865, 11, 309, 307, 13, 865, 13, 2621, 3380, 3109, 13], "temperature": 0.0, "avg_logprob": -0.3197825257207306, "compression_ratio": 1.5056179775280898, "no_speech_prob": 9.755574865266681e-05}, {"id": 488, "seek": 240564, "start": 2405.64, "end": 2415.64, "text": " Although SBD can also be slow to calculate depending on what you're doing. But yeah, we did run into speed speed problems on on Thursday.", "tokens": [5780, 26944, 35, 393, 611, 312, 2964, 281, 8873, 5413, 322, 437, 291, 434, 884, 13, 583, 1338, 11, 321, 630, 1190, 666, 3073, 3073, 2740, 322, 322, 10383, 13], "temperature": 0.0, "avg_logprob": -0.16368722915649414, "compression_ratio": 1.5809128630705394, "no_speech_prob": 1.9222139599150978e-05}, {"id": 489, "seek": 240564, "start": 2415.64, "end": 2421.64, "text": " Jeremy, I was going to say the lack of a diagonal is a problem because it's not as easy to see which topics are important.", "tokens": [17809, 11, 286, 390, 516, 281, 584, 264, 5011, 295, 257, 21539, 307, 257, 1154, 570, 309, 311, 406, 382, 1858, 281, 536, 597, 8378, 366, 1021, 13], "temperature": 0.0, "avg_logprob": -0.16368722915649414, "compression_ratio": 1.5809128630705394, "no_speech_prob": 1.9222139599150978e-05}, {"id": 490, "seek": 240564, "start": 2421.64, "end": 2430.64, "text": " But then I was wondering, can you just look at the norm of each row, the norm of each column, get the same kind of idea?", "tokens": [583, 550, 286, 390, 6359, 11, 393, 291, 445, 574, 412, 264, 2026, 295, 1184, 5386, 11, 264, 2026, 295, 1184, 7738, 11, 483, 264, 912, 733, 295, 1558, 30], "temperature": 0.0, "avg_logprob": -0.16368722915649414, "compression_ratio": 1.5809128630705394, "no_speech_prob": 1.9222139599150978e-05}, {"id": 491, "seek": 243064, "start": 2430.64, "end": 2436.64, "text": " That's a really good point. I actually hadn't thought about that. I would say not having it ordered for you of what's most important.", "tokens": [663, 311, 257, 534, 665, 935, 13, 286, 767, 8782, 380, 1194, 466, 300, 13, 286, 576, 584, 406, 1419, 309, 8866, 337, 291, 295, 437, 311, 881, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1321389425368536, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.5444364180439152e-05}, {"id": 492, "seek": 243064, "start": 2436.64, "end": 2440.64, "text": " And I actually don't know if the norm gives you equivalent information.", "tokens": [400, 286, 767, 500, 380, 458, 498, 264, 2026, 2709, 291, 10344, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1321389425368536, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.5444364180439152e-05}, {"id": 493, "seek": 243064, "start": 2440.64, "end": 2445.64, "text": " The only reason we needed the diagonal before is because of the orthonormal constraint.", "tokens": [440, 787, 1778, 321, 2978, 264, 21539, 949, 307, 570, 295, 264, 420, 11943, 24440, 25534, 13], "temperature": 0.0, "avg_logprob": -0.1321389425368536, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.5444364180439152e-05}, {"id": 494, "seek": 243064, "start": 2445.64, "end": 2449.64, "text": " This doesn't have the constraint. It has the concept of how big each row and each column is.", "tokens": [639, 1177, 380, 362, 264, 25534, 13, 467, 575, 264, 3410, 295, 577, 955, 1184, 5386, 293, 1184, 7738, 307, 13], "temperature": 0.0, "avg_logprob": -0.1321389425368536, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.5444364180439152e-05}, {"id": 495, "seek": 243064, "start": 2449.64, "end": 2453.64, "text": " Maybe I was wrong and that's not a problem.", "tokens": [2704, 286, 390, 2085, 293, 300, 311, 406, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1321389425368536, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.5444364180439152e-05}, {"id": 496, "seek": 245364, "start": 2453.64, "end": 2465.64, "text": " I think it would not be straightforward just in that you've got the topic represented both in terms of how it intersects with the works and W and how it intersects with the vocabulary words and H.", "tokens": [286, 519, 309, 576, 406, 312, 15325, 445, 294, 300, 291, 600, 658, 264, 4829, 10379, 1293, 294, 2115, 295, 577, 309, 27815, 82, 365, 264, 1985, 293, 343, 293, 577, 309, 27815, 82, 365, 264, 19864, 2283, 293, 389, 13], "temperature": 0.0, "avg_logprob": -0.11807272046111351, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.267384146165568e-06}, {"id": 497, "seek": 245364, "start": 2465.64, "end": 2471.64, "text": " So you'd have to do some sort of normalization on the norm. So I would say it's not as straightforward to find the topic importance.", "tokens": [407, 291, 1116, 362, 281, 360, 512, 1333, 295, 2710, 2144, 322, 264, 2026, 13, 407, 286, 576, 584, 309, 311, 406, 382, 15325, 281, 915, 264, 4829, 7379, 13], "temperature": 0.0, "avg_logprob": -0.11807272046111351, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.267384146165568e-06}, {"id": 498, "seek": 245364, "start": 2471.64, "end": 2477.64, "text": " Can you toss it to Tim?", "tokens": [1664, 291, 14432, 309, 281, 7172, 30], "temperature": 0.0, "avg_logprob": -0.11807272046111351, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.267384146165568e-06}, {"id": 499, "seek": 247764, "start": 2477.64, "end": 2484.64, "text": " Did you say that singular value composition was unique? It's the singular values are unique.", "tokens": [2589, 291, 584, 300, 20010, 2158, 12686, 390, 3845, 30, 467, 311, 264, 20010, 4190, 366, 3845, 13], "temperature": 0.0, "avg_logprob": -0.20698296880147543, "compression_ratio": 1.729281767955801, "no_speech_prob": 2.391652924416121e-05}, {"id": 500, "seek": 247764, "start": 2484.64, "end": 2488.64, "text": " Oh, yes. Thank you. Thank you. Yes. It's that. Yeah, that's a great clarification. It's the singular values are unique.", "tokens": [876, 11, 2086, 13, 1044, 291, 13, 1044, 291, 13, 1079, 13, 467, 311, 300, 13, 865, 11, 300, 311, 257, 869, 34449, 13, 467, 311, 264, 20010, 4190, 366, 3845, 13], "temperature": 0.0, "avg_logprob": -0.20698296880147543, "compression_ratio": 1.729281767955801, "no_speech_prob": 2.391652924416121e-05}, {"id": 501, "seek": 247764, "start": 2488.64, "end": 2495.64, "text": " U and V are not unique.", "tokens": [624, 293, 691, 366, 406, 3845, 13], "temperature": 0.0, "avg_logprob": -0.20698296880147543, "compression_ratio": 1.729281767955801, "no_speech_prob": 2.391652924416121e-05}, {"id": 502, "seek": 247764, "start": 2495.64, "end": 2499.64, "text": " I think some of it you can also so definitely you can multiply by negatives.", "tokens": [286, 519, 512, 295, 309, 291, 393, 611, 370, 2138, 291, 393, 12972, 538, 40019, 13], "temperature": 0.0, "avg_logprob": -0.20698296880147543, "compression_ratio": 1.729281767955801, "no_speech_prob": 2.391652924416121e-05}, {"id": 503, "seek": 249964, "start": 2499.64, "end": 2511.64, "text": " This will show up like if you multiply U and V by negatives and then particularly when you're doing the full version for the kind of fill in columns and whichever has the larger dimension.", "tokens": [639, 486, 855, 493, 411, 498, 291, 12972, 624, 293, 691, 538, 40019, 293, 550, 4098, 562, 291, 434, 884, 264, 1577, 3037, 337, 264, 733, 295, 2836, 294, 13766, 293, 24123, 575, 264, 4833, 10139, 13], "temperature": 0.0, "avg_logprob": -0.11089443673892897, "compression_ratio": 1.4429530201342282, "no_speech_prob": 1.3005576875002589e-05}, {"id": 504, "seek": 249964, "start": 2511.64, "end": 2515.64, "text": " Those part are not unique.", "tokens": [3950, 644, 366, 406, 3845, 13], "temperature": 0.0, "avg_logprob": -0.11089443673892897, "compression_ratio": 1.4429530201342282, "no_speech_prob": 1.3005576875002589e-05}, {"id": 505, "seek": 251564, "start": 2515.64, "end": 2529.64, "text": " OK, so another another kind of downside to NMF I was thinking about is that it's an exact. So SVD if you do the full SVD, you can get something that fully reconstructs your matrix when you multiply back through.", "tokens": [2264, 11, 370, 1071, 1071, 733, 295, 25060, 281, 426, 44, 37, 286, 390, 1953, 466, 307, 300, 309, 311, 364, 1900, 13, 407, 31910, 35, 498, 291, 360, 264, 1577, 31910, 35, 11, 291, 393, 483, 746, 300, 4498, 31499, 82, 428, 8141, 562, 291, 12972, 646, 807, 13], "temperature": 0.0, "avg_logprob": -0.1693911729035554, "compression_ratio": 1.3612903225806452, "no_speech_prob": 2.2825222913525067e-05}, {"id": 506, "seek": 252964, "start": 2529.64, "end": 2545.64, "text": " NMF is an exact though because you're not even guaranteed that a non negative W and H exist that perfectly multiply to get your original matrix.", "tokens": [426, 44, 37, 307, 364, 1900, 1673, 570, 291, 434, 406, 754, 18031, 300, 257, 2107, 3671, 343, 293, 389, 2514, 300, 6239, 12972, 281, 483, 428, 3380, 8141, 13], "temperature": 0.0, "avg_logprob": -0.16951577803667853, "compression_ratio": 1.2307692307692308, "no_speech_prob": 1.1478343367343768e-05}, {"id": 507, "seek": 254564, "start": 2545.64, "end": 2569.64, "text": " Yeah. And the idea is you would multiply W and H together and here you can see in blue going row by column. Yes. I also wonder if the fact that they're not orthogonal is a downside like an SVD having orthogonal topics maybe becomes more interpretable because like there's no overlap between them.", "tokens": [865, 13, 400, 264, 1558, 307, 291, 576, 12972, 343, 293, 389, 1214, 293, 510, 291, 393, 536, 294, 3344, 516, 5386, 538, 7738, 13, 1079, 13, 286, 611, 2441, 498, 264, 1186, 300, 436, 434, 406, 41488, 307, 257, 25060, 411, 364, 31910, 35, 1419, 41488, 8378, 1310, 3643, 544, 7302, 712, 570, 411, 456, 311, 572, 19959, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.18515377333669952, "compression_ratio": 1.472636815920398, "no_speech_prob": 2.8854621632490307e-05}, {"id": 508, "seek": 256964, "start": 2569.64, "end": 2583.64, "text": " Where else in this case there might be some overlap and that might be kind of weird. That's true. You could have overlap here or you will have overlap since they're not not orthonormal.", "tokens": [2305, 1646, 294, 341, 1389, 456, 1062, 312, 512, 19959, 293, 300, 1062, 312, 733, 295, 3657, 13, 663, 311, 2074, 13, 509, 727, 362, 19959, 510, 420, 291, 486, 362, 19959, 1670, 436, 434, 406, 406, 420, 11943, 24440, 13], "temperature": 0.0, "avg_logprob": -0.15515536202324762, "compression_ratio": 1.4682539682539681, "no_speech_prob": 1.8341843315283768e-05}, {"id": 509, "seek": 258364, "start": 2583.64, "end": 2599.64, "text": " All right. I have a general question. So in practice I just see how the process goes right. So suppose we do these SVD decomposition and then say we is the satellite we carry out every unique single value.", "tokens": [1057, 558, 13, 286, 362, 257, 2674, 1168, 13, 407, 294, 3124, 286, 445, 536, 577, 264, 1399, 1709, 558, 13, 407, 7297, 321, 360, 613, 31910, 35, 48356, 293, 550, 584, 321, 307, 264, 16016, 321, 3985, 484, 633, 3845, 2167, 2158, 13], "temperature": 0.0, "avg_logprob": -0.26035942633946735, "compression_ratio": 1.3945578231292517, "no_speech_prob": 6.401023711077869e-05}, {"id": 510, "seek": 259964, "start": 2599.64, "end": 2613.64, "text": " For example here you only choose 10 out of maybe 30 topic. So for example we we have 30 eigenvalues we construct the X matrix and then we decide whether we want a top 10 or top 20.", "tokens": [1171, 1365, 510, 291, 787, 2826, 1266, 484, 295, 1310, 2217, 4829, 13, 407, 337, 1365, 321, 321, 362, 2217, 10446, 46033, 321, 7690, 264, 1783, 8141, 293, 550, 321, 4536, 1968, 321, 528, 257, 1192, 1266, 420, 1192, 945, 13], "temperature": 0.0, "avg_logprob": -0.1473868195439728, "compression_ratio": 1.540983606557377, "no_speech_prob": 3.647453922894783e-05}, {"id": 511, "seek": 259964, "start": 2613.64, "end": 2621.64, "text": " And then once we finalize the number of topics then we reconstruct the U and V. Is that how it works.", "tokens": [400, 550, 1564, 321, 2572, 1125, 264, 1230, 295, 8378, 550, 321, 31499, 264, 624, 293, 691, 13, 1119, 300, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1473868195439728, "compression_ratio": 1.540983606557377, "no_speech_prob": 3.647453922894783e-05}, {"id": 512, "seek": 262164, "start": 2621.64, "end": 2632.64, "text": " And so that this is a great question. So I think you're asking kind of do you have to calculate the full SVD of getting all the topics and then just throw away the ones you don't want.", "tokens": [400, 370, 300, 341, 307, 257, 869, 1168, 13, 407, 286, 519, 291, 434, 3365, 733, 295, 360, 291, 362, 281, 8873, 264, 1577, 31910, 35, 295, 1242, 439, 264, 8378, 293, 550, 445, 3507, 1314, 264, 2306, 291, 500, 380, 528, 13], "temperature": 0.0, "avg_logprob": -0.05845344066619873, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.3418538401310798e-05}, {"id": 513, "seek": 262164, "start": 2632.64, "end": 2641.64, "text": " And you do not. And we'll be talking about that in more detail with the new material today. It is possible to just calculate the topics you want for SVD.", "tokens": [400, 291, 360, 406, 13, 400, 321, 603, 312, 1417, 466, 300, 294, 544, 2607, 365, 264, 777, 2527, 965, 13, 467, 307, 1944, 281, 445, 8873, 264, 8378, 291, 528, 337, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.05845344066619873, "compression_ratio": 1.5943396226415094, "no_speech_prob": 1.3418538401310798e-05}, {"id": 514, "seek": 264164, "start": 2641.64, "end": 2658.64, "text": " Although that's kind of a newer approach and there's still a surprising number of materials that recommend you are like found a lot of algorithms online that will kind of be calculating the full SVD and then just throwing information away even though that's much slower.", "tokens": [5780, 300, 311, 733, 295, 257, 17628, 3109, 293, 456, 311, 920, 257, 8830, 1230, 295, 5319, 300, 2748, 291, 366, 411, 1352, 257, 688, 295, 14642, 2950, 300, 486, 733, 295, 312, 28258, 264, 1577, 31910, 35, 293, 550, 445, 10238, 1589, 1314, 754, 1673, 300, 311, 709, 14009, 13], "temperature": 0.0, "avg_logprob": -0.08506212661515421, "compression_ratio": 1.5376884422110553, "no_speech_prob": 2.6425022952025756e-06}, {"id": 515, "seek": 264164, "start": 2658.64, "end": 2662.64, "text": " But yeah you don't have to do that.", "tokens": [583, 1338, 291, 500, 380, 362, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.08506212661515421, "compression_ratio": 1.5376884422110553, "no_speech_prob": 2.6425022952025756e-06}, {"id": 516, "seek": 266264, "start": 2662.64, "end": 2682.64, "text": " Oh wait grab the microphone. So I was just wondering like in PCA do you have to like still because in that sense I guess like you do have to calculate all the eigenvalues and vectors so that you can pick like the top you know eigenvalues that corresponds to like the eigenvectors which are going to be your like principal components.", "tokens": [876, 1699, 4444, 264, 10952, 13, 407, 286, 390, 445, 6359, 411, 294, 6465, 32, 360, 291, 362, 281, 411, 920, 570, 294, 300, 2020, 286, 2041, 411, 291, 360, 362, 281, 8873, 439, 264, 10446, 46033, 293, 18875, 370, 300, 291, 393, 1888, 411, 264, 1192, 291, 458, 10446, 46033, 300, 23249, 281, 411, 264, 10446, 303, 5547, 597, 366, 516, 281, 312, 428, 411, 9716, 6677, 13], "temperature": 0.0, "avg_logprob": -0.15036629977291577, "compression_ratio": 1.6485148514851484, "no_speech_prob": 1.47380014823284e-05}, {"id": 517, "seek": 268264, "start": 2682.64, "end": 2699.64, "text": " Is that the case. It actually depends. So some algorithms with and we'll talk in a later lesson about algorithms for calculating eigenvalues but some algorithms kind of pick out the largest eigenvalues first particularly like iterative algorithms for finding them.", "tokens": [1119, 300, 264, 1389, 13, 467, 767, 5946, 13, 407, 512, 14642, 365, 293, 321, 603, 751, 294, 257, 1780, 6898, 466, 14642, 337, 28258, 10446, 46033, 457, 512, 14642, 733, 295, 1888, 484, 264, 6443, 10446, 46033, 700, 4098, 411, 17138, 1166, 14642, 337, 5006, 552, 13], "temperature": 0.0, "avg_logprob": -0.10280008499438946, "compression_ratio": 1.65, "no_speech_prob": 3.705034760059789e-05}, {"id": 518, "seek": 269964, "start": 2699.64, "end": 2713.64, "text": " Cool. And could you also please talk about like the purpose of doing NFMF because like I remember in the advanced machine learning class we kind of talk about like recommendation system where this thing can be helpful.", "tokens": [8561, 13, 400, 727, 291, 611, 1767, 751, 466, 411, 264, 4334, 295, 884, 13576, 44, 37, 570, 411, 286, 1604, 294, 264, 7339, 3479, 2539, 1508, 321, 733, 295, 751, 466, 411, 11879, 1185, 689, 341, 551, 393, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.2811763972452242, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.428448194928933e-05}, {"id": 519, "seek": 269964, "start": 2713.64, "end": 2720.64, "text": " But like in this sort of like top modeling area why it's helpful. I explained to you we want to do this.", "tokens": [583, 411, 294, 341, 1333, 295, 411, 1192, 15983, 1859, 983, 309, 311, 4961, 13, 286, 8825, 281, 291, 321, 528, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.2811763972452242, "compression_ratio": 1.5528846153846154, "no_speech_prob": 1.428448194928933e-05}, {"id": 520, "seek": 272064, "start": 2720.64, "end": 2734.64, "text": " I think the main reason people argue for NFMF is interpretability and I think I tend to think interpretability is sometimes overblown just in that you can get interpretability from any algorithm by altering your inputs that you put into it.", "tokens": [286, 519, 264, 2135, 1778, 561, 9695, 337, 13576, 44, 37, 307, 7302, 2310, 293, 286, 519, 286, 3928, 281, 519, 7302, 2310, 307, 2171, 670, 5199, 648, 445, 294, 300, 291, 393, 483, 7302, 2310, 490, 604, 9284, 538, 11337, 278, 428, 15743, 300, 291, 829, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.07071012081486164, "compression_ratio": 1.7857142857142858, "no_speech_prob": 3.647171251941472e-05}, {"id": 521, "seek": 272064, "start": 2734.64, "end": 2745.64, "text": " So a lot of kind of supposedly black box algorithms are still interpretable. But that's I would say the main argument I hear for it. But it is definitely something that you that shows up kind of a fair amount.", "tokens": [407, 257, 688, 295, 733, 295, 20581, 2211, 2424, 14642, 366, 920, 7302, 712, 13, 583, 300, 311, 286, 576, 584, 264, 2135, 6770, 286, 1568, 337, 309, 13, 583, 309, 307, 2138, 746, 300, 291, 300, 3110, 493, 733, 295, 257, 3143, 2372, 13], "temperature": 0.0, "avg_logprob": -0.07071012081486164, "compression_ratio": 1.7857142857142858, "no_speech_prob": 3.647171251941472e-05}, {"id": 522, "seek": 274564, "start": 2745.64, "end": 2758.64, "text": " Jeremy I don't know if it's misinterpreting but I thought I heard something else in your question which was about like just given that you have an algorithm that you can select how many columns topics you want.", "tokens": [17809, 286, 500, 380, 458, 498, 309, 311, 3346, 5106, 3712, 783, 457, 286, 1194, 286, 2198, 746, 1646, 294, 428, 1168, 597, 390, 466, 411, 445, 2212, 300, 291, 362, 364, 9284, 300, 291, 393, 3048, 577, 867, 13766, 8378, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.19604170322418213, "compression_ratio": 1.4093959731543624, "no_speech_prob": 3.169115007040091e-05}, {"id": 523, "seek": 275864, "start": 2758.64, "end": 2780.64, "text": " Exists. Do you have to run the whole thing ahead of time before knowing how many to pick off. Like I know later on you're going to show us the little pictures like here's the graph of how the single values decrease. Do you have to like run the whole thing to know to draw that picture and then run it again to pick off.", "tokens": [2111, 1751, 13, 1144, 291, 362, 281, 1190, 264, 1379, 551, 2286, 295, 565, 949, 5276, 577, 867, 281, 1888, 766, 13, 1743, 286, 458, 1780, 322, 291, 434, 516, 281, 855, 505, 264, 707, 5242, 411, 510, 311, 264, 4295, 295, 577, 264, 2167, 4190, 11514, 13, 1144, 291, 362, 281, 411, 1190, 264, 1379, 551, 281, 458, 281, 2642, 300, 3036, 293, 550, 1190, 309, 797, 281, 1888, 766, 13], "temperature": 0.0, "avg_logprob": -0.20824461234243294, "compression_ratio": 1.7243243243243243, "no_speech_prob": 3.0714563763467595e-05}, {"id": 524, "seek": 278064, "start": 2780.64, "end": 2794.64, "text": " The number of columns. OK. And so this this is kind of the question of do you know how your singular singular values are decreasing as you go because that could let you know what a good stopping point is.", "tokens": [440, 1230, 295, 13766, 13, 2264, 13, 400, 370, 341, 341, 307, 733, 295, 264, 1168, 295, 360, 291, 458, 577, 428, 20010, 20010, 4190, 366, 23223, 382, 291, 352, 570, 300, 727, 718, 291, 458, 437, 257, 665, 12767, 935, 307, 13], "temperature": 0.0, "avg_logprob": -0.1452471449020061, "compression_ratio": 1.4366197183098592, "no_speech_prob": 1.863014404079877e-05}, {"id": 525, "seek": 279464, "start": 2794.64, "end": 2813.64, "text": " And yes you can kind of look at your singular values to have a sense of if you want to calculate more. So you would have to calculate the whole thing first in that case to draw the picture to then go back and decide how many to keep.", "tokens": [400, 2086, 291, 393, 733, 295, 574, 412, 428, 20010, 4190, 281, 362, 257, 2020, 295, 498, 291, 528, 281, 8873, 544, 13, 407, 291, 576, 362, 281, 8873, 264, 1379, 551, 700, 294, 300, 1389, 281, 2642, 264, 3036, 281, 550, 352, 646, 293, 4536, 577, 867, 281, 1066, 13], "temperature": 0.0, "avg_logprob": -0.10593222704800692, "compression_ratio": 1.5328947368421053, "no_speech_prob": 6.642761945840903e-06}, {"id": 526, "seek": 281364, "start": 2813.64, "end": 2826.64, "text": " That's true but you I mean you can be calculating kind of calculate it for a set number see if you want to calculate more. Yeah. Yeah. As opposed to doing the whole thing.", "tokens": [663, 311, 2074, 457, 291, 286, 914, 291, 393, 312, 28258, 733, 295, 8873, 309, 337, 257, 992, 1230, 536, 498, 291, 528, 281, 8873, 544, 13, 865, 13, 865, 13, 1018, 8851, 281, 884, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.13226988958933997, "compression_ratio": 1.5060240963855422, "no_speech_prob": 1.2218508345540613e-05}, {"id": 527, "seek": 281364, "start": 2826.64, "end": 2834.64, "text": " When we do robust PCA will actually see an approach that works that way. Yeah.", "tokens": [1133, 321, 360, 13956, 6465, 32, 486, 767, 536, 364, 3109, 300, 1985, 300, 636, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.13226988958933997, "compression_ratio": 1.5060240963855422, "no_speech_prob": 1.2218508345540613e-05}, {"id": 528, "seek": 283464, "start": 2834.64, "end": 2846.64, "text": " OK. Any other questions about kind of this this other view of Adam F. and SVD.", "tokens": [2264, 13, 2639, 661, 1651, 466, 733, 295, 341, 341, 661, 1910, 295, 7938, 479, 13, 293, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.2889957115298412, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.983215224754531e-05}, {"id": 529, "seek": 283464, "start": 2846.64, "end": 2858.64, "text": " Talk about like the pros and cons of like storing a sparse matrix like why sometimes we would rather just for sparse matrix and sometimes we would have like a dense one.", "tokens": [8780, 466, 411, 264, 6267, 293, 1014, 295, 411, 26085, 257, 637, 11668, 8141, 411, 983, 2171, 321, 576, 2831, 445, 337, 637, 11668, 8141, 293, 2171, 321, 576, 362, 411, 257, 18011, 472, 13], "temperature": 0.0, "avg_logprob": -0.2889957115298412, "compression_ratio": 1.5696202531645569, "no_speech_prob": 1.983215224754531e-05}, {"id": 530, "seek": 285864, "start": 2858.64, "end": 2871.64, "text": " And so yeah the pros and cons briefly and we'll go into more detail of this later because we'll see kind of how sci pi handles this and sci pi actually gives you three different ways to store sparse matrices all of which have their own tradeoffs.", "tokens": [400, 370, 1338, 264, 6267, 293, 1014, 10515, 293, 321, 603, 352, 666, 544, 2607, 295, 341, 1780, 570, 321, 603, 536, 733, 295, 577, 2180, 3895, 18722, 341, 293, 2180, 3895, 767, 2709, 291, 1045, 819, 2098, 281, 3531, 637, 11668, 32284, 439, 295, 597, 362, 641, 1065, 4923, 19231, 13], "temperature": 0.0, "avg_logprob": -0.1004031560953381, "compression_ratio": 1.735408560311284, "no_speech_prob": 2.642475919856224e-06}, {"id": 531, "seek": 285864, "start": 2871.64, "end": 2886.64, "text": " But if you don't have that many non zero entries it's a kind of it's like wasting all this memory to just be storing zero and lots of spaces and can take less memory to only store the sparse version.", "tokens": [583, 498, 291, 500, 380, 362, 300, 867, 2107, 4018, 23041, 309, 311, 257, 733, 295, 309, 311, 411, 20457, 439, 341, 4675, 281, 445, 312, 26085, 4018, 293, 3195, 295, 7673, 293, 393, 747, 1570, 4675, 281, 787, 3531, 264, 637, 11668, 3037, 13], "temperature": 0.0, "avg_logprob": -0.1004031560953381, "compression_ratio": 1.735408560311284, "no_speech_prob": 2.642475919856224e-06}, {"id": 532, "seek": 288664, "start": 2886.64, "end": 2897.64, "text": " And this also comes up in algorithms of if you do a lot of computations with just zero you might be doing wasted computations because you know anything time zero is zero.", "tokens": [400, 341, 611, 1487, 493, 294, 14642, 295, 498, 291, 360, 257, 688, 295, 2807, 763, 365, 445, 4018, 291, 1062, 312, 884, 19496, 2807, 763, 570, 291, 458, 1340, 565, 4018, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.09146223935213955, "compression_ratio": 1.4697986577181208, "no_speech_prob": 4.092734343430493e-06}, {"id": 533, "seek": 288664, "start": 2897.64, "end": 2901.64, "text": " So that can be a way to save time as well. Yeah.", "tokens": [407, 300, 393, 312, 257, 636, 281, 3155, 565, 382, 731, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.09146223935213955, "compression_ratio": 1.4697986577181208, "no_speech_prob": 4.092734343430493e-06}, {"id": 534, "seek": 290164, "start": 2901.64, "end": 2917.64, "text": " I was actually thinking with this question about the usefulness of MF actually first just a few pictures I wanted to show to kind of revisit what we talked about. I'll start here on Thursday.", "tokens": [286, 390, 767, 1953, 365, 341, 1168, 466, 264, 4420, 1287, 295, 376, 37, 767, 700, 445, 257, 1326, 5242, 286, 1415, 281, 855, 281, 733, 295, 32676, 437, 321, 2825, 466, 13, 286, 603, 722, 510, 322, 10383, 13], "temperature": 0.0, "avg_logprob": -0.2441227219321511, "compression_ratio": 1.3546099290780143, "no_speech_prob": 7.766382623231038e-06}, {"id": 535, "seek": 291764, "start": 2917.64, "end": 2937.64, "text": " And I've kind of updated the notebook from Thursday just a minor additions if you want to grab that again. But I showed that you know if you actually I guess first I should show kind of using this perspective of matrix matrix multiplication being about taking linear combinations of columns.", "tokens": [400, 286, 600, 733, 295, 10588, 264, 21060, 490, 10383, 445, 257, 6696, 35113, 498, 291, 528, 281, 4444, 300, 797, 13, 583, 286, 4712, 300, 291, 458, 498, 291, 767, 286, 2041, 700, 286, 820, 855, 733, 295, 1228, 341, 4585, 295, 8141, 8141, 27290, 885, 466, 1940, 8213, 21267, 295, 13766, 13], "temperature": 0.0, "avg_logprob": -0.15268733177656008, "compression_ratio": 1.5481171548117154, "no_speech_prob": 7.527407433371991e-06}, {"id": 536, "seek": 291764, "start": 2937.64, "end": 2942.64, "text": " In the case of MF with reconstructing someone's face.", "tokens": [682, 264, 1389, 295, 376, 37, 365, 31499, 278, 1580, 311, 1851, 13], "temperature": 0.0, "avg_logprob": -0.15268733177656008, "compression_ratio": 1.5481171548117154, "no_speech_prob": 7.527407433371991e-06}, {"id": 537, "seek": 291764, "start": 2942.64, "end": 2945.64, "text": " Let me make this bigger.", "tokens": [961, 385, 652, 341, 3801, 13], "temperature": 0.0, "avg_logprob": -0.15268733177656008, "compression_ratio": 1.5481171548117154, "no_speech_prob": 7.527407433371991e-06}, {"id": 538, "seek": 294564, "start": 2945.64, "end": 2957.64, "text": " You can see that here we've got facial features and so this is kind of like the bridge of someone's nose and underneath their eyes. And another feature might be the tip of somebody's nose.", "tokens": [509, 393, 536, 300, 510, 321, 600, 658, 15642, 4122, 293, 370, 341, 307, 733, 295, 411, 264, 7283, 295, 1580, 311, 6690, 293, 7223, 641, 2575, 13, 400, 1071, 4111, 1062, 312, 264, 4125, 295, 2618, 311, 6690, 13], "temperature": 0.0, "avg_logprob": -0.07286447745103103, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.618624446564354e-05}, {"id": 539, "seek": 294564, "start": 2957.64, "end": 2965.64, "text": " And here a feature is somebody's brows and you can think about taking these different features and wanting to add up a combination of them to make somebody's face.", "tokens": [400, 510, 257, 4111, 307, 2618, 311, 8333, 293, 291, 393, 519, 466, 1940, 613, 819, 4122, 293, 7935, 281, 909, 493, 257, 6562, 295, 552, 281, 652, 2618, 311, 1851, 13], "temperature": 0.0, "avg_logprob": -0.07286447745103103, "compression_ratio": 1.7512437810945274, "no_speech_prob": 1.618624446564354e-05}, {"id": 540, "seek": 296564, "start": 2965.64, "end": 2976.64, "text": " And so here kind of each each face is taken by or made by doing a linear combination of these facial feature columns.", "tokens": [400, 370, 510, 733, 295, 1184, 1184, 1851, 307, 2726, 538, 420, 1027, 538, 884, 257, 8213, 6562, 295, 613, 15642, 4111, 13766, 13], "temperature": 0.0, "avg_logprob": -0.07092188500069282, "compression_ratio": 1.8153846153846154, "no_speech_prob": 6.240703442017548e-06}, {"id": 541, "seek": 296564, "start": 2976.64, "end": 2981.64, "text": " So this is kind of a good example of this linear combination perspective.", "tokens": [407, 341, 307, 733, 295, 257, 665, 1365, 295, 341, 8213, 6562, 4585, 13], "temperature": 0.0, "avg_logprob": -0.07092188500069282, "compression_ratio": 1.8153846153846154, "no_speech_prob": 6.240703442017548e-06}, {"id": 542, "seek": 296564, "start": 2981.64, "end": 2990.64, "text": " And so here the coefficients are coming from this column they're telling you the importance of each different facial feature and you take your linear combination.", "tokens": [400, 370, 510, 264, 31994, 366, 1348, 490, 341, 7738, 436, 434, 3585, 291, 264, 7379, 295, 1184, 819, 15642, 4111, 293, 291, 747, 428, 8213, 6562, 13], "temperature": 0.0, "avg_logprob": -0.07092188500069282, "compression_ratio": 1.8153846153846154, "no_speech_prob": 6.240703442017548e-06}, {"id": 543, "seek": 299064, "start": 2990.64, "end": 2995.64, "text": " And then over here you have faces. So this is kind of an example of that.", "tokens": [400, 550, 670, 510, 291, 362, 8475, 13, 407, 341, 307, 733, 295, 364, 1365, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.13778392473856607, "compression_ratio": 1.4497041420118344, "no_speech_prob": 4.710759640147444e-06}, {"id": 544, "seek": 299064, "start": 2995.64, "end": 3012.64, "text": " And going back to kind of the motivation for MF if you were doing something like SBD you would be able to have negative values in the faces which has perhaps less meaning.", "tokens": [400, 516, 646, 281, 733, 295, 264, 12335, 337, 376, 37, 498, 291, 645, 884, 746, 411, 26944, 35, 291, 576, 312, 1075, 281, 362, 3671, 4190, 294, 264, 8475, 597, 575, 4317, 1570, 3620, 13], "temperature": 0.0, "avg_logprob": -0.13778392473856607, "compression_ratio": 1.4497041420118344, "no_speech_prob": 4.710759640147444e-06}, {"id": 545, "seek": 301264, "start": 3012.64, "end": 3028.64, "text": " I guess that's like canceling out some other facial feature but kind of coming up with things that are only non-negative is more immediately interpretable.", "tokens": [286, 2041, 300, 311, 411, 10373, 278, 484, 512, 661, 15642, 4111, 457, 733, 295, 1348, 493, 365, 721, 300, 366, 787, 2107, 12, 28561, 1166, 307, 544, 4258, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.11921364920479911, "compression_ratio": 1.389261744966443, "no_speech_prob": 4.2892211240541656e-06}, {"id": 546, "seek": 301264, "start": 3028.64, "end": 3033.64, "text": " I think we should probably stop for our break soon.", "tokens": [286, 519, 321, 820, 1391, 1590, 337, 527, 1821, 2321, 13], "temperature": 0.0, "avg_logprob": -0.11921364920479911, "compression_ratio": 1.389261744966443, "no_speech_prob": 4.2892211240541656e-06}, {"id": 547, "seek": 303364, "start": 3033.64, "end": 3043.64, "text": " So let's meet back in seven minutes and we will be continuing with some new material later in class.", "tokens": [407, 718, 311, 1677, 646, 294, 3407, 2077, 293, 321, 486, 312, 9289, 365, 512, 777, 2527, 1780, 294, 1508, 13], "temperature": 0.0, "avg_logprob": -0.03423981846503492, "compression_ratio": 1.4522292993630572, "no_speech_prob": 1.933316298163845e-06}, {"id": 548, "seek": 303364, "start": 3043.64, "end": 3053.64, "text": " So I just wanted to briefly show a tweet that I saw this morning that references one of the concepts we talked about last week.", "tokens": [407, 286, 445, 1415, 281, 10515, 855, 257, 15258, 300, 286, 1866, 341, 2446, 300, 15400, 472, 295, 264, 10392, 321, 2825, 466, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.03423981846503492, "compression_ratio": 1.4522292993630572, "no_speech_prob": 1.933316298163845e-06}, {"id": 549, "seek": 305364, "start": 3053.64, "end": 3063.64, "text": " Do you remember what temporaries are?", "tokens": [1144, 291, 1604, 437, 8219, 4889, 366, 30], "temperature": 0.0, "avg_logprob": -0.19187366394769578, "compression_ratio": 1.2956521739130435, "no_speech_prob": 3.882416422129609e-05}, {"id": 550, "seek": 305364, "start": 3063.64, "end": 3073.64, "text": " It's like you're keeping the memory A plus B but when you add C then you have to forget the result of A plus B.", "tokens": [467, 311, 411, 291, 434, 5145, 264, 4675, 316, 1804, 363, 457, 562, 291, 909, 383, 550, 291, 362, 281, 2870, 264, 1874, 295, 316, 1804, 363, 13], "temperature": 0.0, "avg_logprob": -0.19187366394769578, "compression_ratio": 1.2956521739130435, "no_speech_prob": 3.882416422129609e-05}, {"id": 551, "seek": 307364, "start": 3073.64, "end": 3087.64, "text": " So it is that you're having to allocate memory to store A plus B. And if you had a longer computation like we saw an example last week that I forget it was like A times B squared plus the natural log of C.", "tokens": [407, 309, 307, 300, 291, 434, 1419, 281, 35713, 4675, 281, 3531, 316, 1804, 363, 13, 400, 498, 291, 632, 257, 2854, 24903, 411, 321, 1866, 364, 1365, 1036, 1243, 300, 286, 2870, 309, 390, 411, 316, 1413, 363, 8889, 1804, 264, 3303, 3565, 295, 383, 13], "temperature": 0.0, "avg_logprob": -0.10265194201001934, "compression_ratio": 1.385135135135135, "no_speech_prob": 1.5206095667963382e-05}, {"id": 552, "seek": 308764, "start": 3087.64, "end": 3105.64, "text": " NumPy was storing or the old version of NumPy was storing the natural log of C in one place was having to store A squared, A squared times B and allocate all this temporary memory that kind of takes up time and memory to do.", "tokens": [22592, 47, 88, 390, 26085, 420, 264, 1331, 3037, 295, 22592, 47, 88, 390, 26085, 264, 3303, 3565, 295, 383, 294, 472, 1081, 390, 1419, 281, 3531, 316, 8889, 11, 316, 8889, 1413, 363, 293, 35713, 439, 341, 13413, 4675, 300, 733, 295, 2516, 493, 565, 293, 4675, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.10674325769597834, "compression_ratio": 1.5448275862068965, "no_speech_prob": 1.6186539141926914e-05}, {"id": 553, "seek": 310564, "start": 3105.64, "end": 3120.64, "text": " And so I saw this, this is an announcement just from two weeks ago that the newest release of NumPy is allowing the reuse of temporaries. So it is still allocating temporaries but it's allowing to reuse them.", "tokens": [400, 370, 286, 1866, 341, 11, 341, 307, 364, 12847, 445, 490, 732, 3259, 2057, 300, 264, 17569, 4374, 295, 22592, 47, 88, 307, 8293, 264, 26225, 295, 8219, 4889, 13, 407, 309, 307, 920, 12660, 990, 8219, 4889, 457, 309, 311, 8293, 281, 26225, 552, 13], "temperature": 0.0, "avg_logprob": -0.07953904656802907, "compression_ratio": 1.4964028776978417, "no_speech_prob": 1.06140223579132e-05}, {"id": 554, "seek": 312064, "start": 3120.64, "end": 3137.64, "text": " So I thought this was kind of a neat example of how this field is kind of always changing and it's interesting to keep up on it and kind of to see it in the news.", "tokens": [407, 286, 1194, 341, 390, 733, 295, 257, 10654, 1365, 295, 577, 341, 2519, 307, 733, 295, 1009, 4473, 293, 309, 311, 1880, 281, 1066, 493, 322, 309, 293, 733, 295, 281, 536, 309, 294, 264, 2583, 13], "temperature": 0.0, "avg_logprob": -0.08386224224453881, "compression_ratio": 1.3846153846153846, "no_speech_prob": 1.2218031770316884e-05}, {"id": 555, "seek": 313764, "start": 3137.64, "end": 3158.64, "text": " So kind of before we move on I wanted to revisit PyTorch and last time and I just kind of found some material from a PyTorch autograd introduction but does anyone remember what autograd is for PyTorch?", "tokens": [407, 733, 295, 949, 321, 1286, 322, 286, 1415, 281, 32676, 9953, 51, 284, 339, 293, 1036, 565, 293, 286, 445, 733, 295, 1352, 512, 2527, 490, 257, 9953, 51, 284, 339, 1476, 664, 6206, 9339, 457, 775, 2878, 1604, 437, 1476, 664, 6206, 307, 337, 9953, 51, 284, 339, 30], "temperature": 0.0, "avg_logprob": -0.10966631282459606, "compression_ratio": 1.4779411764705883, "no_speech_prob": 4.710774192062672e-06}, {"id": 556, "seek": 315864, "start": 3158.64, "end": 3168.64, "text": " Actually okay even before that let me ask, do you remember why I decided to use PyTorch last time?", "tokens": [5135, 1392, 754, 949, 300, 718, 385, 1029, 11, 360, 291, 1604, 983, 286, 3047, 281, 764, 9953, 51, 284, 339, 1036, 565, 30], "temperature": 0.0, "avg_logprob": -0.10260467529296875, "compression_ratio": 1.52, "no_speech_prob": 2.355012111365795e-05}, {"id": 557, "seek": 315864, "start": 3168.64, "end": 3183.64, "text": " Yeah so I hear a lot of people saying GPU. I wanted to speed things up by running them on the GPU and PyTorch, one of its purposes is it's a deep learning framework, but another purpose is that it's a alternative to NumPy that runs on the GPU.", "tokens": [865, 370, 286, 1568, 257, 688, 295, 561, 1566, 18407, 13, 286, 1415, 281, 3073, 721, 493, 538, 2614, 552, 322, 264, 18407, 293, 9953, 51, 284, 339, 11, 472, 295, 1080, 9932, 307, 309, 311, 257, 2452, 2539, 8388, 11, 457, 1071, 4334, 307, 300, 309, 311, 257, 8535, 281, 22592, 47, 88, 300, 6676, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.10260467529296875, "compression_ratio": 1.52, "no_speech_prob": 2.355012111365795e-05}, {"id": 558, "seek": 318364, "start": 3183.64, "end": 3190.64, "text": " Does anybody remember the method that we use to tell PyTorch to put things on the GPU?", "tokens": [4402, 4472, 1604, 264, 3170, 300, 321, 764, 281, 980, 9953, 51, 284, 339, 281, 829, 721, 322, 264, 18407, 30], "temperature": 0.0, "avg_logprob": -0.12849070900364926, "compression_ratio": 1.6310679611650485, "no_speech_prob": 4.2643729102564976e-05}, {"id": 559, "seek": 318364, "start": 3190.64, "end": 3196.64, "text": " Yes I heard a lot of people say.cuda and so that actually let me go to that.", "tokens": [1079, 286, 2198, 257, 688, 295, 561, 584, 2411, 66, 11152, 293, 370, 300, 767, 718, 385, 352, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.12849070900364926, "compression_ratio": 1.6310679611650485, "no_speech_prob": 4.2643729102564976e-05}, {"id": 560, "seek": 318364, "start": 3196.64, "end": 3209.64, "text": " There was a note that if you're not using a GPU you want to delete the.cuda's that show up here, but that's where we're explicitly telling PyTorch to put things on the GPU.", "tokens": [821, 390, 257, 3637, 300, 498, 291, 434, 406, 1228, 257, 18407, 291, 528, 281, 12097, 264, 2411, 66, 11152, 311, 300, 855, 493, 510, 11, 457, 300, 311, 689, 321, 434, 20803, 3585, 9953, 51, 284, 339, 281, 829, 721, 322, 264, 18407, 13], "temperature": 0.0, "avg_logprob": -0.12849070900364926, "compression_ratio": 1.6310679611650485, "no_speech_prob": 4.2643729102564976e-05}, {"id": 561, "seek": 320964, "start": 3209.64, "end": 3219.64, "text": " Okay so autograd, does anyone remember what that is?", "tokens": [1033, 370, 1476, 664, 6206, 11, 775, 2878, 1604, 437, 300, 307, 30], "temperature": 0.0, "avg_logprob": -0.18181415632659315, "compression_ratio": 1.3333333333333333, "no_speech_prob": 3.0239729312597774e-05}, {"id": 562, "seek": 320964, "start": 3219.64, "end": 3221.64, "text": " Oh yeah?", "tokens": [876, 1338, 30], "temperature": 0.0, "avg_logprob": -0.18181415632659315, "compression_ratio": 1.3333333333333333, "no_speech_prob": 3.0239729312597774e-05}, {"id": 563, "seek": 320964, "start": 3221.64, "end": 3227.64, "text": " I don't think I remember completely but isn't it like an optimization?", "tokens": [286, 500, 380, 519, 286, 1604, 2584, 457, 1943, 380, 309, 411, 364, 19618, 30], "temperature": 0.0, "avg_logprob": -0.18181415632659315, "compression_ratio": 1.3333333333333333, "no_speech_prob": 3.0239729312597774e-05}, {"id": 564, "seek": 320964, "start": 3227.64, "end": 3231.64, "text": " So it is very useful for optimization, yes.", "tokens": [407, 309, 307, 588, 4420, 337, 19618, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.18181415632659315, "compression_ratio": 1.3333333333333333, "no_speech_prob": 3.0239729312597774e-05}, {"id": 565, "seek": 323164, "start": 3231.64, "end": 3252.64, "text": " And so, no that was good, definitely in the very right ballpark. So what do we need, so we talked about stochastic gradient descent last time, what do we need to know to be able to do gradient descent or stochastic gradient descent?", "tokens": [400, 370, 11, 572, 300, 390, 665, 11, 2138, 294, 264, 588, 558, 2594, 31239, 13, 407, 437, 360, 321, 643, 11, 370, 321, 2825, 466, 342, 8997, 2750, 16235, 23475, 1036, 565, 11, 437, 360, 321, 643, 281, 458, 281, 312, 1075, 281, 360, 16235, 23475, 420, 342, 8997, 2750, 16235, 23475, 30], "temperature": 0.0, "avg_logprob": -0.12185708800358559, "compression_ratio": 1.738562091503268, "no_speech_prob": 1.5935471310513094e-05}, {"id": 566, "seek": 323164, "start": 3252.64, "end": 3254.64, "text": " You need to know the derivatives.", "tokens": [509, 643, 281, 458, 264, 33733, 13], "temperature": 0.0, "avg_logprob": -0.12185708800358559, "compression_ratio": 1.738562091503268, "no_speech_prob": 1.5935471310513094e-05}, {"id": 567, "seek": 325464, "start": 3254.64, "end": 3273.64, "text": " Exactly, we need derivatives and so autograd is automatic differentiation because if you don't know the derivative and don't want to have to calculate it, in some cases you may not even be able to calculate it, PyTorch's autograd calculates it for you by letting variables kind of keep track of how they were made.", "tokens": [7587, 11, 321, 643, 33733, 293, 370, 1476, 664, 6206, 307, 12509, 38902, 570, 498, 291, 500, 380, 458, 264, 13760, 293, 500, 380, 528, 281, 362, 281, 8873, 309, 11, 294, 512, 3331, 291, 815, 406, 754, 312, 1075, 281, 8873, 309, 11, 9953, 51, 284, 339, 311, 1476, 664, 6206, 4322, 1024, 309, 337, 291, 538, 8295, 9102, 733, 295, 1066, 2837, 295, 577, 436, 645, 1027, 13], "temperature": 0.0, "avg_logprob": -0.08014855629358536, "compression_ratio": 1.5940594059405941, "no_speech_prob": 6.048723662388511e-06}, {"id": 568, "seek": 325464, "start": 3273.64, "end": 3274.64, "text": " Jeremy?", "tokens": [17809, 30], "temperature": 0.0, "avg_logprob": -0.08014855629358536, "compression_ratio": 1.5940594059405941, "no_speech_prob": 6.048723662388511e-06}, {"id": 569, "seek": 327464, "start": 3274.64, "end": 3288.64, "text": " When I tell people this, they always assume that it must be doing it really slowly by like calculating the function and then the function plus a little bit, finite difference.", "tokens": [1133, 286, 980, 561, 341, 11, 436, 1009, 6552, 300, 309, 1633, 312, 884, 309, 534, 5692, 538, 411, 28258, 264, 2445, 293, 550, 264, 2445, 1804, 257, 707, 857, 11, 19362, 2649, 13], "temperature": 0.0, "avg_logprob": -0.22522952431126644, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.8281690144212916e-05}, {"id": 570, "seek": 327464, "start": 3288.64, "end": 3301.64, "text": " Like even Terrence here at USF, I was telling him and he was like oh too slow, and I had to keep saying to him like no it actually calculates the derivative properly, like it's so amazing that it's possible and I don't believe you.", "tokens": [1743, 754, 6564, 10760, 510, 412, 2546, 37, 11, 286, 390, 3585, 796, 293, 415, 390, 411, 1954, 886, 2964, 11, 293, 286, 632, 281, 1066, 1566, 281, 796, 411, 572, 309, 767, 4322, 1024, 264, 13760, 6108, 11, 411, 309, 311, 370, 2243, 300, 309, 311, 1944, 293, 286, 500, 380, 1697, 291, 13], "temperature": 0.0, "avg_logprob": -0.22522952431126644, "compression_ratio": 1.6345381526104417, "no_speech_prob": 5.8281690144212916e-05}, {"id": 571, "seek": 330164, "start": 3301.64, "end": 3308.64, "text": " Yeah, thank you. Yeah, so this is kind of calculating the derivative very fastly.", "tokens": [865, 11, 1309, 291, 13, 865, 11, 370, 341, 307, 733, 295, 28258, 264, 13760, 588, 2370, 356, 13], "temperature": 0.0, "avg_logprob": -0.12592649014196663, "compression_ratio": 1.6628352490421456, "no_speech_prob": 1.9832268662867136e-05}, {"id": 572, "seek": 330164, "start": 3308.64, "end": 3314.64, "text": " So I just wanted to kind of show some basics of PyTorch again because I think I know what we did last time was very quick.", "tokens": [407, 286, 445, 1415, 281, 733, 295, 855, 512, 14688, 295, 9953, 51, 284, 339, 797, 570, 286, 519, 286, 458, 437, 321, 630, 1036, 565, 390, 588, 1702, 13], "temperature": 0.0, "avg_logprob": -0.12592649014196663, "compression_ratio": 1.6628352490421456, "no_speech_prob": 1.9832268662867136e-05}, {"id": 573, "seek": 330164, "start": 3314.64, "end": 3329.64, "text": " So here creating a variable, this is just a two by two matrix and we're saying requires grad equals true and so that's letting it know, hey we're going to want to be able to get derivatives with respect to this variable later on.", "tokens": [407, 510, 4084, 257, 7006, 11, 341, 307, 445, 257, 732, 538, 732, 8141, 293, 321, 434, 1566, 7029, 2771, 6915, 2074, 293, 370, 300, 311, 8295, 309, 458, 11, 4177, 321, 434, 516, 281, 528, 281, 312, 1075, 281, 483, 33733, 365, 3104, 281, 341, 7006, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.12592649014196663, "compression_ratio": 1.6628352490421456, "no_speech_prob": 1.9832268662867136e-05}, {"id": 574, "seek": 332964, "start": 3329.64, "end": 3336.64, "text": " And then I'm printing X and it tells me this is just, oh and I've initialized it to torch.ones.", "tokens": [400, 550, 286, 478, 14699, 1783, 293, 309, 5112, 385, 341, 307, 445, 11, 1954, 293, 286, 600, 5883, 1602, 309, 281, 27822, 13, 2213, 13], "temperature": 0.0, "avg_logprob": -0.13204123334186832, "compression_ratio": 1.575268817204301, "no_speech_prob": 2.177963324356824e-05}, {"id": 575, "seek": 332964, "start": 3336.64, "end": 3346.64, "text": " NumPy has a very similar method, np.ones that initializes a variable all to ones. There's also zeros that initializes all to zeros.", "tokens": [22592, 47, 88, 575, 257, 588, 2531, 3170, 11, 33808, 13, 2213, 300, 5883, 5660, 257, 7006, 439, 281, 2306, 13, 821, 311, 611, 35193, 300, 5883, 5660, 439, 281, 35193, 13], "temperature": 0.0, "avg_logprob": -0.13204123334186832, "compression_ratio": 1.575268817204301, "no_speech_prob": 2.177963324356824e-05}, {"id": 576, "seek": 332964, "start": 3346.64, "end": 3351.64, "text": " So here we have a two by two tensor where the values are all one.", "tokens": [407, 510, 321, 362, 257, 732, 538, 732, 40863, 689, 264, 4190, 366, 439, 472, 13], "temperature": 0.0, "avg_logprob": -0.13204123334186832, "compression_ratio": 1.575268817204301, "no_speech_prob": 2.177963324356824e-05}, {"id": 577, "seek": 335164, "start": 3351.64, "end": 3360.64, "text": " And a tensor is just a generalization of a matrix. So you can think of it as a matrix, but it can have higher dimensions.", "tokens": [400, 257, 40863, 307, 445, 257, 2674, 2144, 295, 257, 8141, 13, 407, 291, 393, 519, 295, 309, 382, 257, 8141, 11, 457, 309, 393, 362, 2946, 12819, 13], "temperature": 0.0, "avg_logprob": -0.0695733480815646, "compression_ratio": 1.621761658031088, "no_speech_prob": 1.6700914784451015e-05}, {"id": 578, "seek": 335164, "start": 3360.64, "end": 3375.64, "text": " And then X has its data, which in this case is all ones, and then it also has a grad attribute, which is going to store the gradient, which right now is zero because we haven't done anything.", "tokens": [400, 550, 1783, 575, 1080, 1412, 11, 597, 294, 341, 1389, 307, 439, 2306, 11, 293, 550, 309, 611, 575, 257, 2771, 19667, 11, 597, 307, 516, 281, 3531, 264, 16235, 11, 597, 558, 586, 307, 4018, 570, 321, 2378, 380, 1096, 1340, 13], "temperature": 0.0, "avg_logprob": -0.0695733480815646, "compression_ratio": 1.621761658031088, "no_speech_prob": 1.6700914784451015e-05}, {"id": 579, "seek": 337564, "start": 3375.64, "end": 3383.64, "text": " So then we do Y equals X plus two, we print Y, that's all threes.", "tokens": [407, 550, 321, 360, 398, 6915, 1783, 1804, 732, 11, 321, 4482, 398, 11, 300, 311, 439, 258, 4856, 13], "temperature": 0.0, "avg_logprob": -0.09282054096819406, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.952485035872087e-05}, {"id": 580, "seek": 337564, "start": 3383.64, "end": 3390.64, "text": " And we'll come back to this idea in lesson kind of notebook three, but there's something called broadcasting.", "tokens": [400, 321, 603, 808, 646, 281, 341, 1558, 294, 6898, 733, 295, 21060, 1045, 11, 457, 456, 311, 746, 1219, 30024, 13], "temperature": 0.0, "avg_logprob": -0.09282054096819406, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.952485035872087e-05}, {"id": 581, "seek": 337564, "start": 3390.64, "end": 3398.64, "text": " You'll notice here we're adding a scalar to a matrix and what it's done is it's basically just kind of multiplied the scalar to be the right dimensions.", "tokens": [509, 603, 3449, 510, 321, 434, 5127, 257, 39684, 281, 257, 8141, 293, 437, 309, 311, 1096, 307, 309, 311, 1936, 445, 733, 295, 17207, 264, 39684, 281, 312, 264, 558, 12819, 13], "temperature": 0.0, "avg_logprob": -0.09282054096819406, "compression_ratio": 1.569377990430622, "no_speech_prob": 1.952485035872087e-05}, {"id": 582, "seek": 339864, "start": 3398.64, "end": 3407.64, "text": " So basically we added our matrix one one one one two two two two two to get all these threes.", "tokens": [407, 1936, 321, 3869, 527, 8141, 472, 472, 472, 472, 732, 732, 732, 732, 732, 281, 483, 439, 613, 258, 4856, 13], "temperature": 0.0, "avg_logprob": -0.13628678572805306, "compression_ratio": 1.4929577464788732, "no_speech_prob": 6.144045983091928e-06}, {"id": 583, "seek": 339864, "start": 3407.64, "end": 3411.64, "text": " But this idea of broadcasting is pretty important and we'll see more of it.", "tokens": [583, 341, 1558, 295, 30024, 307, 1238, 1021, 293, 321, 603, 536, 544, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13628678572805306, "compression_ratio": 1.4929577464788732, "no_speech_prob": 6.144045983091928e-06}, {"id": 584, "seek": 339864, "start": 3411.64, "end": 3416.64, "text": " Then we do Z equals Y times Y times three.", "tokens": [1396, 321, 360, 1176, 6915, 398, 1413, 398, 1413, 1045, 13], "temperature": 0.0, "avg_logprob": -0.13628678572805306, "compression_ratio": 1.4929577464788732, "no_speech_prob": 6.144045983091928e-06}, {"id": 585, "seek": 341664, "start": 3416.64, "end": 3428.64, "text": " The out equals the sum of Z. So if we print out Z and out, Z is twenty seven, twenty seven, twenty seven, twenty seven, out is one oh eight.", "tokens": [440, 484, 6915, 264, 2408, 295, 1176, 13, 407, 498, 321, 4482, 484, 1176, 293, 484, 11, 1176, 307, 7699, 3407, 11, 7699, 3407, 11, 7699, 3407, 11, 7699, 3407, 11, 484, 307, 472, 1954, 3180, 13], "temperature": 0.0, "avg_logprob": -0.11747872339536065, "compression_ratio": 1.654320987654321, "no_speech_prob": 4.029331194033148e-06}, {"id": 586, "seek": 341664, "start": 3428.64, "end": 3434.64, "text": " And then we do out dot backwards, which is kind of back propagation and it's calculating the gradient.", "tokens": [400, 550, 321, 360, 484, 5893, 12204, 11, 597, 307, 733, 295, 646, 38377, 293, 309, 311, 28258, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.11747872339536065, "compression_ratio": 1.654320987654321, "no_speech_prob": 4.029331194033148e-06}, {"id": 587, "seek": 341664, "start": 3434.64, "end": 3439.64, "text": " And let's take a moment.", "tokens": [400, 718, 311, 747, 257, 1623, 13], "temperature": 0.0, "avg_logprob": -0.11747872339536065, "compression_ratio": 1.654320987654321, "no_speech_prob": 4.029331194033148e-06}, {"id": 588, "seek": 343964, "start": 3439.64, "end": 3448.64, "text": " Let's see if this worked now. Oh, yes, it did. OK. So remember,", "tokens": [961, 311, 536, 498, 341, 2732, 586, 13, 876, 11, 2086, 11, 309, 630, 13, 2264, 13, 407, 1604, 11], "temperature": 0.0, "avg_logprob": -0.28599373175173387, "compression_ratio": 1.1226415094339623, "no_speech_prob": 6.0487018345156685e-06}, {"id": 589, "seek": 343964, "start": 3448.64, "end": 3452.64, "text": " right. Z is.", "tokens": [558, 13, 1176, 307, 13], "temperature": 0.0, "avg_logprob": -0.28599373175173387, "compression_ratio": 1.1226415094339623, "no_speech_prob": 6.0487018345156685e-06}, {"id": 590, "seek": 343964, "start": 3452.64, "end": 3455.64, "text": " So Y was.", "tokens": [407, 398, 390, 13], "temperature": 0.0, "avg_logprob": -0.28599373175173387, "compression_ratio": 1.1226415094339623, "no_speech_prob": 6.0487018345156685e-06}, {"id": 591, "seek": 343964, "start": 3455.64, "end": 3460.64, "text": " X plus two squared.", "tokens": [1783, 1804, 732, 8889, 13], "temperature": 0.0, "avg_logprob": -0.28599373175173387, "compression_ratio": 1.1226415094339623, "no_speech_prob": 6.0487018345156685e-06}, {"id": 592, "seek": 343964, "start": 3460.64, "end": 3463.64, "text": " Times three.", "tokens": [11366, 1045, 13], "temperature": 0.0, "avg_logprob": -0.28599373175173387, "compression_ratio": 1.1226415094339623, "no_speech_prob": 6.0487018345156685e-06}, {"id": 593, "seek": 346364, "start": 3463.64, "end": 3480.64, "text": " And we want to take and then Z is the sum of that and really kind of if we think of X being the individual entries now, the sum of that is going to be equal to four kind of times an individual entry since they're all the same.", "tokens": [400, 321, 528, 281, 747, 293, 550, 1176, 307, 264, 2408, 295, 300, 293, 534, 733, 295, 498, 321, 519, 295, 1783, 885, 264, 2609, 23041, 586, 11, 264, 2408, 295, 300, 307, 516, 281, 312, 2681, 281, 1451, 733, 295, 1413, 364, 2609, 8729, 1670, 436, 434, 439, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1118316650390625, "compression_ratio": 1.59375, "no_speech_prob": 4.42540158474003e-06}, {"id": 594, "seek": 346364, "start": 3480.64, "end": 3486.64, "text": " So that's twelve X plus two.", "tokens": [407, 300, 311, 14390, 1783, 1804, 732, 13], "temperature": 0.0, "avg_logprob": -0.1118316650390625, "compression_ratio": 1.59375, "no_speech_prob": 4.42540158474003e-06}, {"id": 595, "seek": 348664, "start": 3486.64, "end": 3495.64, "text": " Squared. And if we take the derivative, so we're interested in out.", "tokens": [8683, 1642, 13, 400, 498, 321, 747, 264, 13760, 11, 370, 321, 434, 3102, 294, 484, 13], "temperature": 0.0, "avg_logprob": -0.3132607019864596, "compression_ratio": 1.0, "no_speech_prob": 6.338894309010357e-06}, {"id": 596, "seek": 348664, "start": 3495.64, "end": 3500.64, "text": " The acts.", "tokens": [440, 10672, 13], "temperature": 0.0, "avg_logprob": -0.3132607019864596, "compression_ratio": 1.0, "no_speech_prob": 6.338894309010357e-06}, {"id": 597, "seek": 350064, "start": 3500.64, "end": 3515.64, "text": " Say, hold on a moment, maybe.", "tokens": [6463, 11, 1797, 322, 257, 1623, 11, 1310, 13], "temperature": 0.0, "avg_logprob": -0.33332798480987547, "compression_ratio": 0.88, "no_speech_prob": 3.1380238851852482e-06}, {"id": 598, "seek": 350064, "start": 3515.64, "end": 3519.64, "text": " Come back. OK.", "tokens": [2492, 646, 13, 2264, 13], "temperature": 0.0, "avg_logprob": -0.33332798480987547, "compression_ratio": 0.88, "no_speech_prob": 3.1380238851852482e-06}, {"id": 599, "seek": 351964, "start": 3519.64, "end": 3539.64, "text": " So, yeah, I'm not going to sum them yet. So you take the derivative and you should remember kind of with an exponent, you pull it down, subtract one off. So that would be six times X plus two to the one power, which is just itself, times the derivative of what's inside, which is just one.", "tokens": [407, 11, 1338, 11, 286, 478, 406, 516, 281, 2408, 552, 1939, 13, 407, 291, 747, 264, 13760, 293, 291, 820, 1604, 733, 295, 365, 364, 37871, 11, 291, 2235, 309, 760, 11, 16390, 472, 766, 13, 407, 300, 576, 312, 2309, 1413, 1783, 1804, 732, 281, 264, 472, 1347, 11, 597, 307, 445, 2564, 11, 1413, 264, 13760, 295, 437, 311, 1854, 11, 597, 307, 445, 472, 13], "temperature": 0.0, "avg_logprob": -0.13405116616863094, "compression_ratio": 1.5966850828729282, "no_speech_prob": 1.0289014426234644e-05}, {"id": 600, "seek": 353964, "start": 3539.64, "end": 3550.64, "text": " And so we're getting that the derivative is six times X plus two. And remember, we're interested in where X is equal to one. So.", "tokens": [400, 370, 321, 434, 1242, 300, 264, 13760, 307, 2309, 1413, 1783, 1804, 732, 13, 400, 1604, 11, 321, 434, 3102, 294, 689, 1783, 307, 2681, 281, 472, 13, 407, 13], "temperature": 0.0, "avg_logprob": -0.10256136788262261, "compression_ratio": 1.5485714285714285, "no_speech_prob": 2.1233445295365527e-06}, {"id": 601, "seek": 353964, "start": 3550.64, "end": 3556.64, "text": " The derivative is six times three equals eighteen.", "tokens": [440, 13760, 307, 2309, 1413, 1045, 6915, 31755, 13], "temperature": 0.0, "avg_logprob": -0.10256136788262261, "compression_ratio": 1.5485714285714285, "no_speech_prob": 2.1233445295365527e-06}, {"id": 602, "seek": 353964, "start": 3556.64, "end": 3562.64, "text": " Which is what it's telling us. So pie torch has done this for us.", "tokens": [3013, 307, 437, 309, 311, 3585, 505, 13, 407, 1730, 27822, 575, 1096, 341, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.10256136788262261, "compression_ratio": 1.5485714285714285, "no_speech_prob": 2.1233445295365527e-06}, {"id": 603, "seek": 353964, "start": 3562.64, "end": 3566.64, "text": " Any questions about that?", "tokens": [2639, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.10256136788262261, "compression_ratio": 1.5485714285714285, "no_speech_prob": 2.1233445295365527e-06}, {"id": 604, "seek": 356664, "start": 3566.64, "end": 3569.64, "text": " Remind us about the difference between a tensor and a variable.", "tokens": [4080, 471, 505, 466, 264, 2649, 1296, 257, 40863, 293, 257, 7006, 13], "temperature": 0.0, "avg_logprob": -0.15310311317443848, "compression_ratio": 1.5286624203821657, "no_speech_prob": 4.494977474678308e-06}, {"id": 605, "seek": 356664, "start": 3569.64, "end": 3581.64, "text": " So tensors and variables have the same API. These are both kind of pie torch notions, but variables have the auto grad option and that they keep track of how they were created.", "tokens": [407, 10688, 830, 293, 9102, 362, 264, 912, 9362, 13, 1981, 366, 1293, 733, 295, 1730, 27822, 35799, 11, 457, 9102, 362, 264, 8399, 2771, 3614, 293, 300, 436, 1066, 2837, 295, 577, 436, 645, 2942, 13], "temperature": 0.0, "avg_logprob": -0.15310311317443848, "compression_ratio": 1.5286624203821657, "no_speech_prob": 4.494977474678308e-06}, {"id": 606, "seek": 358164, "start": 3581.64, "end": 3597.64, "text": " And so you have if you want to use auto grad, you need to use variables. Other questions? So you can see it's really handy to have it calculated for us.", "tokens": [400, 370, 291, 362, 498, 291, 528, 281, 764, 8399, 2771, 11, 291, 643, 281, 764, 9102, 13, 5358, 1651, 30, 407, 291, 393, 536, 309, 311, 534, 13239, 281, 362, 309, 15598, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.141802978515625, "compression_ratio": 1.2773109243697478, "no_speech_prob": 1.9033361695619533e-06}, {"id": 607, "seek": 359764, "start": 3597.64, "end": 3612.64, "text": " OK, so I want to return to kind of where we left off now.", "tokens": [2264, 11, 370, 286, 528, 281, 2736, 281, 733, 295, 689, 321, 1411, 766, 586, 13], "temperature": 0.0, "avg_logprob": -0.14509714126586915, "compression_ratio": 1.2481751824817517, "no_speech_prob": 5.255157702777069e-06}, {"id": 608, "seek": 359764, "start": 3612.64, "end": 3622.64, "text": " Let's start back with comparing approaches. So Scikit learns NMF. It was fast. We didn't have to tune parameters.", "tokens": [961, 311, 722, 646, 365, 15763, 11587, 13, 407, 16942, 22681, 27152, 426, 44, 37, 13, 467, 390, 2370, 13, 492, 994, 380, 362, 281, 10864, 9834, 13], "temperature": 0.0, "avg_logprob": -0.14509714126586915, "compression_ratio": 1.2481751824817517, "no_speech_prob": 5.255157702777069e-06}, {"id": 609, "seek": 362264, "start": 3622.64, "end": 3628.64, "text": " The people that created it used decades of academic research and it was pretty specific to NMF.", "tokens": [440, 561, 300, 2942, 309, 1143, 7878, 295, 7778, 2132, 293, 309, 390, 1238, 2685, 281, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.09950729563266417, "compression_ratio": 1.5179487179487179, "no_speech_prob": 5.593956302618608e-06}, {"id": 610, "seek": 362264, "start": 3628.64, "end": 3636.64, "text": " And then this is a list of kind of some relatively recent research in NMF.", "tokens": [400, 550, 341, 307, 257, 1329, 295, 733, 295, 512, 7226, 5162, 2132, 294, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.09950729563266417, "compression_ratio": 1.5179487179487179, "no_speech_prob": 5.593956302618608e-06}, {"id": 611, "seek": 362264, "start": 3636.64, "end": 3642.64, "text": " So you can see it's kind of an active field.", "tokens": [407, 291, 393, 536, 309, 311, 733, 295, 364, 4967, 2519, 13], "temperature": 0.0, "avg_logprob": -0.09950729563266417, "compression_ratio": 1.5179487179487179, "no_speech_prob": 5.593956302618608e-06}, {"id": 612, "seek": 362264, "start": 3642.64, "end": 3648.64, "text": " And this comes from a Python library called NIMFA that's specifically about NMF.", "tokens": [400, 341, 1487, 490, 257, 15329, 6405, 1219, 426, 6324, 19684, 300, 311, 4682, 466, 426, 44, 37, 13], "temperature": 0.0, "avg_logprob": -0.09950729563266417, "compression_ratio": 1.5179487179487179, "no_speech_prob": 5.593956302618608e-06}, {"id": 613, "seek": 364864, "start": 3648.64, "end": 3656.64, "text": " So then we decided we wanted something that we could use or build ourselves. So we used pie torch and stochastic gradient descent.", "tokens": [407, 550, 321, 3047, 321, 1415, 746, 300, 321, 727, 764, 420, 1322, 4175, 13, 407, 321, 1143, 1730, 27822, 293, 342, 8997, 2750, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.07448671545301165, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.3006265362491831e-05}, {"id": 614, "seek": 364864, "start": 3656.64, "end": 3666.64, "text": " Stochastic gradient descent is a very general purpose optimization algorithm. So we were just choosing to apply it to NMF, but it works for", "tokens": [745, 8997, 2750, 16235, 23475, 307, 257, 588, 2674, 4334, 19618, 9284, 13, 407, 321, 645, 445, 10875, 281, 3079, 309, 281, 426, 44, 37, 11, 457, 309, 1985, 337], "temperature": 0.0, "avg_logprob": -0.07448671545301165, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.3006265362491831e-05}, {"id": 615, "seek": 364864, "start": 3666.64, "end": 3671.64, "text": " many, many different optimization problems. So it's a very good general purpose tool to have.", "tokens": [867, 11, 867, 819, 19618, 2740, 13, 407, 309, 311, 257, 588, 665, 2674, 4334, 2290, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.07448671545301165, "compression_ratio": 1.7009345794392523, "no_speech_prob": 1.3006265362491831e-05}, {"id": 616, "seek": 367164, "start": 3671.64, "end": 3678.64, "text": " It didn't take us that long to implement. We did have to do more fiddling with the parameters, though. So remember we had a learning rate,", "tokens": [467, 994, 380, 747, 505, 300, 938, 281, 4445, 13, 492, 630, 362, 281, 360, 544, 283, 14273, 1688, 365, 264, 9834, 11, 1673, 13, 407, 1604, 321, 632, 257, 2539, 3314, 11], "temperature": 0.0, "avg_logprob": -0.09492620580336626, "compression_ratio": 1.5023696682464456, "no_speech_prob": 2.0460731320781633e-05}, {"id": 617, "seek": 367164, "start": 3678.64, "end": 3685.64, "text": " kind of keeping track of what size steps we wanted to take. And it was also not as fast.", "tokens": [733, 295, 5145, 2837, 295, 437, 2744, 4439, 321, 1415, 281, 747, 13, 400, 309, 390, 611, 406, 382, 2370, 13], "temperature": 0.0, "avg_logprob": -0.09492620580336626, "compression_ratio": 1.5023696682464456, "no_speech_prob": 2.0460731320781633e-05}, {"id": 618, "seek": 367164, "start": 3685.64, "end": 3691.64, "text": " We initially tried it in NumPy and because it was so slow, we had to switch to pie torch.", "tokens": [492, 9105, 3031, 309, 294, 22592, 47, 88, 293, 570, 309, 390, 370, 2964, 11, 321, 632, 281, 3679, 281, 1730, 27822, 13], "temperature": 0.0, "avg_logprob": -0.09492620580336626, "compression_ratio": 1.5023696682464456, "no_speech_prob": 2.0460731320781633e-05}, {"id": 619, "seek": 369164, "start": 3691.64, "end": 3701.64, "text": " Jeremy? And we also found one benefit of it, another benefit of it, which was because we made the non-negativity a penalty rather than a constraint.", "tokens": [17809, 30, 400, 321, 611, 1352, 472, 5121, 295, 309, 11, 1071, 5121, 295, 309, 11, 597, 390, 570, 321, 1027, 264, 2107, 12, 28561, 30142, 257, 16263, 2831, 813, 257, 25534, 13], "temperature": 0.0, "avg_logprob": -0.12694187959035239, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.00011060040560550988}, {"id": 620, "seek": 369164, "start": 3701.64, "end": 3710.64, "text": " We had just a few small negatives and that allowed us actually to have a more accurate decomposition, which maybe created some better topics.", "tokens": [492, 632, 445, 257, 1326, 1359, 40019, 293, 300, 4350, 505, 767, 281, 362, 257, 544, 8559, 48356, 11, 597, 1310, 2942, 512, 1101, 8378, 13], "temperature": 0.0, "avg_logprob": -0.12694187959035239, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.00011060040560550988}, {"id": 621, "seek": 369164, "start": 3710.64, "end": 3717.64, "text": " That's true, yes. And that did give us, though, more parameters that we were having to tune of how much weight to give,", "tokens": [663, 311, 2074, 11, 2086, 13, 400, 300, 630, 976, 505, 11, 1673, 11, 544, 9834, 300, 321, 645, 1419, 281, 10864, 295, 577, 709, 3364, 281, 976, 11], "temperature": 0.0, "avg_logprob": -0.12694187959035239, "compression_ratio": 1.6141732283464567, "no_speech_prob": 0.00011060040560550988}, {"id": 622, "seek": 371764, "start": 3717.64, "end": 3724.64, "text": " wanting it to be non-negative versus wanting it to multiply to give us the right answer.", "tokens": [7935, 309, 281, 312, 2107, 12, 28561, 1166, 5717, 7935, 309, 281, 12972, 281, 976, 505, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.09583326414519665, "compression_ratio": 1.3120567375886525, "no_speech_prob": 6.0487900555017404e-06}, {"id": 623, "seek": 371764, "start": 3724.64, "end": 3733.64, "text": " So any questions about NMF or these different approaches?", "tokens": [407, 604, 1651, 466, 426, 44, 37, 420, 613, 819, 11587, 30], "temperature": 0.0, "avg_logprob": -0.09583326414519665, "compression_ratio": 1.3120567375886525, "no_speech_prob": 6.0487900555017404e-06}, {"id": 624, "seek": 371764, "start": 3733.64, "end": 3739.64, "text": " Okay, so we're going to return to SPD.", "tokens": [1033, 11, 370, 321, 434, 516, 281, 2736, 281, 19572, 13], "temperature": 0.0, "avg_logprob": -0.09583326414519665, "compression_ratio": 1.3120567375886525, "no_speech_prob": 6.0487900555017404e-06}, {"id": 625, "seek": 373964, "start": 3739.64, "end": 3752.64, "text": " And we've kind of talked about this sum, this idea of truncated SPD. So for full SPD would be calculating kind of the full dimension of topics.", "tokens": [400, 321, 600, 733, 295, 2825, 466, 341, 2408, 11, 341, 1558, 295, 504, 409, 66, 770, 19572, 13, 407, 337, 1577, 19572, 576, 312, 28258, 733, 295, 264, 1577, 10139, 295, 8378, 13], "temperature": 0.0, "avg_logprob": -0.07273694650450749, "compression_ratio": 1.5251396648044693, "no_speech_prob": 8.664062079333235e-06}, {"id": 626, "seek": 373964, "start": 3752.64, "end": 3758.64, "text": " But we've already seen that it's handy to just have kind of a limited number of topics because those are the most important ones.", "tokens": [583, 321, 600, 1217, 1612, 300, 309, 311, 13239, 281, 445, 362, 733, 295, 257, 5567, 1230, 295, 8378, 570, 729, 366, 264, 881, 1021, 2306, 13], "temperature": 0.0, "avg_logprob": -0.07273694650450749, "compression_ratio": 1.5251396648044693, "no_speech_prob": 8.664062079333235e-06}, {"id": 627, "seek": 375864, "start": 3758.64, "end": 3770.64, "text": " This is also how SPD is used in data compression, where you are kind of choosing a smaller value because you want to kind of save space with your data.", "tokens": [639, 307, 611, 577, 19572, 307, 1143, 294, 1412, 19355, 11, 689, 291, 366, 733, 295, 10875, 257, 4356, 2158, 570, 291, 528, 281, 733, 295, 3155, 1901, 365, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.0493738159300789, "compression_ratio": 1.441860465116279, "no_speech_prob": 2.52118502430676e-06}, {"id": 628, "seek": 375864, "start": 3770.64, "end": 3780.64, "text": " And so, yeah, this is truncated SPD. This is the picture from the Facebook blog post again here.", "tokens": [400, 370, 11, 1338, 11, 341, 307, 504, 409, 66, 770, 19572, 13, 639, 307, 264, 3036, 490, 264, 4384, 6968, 2183, 797, 510, 13], "temperature": 0.0, "avg_logprob": -0.0493738159300789, "compression_ratio": 1.441860465116279, "no_speech_prob": 2.52118502430676e-06}, {"id": 629, "seek": 378064, "start": 3780.64, "end": 3789.64, "text": " And I'm also, I realized after class Thursday that some of the examples, I think, were documents by words and some were words by documents.", "tokens": [400, 286, 478, 611, 11, 286, 5334, 934, 1508, 10383, 300, 512, 295, 264, 5110, 11, 286, 519, 11, 645, 8512, 538, 2283, 293, 512, 645, 2283, 538, 8512, 13], "temperature": 0.0, "avg_logprob": -0.08699243279952037, "compression_ratio": 1.558252427184466, "no_speech_prob": 2.111109824909363e-05}, {"id": 630, "seek": 378064, "start": 3789.64, "end": 3797.64, "text": " So I think I may have misspoke a few times on Thursday about which order they were because we do see both in here.", "tokens": [407, 286, 519, 286, 815, 362, 1713, 48776, 257, 1326, 1413, 322, 10383, 466, 597, 1668, 436, 645, 570, 321, 360, 536, 1293, 294, 510, 13], "temperature": 0.0, "avg_logprob": -0.08699243279952037, "compression_ratio": 1.558252427184466, "no_speech_prob": 2.111109824909363e-05}, {"id": 631, "seek": 378064, "start": 3797.64, "end": 3804.64, "text": " Here, this is saying the words are rows, the hashtags are columns.", "tokens": [1692, 11, 341, 307, 1566, 264, 2283, 366, 13241, 11, 264, 50016, 366, 13766, 13], "temperature": 0.0, "avg_logprob": -0.08699243279952037, "compression_ratio": 1.558252427184466, "no_speech_prob": 2.111109824909363e-05}, {"id": 632, "seek": 380464, "start": 3804.64, "end": 3818.64, "text": " So words, this would basically be words by topics then, topics by topics, and then topics by hashtags.", "tokens": [407, 2283, 11, 341, 576, 1936, 312, 2283, 538, 8378, 550, 11, 8378, 538, 8378, 11, 293, 550, 8378, 538, 50016, 13], "temperature": 0.0, "avg_logprob": -0.08102750248379177, "compression_ratio": 1.6891891891891893, "no_speech_prob": 4.63772312286892e-06}, {"id": 633, "seek": 380464, "start": 3818.64, "end": 3824.64, "text": " And so we're going to be looking at using a randomized algorithm to calculate the truncated SPD.", "tokens": [400, 370, 321, 434, 516, 281, 312, 1237, 412, 1228, 257, 38513, 9284, 281, 8873, 264, 504, 409, 66, 770, 19572, 13], "temperature": 0.0, "avg_logprob": -0.08102750248379177, "compression_ratio": 1.6891891891891893, "no_speech_prob": 4.63772312286892e-06}, {"id": 634, "seek": 380464, "start": 3824.64, "end": 3833.64, "text": " And so this is going to be a quick method. And this kind of gets back to April's question earlier about, do you have to calculate the full SPD and then throw away information?", "tokens": [400, 370, 341, 307, 516, 281, 312, 257, 1702, 3170, 13, 400, 341, 733, 295, 2170, 646, 281, 6929, 311, 1168, 3071, 466, 11, 360, 291, 362, 281, 8873, 264, 1577, 19572, 293, 550, 3507, 1314, 1589, 30], "temperature": 0.0, "avg_logprob": -0.08102750248379177, "compression_ratio": 1.6891891891891893, "no_speech_prob": 4.63772312286892e-06}, {"id": 635, "seek": 383364, "start": 3833.64, "end": 3845.64, "text": " We're going to use a randomized approach to just calculate, well, what we want plus a little bit more of buffer, but to not have to calculate the full thing.", "tokens": [492, 434, 516, 281, 764, 257, 38513, 3109, 281, 445, 8873, 11, 731, 11, 437, 321, 528, 1804, 257, 707, 857, 544, 295, 21762, 11, 457, 281, 406, 362, 281, 8873, 264, 1577, 551, 13], "temperature": 0.0, "avg_logprob": -0.07520001042972911, "compression_ratio": 1.5708502024291497, "no_speech_prob": 6.438907348638168e-06}, {"id": 636, "seek": 383364, "start": 3845.64, "end": 3854.64, "text": " And so we talked about, you know, shortcomings of classical algorithms, just matrices are so large and often data is missing or inaccurate.", "tokens": [400, 370, 321, 2825, 466, 11, 291, 458, 11, 2099, 49886, 295, 13735, 14642, 11, 445, 32284, 366, 370, 2416, 293, 2049, 1412, 307, 5361, 420, 46443, 13], "temperature": 0.0, "avg_logprob": -0.07520001042972911, "compression_ratio": 1.5708502024291497, "no_speech_prob": 6.438907348638168e-06}, {"id": 637, "seek": 383364, "start": 3854.64, "end": 3860.64, "text": " So why spend the extra computation when your result is not going to be perfectly accurate?", "tokens": [407, 983, 3496, 264, 2857, 24903, 562, 428, 1874, 307, 406, 516, 281, 312, 6239, 8559, 30], "temperature": 0.0, "avg_logprob": -0.07520001042972911, "compression_ratio": 1.5708502024291497, "no_speech_prob": 6.438907348638168e-06}, {"id": 638, "seek": 386064, "start": 3860.64, "end": 3866.64, "text": " Another key theme of the course that data transfer is a major role in the time of algorithms.", "tokens": [3996, 2141, 6314, 295, 264, 1164, 300, 1412, 5003, 307, 257, 2563, 3090, 294, 264, 565, 295, 14642, 13], "temperature": 0.0, "avg_logprob": -0.12028161410627694, "compression_ratio": 1.3742331288343559, "no_speech_prob": 4.710816028818954e-06}, {"id": 639, "seek": 386064, "start": 3866.64, "end": 3872.64, "text": " So it's not just about computational complexity.", "tokens": [407, 309, 311, 406, 445, 466, 28270, 14024, 13], "temperature": 0.0, "avg_logprob": -0.12028161410627694, "compression_ratio": 1.3742331288343559, "no_speech_prob": 4.710816028818954e-06}, {"id": 640, "seek": 386064, "start": 3872.64, "end": 3880.64, "text": " And then so we're going to be referencing this paper by HALCO. Let me pull it up.", "tokens": [400, 550, 370, 321, 434, 516, 281, 312, 40582, 341, 3035, 538, 389, 3427, 12322, 13, 961, 385, 2235, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.12028161410627694, "compression_ratio": 1.3742331288343559, "no_speech_prob": 4.710816028818954e-06}, {"id": 641, "seek": 388064, "start": 3880.64, "end": 3900.64, "text": " It's called Finding Structure with Randomness, and I think it's very well written. I particularly like the introduction, so you might want to check it out. But this is where a lot of material in this lecture, as well as I think some in the next lecture comes from.", "tokens": [467, 311, 1219, 31947, 745, 2885, 365, 37603, 1287, 11, 293, 286, 519, 309, 311, 588, 731, 3720, 13, 286, 4098, 411, 264, 9339, 11, 370, 291, 1062, 528, 281, 1520, 309, 484, 13, 583, 341, 307, 689, 257, 688, 295, 2527, 294, 341, 7991, 11, 382, 731, 382, 286, 519, 512, 294, 264, 958, 7991, 1487, 490, 13], "temperature": 0.0, "avg_logprob": -0.12593263387680054, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.6186691937036812e-05}, {"id": 642, "seek": 388064, "start": 3900.64, "end": 3908.64, "text": " And they give different examples. So just know that that's out there.", "tokens": [400, 436, 976, 819, 5110, 13, 407, 445, 458, 300, 300, 311, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.12593263387680054, "compression_ratio": 1.518181818181818, "no_speech_prob": 1.6186691937036812e-05}, {"id": 643, "seek": 390864, "start": 3908.64, "end": 3917.64, "text": " So Scikit-learn has a randomized SVD built in, and this is from the decomposition module.", "tokens": [407, 16942, 22681, 12, 306, 1083, 575, 257, 38513, 31910, 35, 3094, 294, 11, 293, 341, 307, 490, 264, 48356, 10088, 13], "temperature": 0.0, "avg_logprob": -0.11324213772285276, "compression_ratio": 1.4495412844036697, "no_speech_prob": 4.264393646735698e-05}, {"id": 644, "seek": 390864, "start": 3917.64, "end": 3925.64, "text": " Actually, I should probably not start running things now because I don't know what's been run.", "tokens": [5135, 11, 286, 820, 1391, 406, 722, 2614, 721, 586, 570, 286, 500, 380, 458, 437, 311, 668, 1190, 13], "temperature": 0.0, "avg_logprob": -0.11324213772285276, "compression_ratio": 1.4495412844036697, "no_speech_prob": 4.264393646735698e-05}, {"id": 645, "seek": 390864, "start": 3925.64, "end": 3932.64, "text": " You'll see here we can request how many components we want or how many singular values.", "tokens": [509, 603, 536, 510, 321, 393, 5308, 577, 867, 6677, 321, 528, 420, 577, 867, 20010, 4190, 13], "temperature": 0.0, "avg_logprob": -0.11324213772285276, "compression_ratio": 1.4495412844036697, "no_speech_prob": 4.264393646735698e-05}, {"id": 646, "seek": 390864, "start": 3932.64, "end": 3937.64, "text": " So we're saying five, and it's quite quick.", "tokens": [407, 321, 434, 1566, 1732, 11, 293, 309, 311, 1596, 1702, 13], "temperature": 0.0, "avg_logprob": -0.11324213772285276, "compression_ratio": 1.4495412844036697, "no_speech_prob": 4.264393646735698e-05}, {"id": 647, "seek": 393764, "start": 3937.64, "end": 3942.64, "text": " So what we get back, and so just to remember the data set that we were using here was from newsgroups.", "tokens": [407, 437, 321, 483, 646, 11, 293, 370, 445, 281, 1604, 264, 1412, 992, 300, 321, 645, 1228, 510, 390, 490, 2583, 17377, 82, 13], "temperature": 0.0, "avg_logprob": -0.11416008414291753, "compression_ratio": 1.599009900990099, "no_speech_prob": 5.3897798352409154e-05}, {"id": 648, "seek": 393764, "start": 3942.64, "end": 3949.64, "text": " So these are kind of discussion boards on the internet on different topics. We had 2,000 posts to newsgroups.", "tokens": [407, 613, 366, 733, 295, 5017, 13293, 322, 264, 4705, 322, 819, 8378, 13, 492, 632, 568, 11, 1360, 12300, 281, 2583, 17377, 82, 13], "temperature": 0.0, "avg_logprob": -0.11416008414291753, "compression_ratio": 1.599009900990099, "no_speech_prob": 5.3897798352409154e-05}, {"id": 649, "seek": 393764, "start": 3949.64, "end": 3957.64, "text": " We're saying that we just want to get five topics, and then there were 26,000 different vocabulary words used.", "tokens": [492, 434, 1566, 300, 321, 445, 528, 281, 483, 1732, 8378, 11, 293, 550, 456, 645, 7551, 11, 1360, 819, 19864, 2283, 1143, 13], "temperature": 0.0, "avg_logprob": -0.11416008414291753, "compression_ratio": 1.599009900990099, "no_speech_prob": 5.3897798352409154e-05}, {"id": 650, "seek": 395764, "start": 3957.64, "end": 3970.64, "text": " And so the top five topics, you'll remember we just requested things from four categories, and those categories were space, graphics, religion, and atheism.", "tokens": [400, 370, 264, 1192, 1732, 8378, 11, 291, 603, 1604, 321, 445, 16436, 721, 490, 1451, 10479, 11, 293, 729, 10479, 645, 1901, 11, 11837, 11, 7561, 11, 293, 27033, 1434, 13], "temperature": 0.0, "avg_logprob": -0.07685175067500065, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5072305258363485e-06}, {"id": 651, "seek": 395764, "start": 3970.64, "end": 3981.64, "text": " And the top five topics that we see are JPEG, image, EDU, file, graphics, images, GIF, data. So these all have to do with graphics.", "tokens": [400, 264, 1192, 1732, 8378, 300, 321, 536, 366, 508, 5208, 38, 11, 3256, 11, 18050, 52, 11, 3991, 11, 11837, 11, 5267, 11, 460, 12775, 11, 1412, 13, 407, 613, 439, 362, 281, 360, 365, 11837, 13], "temperature": 0.0, "avg_logprob": -0.07685175067500065, "compression_ratio": 1.5319148936170213, "no_speech_prob": 5.5072305258363485e-06}, {"id": 652, "seek": 398164, "start": 3981.64, "end": 3994.64, "text": " So does the second topic. This next one seems like a mix of space and religion.", "tokens": [407, 775, 264, 1150, 4829, 13, 639, 958, 472, 2544, 411, 257, 2890, 295, 1901, 293, 7561, 13], "temperature": 0.0, "avg_logprob": -0.09639236132303873, "compression_ratio": 1.4035087719298245, "no_speech_prob": 1.3287561841934803e-06}, {"id": 653, "seek": 398164, "start": 3994.64, "end": 4008.64, "text": " You do have graphics showing up. So this, I don't know, there's some problems with these not being kind of fully separated out, but overall they're pretty good.", "tokens": [509, 360, 362, 11837, 4099, 493, 13, 407, 341, 11, 286, 500, 380, 458, 11, 456, 311, 512, 2740, 365, 613, 406, 885, 733, 295, 4498, 12005, 484, 11, 457, 4787, 436, 434, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.09639236132303873, "compression_ratio": 1.4035087719298245, "no_speech_prob": 1.3287561841934803e-06}, {"id": 654, "seek": 400864, "start": 4008.64, "end": 4016.64, "text": " So I'm going to talk about the approach of kind of how this randomized SVD was calculated, because it is really handy to not have to calculate the full thing.", "tokens": [407, 286, 478, 516, 281, 751, 466, 264, 3109, 295, 733, 295, 577, 341, 38513, 31910, 35, 390, 15598, 11, 570, 309, 307, 534, 13239, 281, 406, 362, 281, 8873, 264, 1577, 551, 13], "temperature": 0.0, "avg_logprob": -0.0771399865667504, "compression_ratio": 1.5594713656387664, "no_speech_prob": 8.52991252031643e-06}, {"id": 655, "seek": 400864, "start": 4016.64, "end": 4022.64, "text": " And so the basic approach is that we want to find an approximation to the range of A.", "tokens": [400, 370, 264, 3875, 3109, 307, 300, 321, 528, 281, 915, 364, 28023, 281, 264, 3613, 295, 316, 13], "temperature": 0.0, "avg_logprob": -0.0771399865667504, "compression_ratio": 1.5594713656387664, "no_speech_prob": 8.52991252031643e-06}, {"id": 656, "seek": 400864, "start": 4022.64, "end": 4035.64, "text": " And does anyone remember what the range of a matrix is? This is kind of a linear algebra vocabulary question.", "tokens": [400, 775, 2878, 1604, 437, 264, 3613, 295, 257, 8141, 307, 30, 639, 307, 733, 295, 257, 8213, 21989, 19864, 1168, 13], "temperature": 0.0, "avg_logprob": -0.0771399865667504, "compression_ratio": 1.5594713656387664, "no_speech_prob": 8.52991252031643e-06}, {"id": 657, "seek": 403564, "start": 4035.64, "end": 4039.64, "text": " Anyone?", "tokens": [14643, 30], "temperature": 0.0, "avg_logprob": -0.16595723628997802, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.8853979529230855e-05}, {"id": 658, "seek": 403564, "start": 4039.64, "end": 4044.64, "text": " Okay, yeah, go for it.", "tokens": [1033, 11, 1338, 11, 352, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.16595723628997802, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.8853979529230855e-05}, {"id": 659, "seek": 403564, "start": 4044.64, "end": 4047.64, "text": " Is it the space covered by the column basis?", "tokens": [1119, 309, 264, 1901, 5343, 538, 264, 7738, 5143, 30], "temperature": 0.0, "avg_logprob": -0.16595723628997802, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.8853979529230855e-05}, {"id": 660, "seek": 403564, "start": 4047.64, "end": 4062.64, "text": " Exactly, yeah. Yeah, the space covered by the column basis. And that's actually, that's really great language because that is kind of getting back to this idea of thinking of the, you know, the taking linear combinations of the columns.", "tokens": [7587, 11, 1338, 13, 865, 11, 264, 1901, 5343, 538, 264, 7738, 5143, 13, 400, 300, 311, 767, 11, 300, 311, 534, 869, 2856, 570, 300, 307, 733, 295, 1242, 646, 281, 341, 1558, 295, 1953, 295, 264, 11, 291, 458, 11, 264, 1940, 8213, 21267, 295, 264, 13766, 13], "temperature": 0.0, "avg_logprob": -0.16595723628997802, "compression_ratio": 1.6774193548387097, "no_speech_prob": 2.8853979529230855e-05}, {"id": 661, "seek": 406264, "start": 4062.64, "end": 4078.64, "text": " But so one way to write it would be that the range of a matrix A is all Y such that AX equals Y for some X.", "tokens": [583, 370, 472, 636, 281, 2464, 309, 576, 312, 300, 264, 3613, 295, 257, 8141, 316, 307, 439, 398, 1270, 300, 316, 55, 6915, 398, 337, 512, 1783, 13], "temperature": 0.0, "avg_logprob": -0.11134326096737024, "compression_ratio": 1.1630434782608696, "no_speech_prob": 8.397736564802472e-06}, {"id": 662, "seek": 407864, "start": 4078.64, "end": 4099.639999999999, "text": " And so if you think about a lot of times, so this is going back now to the image of A being some transformation, but if A is kind of transforming, you know, taking vectors X and transforming them to Y, the range is everything that you can hit over here by multiplying by A.", "tokens": [400, 370, 498, 291, 519, 466, 257, 688, 295, 1413, 11, 370, 341, 307, 516, 646, 586, 281, 264, 3256, 295, 316, 885, 512, 9887, 11, 457, 498, 316, 307, 733, 295, 27210, 11, 291, 458, 11, 1940, 18875, 1783, 293, 27210, 552, 281, 398, 11, 264, 3613, 307, 1203, 300, 291, 393, 2045, 670, 510, 538, 30955, 538, 316, 13], "temperature": 0.0, "avg_logprob": -0.05259513121384841, "compression_ratio": 1.5689655172413792, "no_speech_prob": 8.267621524282731e-06}, {"id": 663, "seek": 409964, "start": 4099.64, "end": 4114.64, "text": " But yeah, I really like Kelsey's definition of thinking of the columns as a basis and saying, you know, what are the columns span?", "tokens": [583, 1338, 11, 286, 534, 411, 44714, 311, 7123, 295, 1953, 295, 264, 13766, 382, 257, 5143, 293, 1566, 11, 291, 458, 11, 437, 366, 264, 13766, 16174, 30], "temperature": 0.0, "avg_logprob": -0.10259370600923579, "compression_ratio": 1.323076923076923, "no_speech_prob": 4.5658489398192614e-06}, {"id": 664, "seek": 409964, "start": 4114.64, "end": 4121.64, "text": " I have a good one off the top of my head.", "tokens": [286, 362, 257, 665, 472, 766, 264, 1192, 295, 452, 1378, 13], "temperature": 0.0, "avg_logprob": -0.10259370600923579, "compression_ratio": 1.323076923076923, "no_speech_prob": 4.5658489398192614e-06}, {"id": 665, "seek": 412164, "start": 4121.64, "end": 4132.64, "text": " Well, okay, so Jeremy's asking what a basis is. Can anyone answer what a basis is?", "tokens": [1042, 11, 1392, 11, 370, 17809, 311, 3365, 437, 257, 5143, 307, 13, 1664, 2878, 1867, 437, 257, 5143, 307, 30], "temperature": 0.0, "avg_logprob": -0.13730201488587915, "compression_ratio": 1.293103448275862, "no_speech_prob": 1.4063405615161173e-05}, {"id": 666, "seek": 412164, "start": 4132.64, "end": 4136.64, "text": " You think of your basis as like the coordinate axes for your space?", "tokens": [509, 519, 295, 428, 5143, 382, 411, 264, 15670, 35387, 337, 428, 1901, 30], "temperature": 0.0, "avg_logprob": -0.13730201488587915, "compression_ratio": 1.293103448275862, "no_speech_prob": 1.4063405615161173e-05}, {"id": 667, "seek": 413664, "start": 4136.64, "end": 4151.64, "text": " You could, so, yes. Yeah, so kind of, yeah, depending on the space. So if we're talking about all the numbers in R2, then the coordinate axes are the standard basis. Yeah.", "tokens": [509, 727, 11, 370, 11, 2086, 13, 865, 11, 370, 733, 295, 11, 1338, 11, 5413, 322, 264, 1901, 13, 407, 498, 321, 434, 1417, 466, 439, 264, 3547, 294, 497, 17, 11, 550, 264, 15670, 35387, 366, 264, 3832, 5143, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.167838454246521, "compression_ratio": 1.3464566929133859, "no_speech_prob": 1.0289137208019383e-05}, {"id": 668, "seek": 415164, "start": 4151.64, "end": 4166.64, "text": " Yeah. And so really kind of depending on your space, the basis are vectors that you can take linear combinations of to get any value in the space that you're talking about.", "tokens": [865, 13, 400, 370, 534, 733, 295, 5413, 322, 428, 1901, 11, 264, 5143, 366, 18875, 300, 291, 393, 747, 8213, 21267, 295, 281, 483, 604, 2158, 294, 264, 1901, 300, 291, 434, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.10850118398666382, "compression_ratio": 1.5480769230769231, "no_speech_prob": 5.594179128820542e-06}, {"id": 669, "seek": 415164, "start": 4166.64, "end": 4176.64, "text": " I think it's like, just for R2, you can interpret it as like, what area can you tile over with parallelograms where the waves are, what's your basis?", "tokens": [286, 519, 309, 311, 411, 11, 445, 337, 497, 17, 11, 291, 393, 7302, 309, 382, 411, 11, 437, 1859, 393, 291, 20590, 670, 365, 8952, 12820, 82, 689, 264, 9417, 366, 11, 437, 311, 428, 5143, 30], "temperature": 0.0, "avg_logprob": -0.10850118398666382, "compression_ratio": 1.5480769230769231, "no_speech_prob": 5.594179128820542e-06}, {"id": 670, "seek": 417664, "start": 4176.64, "end": 4191.64, "text": " Yes. Yeah. Yes. Okay. I actually wasn't going to show this till later, but I really think this is like the perfect time probably to watch the 3Blue1Brown video about bases.", "tokens": [1079, 13, 865, 13, 1079, 13, 1033, 13, 286, 767, 2067, 380, 516, 281, 855, 341, 4288, 1780, 11, 457, 286, 534, 519, 341, 307, 411, 264, 2176, 565, 1391, 281, 1159, 264, 805, 45231, 16, 22170, 648, 960, 466, 17949, 13], "temperature": 0.0, "avg_logprob": -0.12027654768545416, "compression_ratio": 1.502415458937198, "no_speech_prob": 9.972591215046123e-06}, {"id": 671, "seek": 417664, "start": 4191.64, "end": 4204.64, "text": " So if you're not familiar with 3Blue1Brown, these are really fantastic videos about linear algebra. Well, he makes them about many topics.", "tokens": [407, 498, 291, 434, 406, 4963, 365, 805, 45231, 16, 22170, 648, 11, 613, 366, 534, 5456, 2145, 466, 8213, 21989, 13, 1042, 11, 415, 1669, 552, 466, 867, 8378, 13], "temperature": 0.0, "avg_logprob": -0.12027654768545416, "compression_ratio": 1.502415458937198, "no_speech_prob": 9.972591215046123e-06}, {"id": 672, "seek": 420464, "start": 4204.64, "end": 4221.64, "text": " And you're going to switch over. Let me just get to the video. But I highly recommend these. And this I've kind of specifically chosen.", "tokens": [400, 291, 434, 516, 281, 3679, 670, 13, 961, 385, 445, 483, 281, 264, 960, 13, 583, 286, 5405, 2748, 613, 13, 400, 341, 286, 600, 733, 295, 4682, 8614, 13], "temperature": 0.0, "avg_logprob": -0.15363236836024693, "compression_ratio": 1.1946902654867257, "no_speech_prob": 4.19816751673352e-05}, {"id": 673, "seek": 422164, "start": 4221.64, "end": 4244.64, "text": " Actually, okay. So I was going to show the change of basis video, which I particularly like. But do you feel like you need to see the linear combination span and basis intro video before you see the one on change of basis?", "tokens": [5135, 11, 1392, 13, 407, 286, 390, 516, 281, 855, 264, 1319, 295, 5143, 960, 11, 597, 286, 4098, 411, 13, 583, 360, 291, 841, 411, 291, 643, 281, 536, 264, 8213, 6562, 16174, 293, 5143, 12897, 960, 949, 291, 536, 264, 472, 322, 1319, 295, 5143, 30], "temperature": 0.0, "avg_logprob": -0.08696522162510799, "compression_ratio": 1.48, "no_speech_prob": 1.6186770153581165e-05}, {"id": 674, "seek": 424464, "start": 4244.64, "end": 4256.64, "text": " How? Okay. So no shame about your answers. Raise your hand if you feel really comfortable with the idea of bases and span.", "tokens": [1012, 30, 1033, 13, 407, 572, 10069, 466, 428, 6338, 13, 30062, 428, 1011, 498, 291, 841, 534, 4619, 365, 264, 1558, 295, 17949, 293, 16174, 13], "temperature": 0.0, "avg_logprob": -0.06110381291917533, "compression_ratio": 1.8270676691729324, "no_speech_prob": 2.282654349983204e-05}, {"id": 675, "seek": 424464, "start": 4256.64, "end": 4262.64, "text": " Okay. And then raise your hand if you feel like you need a refresher on the ideas of bases and span. Okay. So it was about half and half.", "tokens": [1033, 13, 400, 550, 5300, 428, 1011, 498, 291, 841, 411, 291, 643, 257, 17368, 511, 322, 264, 3487, 295, 17949, 293, 16174, 13, 1033, 13, 407, 309, 390, 466, 1922, 293, 1922, 13], "temperature": 0.0, "avg_logprob": -0.06110381291917533, "compression_ratio": 1.8270676691729324, "no_speech_prob": 2.282654349983204e-05}, {"id": 676, "seek": 424464, "start": 4262.64, "end": 4273.64, "text": " So I'm going to go with the review. Hopefully, even for those of you that feel really comfortable with these ideas, I find 3Blue1Brown to just be a really new perspective that you don't see in a lot of linear algebra classes.", "tokens": [407, 286, 478, 516, 281, 352, 365, 264, 3131, 13, 10429, 11, 754, 337, 729, 295, 291, 300, 841, 534, 4619, 365, 613, 3487, 11, 286, 915, 805, 45231, 16, 22170, 648, 281, 445, 312, 257, 534, 777, 4585, 300, 291, 500, 380, 536, 294, 257, 688, 295, 8213, 21989, 5359, 13], "temperature": 0.0, "avg_logprob": -0.06110381291917533, "compression_ratio": 1.8270676691729324, "no_speech_prob": 2.282654349983204e-05}, {"id": 677, "seek": 427364, "start": 4273.64, "end": 4291.64, "text": " So I think that you'll still kind of gain something from this.", "tokens": [407, 286, 519, 300, 291, 603, 920, 733, 295, 6052, 746, 490, 341, 13], "temperature": 0.0, "avg_logprob": -0.11204719543457031, "compression_ratio": 0.9841269841269841, "no_speech_prob": 8.347645780304447e-05}, {"id": 678, "seek": 429164, "start": 4291.64, "end": 4307.64, "text": " Jeremy, do I need to do anything else?", "tokens": [50364, 17809, 11, 360, 286, 643, 281, 360, 1340, 1646, 30, 51164], "temperature": 0.0, "avg_logprob": -0.31708200161273664, "compression_ratio": 0.8837209302325582, "no_speech_prob": 0.00028633835609070957}, {"id": 679, "seek": 432164, "start": 4321.64, "end": 4343.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.7016270160675049, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.7187831401824951}, {"id": 680, "seek": 434364, "start": 4343.64, "end": 4365.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.4, "avg_logprob": -0.8755233287811279, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0002909870818257332}, {"id": 681, "seek": 436564, "start": 4365.64, "end": 4387.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5868459939956665, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.000286503549432382}, {"id": 682, "seek": 438764, "start": 4387.64, "end": 4402.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.6066027879714966, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00038500409573316574}, {"id": 683, "seek": 440264, "start": 4402.64, "end": 4417.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5645252068837484, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0001375107967760414}, {"id": 684, "seek": 441764, "start": 4417.64, "end": 4432.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.292064110438029, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00011950040789088234}, {"id": 685, "seek": 443264, "start": 4432.64, "end": 4447.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3444737195968628, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0005265211802907288}, {"id": 686, "seek": 444764, "start": 4447.64, "end": 4462.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.4163065354029338, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0001375636929878965}, {"id": 687, "seek": 446264, "start": 4462.64, "end": 4477.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.30605117479960126, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00012721316306851804}, {"id": 688, "seek": 447764, "start": 4477.64, "end": 4492.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2845587333043416, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0003914856642950326}, {"id": 689, "seek": 449264, "start": 4492.64, "end": 4507.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39896259307861326, "compression_ratio": 0.6875, "no_speech_prob": 0.0003793438372667879}, {"id": 690, "seek": 449264, "start": 4507.64, "end": 4521.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39896259307861326, "compression_ratio": 0.6875, "no_speech_prob": 0.0003793438372667879}, {"id": 691, "seek": 452164, "start": 4521.64, "end": 4536.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5389701922734579, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00031453234259970486}, {"id": 692, "seek": 453664, "start": 4536.64, "end": 4551.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3067646225293477, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00017380811914335936}, {"id": 693, "seek": 455164, "start": 4551.64, "end": 4566.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3833097696304321, "compression_ratio": 0.6875, "no_speech_prob": 0.00010065029346151277}, {"id": 694, "seek": 455164, "start": 4566.64, "end": 4580.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3833097696304321, "compression_ratio": 0.6875, "no_speech_prob": 0.00010065029346151277}, {"id": 695, "seek": 458064, "start": 4580.64, "end": 4595.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5278694152832031, "compression_ratio": 0.6875, "no_speech_prob": 0.00030975238769315183}, {"id": 696, "seek": 458064, "start": 4595.64, "end": 4609.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5278694152832031, "compression_ratio": 0.6875, "no_speech_prob": 0.00030975238769315183}, {"id": 697, "seek": 460964, "start": 4609.64, "end": 4624.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5021426200866699, "compression_ratio": 0.6875, "no_speech_prob": 0.0003976911539211869}, {"id": 698, "seek": 460964, "start": 4624.64, "end": 4637.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5021426200866699, "compression_ratio": 0.6875, "no_speech_prob": 0.0003976911539211869}, {"id": 699, "seek": 463764, "start": 4637.64, "end": 4652.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5693890651067098, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0002485407458152622}, {"id": 700, "seek": 465264, "start": 4652.64, "end": 4667.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.30019434293111164, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.00017639115685597062}, {"id": 701, "seek": 466764, "start": 4667.64, "end": 4682.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.36954212188720703, "compression_ratio": 0.6875, "no_speech_prob": 0.00034026382490992546}, {"id": 702, "seek": 466764, "start": 4682.64, "end": 4696.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.36954212188720703, "compression_ratio": 0.6875, "no_speech_prob": 0.00034026382490992546}, {"id": 703, "seek": 469664, "start": 4696.64, "end": 4701.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.529519966670445, "compression_ratio": 1.0625, "no_speech_prob": 0.0001820356847019866}, {"id": 704, "seek": 469664, "start": 4701.64, "end": 4711.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.529519966670445, "compression_ratio": 1.0625, "no_speech_prob": 0.0001820356847019866}, {"id": 705, "seek": 469664, "start": 4711.64, "end": 4719.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.529519966670445, "compression_ratio": 1.0625, "no_speech_prob": 0.0001820356847019866}, {"id": 706, "seek": 471964, "start": 4719.64, "end": 4730.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5106799602508545, "compression_ratio": 0.6875, "no_speech_prob": 8.601609442848712e-05}, {"id": 707, "seek": 471964, "start": 4730.64, "end": 4740.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5106799602508545, "compression_ratio": 0.6875, "no_speech_prob": 8.601609442848712e-05}, {"id": 708, "seek": 474064, "start": 4740.64, "end": 4750.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5020010254599832, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.915676956647076e-05}, {"id": 709, "seek": 474064, "start": 4750.64, "end": 4757.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5020010254599832, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.915676956647076e-05}, {"id": 710, "seek": 474064, "start": 4757.64, "end": 4761.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5020010254599832, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.915676956647076e-05}, {"id": 711, "seek": 474064, "start": 4761.64, "end": 4763.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5020010254599832, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.915676956647076e-05}, {"id": 712, "seek": 474064, "start": 4763.64, "end": 4765.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5020010254599832, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.915676956647076e-05}, {"id": 713, "seek": 476564, "start": 4765.64, "end": 4775.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.4, "avg_logprob": -0.5641914776393345, "compression_ratio": 1.0625, "no_speech_prob": 0.0006863054586574435}, {"id": 714, "seek": 476564, "start": 4775.64, "end": 4778.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.4, "avg_logprob": -0.5641914776393345, "compression_ratio": 1.0625, "no_speech_prob": 0.0006863054586574435}, {"id": 715, "seek": 476564, "start": 4778.64, "end": 4784.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.4, "avg_logprob": -0.5641914776393345, "compression_ratio": 1.0625, "no_speech_prob": 0.0006863054586574435}, {"id": 716, "seek": 478464, "start": 4784.64, "end": 4796.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.2, "avg_logprob": -0.44110426902770994, "compression_ratio": 0.6875, "no_speech_prob": 0.00022311128850560635}, {"id": 717, "seek": 478464, "start": 4796.64, "end": 4805.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.2, "avg_logprob": -0.44110426902770994, "compression_ratio": 0.6875, "no_speech_prob": 0.00022311128850560635}, {"id": 718, "seek": 480564, "start": 4805.64, "end": 4815.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.45825516093860974, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00021287995332386345}, {"id": 719, "seek": 480564, "start": 4815.64, "end": 4822.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.45825516093860974, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00021287995332386345}, {"id": 720, "seek": 480564, "start": 4822.64, "end": 4826.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.45825516093860974, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00021287995332386345}, {"id": 721, "seek": 480564, "start": 4826.64, "end": 4828.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.45825516093860974, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00021287995332386345}, {"id": 722, "seek": 480564, "start": 4828.64, "end": 4830.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.45825516093860974, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.00021287995332386345}, {"id": 723, "seek": 483064, "start": 4830.64, "end": 4838.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 1.0, "avg_logprob": -0.6267148017883301, "compression_ratio": 0.6875, "no_speech_prob": 0.0005604641046375036}, {"id": 724, "seek": 483064, "start": 4838.64, "end": 4853.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 1.0, "avg_logprob": -0.6267148017883301, "compression_ratio": 0.6875, "no_speech_prob": 0.0005604641046375036}, {"id": 725, "seek": 485364, "start": 4853.64, "end": 4860.360000000001, "text": " definition of a basis of a space is a set of linearly independent vectors that span that space.", "tokens": [7123, 295, 257, 5143, 295, 257, 1901, 307, 257, 992, 295, 43586, 6695, 18875, 300, 16174, 300, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11623994927657277, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.40458542108535767}, {"id": 726, "seek": 485364, "start": 4862.200000000001, "end": 4866.92, "text": " Now given how I described a basis earlier and given your current understanding of the words", "tokens": [823, 2212, 577, 286, 7619, 257, 5143, 3071, 293, 2212, 428, 2190, 3701, 295, 264, 2283], "temperature": 0.0, "avg_logprob": -0.11623994927657277, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.40458542108535767}, {"id": 727, "seek": 485364, "start": 4866.92, "end": 4871.88, "text": " span and linearly independent, think about why this definition would make sense.", "tokens": [16174, 293, 43586, 6695, 11, 519, 466, 983, 341, 7123, 576, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.11623994927657277, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.40458542108535767}, {"id": 728, "seek": 485364, "start": 4874.360000000001, "end": 4878.280000000001, "text": " In the next video I'll get into matrices and transforming space. See you then.", "tokens": [682, 264, 958, 960, 286, 603, 483, 666, 32284, 293, 27210, 1901, 13, 3008, 291, 550, 13], "temperature": 0.0, "avg_logprob": -0.11623994927657277, "compression_ratio": 1.6844660194174756, "no_speech_prob": 0.40458542108535767}, {"id": 729, "seek": 487828, "start": 4878.28, "end": 4885.639999999999, "text": " Okay great.", "tokens": [1033, 869, 13], "temperature": 0.0, "avg_logprob": -0.13641532897949218, "compression_ratio": 1.375, "no_speech_prob": 0.00015352477203123271}, {"id": 730, "seek": 487828, "start": 4890.2, "end": 4894.759999999999, "text": " Is there anything that you found particularly noteworthy or interesting about this?", "tokens": [1119, 456, 1340, 300, 291, 1352, 4098, 406, 1023, 2652, 88, 420, 1880, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.13641532897949218, "compression_ratio": 1.375, "no_speech_prob": 0.00015352477203123271}, {"id": 731, "seek": 487828, "start": 4899.8, "end": 4904.5199999999995, "text": " It's okay if not. It can also just be a good review of the terminology. I also just want to", "tokens": [467, 311, 1392, 498, 406, 13, 467, 393, 611, 445, 312, 257, 665, 3131, 295, 264, 27575, 13, 286, 611, 445, 528, 281], "temperature": 0.0, "avg_logprob": -0.13641532897949218, "compression_ratio": 1.375, "no_speech_prob": 0.00015352477203123271}, {"id": 732, "seek": 490452, "start": 4904.52, "end": 4911.160000000001, "text": " highlight that he has some videos not related to linear algebra that are just really interesting", "tokens": [5078, 300, 415, 575, 512, 2145, 406, 4077, 281, 8213, 21989, 300, 366, 445, 534, 1880], "temperature": 0.0, "avg_logprob": -0.09567670226097107, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00026112262276001275}, {"id": 733, "seek": 490452, "start": 4911.160000000001, "end": 4916.52, "text": " and beautiful. I particularly recommend his one on the towers of Hanoi which is kind of this puzzle", "tokens": [293, 2238, 13, 286, 4098, 2748, 702, 472, 322, 264, 25045, 295, 389, 3730, 72, 597, 307, 733, 295, 341, 12805], "temperature": 0.0, "avg_logprob": -0.09567670226097107, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00026112262276001275}, {"id": 734, "seek": 490452, "start": 4917.400000000001, "end": 4923.72, "text": " combined with binary and with Sierpinski triangles. And so there's some really", "tokens": [9354, 365, 17434, 293, 365, 318, 811, 79, 38984, 29896, 13, 400, 370, 456, 311, 512, 534], "temperature": 0.0, "avg_logprob": -0.09567670226097107, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00026112262276001275}, {"id": 735, "seek": 490452, "start": 4925.160000000001, "end": 4929.080000000001, "text": " surprising connections between those three topics that I did not know about and also some really", "tokens": [8830, 9271, 1296, 729, 1045, 8378, 300, 286, 630, 406, 458, 466, 293, 611, 512, 534], "temperature": 0.0, "avg_logprob": -0.09567670226097107, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00026112262276001275}, {"id": 736, "seek": 492908, "start": 4929.08, "end": 4937.48, "text": " nice visualizations. I don't know this person at all but I did just start supporting him through", "tokens": [1481, 5056, 14455, 13, 286, 500, 380, 458, 341, 954, 412, 439, 457, 286, 630, 445, 722, 7231, 796, 807], "temperature": 0.0, "avg_logprob": -0.16307623974688643, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0001195637378259562}, {"id": 737, "seek": 492908, "start": 4937.48, "end": 4943.24, "text": " Patreon because I really love his videos. I just mentioned it because I mean hopefully people", "tokens": [15692, 570, 286, 534, 959, 702, 2145, 13, 286, 445, 2835, 309, 570, 286, 914, 4696, 561], "temperature": 0.0, "avg_logprob": -0.16307623974688643, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0001195637378259562}, {"id": 738, "seek": 492908, "start": 4943.24, "end": 4948.5199999999995, "text": " see this video in the future and if they do then yeah but I think it's yeah very worthwhile thing", "tokens": [536, 341, 960, 294, 264, 2027, 293, 498, 436, 360, 550, 1338, 457, 286, 519, 309, 311, 1338, 588, 28159, 551], "temperature": 0.0, "avg_logprob": -0.16307623974688643, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0001195637378259562}, {"id": 739, "seek": 492908, "start": 4948.5199999999995, "end": 4950.68, "text": " to to support because they're really beautiful.", "tokens": [281, 281, 1406, 570, 436, 434, 534, 2238, 13], "temperature": 0.0, "avg_logprob": -0.16307623974688643, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0001195637378259562}, {"id": 740, "seek": 495068, "start": 4950.68, "end": 4955.400000000001, "text": " Okay so kind of going back to and so we'll return another day to kind of talk about change of basis", "tokens": [1033, 370, 733, 295, 516, 646, 281, 293, 370, 321, 603, 2736, 1071, 786, 281, 733, 295, 751, 466, 1319, 295, 5143], "temperature": 0.0, "avg_logprob": -0.2847154244132664, "compression_ratio": 1.7582938388625593, "no_speech_prob": 1.9219240130041726e-05}, {"id": 741, "seek": 495068, "start": 4956.76, "end": 4962.280000000001, "text": " but going back to this idea of the range of A. So as Kelsey said that's the", "tokens": [457, 516, 646, 281, 341, 1558, 295, 264, 3613, 295, 316, 13, 407, 382, 44714, 848, 300, 311, 264], "temperature": 0.0, "avg_logprob": -0.2847154244132664, "compression_ratio": 1.7582938388625593, "no_speech_prob": 1.9219240130041726e-05}, {"id": 742, "seek": 495068, "start": 4963.8, "end": 4971.8, "text": " space spanned by the columns of A and so hopefully this kind of helps you get up to speed on those", "tokens": [1901, 637, 5943, 538, 264, 13766, 295, 316, 293, 370, 4696, 341, 733, 295, 3665, 291, 483, 493, 281, 3073, 322, 729], "temperature": 0.0, "avg_logprob": -0.2847154244132664, "compression_ratio": 1.7582938388625593, "no_speech_prob": 1.9219240130041726e-05}, {"id": 743, "seek": 495068, "start": 4971.8, "end": 4974.92, "text": " concepts of columns of A. We're taking all possible linear algebra and we're taking all possible", "tokens": [10392, 295, 13766, 295, 316, 13, 492, 434, 1940, 439, 1944, 8213, 21989, 293, 321, 434, 1940, 439, 1944], "temperature": 0.0, "avg_logprob": -0.2847154244132664, "compression_ratio": 1.7582938388625593, "no_speech_prob": 1.9219240130041726e-05}, {"id": 744, "seek": 497492, "start": 4974.92, "end": 4981.32, "text": " linear combinations of them and that's the range of A. And so our goal and just to kind of to remind", "tokens": [8213, 21267, 295, 552, 293, 300, 311, 264, 3613, 295, 316, 13, 400, 370, 527, 3387, 293, 445, 281, 733, 295, 281, 4160], "temperature": 0.0, "avg_logprob": -0.15891608213767028, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.3550488549517468e-05}, {"id": 745, "seek": 497492, "start": 4981.32, "end": 4989.88, "text": " you what we're doing our goal is to come up with an algorithm for the randomized SVD and so to be", "tokens": [291, 437, 321, 434, 884, 527, 3387, 307, 281, 808, 493, 365, 364, 9284, 337, 264, 38513, 31910, 35, 293, 370, 281, 312], "temperature": 0.0, "avg_logprob": -0.15891608213767028, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.3550488549517468e-05}, {"id": 746, "seek": 497492, "start": 4989.88, "end": 4998.4400000000005, "text": " or for a truncated SVD using randomized values. So we want to be able to just calculate as many", "tokens": [420, 337, 257, 504, 409, 66, 770, 31910, 35, 1228, 38513, 4190, 13, 407, 321, 528, 281, 312, 1075, 281, 445, 8873, 382, 867], "temperature": 0.0, "avg_logprob": -0.15891608213767028, "compression_ratio": 1.6065573770491803, "no_speech_prob": 2.3550488549517468e-05}, {"id": 747, "seek": 499844, "start": 4998.44, "end": 5004.2, "text": " topics as we're interested in not have to calculate them all we're going to use randomization", "tokens": [8378, 382, 321, 434, 3102, 294, 406, 362, 281, 8873, 552, 439, 321, 434, 516, 281, 764, 4974, 2144], "temperature": 0.0, "avg_logprob": -0.21042263217088653, "compression_ratio": 1.5784313725490196, "no_speech_prob": 6.540212325489847e-06}, {"id": 748, "seek": 499844, "start": 5004.2, "end": 5011.16, "text": " and this is kind of outlining a path for us. So the first step is we want to find a matrix Q", "tokens": [293, 341, 307, 733, 295, 484, 31079, 257, 3100, 337, 505, 13, 407, 264, 700, 1823, 307, 321, 528, 281, 915, 257, 8141, 1249], "temperature": 0.0, "avg_logprob": -0.21042263217088653, "compression_ratio": 1.5784313725490196, "no_speech_prob": 6.540212325489847e-06}, {"id": 749, "seek": 499844, "start": 5011.16, "end": 5018.919999999999, "text": " that has r orthonormal columns such that A is approximately Q times Q transpose times A", "tokens": [300, 575, 367, 420, 11943, 24440, 13766, 1270, 300, 316, 307, 10447, 1249, 1413, 1249, 25167, 1413, 316], "temperature": 0.0, "avg_logprob": -0.21042263217088653, "compression_ratio": 1.5784313725490196, "no_speech_prob": 6.540212325489847e-06}, {"id": 750, "seek": 499844, "start": 5019.719999999999, "end": 5023.4, "text": " and so the thing to note here let me write this", "tokens": [293, 370, 264, 551, 281, 3637, 510, 718, 385, 2464, 341], "temperature": 0.0, "avg_logprob": -0.21042263217088653, "compression_ratio": 1.5784313725490196, "no_speech_prob": 6.540212325489847e-06}, {"id": 751, "seek": 502340, "start": 5023.4, "end": 5035.0, "text": " down maybe is that you know suppose A is m by n and Q is just going to be m by r so there could be a", "tokens": [760, 1310, 307, 300, 291, 458, 7297, 316, 307, 275, 538, 297, 293, 1249, 307, 445, 516, 281, 312, 275, 538, 367, 370, 456, 727, 312, 257], "temperature": 0.0, "avg_logprob": -0.47271101815359934, "compression_ratio": 1.6709677419354838, "no_speech_prob": 1.1478401575004682e-05}, {"id": 752, "seek": 502340, "start": 5035.0, "end": 5041.879999999999, "text": " lot of space savings there and so Q times Q transpose actually this is a question for you", "tokens": [688, 295, 1901, 13454, 456, 293, 370, 1249, 1413, 1249, 25167, 767, 341, 307, 257, 1168, 337, 291], "temperature": 0.0, "avg_logprob": -0.47271101815359934, "compression_ratio": 1.6709677419354838, "no_speech_prob": 1.1478401575004682e-05}, {"id": 753, "seek": 502340, "start": 5043.4, "end": 5049.879999999999, "text": " is Q by n times Q transpose times A is equal to Q transpose times A.", "tokens": [307, 1249, 538, 297, 1413, 1249, 25167, 1413, 316, 307, 2681, 281, 1249, 25167, 1413, 316, 13], "temperature": 0.0, "avg_logprob": -0.47271101815359934, "compression_ratio": 1.6709677419354838, "no_speech_prob": 1.1478401575004682e-05}, {"id": 754, "seek": 504988, "start": 5049.88, "end": 5056.2, "text": " So that's a question for you. Is Q by Q transpose going to be the identity?", "tokens": [407, 300, 311, 257, 1168, 337, 291, 13, 1119, 1249, 538, 1249, 25167, 516, 281, 312, 264, 6575, 30], "temperature": 0.0, "avg_logprob": -0.33231544494628906, "compression_ratio": 1.6624203821656052, "no_speech_prob": 2.684177843548241e-06}, {"id": 755, "seek": 504988, "start": 5059.0, "end": 5060.68, "text": " You can just shout out your guesses.", "tokens": [509, 393, 445, 8043, 484, 428, 42703, 13], "temperature": 0.0, "avg_logprob": -0.33231544494628906, "compression_ratio": 1.6624203821656052, "no_speech_prob": 2.684177843548241e-06}, {"id": 756, "seek": 504988, "start": 5063.0, "end": 5070.68, "text": " Okay I see both nodding and shaking heads. Okay who wants to vote for yes it will be the identity?", "tokens": [1033, 286, 536, 1293, 15224, 3584, 293, 15415, 8050, 13, 1033, 567, 2738, 281, 4740, 337, 2086, 309, 486, 312, 264, 6575, 30], "temperature": 0.0, "avg_logprob": -0.33231544494628906, "compression_ratio": 1.6624203821656052, "no_speech_prob": 2.684177843548241e-06}, {"id": 757, "seek": 504988, "start": 5072.52, "end": 5075.16, "text": " Who wants to vote no it will not be the identity?", "tokens": [2102, 2738, 281, 4740, 572, 309, 486, 406, 312, 264, 6575, 30], "temperature": 0.0, "avg_logprob": -0.33231544494628906, "compression_ratio": 1.6624203821656052, "no_speech_prob": 2.684177843548241e-06}, {"id": 758, "seek": 507516, "start": 5075.16, "end": 5078.68, "text": " Okay does anyone want to say why they voted the way they did?", "tokens": [1033, 775, 2878, 528, 281, 584, 983, 436, 13415, 264, 636, 436, 630, 30], "temperature": 0.0, "avg_logprob": -0.20431991362235916, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.296184776350856e-06}, {"id": 759, "seek": 507516, "start": 5083.24, "end": 5086.28, "text": " Okay I'll say so if oh what", "tokens": [1033, 286, 603, 584, 370, 498, 1954, 437], "temperature": 0.0, "avg_logprob": -0.20431991362235916, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.296184776350856e-06}, {"id": 760, "seek": 507516, "start": 5091.72, "end": 5097.32, "text": " because it's columns but the row actually the multiplication of the rows might not be orthographic.", "tokens": [570, 309, 311, 13766, 457, 264, 5386, 767, 264, 27290, 295, 264, 13241, 1062, 406, 312, 19052, 12295, 13], "temperature": 0.0, "avg_logprob": -0.20431991362235916, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.296184776350856e-06}, {"id": 761, "seek": 507516, "start": 5097.32, "end": 5104.12, "text": " Exactly yes yeah so this was kind of a tricky question if both the columns and rows of Q had", "tokens": [7587, 2086, 1338, 370, 341, 390, 733, 295, 257, 12414, 1168, 498, 1293, 264, 13766, 293, 13241, 295, 1249, 632], "temperature": 0.0, "avg_logprob": -0.20431991362235916, "compression_ratio": 1.5161290322580645, "no_speech_prob": 7.296184776350856e-06}, {"id": 762, "seek": 510412, "start": 5104.12, "end": 5113.5599999999995, "text": " been orthonormal then it would be the identity and even yeah so if both the columns and the rows were", "tokens": [668, 420, 11943, 24440, 550, 309, 576, 312, 264, 6575, 293, 754, 1338, 370, 498, 1293, 264, 13766, 293, 264, 13241, 645], "temperature": 0.0, "avg_logprob": -0.04872393334048918, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.187525180692319e-06}, {"id": 763, "seek": 510412, "start": 5114.5199999999995, "end": 5119.0, "text": " orthonormal we would get the identity however it's just the columns and so we kind of have", "tokens": [420, 11943, 24440, 321, 576, 483, 264, 6575, 4461, 309, 311, 445, 264, 13766, 293, 370, 321, 733, 295, 362], "temperature": 0.0, "avg_logprob": -0.04872393334048918, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.187525180692319e-06}, {"id": 764, "seek": 510412, "start": 5119.0, "end": 5128.2, "text": " this tall skinny matrix so Q kind of looks like this and we multiply Q by Q transpose.", "tokens": [341, 6764, 25193, 8141, 370, 1249, 733, 295, 1542, 411, 341, 293, 321, 12972, 1249, 538, 1249, 25167, 13], "temperature": 0.0, "avg_logprob": -0.04872393334048918, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.187525180692319e-06}, {"id": 765, "seek": 510412, "start": 5128.2, "end": 5132.04, "text": " We're getting something we want something that acts kind of like the identity for A", "tokens": [492, 434, 1242, 746, 321, 528, 746, 300, 10672, 733, 295, 411, 264, 6575, 337, 316], "temperature": 0.0, "avg_logprob": -0.04872393334048918, "compression_ratio": 1.7794117647058822, "no_speech_prob": 3.187525180692319e-06}, {"id": 766, "seek": 513204, "start": 5132.04, "end": 5137.0, "text": " but it's not actually going to be the identity because we didn't have enough inputs kind of", "tokens": [457, 309, 311, 406, 767, 516, 281, 312, 264, 6575, 570, 321, 994, 380, 362, 1547, 15743, 733, 295], "temperature": 0.0, "avg_logprob": -0.13615086373318447, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.1125152923341375e-05}, {"id": 767, "seek": 513204, "start": 5137.0, "end": 5142.2, "text": " going into it but we're hoping to kind of find something so that Q by Q transpose", "tokens": [516, 666, 309, 457, 321, 434, 7159, 281, 733, 295, 915, 746, 370, 300, 1249, 538, 1249, 25167], "temperature": 0.0, "avg_logprob": -0.13615086373318447, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.1125152923341375e-05}, {"id": 768, "seek": 513204, "start": 5142.2, "end": 5149.08, "text": " at least acts similar to the identity for A. And so we'll come back to the question of how do we", "tokens": [412, 1935, 10672, 2531, 281, 264, 6575, 337, 316, 13, 400, 370, 321, 603, 808, 646, 281, 264, 1168, 295, 577, 360, 321], "temperature": 0.0, "avg_logprob": -0.13615086373318447, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.1125152923341375e-05}, {"id": 769, "seek": 513204, "start": 5149.08, "end": 5155.56, "text": " actually find such a Q but for now just know that that's what that it is possible.", "tokens": [767, 915, 1270, 257, 1249, 457, 337, 586, 445, 458, 300, 300, 311, 437, 300, 309, 307, 1944, 13], "temperature": 0.0, "avg_logprob": -0.13615086373318447, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.1125152923341375e-05}, {"id": 770, "seek": 515556, "start": 5155.56, "end": 5165.4800000000005, "text": " So then we want to construct B equals Q transpose times A and actually probably should", "tokens": [407, 550, 321, 528, 281, 7690, 363, 6915, 1249, 25167, 1413, 316, 293, 767, 1391, 820], "temperature": 0.0, "avg_logprob": -0.1409339037808505, "compression_ratio": 1.3157894736842106, "no_speech_prob": 1.0129834663530346e-05}, {"id": 771, "seek": 515556, "start": 5173.080000000001, "end": 5176.360000000001, "text": " okay I guess I have to pull this up here I'll sorry I'll write that again but I will not", "tokens": [1392, 286, 2041, 286, 362, 281, 2235, 341, 493, 510, 286, 603, 2597, 286, 603, 2464, 300, 797, 457, 286, 486, 406], "temperature": 0.0, "avg_logprob": -0.1409339037808505, "compression_ratio": 1.3157894736842106, "no_speech_prob": 1.0129834663530346e-05}, {"id": 772, "seek": 517636, "start": 5176.36, "end": 5187.08, "text": " erase it this time so remember A is m by n Q is m by r so then when we do Q transpose times A", "tokens": [23525, 309, 341, 565, 370, 1604, 316, 307, 275, 538, 297, 1249, 307, 275, 538, 367, 370, 550, 562, 321, 360, 1249, 25167, 1413, 316], "temperature": 0.0, "avg_logprob": -0.09010917965958758, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1233511233731406e-06}, {"id": 773, "seek": 517636, "start": 5187.639999999999, "end": 5195.5599999999995, "text": " that's something that's r by m times m by n and we get that the product is r by n so B is a lot", "tokens": [300, 311, 746, 300, 311, 367, 538, 275, 1413, 275, 538, 297, 293, 321, 483, 300, 264, 1674, 307, 367, 538, 297, 370, 363, 307, 257, 688], "temperature": 0.0, "avg_logprob": -0.09010917965958758, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1233511233731406e-06}, {"id": 774, "seek": 517636, "start": 5195.5599999999995, "end": 5203.48, "text": " smaller than A. So we can compute the SVD of B by standard methods and this will be much quicker", "tokens": [4356, 813, 316, 13, 407, 321, 393, 14722, 264, 31910, 35, 295, 363, 538, 3832, 7150, 293, 341, 486, 312, 709, 16255], "temperature": 0.0, "avg_logprob": -0.09010917965958758, "compression_ratio": 1.5714285714285714, "no_speech_prob": 2.1233511233731406e-06}, {"id": 775, "seek": 520348, "start": 5203.48, "end": 5211.4, "text": " than it would have been to compute the SVD of A and then plugging back in so kind of here", "tokens": [813, 309, 576, 362, 668, 281, 14722, 264, 31910, 35, 295, 316, 293, 550, 42975, 646, 294, 370, 733, 295, 510], "temperature": 0.0, "avg_logprob": -0.08888484954833985, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465388125681784e-06}, {"id": 776, "seek": 520348, "start": 5212.5199999999995, "end": 5221.4, "text": " that's the formula for S oh that's actually a typo that B is U times sigma times V transpose", "tokens": [300, 311, 264, 8513, 337, 318, 1954, 300, 311, 767, 257, 2125, 78, 300, 363, 307, 624, 1413, 12771, 1413, 691, 25167], "temperature": 0.0, "avg_logprob": -0.08888484954833985, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465388125681784e-06}, {"id": 777, "seek": 520348, "start": 5223.5599999999995, "end": 5229.879999999999, "text": " typical formula for the SVD oh no it's not a typo I called it S because it's a different one", "tokens": [7476, 8513, 337, 264, 31910, 35, 1954, 572, 309, 311, 406, 257, 2125, 78, 286, 1219, 309, 318, 570, 309, 311, 257, 819, 472], "temperature": 0.0, "avg_logprob": -0.08888484954833985, "compression_ratio": 1.5714285714285714, "no_speech_prob": 3.4465388125681784e-06}, {"id": 778, "seek": 522988, "start": 5229.88, "end": 5239.32, "text": " uh we're going to have U later on then remember A is approximately Q times Q transpose times A", "tokens": [2232, 321, 434, 516, 281, 362, 624, 1780, 322, 550, 1604, 316, 307, 10447, 1249, 1413, 1249, 25167, 1413, 316], "temperature": 0.0, "avg_logprob": -0.10026142265223249, "compression_ratio": 1.7975460122699387, "no_speech_prob": 8.939528015616816e-06}, {"id": 779, "seek": 522988, "start": 5239.96, "end": 5248.2, "text": " so we plug in for Q transpose times A plug in this SVD for B and we get A is approximately Q times S", "tokens": [370, 321, 5452, 294, 337, 1249, 25167, 1413, 316, 5452, 294, 341, 31910, 35, 337, 363, 293, 321, 483, 316, 307, 10447, 1249, 1413, 318], "temperature": 0.0, "avg_logprob": -0.10026142265223249, "compression_ratio": 1.7975460122699387, "no_speech_prob": 8.939528015616816e-06}, {"id": 780, "seek": 522988, "start": 5248.84, "end": 5257.96, "text": " times sigma times B transpose and we can set U equal to QS and now we have a SVD for A a low rank", "tokens": [1413, 12771, 1413, 363, 25167, 293, 321, 393, 992, 624, 2681, 281, 1249, 50, 293, 586, 321, 362, 257, 31910, 35, 337, 316, 257, 2295, 6181], "temperature": 0.0, "avg_logprob": -0.10026142265223249, "compression_ratio": 1.7975460122699387, "no_speech_prob": 8.939528015616816e-06}, {"id": 781, "seek": 525796, "start": 5257.96, "end": 5266.76, "text": " SVD so I have a question why uh why is it okay to say U is QS because we want you to be or", "tokens": [31910, 35, 370, 286, 362, 257, 1168, 983, 2232, 983, 307, 309, 1392, 281, 584, 624, 307, 1249, 50, 570, 321, 528, 291, 281, 312, 420], "temperature": 0.0, "avg_logprob": -0.25611242881188023, "compression_ratio": 1.3140495867768596, "no_speech_prob": 8.66438767843647e-06}, {"id": 782, "seek": 525796, "start": 5267.4, "end": 5268.92, "text": " or have orthonormal columns", "tokens": [420, 362, 420, 11943, 24440, 13766], "temperature": 0.0, "avg_logprob": -0.25611242881188023, "compression_ratio": 1.3140495867768596, "no_speech_prob": 8.66438767843647e-06}, {"id": 783, "seek": 525796, "start": 5271.32, "end": 5271.72, "text": " Kelsey", "tokens": [44714], "temperature": 0.0, "avg_logprob": -0.25611242881188023, "compression_ratio": 1.3140495867768596, "no_speech_prob": 8.66438767843647e-06}, {"id": 784, "seek": 525796, "start": 5276.36, "end": 5280.12, "text": " I think U is just a rotation of S", "tokens": [286, 519, 624, 307, 445, 257, 12447, 295, 318], "temperature": 0.0, "avg_logprob": -0.25611242881188023, "compression_ratio": 1.3140495867768596, "no_speech_prob": 8.66438767843647e-06}, {"id": 785, "seek": 528012, "start": 5280.12, "end": 5284.12, "text": " yes um", "tokens": [2086, 1105], "temperature": 0.0, "avg_logprob": -0.18416327964968798, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.0451300113345496e-05}, {"id": 786, "seek": 528012, "start": 5284.68, "end": 5285.4, "text": " but if you change", "tokens": [457, 498, 291, 1319], "temperature": 0.0, "avg_logprob": -0.18416327964968798, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.0451300113345496e-05}, {"id": 787, "seek": 528012, "start": 5288.28, "end": 5294.04, "text": " S is orthonormal and Q is orthonormal yeah so that's yeah that's the key yeah since S and Q", "tokens": [318, 307, 420, 11943, 24440, 293, 1249, 307, 420, 11943, 24440, 1338, 370, 300, 311, 1338, 300, 311, 264, 2141, 1338, 1670, 318, 293, 1249], "temperature": 0.0, "avg_logprob": -0.18416327964968798, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.0451300113345496e-05}, {"id": 788, "seek": 528012, "start": 5294.04, "end": 5301.72, "text": " are orthonormal we'll get something else that's orthonormal exactly yeah and I guess um", "tokens": [366, 420, 11943, 24440, 321, 603, 483, 746, 1646, 300, 311, 420, 11943, 24440, 2293, 1338, 293, 286, 2041, 1105], "temperature": 0.0, "avg_logprob": -0.18416327964968798, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.0451300113345496e-05}, {"id": 789, "seek": 528012, "start": 5305.16, "end": 5309.32, "text": " yeah we'll talk about the topic of rotations later but yeah S and Q are orthonormal", "tokens": [1338, 321, 603, 751, 466, 264, 4829, 295, 44796, 1780, 457, 1338, 318, 293, 1249, 366, 420, 11943, 24440], "temperature": 0.0, "avg_logprob": -0.18416327964968798, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.0451300113345496e-05}, {"id": 790, "seek": 530932, "start": 5309.32, "end": 5313.719999999999, "text": " so that works so now we're going to kind of return to these questions of okay how did we", "tokens": [370, 300, 1985, 370, 586, 321, 434, 516, 281, 733, 295, 2736, 281, 613, 1651, 295, 1392, 577, 630, 321], "temperature": 0.0, "avg_logprob": -0.12380112772402556, "compression_ratio": 1.6954545454545455, "no_speech_prob": 1.4285003089753445e-05}, {"id": 791, "seek": 530932, "start": 5313.719999999999, "end": 5317.799999999999, "text": " find Q but are there any questions just about kind of this plan of what we're going to do", "tokens": [915, 1249, 457, 366, 456, 604, 1651, 445, 466, 733, 295, 341, 1393, 295, 437, 321, 434, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.12380112772402556, "compression_ratio": 1.6954545454545455, "no_speech_prob": 1.4285003089753445e-05}, {"id": 792, "seek": 530932, "start": 5319.16, "end": 5323.32, "text": " so you happen to remember the computational complexity of SVD so when we reduced the size", "tokens": [370, 291, 1051, 281, 1604, 264, 28270, 14024, 295, 31910, 35, 370, 562, 321, 9212, 264, 2744], "temperature": 0.0, "avg_logprob": -0.12380112772402556, "compression_ratio": 1.6954545454545455, "no_speech_prob": 1.4285003089753445e-05}, {"id": 793, "seek": 530932, "start": 5323.32, "end": 5331.32, "text": " but are we reducing its speed by I do not remember let me write that down I'll", "tokens": [457, 366, 321, 12245, 1080, 3073, 538, 286, 360, 406, 1604, 718, 385, 2464, 300, 760, 286, 603], "temperature": 0.0, "avg_logprob": -0.12380112772402556, "compression_ratio": 1.6954545454545455, "no_speech_prob": 1.4285003089753445e-05}, {"id": 794, "seek": 530932, "start": 5333.639999999999, "end": 5335.08, "text": " talk about that next time", "tokens": [751, 466, 300, 958, 565], "temperature": 0.0, "avg_logprob": -0.12380112772402556, "compression_ratio": 1.6954545454545455, "no_speech_prob": 1.4285003089753445e-05}, {"id": 795, "seek": 533508, "start": 5335.08, "end": 5341.32, "text": " okay it might be squared in the number of columns that is might be a C-value", "tokens": [1392, 309, 1062, 312, 8889, 294, 264, 1230, 295, 13766, 300, 307, 1062, 312, 257, 383, 12, 29155], "temperature": 0.0, "avg_logprob": -0.30748973675628205, "compression_ratio": 1.463276836158192, "no_speech_prob": 8.267604243883397e-06}, {"id": 796, "seek": 533508, "start": 5342.76, "end": 5348.92, "text": " yeah I would I would believe that but I will check yeah so general idea is kind of we're", "tokens": [1338, 286, 576, 286, 576, 1697, 300, 457, 286, 486, 1520, 1338, 370, 2674, 1558, 307, 733, 295, 321, 434], "temperature": 0.0, "avg_logprob": -0.30748973675628205, "compression_ratio": 1.463276836158192, "no_speech_prob": 8.267604243883397e-06}, {"id": 797, "seek": 533508, "start": 5348.92, "end": 5356.44, "text": " finding this special Q then we find the SVD on a smaller matrix Q transpose times A and we're", "tokens": [5006, 341, 2121, 1249, 550, 321, 915, 264, 31910, 35, 322, 257, 4356, 8141, 1249, 25167, 1413, 316, 293, 321, 434], "temperature": 0.0, "avg_logprob": -0.30748973675628205, "compression_ratio": 1.463276836158192, "no_speech_prob": 8.267604243883397e-06}, {"id": 798, "seek": 535644, "start": 5356.44, "end": 5365.0, "text": " able to plug that back in to have our our truncated SVD for A okay so first question", "tokens": [1075, 281, 5452, 300, 646, 294, 281, 362, 527, 527, 504, 409, 66, 770, 31910, 35, 337, 316, 1392, 370, 700, 1168], "temperature": 0.0, "avg_logprob": -0.0839384864358341, "compression_ratio": 1.4787878787878788, "no_speech_prob": 1.1189370070496807e-06}, {"id": 799, "seek": 535644, "start": 5369.879999999999, "end": 5370.839999999999, "text": " how do we find Q", "tokens": [577, 360, 321, 915, 1249], "temperature": 0.0, "avg_logprob": -0.0839384864358341, "compression_ratio": 1.4787878787878788, "no_speech_prob": 1.1189370070496807e-06}, {"id": 800, "seek": 535644, "start": 5373.799999999999, "end": 5377.4, "text": " and so it turns out we can just take a bunch of random vectors", "tokens": [293, 370, 309, 4523, 484, 321, 393, 445, 747, 257, 3840, 295, 4974, 18875], "temperature": 0.0, "avg_logprob": -0.0839384864358341, "compression_ratio": 1.4787878787878788, "no_speech_prob": 1.1189370070496807e-06}, {"id": 801, "seek": 535644, "start": 5377.4, "end": 5384.04, "text": " W and look at the subspace formed by A W for the these different random vectors", "tokens": [343, 293, 574, 412, 264, 2090, 17940, 8693, 538, 316, 343, 337, 264, 613, 819, 4974, 18875], "temperature": 0.0, "avg_logprob": -0.0839384864358341, "compression_ratio": 1.4787878787878788, "no_speech_prob": 1.1189370070496807e-06}, {"id": 802, "seek": 538404, "start": 5384.04, "end": 5392.04, "text": " we'll form a matrix W with the W's as its columns and we take the QR decomposition of A W equals", "tokens": [321, 603, 1254, 257, 8141, 343, 365, 264, 343, 311, 382, 1080, 13766, 293, 321, 747, 264, 32784, 48356, 295, 316, 343, 6915], "temperature": 0.0, "avg_logprob": -0.15477346394160021, "compression_ratio": 1.6941176470588235, "no_speech_prob": 9.080129530047998e-06}, {"id": 803, "seek": 538404, "start": 5392.04, "end": 5399.88, "text": " QR and I'll so we will be talking about the QR decomposition a lot for now all you need to know", "tokens": [32784, 293, 286, 603, 370, 321, 486, 312, 1417, 466, 264, 32784, 48356, 257, 688, 337, 586, 439, 291, 643, 281, 458], "temperature": 0.0, "avg_logprob": -0.15477346394160021, "compression_ratio": 1.6941176470588235, "no_speech_prob": 9.080129530047998e-06}, {"id": 804, "seek": 538404, "start": 5399.88, "end": 5407.72, "text": " is that the QR decomposition exists for any matrix and it's an orthonormal matrix times a upper", "tokens": [307, 300, 264, 32784, 48356, 8198, 337, 604, 8141, 293, 309, 311, 364, 420, 11943, 24440, 8141, 1413, 257, 6597], "temperature": 0.0, "avg_logprob": -0.15477346394160021, "compression_ratio": 1.6941176470588235, "no_speech_prob": 9.080129530047998e-06}, {"id": 805, "seek": 540772, "start": 5407.72, "end": 5416.4400000000005, "text": " upper triangular matrix and so this is something I think nice about linear algebra is you kind of", "tokens": [6597, 38190, 8141, 293, 370, 341, 307, 746, 286, 519, 1481, 466, 8213, 21989, 307, 291, 733, 295], "temperature": 0.0, "avg_logprob": -0.07747691869735718, "compression_ratio": 1.7570093457943925, "no_speech_prob": 3.0415408218686935e-06}, {"id": 806, "seek": 540772, "start": 5416.4400000000005, "end": 5421.4800000000005, "text": " have these standard naming conventions and so pretty much anytime you see my matrix named Q", "tokens": [362, 613, 3832, 25290, 33520, 293, 370, 1238, 709, 13038, 291, 536, 452, 8141, 4926, 1249], "temperature": 0.0, "avg_logprob": -0.07747691869735718, "compression_ratio": 1.7570093457943925, "no_speech_prob": 3.0415408218686935e-06}, {"id": 807, "seek": 540772, "start": 5421.4800000000005, "end": 5426.84, "text": " you can assume it's orthonormal because that's just a commonly used convention but the QR", "tokens": [291, 393, 6552, 309, 311, 420, 11943, 24440, 570, 300, 311, 445, 257, 12719, 1143, 10286, 457, 264, 32784], "temperature": 0.0, "avg_logprob": -0.07747691869735718, "compression_ratio": 1.7570093457943925, "no_speech_prob": 3.0415408218686935e-06}, {"id": 808, "seek": 540772, "start": 5426.84, "end": 5433.08, "text": " decomposition is all about getting an orthonormal matrix times an upper triangular matrix and we", "tokens": [48356, 307, 439, 466, 1242, 364, 420, 11943, 24440, 8141, 1413, 364, 6597, 38190, 8141, 293, 321], "temperature": 0.0, "avg_logprob": -0.07747691869735718, "compression_ratio": 1.7570093457943925, "no_speech_prob": 3.0415408218686935e-06}, {"id": 809, "seek": 543308, "start": 5433.08, "end": 5442.12, "text": " will we'll learn how to do that in a later lesson so we'll take A W W is random get the QR and the", "tokens": [486, 321, 603, 1466, 577, 281, 360, 300, 294, 257, 1780, 6898, 370, 321, 603, 747, 316, 343, 343, 307, 4974, 483, 264, 32784, 293, 264], "temperature": 0.0, "avg_logprob": -0.09391387716516272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 7.811364071130811e-07}, {"id": 810, "seek": 543308, "start": 5442.12, "end": 5447.32, "text": " Q this is kind of a property of from the QR decomposition the Q forms an orthonormal basis for", "tokens": [1249, 341, 307, 733, 295, 257, 4707, 295, 490, 264, 32784, 48356, 264, 1249, 6422, 364, 420, 11943, 24440, 5143, 337], "temperature": 0.0, "avg_logprob": -0.09391387716516272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 7.811364071130811e-07}, {"id": 811, "seek": 543308, "start": 5447.32, "end": 5457.24, "text": " A W and A W is giving us the range of A since it's kind of A times these different values", "tokens": [316, 343, 293, 316, 343, 307, 2902, 505, 264, 3613, 295, 316, 1670, 309, 311, 733, 295, 316, 1413, 613, 819, 4190], "temperature": 0.0, "avg_logprob": -0.09391387716516272, "compression_ratio": 1.5988700564971752, "no_speech_prob": 7.811364071130811e-07}, {"id": 812, "seek": 545724, "start": 5457.24, "end": 5467.48, "text": " and since A W has far more rows than columns it turns out that it works in practice that these", "tokens": [293, 1670, 316, 343, 575, 1400, 544, 13241, 813, 13766, 309, 4523, 484, 300, 309, 1985, 294, 3124, 300, 613], "temperature": 0.0, "avg_logprob": -0.16186697441234923, "compression_ratio": 1.5657142857142856, "no_speech_prob": 1.7704012122976565e-07}, {"id": 813, "seek": 545724, "start": 5467.48, "end": 5474.36, "text": " columns are approximately orthonormal it's just really unlikely that you would get columns that", "tokens": [13766, 366, 10447, 420, 11943, 24440, 309, 311, 445, 534, 17518, 300, 291, 576, 483, 13766, 300], "temperature": 0.0, "avg_logprob": -0.16186697441234923, "compression_ratio": 1.5657142857142856, "no_speech_prob": 1.7704012122976565e-07}, {"id": 814, "seek": 547436, "start": 5474.36, "end": 5487.639999999999, "text": " were linearly dependent when you're choosing random values any questions about this", "tokens": [645, 43586, 12334, 562, 291, 434, 10875, 4974, 4190, 604, 1651, 466, 341], "temperature": 0.0, "avg_logprob": -0.3346579451310007, "compression_ratio": 1.3333333333333333, "no_speech_prob": 9.81784160103416e-06}, {"id": 815, "seek": 547436, "start": 5491.24, "end": 5499.16, "text": " and we'll go ahead so it was talking about why A W is the range of a roughly", "tokens": [293, 321, 603, 352, 2286, 370, 309, 390, 1417, 466, 983, 316, 343, 307, 264, 3613, 295, 257, 9810], "temperature": 0.0, "avg_logprob": -0.3346579451310007, "compression_ratio": 1.3333333333333333, "no_speech_prob": 9.81784160103416e-06}, {"id": 816, "seek": 549916, "start": 5499.16, "end": 5503.8, "text": " we'll come back to that under how we should choose R I think", "tokens": [321, 603, 808, 646, 281, 300, 833, 577, 321, 820, 2826, 497, 286, 519], "temperature": 0.0, "avg_logprob": -0.16135137346055772, "compression_ratio": 1.6475770925110131, "no_speech_prob": 3.966899839724647e-06}, {"id": 817, "seek": 549916, "start": 5507.24, "end": 5513.24, "text": " and yeah this idea of the Q or really the QR decomposition will show up in almost every lesson", "tokens": [293, 1338, 341, 1558, 295, 264, 1249, 420, 534, 264, 32784, 48356, 486, 855, 493, 294, 1920, 633, 6898], "temperature": 0.0, "avg_logprob": -0.16135137346055772, "compression_ratio": 1.6475770925110131, "no_speech_prob": 3.966899839724647e-06}, {"id": 818, "seek": 549916, "start": 5513.24, "end": 5517.8, "text": " so you will get to see a lot of the QR decomposition in this class it's pretty", "tokens": [370, 291, 486, 483, 281, 536, 257, 688, 295, 264, 32784, 48356, 294, 341, 1508, 309, 311, 1238], "temperature": 0.0, "avg_logprob": -0.16135137346055772, "compression_ratio": 1.6475770925110131, "no_speech_prob": 3.966899839724647e-06}, {"id": 819, "seek": 549916, "start": 5517.8, "end": 5520.36, "text": " pretty foundational to numerical linear algebra", "tokens": [1238, 32195, 281, 29054, 8213, 21989], "temperature": 0.0, "avg_logprob": -0.16135137346055772, "compression_ratio": 1.6475770925110131, "no_speech_prob": 3.966899839724647e-06}, {"id": 820, "seek": 549916, "start": 5523.08, "end": 5528.04, "text": " and so yeah for now you just need to know that Q consists of orthonormal columns R is upper", "tokens": [293, 370, 1338, 337, 586, 291, 445, 643, 281, 458, 300, 1249, 14689, 295, 420, 11943, 24440, 13766, 497, 307, 6597], "temperature": 0.0, "avg_logprob": -0.16135137346055772, "compression_ratio": 1.6475770925110131, "no_speech_prob": 3.966899839724647e-06}, {"id": 821, "seek": 552804, "start": 5528.04, "end": 5534.68, "text": " triangular when I say upper triangular that means that everything below the diagonal is zero", "tokens": [38190, 562, 286, 584, 6597, 38190, 300, 1355, 300, 1203, 2507, 264, 21539, 307, 4018], "temperature": 0.0, "avg_logprob": -0.14159394800662994, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.684158289412153e-06}, {"id": 822, "seek": 552804, "start": 5535.64, "end": 5541.0, "text": " trevathan says the QR decomposition is the most important idea in numerical linear algebra so that's", "tokens": [2192, 85, 9390, 1619, 264, 32784, 48356, 307, 264, 881, 1021, 1558, 294, 29054, 8213, 21989, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.14159394800662994, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.684158289412153e-06}, {"id": 823, "seek": 552804, "start": 5541.72, "end": 5542.76, "text": " in pretty high praise", "tokens": [294, 1238, 1090, 13286], "temperature": 0.0, "avg_logprob": -0.14159394800662994, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.684158289412153e-06}, {"id": 824, "seek": 552804, "start": 5546.36, "end": 5552.5199999999995, "text": " and then this question so remember we chose Q to have R orthonormal columns", "tokens": [293, 550, 341, 1168, 370, 1604, 321, 5111, 1249, 281, 362, 497, 420, 11943, 24440, 13766], "temperature": 0.0, "avg_logprob": -0.14159394800662994, "compression_ratio": 1.5315789473684212, "no_speech_prob": 2.684158289412153e-06}, {"id": 825, "seek": 555252, "start": 5552.52, "end": 5559.080000000001, "text": " and then R is giving us what the dimension of B is going to be so how do we want to choose R", "tokens": [293, 550, 497, 307, 2902, 505, 437, 264, 10139, 295, 363, 307, 516, 281, 312, 370, 577, 360, 321, 528, 281, 2826, 497], "temperature": 0.0, "avg_logprob": -0.12513821099394112, "compression_ratio": 1.7123287671232876, "no_speech_prob": 9.817998034122866e-06}, {"id": 826, "seek": 555252, "start": 5560.280000000001, "end": 5566.120000000001, "text": " so if we wanted suppose we had a matrix with 100 columns and we wanted to get five so in the", "tokens": [370, 498, 321, 1415, 7297, 321, 632, 257, 8141, 365, 2319, 13766, 293, 321, 1415, 281, 483, 1732, 370, 294, 264], "temperature": 0.0, "avg_logprob": -0.12513821099394112, "compression_ratio": 1.7123287671232876, "no_speech_prob": 9.817998034122866e-06}, {"id": 827, "seek": 555252, "start": 5566.120000000001, "end": 5573.400000000001, "text": " question of going back to our like our literary example we just want five topics to be safe", "tokens": [1168, 295, 516, 646, 281, 527, 411, 527, 24194, 1365, 321, 445, 528, 1732, 8378, 281, 312, 3273], "temperature": 0.0, "avg_logprob": -0.12513821099394112, "compression_ratio": 1.7123287671232876, "no_speech_prob": 9.817998034122866e-06}, {"id": 828, "seek": 555252, "start": 5573.400000000001, "end": 5579.160000000001, "text": " we're going to choose something larger than five so you could just kind of as a rule of thumb say", "tokens": [321, 434, 516, 281, 2826, 746, 4833, 813, 1732, 370, 291, 727, 445, 733, 295, 382, 257, 4978, 295, 9298, 584], "temperature": 0.0, "avg_logprob": -0.12513821099394112, "compression_ratio": 1.7123287671232876, "no_speech_prob": 9.817998034122866e-06}, {"id": 829, "seek": 557916, "start": 5579.16, "end": 5584.5199999999995, "text": " let's add 10 to what we're doing so let's do it with 15 so you don't want to calculate exactly", "tokens": [718, 311, 909, 1266, 281, 437, 321, 434, 884, 370, 718, 311, 360, 309, 365, 2119, 370, 291, 500, 380, 528, 281, 8873, 2293], "temperature": 0.0, "avg_logprob": -0.14709153232804265, "compression_ratio": 1.596244131455399, "no_speech_prob": 2.85726264337427e-06}, {"id": 830, "seek": 557916, "start": 5584.5199999999995, "end": 5591.639999999999, "text": " five because we've got this randomized component so it's kind of safer to give yourself some", "tokens": [1732, 570, 321, 600, 658, 341, 38513, 6542, 370, 309, 311, 733, 295, 15856, 281, 976, 1803, 512], "temperature": 0.0, "avg_logprob": -0.14709153232804265, "compression_ratio": 1.596244131455399, "no_speech_prob": 2.85726264337427e-06}, {"id": 831, "seek": 557916, "start": 5591.639999999999, "end": 5595.0, "text": " buffer but we don't need to calculate the full 100 topics", "tokens": [21762, 457, 321, 500, 380, 643, 281, 8873, 264, 1577, 2319, 8378], "temperature": 0.0, "avg_logprob": -0.14709153232804265, "compression_ratio": 1.596244131455399, "no_speech_prob": 2.85726264337427e-06}, {"id": 832, "seek": 557916, "start": 5597.96, "end": 5603.24, "text": " and so our projection was only approximate so we're making it a little bit bigger than we need", "tokens": [293, 370, 527, 22743, 390, 787, 30874, 370, 321, 434, 1455, 309, 257, 707, 857, 3801, 813, 321, 643], "temperature": 0.0, "avg_logprob": -0.14709153232804265, "compression_ratio": 1.596244131455399, "no_speech_prob": 2.85726264337427e-06}, {"id": 833, "seek": 560324, "start": 5603.24, "end": 5614.44, "text": " bigger than we need now let me show you what this looks like in code so we above we used scikit", "tokens": [3801, 813, 321, 643, 586, 718, 385, 855, 291, 437, 341, 1542, 411, 294, 3089, 370, 321, 3673, 321, 1143, 2180, 22681], "temperature": 0.0, "avg_logprob": -0.09074389766639387, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.3687193788646255e-06}, {"id": 834, "seek": 560324, "start": 5614.44, "end": 5621.96, "text": " learns implementation now i'm going to write my own which is based off of the scikit learn source", "tokens": [27152, 11420, 586, 741, 478, 516, 281, 2464, 452, 1065, 597, 307, 2361, 766, 295, 264, 2180, 22681, 1466, 4009], "temperature": 0.0, "avg_logprob": -0.09074389766639387, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.3687193788646255e-06}, {"id": 835, "seek": 560324, "start": 5621.96, "end": 5628.2, "text": " code only it takes into account fewer special cases so this is a less robust version but i think it's", "tokens": [3089, 787, 309, 2516, 666, 2696, 13366, 2121, 3331, 370, 341, 307, 257, 1570, 13956, 3037, 457, 741, 519, 309, 311], "temperature": 0.0, "avg_logprob": -0.09074389766639387, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.3687193788646255e-06}, {"id": 836, "seek": 562820, "start": 5628.2, "end": 5635.96, "text": " kind of clearer what's happening first we want a randomized range finder and so this is what just", "tokens": [733, 295, 26131, 437, 311, 2737, 700, 321, 528, 257, 38513, 3613, 915, 260, 293, 370, 341, 307, 437, 445], "temperature": 0.0, "avg_logprob": -0.0846654549241066, "compression_ratio": 1.685897435897436, "no_speech_prob": 9.223243068845477e-06}, {"id": 837, "seek": 562820, "start": 5635.96, "end": 5642.12, "text": " finds the q that we saw above in step one when we said like okay we want q such that a is approximately", "tokens": [10704, 264, 9505, 300, 321, 1866, 3673, 294, 1823, 472, 562, 321, 848, 411, 1392, 321, 528, 9505, 1270, 300, 257, 307, 10447], "temperature": 0.0, "avg_logprob": -0.0846654549241066, "compression_ratio": 1.685897435897436, "no_speech_prob": 9.223243068845477e-06}, {"id": 838, "seek": 562820, "start": 5643.32, "end": 5647.5599999999995, "text": " q times q transpose times or yeah q times q transpose times a", "tokens": [9505, 1413, 9505, 25167, 1413, 420, 1338, 9505, 1413, 9505, 25167, 1413, 257], "temperature": 0.0, "avg_logprob": -0.0846654549241066, "compression_ratio": 1.685897435897436, "no_speech_prob": 9.223243068845477e-06}, {"id": 839, "seek": 564756, "start": 5647.56, "end": 5659.64, "text": " okay so what we'll do in here is just randomly initialize a matrix to our size and here", "tokens": [1392, 370, 437, 321, 603, 360, 294, 510, 307, 445, 16979, 5883, 1125, 257, 8141, 281, 527, 2744, 293, 510], "temperature": 0.0, "avg_logprob": -0.09149845207438749, "compression_ratio": 1.606936416184971, "no_speech_prob": 4.860349235968897e-06}, {"id": 840, "seek": 564756, "start": 5663.88, "end": 5669.320000000001, "text": " yeah and here size we're kind of telling it how many columns we want and then for now let's", "tokens": [1338, 293, 510, 2744, 321, 434, 733, 295, 3585, 309, 577, 867, 13766, 321, 528, 293, 550, 337, 586, 718, 311], "temperature": 0.0, "avg_logprob": -0.09149845207438749, "compression_ratio": 1.606936416184971, "no_speech_prob": 4.860349235968897e-06}, {"id": 841, "seek": 564756, "start": 5669.320000000001, "end": 5674.92, "text": " imagine that the number of iterations were zero so we can ignore this inner for loop then we could", "tokens": [3811, 300, 264, 1230, 295, 36540, 645, 4018, 370, 321, 393, 11200, 341, 7284, 337, 6367, 550, 321, 727], "temperature": 0.0, "avg_logprob": -0.09149845207438749, "compression_ratio": 1.606936416184971, "no_speech_prob": 4.860349235968897e-06}, {"id": 842, "seek": 567492, "start": 5674.92, "end": 5686.84, "text": " just call the qr decomposition on a times q and we get back kind of q and r and we'll return our q", "tokens": [445, 818, 264, 9505, 81, 48356, 322, 257, 1413, 9505, 293, 321, 483, 646, 733, 295, 9505, 293, 367, 293, 321, 603, 2736, 527, 9505], "temperature": 0.0, "avg_logprob": -0.12976486342293875, "compression_ratio": 1.396039603960396, "no_speech_prob": 2.6841771614272147e-06}, {"id": 843, "seek": 567492, "start": 5687.8, "end": 5691.4800000000005, "text": " and so that's giving us the q that we want", "tokens": [293, 370, 300, 311, 2902, 505, 264, 9505, 300, 321, 528], "temperature": 0.0, "avg_logprob": -0.12976486342293875, "compression_ratio": 1.396039603960396, "no_speech_prob": 2.6841771614272147e-06}, {"id": 844, "seek": 569148, "start": 5691.48, "end": 5696.28, "text": " to approximate", "tokens": [281, 30874], "temperature": 0.0, "avg_logprob": -0.2228220784386923, "compression_ratio": 1.5754716981132075, "no_speech_prob": 8.664466804475524e-06}, {"id": 845, "seek": 569148, "start": 5701.16, "end": 5704.679999999999, "text": " questions about the randomized range finder so this is kind of just finding", "tokens": [1651, 466, 264, 38513, 3613, 915, 260, 370, 341, 307, 733, 295, 445, 5006], "temperature": 0.0, "avg_logprob": -0.2228220784386923, "compression_ratio": 1.5754716981132075, "no_speech_prob": 8.664466804475524e-06}, {"id": 846, "seek": 569148, "start": 5705.639999999999, "end": 5708.599999999999, "text": " finding that q we want to kind of approximate the range of a", "tokens": [5006, 300, 9505, 321, 528, 281, 733, 295, 30874, 264, 3613, 295, 257], "temperature": 0.0, "avg_logprob": -0.2228220784386923, "compression_ratio": 1.5754716981132075, "no_speech_prob": 8.664466804475524e-06}, {"id": 847, "seek": 569148, "start": 5713.16, "end": 5714.04, "text": " and then in the", "tokens": [293, 550, 294, 264], "temperature": 0.0, "avg_logprob": -0.2228220784386923, "compression_ratio": 1.5754716981132075, "no_speech_prob": 8.664466804475524e-06}, {"id": 848, "seek": 571404, "start": 5714.04, "end": 5720.04, "text": " next lesson we'll be covering lu decomposition but lu decomposition", "tokens": [958, 6898, 321, 603, 312, 10322, 10438, 48356, 457, 10438, 48356], "temperature": 0.0, "avg_logprob": -0.23207740783691405, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.750226874719374e-05}, {"id": 849, "seek": 571404, "start": 5720.84, "end": 5725.8, "text": " decomposes a matrix into a lower triangular matrix times an upper triangular matrix", "tokens": [22867, 4201, 257, 8141, 666, 257, 3126, 38190, 8141, 1413, 364, 6597, 38190, 8141], "temperature": 0.0, "avg_logprob": -0.23207740783691405, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.750226874719374e-05}, {"id": 850, "seek": 571404, "start": 5726.5199999999995, "end": 5733.56, "text": " and we can add some iterations of that here in the middle that this will make our result more", "tokens": [293, 321, 393, 909, 512, 36540, 295, 300, 510, 294, 264, 2808, 300, 341, 486, 652, 527, 1874, 544], "temperature": 0.0, "avg_logprob": -0.23207740783691405, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.750226874719374e-05}, {"id": 851, "seek": 571404, "start": 5734.28, "end": 5740.6, "text": " more accurate and it's basically kind of gives us this chance so we kind of want to imprint a like", "tokens": [544, 8559, 293, 309, 311, 1936, 733, 295, 2709, 505, 341, 2931, 370, 321, 733, 295, 528, 281, 44615, 257, 411], "temperature": 0.0, "avg_logprob": -0.23207740783691405, "compression_ratio": 1.7551020408163265, "no_speech_prob": 1.750226874719374e-05}, {"id": 852, "seek": 574060, "start": 5740.6, "end": 5747.0, "text": " we're really interested in the range of a so we want to imprint a again and again and so it would", "tokens": [321, 434, 534, 3102, 294, 264, 3613, 295, 257, 370, 321, 528, 281, 44615, 257, 797, 293, 797, 293, 370, 309, 576], "temperature": 0.0, "avg_logprob": -0.12183222872145633, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.43888824924943e-06}, {"id": 853, "seek": 574060, "start": 5747.0, "end": 5751.64, "text": " be nice to just kind of keep multiplying by a you know because if you multiply by a a bunch of times", "tokens": [312, 1481, 281, 445, 733, 295, 1066, 30955, 538, 257, 291, 458, 570, 498, 291, 12972, 538, 257, 257, 3840, 295, 1413], "temperature": 0.0, "avg_logprob": -0.12183222872145633, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.43888824924943e-06}, {"id": 854, "seek": 574060, "start": 5751.64, "end": 5757.64, "text": " you're getting something that's like super in the range of a the issue with just doing that directly", "tokens": [291, 434, 1242, 746, 300, 311, 411, 1687, 294, 264, 3613, 295, 257, 264, 2734, 365, 445, 884, 300, 3838], "temperature": 0.0, "avg_logprob": -0.12183222872145633, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.43888824924943e-06}, {"id": 855, "seek": 574060, "start": 5757.64, "end": 5764.200000000001, "text": " is that you know it could shrink to zero or it could explode that's very unstable to kind of", "tokens": [307, 300, 291, 458, 309, 727, 23060, 281, 4018, 420, 309, 727, 21411, 300, 311, 588, 23742, 281, 733, 295], "temperature": 0.0, "avg_logprob": -0.12183222872145633, "compression_ratio": 1.798165137614679, "no_speech_prob": 6.43888824924943e-06}, {"id": 856, "seek": 576420, "start": 5764.2, "end": 5771.4, "text": " multiply by the same number again and again unless that number happens to be one and so taking the lu", "tokens": [12972, 538, 264, 912, 1230, 797, 293, 797, 5969, 300, 1230, 2314, 281, 312, 472, 293, 370, 1940, 264, 10438], "temperature": 0.0, "avg_logprob": -0.17460254069124714, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.338022674550302e-06}, {"id": 857, "seek": 576420, "start": 5771.4, "end": 5777.639999999999, "text": " decomposition is a way to kind of normalize it and take into account like okay we want something", "tokens": [48356, 307, 257, 636, 281, 733, 295, 2710, 1125, 309, 293, 747, 666, 2696, 411, 1392, 321, 528, 746], "temperature": 0.0, "avg_logprob": -0.17460254069124714, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.338022674550302e-06}, {"id": 858, "seek": 576420, "start": 5777.639999999999, "end": 5783.48, "text": " that's normalized so that it doesn't explode or vanish so it's just kind of like a very", "tokens": [300, 311, 48704, 370, 300, 309, 1177, 380, 21411, 420, 43584, 370, 309, 311, 445, 733, 295, 411, 257, 588], "temperature": 0.0, "avg_logprob": -0.17460254069124714, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.338022674550302e-06}, {"id": 859, "seek": 576420, "start": 5784.28, "end": 5791.08, "text": " kind of high level intuitive idea of what this is doing and then how that's how that's used", "tokens": [733, 295, 1090, 1496, 21769, 1558, 295, 437, 341, 307, 884, 293, 550, 577, 300, 311, 577, 300, 311, 1143], "temperature": 0.0, "avg_logprob": -0.17460254069124714, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.338022674550302e-06}, {"id": 860, "seek": 579108, "start": 5791.08, "end": 5799.0, "text": " inside the a randomized svd is first the number of random columns we're going to create is the", "tokens": [1854, 264, 257, 38513, 17342, 67, 307, 700, 264, 1230, 295, 4974, 13766, 321, 434, 516, 281, 1884, 307, 264], "temperature": 0.0, "avg_logprob": -0.14771886204564294, "compression_ratio": 1.853658536585366, "no_speech_prob": 6.240802576940041e-06}, {"id": 861, "seek": 579108, "start": 5799.0, "end": 5804.44, "text": " number of components we want plus the number of over samples and so i mentioned since this is", "tokens": [1230, 295, 6677, 321, 528, 1804, 264, 1230, 295, 670, 10938, 293, 370, 741, 2835, 1670, 341, 307], "temperature": 0.0, "avg_logprob": -0.14771886204564294, "compression_ratio": 1.853658536585366, "no_speech_prob": 6.240802576940041e-06}, {"id": 862, "seek": 579108, "start": 5804.44, "end": 5810.92, "text": " random we're going to give ourselves a buffer and kind of over sample so that's defaulting to 10", "tokens": [4974, 321, 434, 516, 281, 976, 4175, 257, 21762, 293, 733, 295, 670, 6889, 370, 300, 311, 7576, 278, 281, 1266], "temperature": 0.0, "avg_logprob": -0.14771886204564294, "compression_ratio": 1.853658536585366, "no_speech_prob": 6.240802576940041e-06}, {"id": 863, "seek": 579108, "start": 5810.92, "end": 5816.92, "text": " but you could choose another another number if you wanted then we'll call the randomized range", "tokens": [457, 291, 727, 2826, 1071, 1071, 1230, 498, 291, 1415, 550, 321, 603, 818, 264, 38513, 3613], "temperature": 0.0, "avg_logprob": -0.14771886204564294, "compression_ratio": 1.853658536585366, "no_speech_prob": 6.240802576940041e-06}, {"id": 864, "seek": 581692, "start": 5816.92, "end": 5822.84, "text": " finder with our matrix and with the sum of the number of components plus the number of over", "tokens": [915, 260, 365, 527, 8141, 293, 365, 264, 2408, 295, 264, 1230, 295, 6677, 1804, 264, 1230, 295, 670], "temperature": 0.0, "avg_logprob": -0.1608754378098708, "compression_ratio": 1.6228571428571428, "no_speech_prob": 4.092869858141057e-06}, {"id": 865, "seek": 581692, "start": 5822.84, "end": 5832.52, "text": " samples that's saying how many columns we want to find then we do q transpose times m another way", "tokens": [10938, 300, 311, 1566, 577, 867, 13766, 321, 528, 281, 915, 550, 321, 360, 9505, 25167, 1413, 275, 1071, 636], "temperature": 0.0, "avg_logprob": -0.1608754378098708, "compression_ratio": 1.6228571428571428, "no_speech_prob": 4.092869858141057e-06}, {"id": 866, "seek": 581692, "start": 5832.52, "end": 5843.16, "text": " to think about that is projecting m to this k plus p dimensional space using the basis vectors", "tokens": [281, 519, 466, 300, 307, 43001, 275, 281, 341, 350, 1804, 280, 18795, 1901, 1228, 264, 5143, 18875], "temperature": 0.0, "avg_logprob": -0.1608754378098708, "compression_ratio": 1.6228571428571428, "no_speech_prob": 4.092869858141057e-06}, {"id": 867, "seek": 584316, "start": 5843.16, "end": 5847.4, "text": " space i'm using the basis vectors so", "tokens": [1901, 741, 478, 1228, 264, 5143, 18875, 370], "temperature": 0.0, "avg_logprob": -0.12655410431978995, "compression_ratio": 1.5673758865248226, "no_speech_prob": 2.8572892460942967e-06}, {"id": 868, "seek": 584316, "start": 5851.639999999999, "end": 5861.32, "text": " q is the yeah the k plus or kind of giving this giving us this basis of a k plus p dimensional space", "tokens": [9505, 307, 264, 1338, 264, 350, 1804, 420, 733, 295, 2902, 341, 2902, 505, 341, 5143, 295, 257, 350, 1804, 280, 18795, 1901], "temperature": 0.0, "avg_logprob": -0.12655410431978995, "compression_ratio": 1.5673758865248226, "no_speech_prob": 2.8572892460942967e-06}, {"id": 869, "seek": 584316, "start": 5863.96, "end": 5869.5599999999995, "text": " then we calculate our svd on b remember b is a lot smaller than our original matrix", "tokens": [550, 321, 8873, 527, 17342, 67, 322, 272, 1604, 272, 307, 257, 688, 4356, 813, 527, 3380, 8141], "temperature": 0.0, "avg_logprob": -0.12655410431978995, "compression_ratio": 1.5673758865248226, "no_speech_prob": 2.8572892460942967e-06}, {"id": 870, "seek": 586956, "start": 5869.56, "end": 5877.96, "text": " and we get that back here we delete b to free up the memory and then we say u is equal to our q", "tokens": [293, 321, 483, 300, 646, 510, 321, 12097, 272, 281, 1737, 493, 264, 4675, 293, 550, 321, 584, 344, 307, 2681, 281, 527, 9505], "temperature": 0.0, "avg_logprob": -0.05903841898991512, "compression_ratio": 1.7819905213270142, "no_speech_prob": 3.844913862849353e-06}, {"id": 871, "seek": 586956, "start": 5878.6, "end": 5885.320000000001, "text": " times the the u that came back for b and we'll just return the number of components", "tokens": [1413, 264, 264, 344, 300, 1361, 646, 337, 272, 293, 321, 603, 445, 2736, 264, 1230, 295, 6677], "temperature": 0.0, "avg_logprob": -0.05903841898991512, "compression_ratio": 1.7819905213270142, "no_speech_prob": 3.844913862849353e-06}, {"id": 872, "seek": 586956, "start": 5886.04, "end": 5890.76, "text": " and so remember we've calculated the number of components plus over samples so we don't want to", "tokens": [293, 370, 1604, 321, 600, 15598, 264, 1230, 295, 6677, 1804, 670, 10938, 370, 321, 500, 380, 528, 281], "temperature": 0.0, "avg_logprob": -0.05903841898991512, "compression_ratio": 1.7819905213270142, "no_speech_prob": 3.844913862849353e-06}, {"id": 873, "seek": 586956, "start": 5890.76, "end": 5898.76, "text": " return everything we'll kind of chop off the the last 10 last 10 columns the last 10 singular values", "tokens": [2736, 1203, 321, 603, 733, 295, 7931, 766, 264, 264, 1036, 1266, 1036, 1266, 13766, 264, 1036, 1266, 20010, 4190], "temperature": 0.0, "avg_logprob": -0.05903841898991512, "compression_ratio": 1.7819905213270142, "no_speech_prob": 3.844913862849353e-06}, {"id": 874, "seek": 589876, "start": 5898.76, "end": 5905.16, "text": " and just return what else is there and so we can try this out", "tokens": [293, 445, 2736, 437, 1646, 307, 456, 293, 370, 321, 393, 853, 341, 484], "temperature": 0.0, "avg_logprob": -0.13826013960928288, "compression_ratio": 1.5811965811965811, "no_speech_prob": 1.9333463114890037e-06}, {"id": 875, "seek": 589876, "start": 5907.8, "end": 5913.72, "text": " see that we get stuff back and this is um and this is i let me think i think this is faster", "tokens": [536, 300, 321, 483, 1507, 646, 293, 341, 307, 1105, 293, 341, 307, 741, 718, 385, 519, 741, 519, 341, 307, 4663], "temperature": 0.0, "avg_logprob": -0.13826013960928288, "compression_ratio": 1.5811965811965811, "no_speech_prob": 1.9333463114890037e-06}, {"id": 876, "seek": 589876, "start": 5915.08, "end": 5915.64, "text": " let me check", "tokens": [718, 385, 1520], "temperature": 0.0, "avg_logprob": -0.13826013960928288, "compression_ratio": 1.5811965811965811, "no_speech_prob": 1.9333463114890037e-06}, {"id": 877, "seek": 589876, "start": 5920.92, "end": 5922.360000000001, "text": " just marginally um", "tokens": [445, 10270, 379, 1105], "temperature": 0.0, "avg_logprob": -0.13826013960928288, "compression_ratio": 1.5811965811965811, "no_speech_prob": 1.9333463114890037e-06}, {"id": 878, "seek": 592236, "start": 5922.36, "end": 5928.679999999999, "text": " and we're still getting topics that seem reasonable so this is this is working um that even though we", "tokens": [293, 321, 434, 920, 1242, 8378, 300, 1643, 10585, 370, 341, 307, 341, 307, 1364, 1105, 300, 754, 1673, 321], "temperature": 0.0, "avg_logprob": -0.19775746573864575, "compression_ratio": 1.6043956043956045, "no_speech_prob": 1.669992707320489e-05}, {"id": 879, "seek": 592236, "start": 5928.679999999999, "end": 5937.32, "text": " had 2000 posts by 25 000 words if we were going to do a full svd that would be finding 2000 topics", "tokens": [632, 8132, 12300, 538, 3552, 13711, 2283, 498, 321, 645, 516, 281, 360, 257, 1577, 17342, 67, 300, 576, 312, 5006, 8132, 8378], "temperature": 0.0, "avg_logprob": -0.19775746573864575, "compression_ratio": 1.6043956043956045, "no_speech_prob": 1.669992707320489e-05}, {"id": 880, "seek": 592236, "start": 5937.32, "end": 5943.32, "text": " we've said we just want five we're going to calculate 15 and we get back pretty good topics", "tokens": [321, 600, 848, 321, 445, 528, 1732, 321, 434, 516, 281, 8873, 2119, 293, 321, 483, 646, 1238, 665, 8378], "temperature": 0.0, "avg_logprob": -0.19775746573864575, "compression_ratio": 1.6043956043956045, "no_speech_prob": 1.669992707320489e-05}, {"id": 881, "seek": 594332, "start": 5943.32, "end": 5952.759999999999, "text": " and we get back pretty good topics oh yes i looked up the computational complexity and", "tokens": [293, 321, 483, 646, 1238, 665, 8378, 1954, 2086, 741, 2956, 493, 264, 28270, 14024, 293], "temperature": 0.0, "avg_logprob": -0.10295302217656915, "compression_ratio": 1.8195121951219513, "no_speech_prob": 1.363061437587021e-05}, {"id": 882, "seek": 594332, "start": 5952.759999999999, "end": 5959.88, "text": " for an m by n matrix svd's form computational complexity is m squared n plus n cubed oh wow so", "tokens": [337, 364, 275, 538, 297, 8141, 17342, 67, 311, 1254, 28270, 14024, 307, 275, 8889, 297, 1804, 297, 36510, 1954, 6076, 370], "temperature": 0.0, "avg_logprob": -0.10295302217656915, "compression_ratio": 1.8195121951219513, "no_speech_prob": 1.363061437587021e-05}, {"id": 883, "seek": 594332, "start": 5959.88, "end": 5965.4, "text": " this is um massive massive massive improvement okay thank you jeremy yeah so what jeremy was", "tokens": [341, 307, 1105, 5994, 5994, 5994, 10444, 1392, 1309, 291, 361, 13579, 1338, 370, 437, 361, 13579, 390], "temperature": 0.0, "avg_logprob": -0.10295302217656915, "compression_ratio": 1.8195121951219513, "no_speech_prob": 1.363061437587021e-05}, {"id": 884, "seek": 594332, "start": 5965.4, "end": 5972.599999999999, "text": " saying is it's m squared n plus n cubed so both both of those terms are cubic which is really slow", "tokens": [1566, 307, 309, 311, 275, 8889, 297, 1804, 297, 36510, 370, 1293, 1293, 295, 729, 2115, 366, 28733, 597, 307, 534, 2964], "temperature": 0.0, "avg_logprob": -0.10295302217656915, "compression_ratio": 1.8195121951219513, "no_speech_prob": 1.363061437587021e-05}, {"id": 885, "seek": 597260, "start": 5972.6, "end": 5979.0, "text": " so it is awesome to be able to slice stuff off of there because you know before we would have been", "tokens": [370, 309, 307, 3476, 281, 312, 1075, 281, 13153, 1507, 766, 295, 456, 570, 291, 458, 949, 321, 576, 362, 668], "temperature": 0.0, "avg_logprob": -0.053529699643452965, "compression_ratio": 1.6067415730337078, "no_speech_prob": 3.7852621517231455e-06}, {"id": 886, "seek": 597260, "start": 5979.0, "end": 5986.120000000001, "text": " and we are still i guess our m is staying the same for this smaller matrix b but n is changing", "tokens": [293, 321, 366, 920, 741, 2041, 527, 275, 307, 7939, 264, 912, 337, 341, 4356, 8141, 272, 457, 297, 307, 4473], "temperature": 0.0, "avg_logprob": -0.053529699643452965, "compression_ratio": 1.6067415730337078, "no_speech_prob": 3.7852621517231455e-06}, {"id": 887, "seek": 597260, "start": 5988.92, "end": 5996.120000000001, "text": " yeah and then we are almost out of time maybe we'll start here at the end i just had a quick", "tokens": [1338, 293, 550, 321, 366, 1920, 484, 295, 565, 1310, 321, 603, 722, 510, 412, 264, 917, 741, 445, 632, 257, 1702], "temperature": 0.0, "avg_logprob": -0.053529699643452965, "compression_ratio": 1.6067415730337078, "no_speech_prob": 3.7852621517231455e-06}, {"id": 888, "seek": 599612, "start": 5996.12, "end": 6004.76, "text": " exercise for you um but we'll do that next time yeah so next time we'll kind of finish up um", "tokens": [5380, 337, 291, 1105, 457, 321, 603, 360, 300, 958, 565, 1338, 370, 958, 565, 321, 603, 733, 295, 2413, 493, 1105], "temperature": 0.0, "avg_logprob": -0.05766199464383333, "compression_ratio": 1.7045454545454546, "no_speech_prob": 9.368283826916013e-06}, {"id": 889, "seek": 599612, "start": 6005.4, "end": 6010.04, "text": " talk about this a little bit more and i think that'll be good to kind of return to it after", "tokens": [751, 466, 341, 257, 707, 857, 544, 293, 741, 519, 300, 603, 312, 665, 281, 733, 295, 2736, 281, 309, 934], "temperature": 0.0, "avg_logprob": -0.05766199464383333, "compression_ratio": 1.7045454545454546, "no_speech_prob": 9.368283826916013e-06}, {"id": 890, "seek": 599612, "start": 6010.04, "end": 6014.76, "text": " after two days and then we'll be getting into background removal after that which is exciting", "tokens": [934, 732, 1708, 293, 550, 321, 603, 312, 1242, 666, 3678, 17933, 934, 300, 597, 307, 4670], "temperature": 0.0, "avg_logprob": -0.05766199464383333, "compression_ratio": 1.7045454545454546, "no_speech_prob": 9.368283826916013e-06}, {"id": 891, "seek": 599612, "start": 6016.28, "end": 6022.5199999999995, "text": " i also wanted to remind you there's homework one is available on github and that is due thursday", "tokens": [741, 611, 1415, 281, 4160, 291, 456, 311, 14578, 472, 307, 2435, 322, 290, 355, 836, 293, 300, 307, 3462, 258, 10125], "temperature": 0.0, "avg_logprob": -0.05766199464383333, "compression_ratio": 1.7045454545454546, "no_speech_prob": 9.368283826916013e-06}, {"id": 892, "seek": 602252, "start": 6022.52, "end": 6027.64, "text": " yes tim", "tokens": [2086, 524], "temperature": 0.0, "avg_logprob": -0.18896211362352558, "compression_ratio": 1.5169491525423728, "no_speech_prob": 1.2218007213959936e-05}, {"id": 893, "seek": 602252, "start": 6031.0, "end": 6043.0, "text": " um that's right i i think either is fine yeah so whatever um maybe print it", "tokens": [1105, 300, 311, 558, 741, 741, 519, 2139, 307, 2489, 1338, 370, 2035, 1105, 1310, 4482, 309], "temperature": 0.0, "avg_logprob": -0.18896211362352558, "compression_ratio": 1.5169491525423728, "no_speech_prob": 1.2218007213959936e-05}, {"id": 894, "seek": 602252, "start": 6044.68, "end": 6049.96, "text": " but yeah maybe print a pdf would be best yeah so print a pdf i'll say that in the slack channel", "tokens": [457, 1338, 1310, 4482, 257, 280, 45953, 576, 312, 1151, 1338, 370, 4482, 257, 280, 45953, 741, 603, 584, 300, 294, 264, 29767, 2269], "temperature": 0.0, "avg_logprob": -0.18896211362352558, "compression_ratio": 1.5169491525423728, "no_speech_prob": 1.2218007213959936e-05}, {"id": 895, "seek": 604996, "start": 6049.96, "end": 6055.0, "text": " as well but yeah print a pdf for the homework um do on thursday", "tokens": [382, 731, 457, 1338, 4482, 257, 280, 45953, 337, 264, 14578, 1105, 360, 322, 258, 10125], "temperature": 0.0, "avg_logprob": -0.2648333100711598, "compression_ratio": 1.263157894736842, "no_speech_prob": 3.6424302379600704e-05}, {"id": 896, "seek": 605500, "start": 6055.0, "end": 6077.96, "text": " um to email us yeah that's good all right great thank you", "tokens": [50364, 1105, 281, 3796, 505, 1338, 300, 311, 665, 439, 558, 869, 1309, 291, 51512], "temperature": 0.0, "avg_logprob": -0.4412129521369934, "compression_ratio": 0.95, "no_speech_prob": 8.328998228535056e-05}], "language": "en"}