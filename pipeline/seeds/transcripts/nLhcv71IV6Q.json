{"text": " Hi folks, can you hear me okay? Can you see everything okay? Great. Oh, Pedro's here too. Hello Pedro. Hello everybody. Okay, so if you hadn't noticed yet, I just pinned the daily code walkthroughs thread, and at the top of it, I'm just going to keep the video and notes for each one. And so special extra big thanks to Elena on ictost again for her notes, and also to Vishnu. So we've got two sets of notes at the moment, which is super helpful, I think. So let's see, and if anybody has any questions about previous lessons after they've had a chance to look at them, feel free to ask during the calls as well, of course, or feel free to ask about pretty much anything, although I reserve the right to not answer, but I will certainly answer if I think it might be of general interest to people. So we were looking at data core yesterday. Before we look at that, I might point out 08. I made some minor changes to one of the 08 versions, which specifically is the Siamese model dataset one. I moved the Siamese image type, which derives from tuple, up to the top, and I then removed the Siamese image.create method, because we weren't really using it for anything, and instead, I realized we can actually just make the pipeline just say Siamese image, so that's going to call Siamese image as a function at the end. In other words, it's going to call its constructor, so it's going to create the Siamese image from the tuple. I also, rather than deriving from tuple transform to create open and resize, this is actually an easier way to do it, is you can just create a tuple transform and pass in a function. So in this case, you just have to make sure that the function resized image is defined to such that the first parameter has a type, if you want your transform to be typed. Since we're using these kind of mid-level things, it's a little bit more complicated than usual. Normally, you don't have to create a tuple transform. If you create a data source or a transformed list or a transformed dataset, then it will automatically turn any functions into the right kind of transform. So we're kind of doing more advanced stuff than probably most users would have to worry about, but I think that people who are on this call are interested in learning about the more advanced stuff. But if you look at the versions after that, we didn't have to do anything much in terms of making this all work. Okay. So, yeah, we didn't have to worry about tuple transforms or item transforms or whatever, because these things like tiff and ds know how to handle different parts of the pipeline appropriately. Okay. So that's something I thought I would mention. And you can also see that for segmentation. Again, we didn't have to do anything much at all. We just defined normal functions and stuff like that. Okay. So we were looking at 05. And I think we did, we finished with mnist, right? And then we were starting to look at tiff and dl. And so rather than going into the details of tiff and dl, let's just look at some examples of things that can use tiff and dl. So in data core, we have to find a few more transforms. As you can see, here's an example of a transform, which is CUDA. So CUDA is a transform which will move a batch to some device, which defaults, this is now incorrect documentation. So let's fix it. Default device is what it defaults to. So this might be interesting to some of you. Default device, as you can see, returns a device. And so I share a server with some other people. So we've each decided which GPU is going to be our device. So I'm using device number 5. You can change the default device by just passing in a device. So in fact, you can just say, forch.cuda.setDevice. And then that will change it. The reason we have a default device function is that you can pass in useCuda true or false. And so it's kind of handy. You can just pass in useCuda equals false, and it will set the default device to CPU. Otherwise, it will set the default device to whatever you put into torch.cuda.setDevice. This is a really nice, easy way to cause all of fast AI to go between CUDA and non-CUDA, and also to ensure that the same device is used everywhere if you wish for it to be used everywhere. You never have to use the default device. You can also always just pass in a specific device to the transform. But if you pass in none, then it's going to use the default. So I mentioned transforms look a lot like functions. As you can see, here we're creating the CUDA transform, and then we call it just like a function. And when we do call them like a function, they're going to call in codes. And the reasons that we don't just use a function, there's a couple. The first is that you might want to have some state. So in this case, we wanted some state here. Now, of course, you could do that with a partial instead. But this is just a nice, simple way to consistently do it. And then the second reason is because it's nice to have this decodes method. And so this is quite nice that we now have a CUDA method that automatically puts things back on the CPU when you're done. It's going to really help to avoid memory leaks and stuff like that. So like a lot of the more advanced functionality in Fast AI tends to be stuff which is like pretty optional. So you don't have to use a CUDA transform. You could just put in a pipeline to device as a function. And that'll work absolutely fine. But it's not going to do the extra stuff of making sure that you don't have memory leaks and stuff by automatically calling decodes for you. So most of the more advanced functionality in Fast AI is designed to both be optional and also have intuitive behavior, like do what you would expect it to do. So that's the CUDA transform. One interesting feature that you might have noticed we've seen before is there's a decorator called at docs that we use quite a lot. And what does at docs do? Well, you can see it underneath when I say show doc CUDA.decodes. It's returning this doc string. And the doc string is over here. So basically what at docs is going to do is it's going to try and find an attribute called underscore docs, which is going to be a dictionary. And it's going to use that for the doc strings. You don't have to use it for all the doc strings. But we very, very often, because our code is so simple, we very, very often have one liners, which if you add a doc string to that, it's going to become a three liner, which is just going to take up a whole lot of stuff. So this way, you can have much more concise at all classes. I also quite like having all the documentation in one place, personally. So Joseph asks, what is B? So B is being, so let's look at that from a couple of directions. The first is what's being passed to encodes. Encodes is going to be passed whatever you pass to the callable, which in this case would be tensor 1. Just like in nn.module forward, you'll get passed whatever is passed to the callable. Specifically, B in this case is standing for as Kevin's guest batch. And so basically, if you put this transform into the data loader in after batch, then it's going to get a complete batch. And it'll pop it onto whatever device you request. Which by default, if you have a GPU, will be your first GPU. You'll see here that the test is passing in a tuple containing a single tensor. And of course, that's normally what batches contain. They contain tuples of tensors. Normally they'd have two tensors, x and a y. And so things like two device and two CPU will work perfectly fine on tuples as well. They will just move everything in that tuple across to the device or the CPU. The tuples could also contain dictionaries or lists. It just does it recursively. So if we have a look at two device, you'll see that what it does is it sets up a function which calls the.to function in PyTorch. And it's going to pass it through to the device. But then it doesn't just call that function. It calls apply that function to that batch. And so what apply does is it's super handy function for PyTorch is it's going to apply this function recursively to any lists, dictionaries, and tuples inside that argument. So apply. As you can see, it's basically looking for things that are lists or instances and calling them appropriately. PyTorch has an apply as well. But this is actually a bit different because rather than changing something, it's actually returning the result of applying. And it's making every effort to keep all the types consistent as we do everywhere. So for example, if your list-y thing is actually some subclass, a tuple or list, it'll make sure that it keeps that subclass all the way through. OK. For those of you that haven't looked at the documentation framework in Fast.ai V1, you might not have seen showDoc before. showDoc is a function which returns, as you see, documentation. So if we go to the documentation, let's take a look at Puda. Here is, you can see, Puda.encodes return batch to CPU, which is obviously the wrong way around. That should say encodes. Puda.encodes. Right. Never mind. Let's do decodes since I had that the right way around. Puda.decodes return batch to CPU. So basically what showDoc does is it creates the markdown that's going to appear in the documentation. This is a bit different to how a lot of documentation generators work. I really like it because it basically allows us to auto-include the kind of automatic documentation anywhere we like. But then we can add our own markdown and code at any point. So it kind of lets us construct documentation which has auto-generated bits and also manual bits as well. And we find that super handy. When you export the Notebooks to HTML, it will automatically remove the showDoc line. So as you can see, you can't actually see it saying showDoc. The other thing to note is that for classes and functions, you don't have to include the showDoc. It will automatically add it for you. But for methods, you do have to put it there. All right. So that's kind of everything there. And you can see in the documentation, the showDoc kind of creates things like the source links which, if you look at the bottom of the screen, you can see that the source link when I'm inside the Notebook will link to the Notebook that defines the function. Whereas in the documentation, the source link will link to the... actually, this isn't quite working yet. I think once this is working, this should be linking to the GitHub where that's defined. So that's something I should note down to fix. It's working in Fast.io v1, so it should be easy to fix. Okay. All right. So you can see here our tests. So we create the transform. We call it as if it's a function. We test that it is a tuple that contains this. And we test that the type is something that's now codified. And then we should be able to decode that thing we just created. And now it should be uncodified. So as I mentioned last time, putting something that's a byte, making it into a float, takes a surprisingly long time on the CPU. So by doing this as a PyTorch transform, we can automatically have it run on the GPU if we put it in that part of the pipeline. And so here you'll see we've got one pair of encodes decodes for a tensor image and one pair of encodes decodes for a tensor mask. And the reason why, as you can see, let's move them so we put the encodes next to each other. The... I shouldn't need that. The tensor mask version converts to a long if you're trying to divide by 255, if you requested it. In fact, it's going to convert it to a long either way, which makes sense because masks have to be long. And then the decodes, if you're decoding a tensor image, then sometimes you end up with like 1.00001. So we clamp it to avoid those redundant errors. For masks, we don't have to because it was a long, so we don't have those floating point issues. So this is kind of... Yeah, this is where this auto dispatch stuff is super handy. And again, you don't have to use any of this, but it makes life a lot easier for a lot of things if you do. So Kevin, I think I've answered your question, but let me know if I haven't. I think the answer is it doesn't need it. And the reason it shouldn't need it is that it's got a tensor mask coming in. And so by default, if you don't have a return type and it returns a superclass of the thing that it gets, it will cast it to the subclass. So yeah, this is all passing without that. Okay. So I think that since it's doing nothing, I don't think we should need a decodes at all because by definition decodes does nothing. Okay. Great. That's interesting. There's about a 20 second delay. I think there's an option in YouTube video to say low latency. We could try that. I think the quality might be a little bit less good. So we'll try that next time. Remind me if I forget and we'll see if the quality is still good enough. Okay. So normalization, like here's an example of where the transform stuff is just so, so, so, so helpful. Because it's so annoying in fast.ai v1 and every other library having to remember to denormalize stuff when you want to display it. But thanks to decodable transforms, we have state containing the mean and standard deviation that we want. And so now we have decodes. So that is super helpful, right? So here's how it works. Let's see, normalize. So we can create some data loader transform pipeline that does CUDA, then converts to a float, and then normalizes with some mean and standard deviation. So then we can get a transform data loader from that. So that's our after batch transform pipeline. And so now we can grab a batch and we can decode it. And as you can see, we end up with the right means and standard deviations for everything and we can display it. So that is super great. Okay, so that's a slightly strange test. I think we're just getting lucky there because x.mean. I see. It's because we're not using a real mean and standard deviation. So the mean ends up being less than zero. That makes sense. BroadcastVec is just a minor little convenience function which our mean and standard deviation needs to be broadcast over the batch dimension correctly. So that's what BroadcastVec is doing is it's broadcasting to create a rank four tensor by broadcasting over the first dimension. And so there's certainly room for us to improve normalize which we should do I guess by maybe adding a setup method which automatically calculates a mean and standard deviation from one batch of data or something like that. For now, as you can see, it's all pretty manual. Maybe we should make BroadcastVec happen automatically as well. Let's add that. Normalize.setup. So remember setup is the thing that is going to be passed the data, the training data that we're using so we can use that to automatically set things up. Okay. So that's normalize. So one of the things that sometimes you fast AI users complain about is data bunch as being like an abstraction that gets used quite widely in fast AI and things like a complex thing to understand. So here is the entire definition of data bunch in fast AI version 2. So hopefully we can all agree that this is not something we're going to have to spend a lot of time getting our heads around. A data bunch quite literally is simply something that contains a training data loader and a validation data loader. So that's literally all it is. And so if you don't want to use our data bunch class, you could literally just go, you know, data bunch equals named tuple trainDL equals whatever, validDL equals whatever, and now you can use that as a data bunch. But there's no particular reason to because you could just go data bunch equals data bunch and pass in your training data loader and your validation data loader and again you're all done. So as long as you've got some kind of object that has a trainDL and a validDL. But so data bunch is just something which creates that object for you. There's quite a few little tricks though to make it nice and concise and again you don't need to learn all these tricks if you're not interested in kind of finding ways to write Python in concise ways. But if you are interested, you might like some of these tricks. The first one is getAtra. So getAtra is basically a wrapper around thunder getAtra which is part of Python and it's a really handy thing in Python. What it lets you do is if you define it in a class and then you call some attribute of that class that doesn't exist, then it called Python will call getAtra. So you can Google it. There's lots of information online about that if you want to know about it. But defining getAtra has a couple of problems. The first problem is that it's going to grab everything that isn't defined and that can hide errors because you might be calling something that doesn't exist by mistake by some typo or something and you end up with some like weird exception or worse still you might end up with unexpected behavior and not an exception. The second problem is that you don't get tab completion because the stuff that's in thunder getAtra, Python doesn't know what can be handled there so it has no way to do tab completion. So I will show you a couple of cool things though. If we define use the base class getAtra, it's going to give us exactly the same behavior as thunder getAtra does in Python. And specifically it's going to look for an attribute called default in your class and anything that's not understood, it's going to instead look for that attribute in default. And the reason for that we want that in data bunch is it's very handy to be able to say for example, so data bunch dot trainDL instance. For example, there's a data set. But if you just say data bunch dot data set, that would be nice to be able to assume that I'm talking about the training set by default. And so this is where getAtra is super handy rather than having to define, you know, another example would be one batch. You know, this is the same as trainDL dot one batch. Yes, Petro, that would be very helpful to add those tasks as GitHub issues. Although in this case, they kind of say consider blah. So the issue should say consider blah rather than do it because I'm not quite sure yet if it's a good idea. The first one is certainly something to fix. So thanks for that suggestion. Okay. So yeah, we'd love to be able to, you know, not have to write all those different versions. So done to getAtra is super handy for that. But as I said, you know, you have these problems of things like if you pass through some typo, so I've accidentally said not one batch but on batch, I'd get some weird error or it might even give me the wrong behavior. GetAtra subclass fixes all that. So for example, this correctly gives me an attribute error on batch. So it tells me clearly that this is an attribute that doesn't exist. And here's the other cool thing. If I press tab, I get tab completion. Or press tab, I can see all the possible things. So getAtra fixes the subclass, fixes all of the things I mentioned that I'm aware of as problems in done to getAtra in Python. So like this is an example of where we're trying to take the stuff that's already in Python and make it better. You know, more self-documenting, harder to make mistakes, stuff like that. So the way that this works is you both inherit from getAtra. You have to add some attribute called default. And that's what any unknown attributes will be passed down to. And then optionally, you can define a special attribute called underscore extra that contains a listed strings. And that will be only those things will be delegated to. So it won't delegate to every possible thing. You don't have to define underscore extra. So I'll show you. If I comment it out, right, and then I go data bunch. And I press tab. You'll see I've got more things here now. And so what it does by default is that underscore extra by default will actually dynamically find all of the attributes inside self.default. If underscore extra is not defined, it will include all of the ones that don't start with an underscore. And they're all going to be included in your tab completion list. In this case, we didn't really want to include everything. So we kind of try to keep things manageable by saying this is the subset of stuff we expect. OK. So that's one nice thing. You'll see we're defining at docs, which allows us to add documentation to everything. Ethan's asked a question about Swift. It would be best to probably ask that on the Swift forum. So Kevin's asking about tab completion on chain commands. Not really. This is the issue with Jupyter, basically, which is really an issue with Python. It can't really do tab completion on something that calls a function because it would need to call the function to know how to tab complete it. So in Jupyter, you just have to create a separate line that calls each function. But we're doing a lot less chain commands in FastAI version 2, partly for that reason and partly because of some of the ideas that came out of the Swift work. So you'll find it's less of a problem. But for things that are properties like this, the tab completion does chain correctly. OK. So that's what underscore extra is. So now you understand what the DUNDA init is. So important to recognize that a data bunch is super flexible because you can pass in as many data loaders as you like. So you can have not just a training and validation set, but a test set, multiple validation sets, and so forth. So then we define DUNDA get item so that you can, as you see, index directly into your data bunch to get the nth data loader out of it. So as you can see, it's just returning soft.dls. And so now that we've defined get item, we basically want to be able to say at property def train dl self, self return self 0. And we want it to be able to do valid dl. And that would be self 1. And so that's a lot of lines of code for not doing very much stuff. So in FastAI, there's a thing called add props, which stands for add properties. And that is going to go through numbers from 0 to n minus 1, which by default is 2. It will create a new function with this definition, and it will make it into a property. So here we're creating those two properties in one go, and the properties are called train dl and valid dl, and they respectively are going to return x0 and x1. So they'll do exactly the same as this. And so here's the same thing for getting x0 and x1.dataset as properties. So again, these are shortcuts which come up quite a lot in FastAI code, because a lot often we want the train version of something and a validation version of something. OK. So we end up with a super little concise bit of code, and that's our data bunch. We can grab one batch. In fact, for train dl, we don't need to say train dl because we have gedactra, which you can see is tested here. And one batch is simply calling, as we saw last time, next.idr.tdl. So we're just making sure that they're all the same, which they are. And then there's the test of getItem. And that should be up there. OK. All right. So that is data core. If people have requests as to where to go next, feel free to tell me. Otherwise, I think what we'll do is we'll go and have a look at the data loader. And so we're getting into some deep code now. Stuff that starts with O1 is going to be deep code. And the reason I'm going here now is so that we can actually have an excuse to look at some of the deep code. The data loader, which is here, is designed to be a replacement for high torch data loader. Yes, meta classes, I think we will get to, Kevin, pretty soon. We're kind of heading in that direction. But I want to kind of see examples of them first. So I think we're going to go from here. Then we might go to transforms. Then we might end up at meta classes. So this is designed to be a replacement for the high torch data loader. Why replace the high torch data loader? There's a few reasons. The biggest one is that I just kept finding that there were things we wanted to do with the high torch data loader that it didn't provide the hooks to do. So we had to keep creating our own new classes. And so we just had a lot of complicated code. The second reason is that the high torch data loader is very, I really don't like the code in it. It's very awkward. It's hard to understand. And there's lots of different pieces that are all tightly coupled together and make all kinds of assumptions about each other. It's really hard to work through and understand and fix issues and add things to it. So it just, yeah, I was really pretty dissatisfied with it. So we've created our own. Having said that, high torch data loader is based on a very rock solid, well tested, fast processing system. And we wanted to use that. And so the good news is that that multi-processing system is actually pulled out in high torch into something called underscore multi-processing data loader. So we can just use that. So we don't have to rewrite any of that. Unfortunately, as I mentioned, high torch classes are all kind of tightly coupled and make lots of assumptions about each other. So to actually use this, we've had to do a little bit of slightly ugly code, but it's not too much. Specifically, it's these lines of code here. But we'll come back to them later. But the only reason they exist is so that we can kind of sneak our way into the high torch data loading system. So a data loader, you can use it in much the same way as the normal high torch data loader. So if we start with a data set, and remember a data set is anything that has a length and you can index into. So for example, this list of letters is a data set. It works as a data set. So we can say data loader, pass in our data set, pass in a batch size, say whether or not you want to drop the last batch if it's not a size four, and say how many multi-processing workers to do. And then if we take that and we run two epochs of it, grab all the elements in each epoch, and then join each batch together with no separator, and then join each set of batches together with a space between them, we get back that. OK, so there's all the letters with the, as you can see, the last batch disappears. If we do it without dropLast equals true, then we do get the last bit. And we can have more than as many workers as we like by just passing that in. You can pass in things that can be turned into tensors, like for example ints, and just like the, this is all the same as the PyTorch data loader, it will turn those into batches of tensors, as you can see. So this is the testing, test equal type tests that this thing and this thing are the same, and even have exactly the same types. Normally test equals just checks that collections have the same contents. OK, so that's kind of the basic behavior that you would expect to see in a normal PyTorch data loader. But we also have some hooks that you can add in. So one of the hooks is afterItter. And so afterItter is a hook that will run at the end of each iteration. And so this is just something that's going to, let's see what T3 is. T3 is just some tensor, and so it's just something that's going to just put, set some, set T3.f to something. And so after we run this, T3.f equals that thing. So you can add code that runs after an iteration. Then let's see this one. That's just the same as before. You can also pass a generator to a data loader, and it will work fine as well. OK, so that's all kind of normal data loader behavior. Then there's more stuff we can do. And specifically, we can define, rather than just saying afterItter, there's actually a whole list of callbacks that we can, as you can see, that we can define. And we use them all over the place throughout Fast.ai. We've already seen afterItter, which runs after each iteration. There's, sorry, after all the iterations. There's beforeItter, which will run before all the iterations. And then if you look at the code, you can see here is the iterator. So here's beforeItter, here's afterItter. This is the slightly awkward thing that we need to fit in with the PyTorch multi-processing data loader. But what it's going to do then is it's going to call, it's going to basically use this thing here, which is going to call sampler to get the samples, which is basically using this as something very similar to the PyTorch's samplers. And it's going to call createBatches for each iteration. And createBatches is going to go through everything in the sampler. And it's going to map doItem over it. And doItem will first call createItem, and then it will call afterItem. And then after that, it will then use a function called chunked, which is basically going to take, it's basically going to create batches out of our list. But it's all done lazily. And then they're going to call doBatch. So just like we had doItem to create our items, doBatch creates our batches. And so that will call beforeBatch, and then createBatch. This is the thing to retain types. And then finally, afterBatch. And so the idea here is that you can replace any of those things. Things like beforeBatch, afterBatch, and afterItem are all things which default to no up to. So all these things default to no operation. So in other words, you can just use them as callbacks. But you can actually change everything. So we'll see examples of that over time, but I'll show you some examples. So example number one is here's a subclass of Dataloader. And in this subclass of Dataloader, we override createItem. So createItem normally grabs the ith element of a data set, assuming we have some sample that we want. That's what createItem normally does. So now we're overriding it to do something else. And specifically, it's going to return some random number. And so you can see it's going to return that random number if it's less than 0.95. Otherwise, it will stop. What does stop? Stop is simply something that will raise a stop iteration exception. For those of you that don't know, in Python, the way that generators, iterators, stuff like that say they've finished is they raise a stop iteration exception. So Python actually uses exceptions for control flow, which is a really interesting insight. So in this case, we can create I mean, obviously, this particular example is not something we're doing in the real world. But you can imagine creating a createItem that keeps reading something from a network stream, for example. And when it gets some, I don't know, end of network stream error, it will stop. So you can kind of easily create streaming data loaders in this way. And so here's an example, basically, of a simple streaming data loader. So that's pretty fun. And one of the interesting things about this is you can pass in numWorkers to be something other than zero. And what that's going to do is it's going to create, in this case, four streaming data loaders, which is a kind of really interesting idea. So as you can see, you end up with more batches than the zero numWorkers version, because you've got more streaming workers, and they're all doing this job. So they're all doing it totally independently. So that's pretty interesting, I think. So if you don't set a batch size, so here I've got batch size equals four, if you don't set a batch size, then we don't do any batching. And this is actually something which is also built into the new PyTorch 1.2 data loader. This idea of you can have a batch size of none. And so if you don't pass a batch size in, so here I'm, remember that letters is all the letters of the alphabet, lowercase. So if I just do a data loader on letters, then if I listify that data loader, I literally am going to end up with exactly the same thing that I started with, because it's not turning them into batches at all. So I'll get back 26 things. I can shuffle a data loader, of course. And if I do that, I should end up with exactly the same thing that I started with, but in a shuffled version. And so we actually have a thing called testShuffled that checks that the two arguments have the same contents in different orders. For randl, you could pass in a keyword argument whose value can be set to 0.95 or any other number less than 1. I'm not sure I understand that. Sorry. OK. So that's what that is. Something else that you can do is, you can see I'm passing in a data set here to my data loader, which is pretty normal. But sometimes your data set might not have a done to get item. So you might have a data set that only has a next. And so this would be the case if, for example, your data set was some kind of infinite stream, like a network link. Or it could be like some coming from a file system containing hundreds of millions of files, and it's so big you don't want to enumerate the whole lot, or something like that. And again, this is something that's also in PyTorch 1.2. It's the idea of kind of non-indexed or iterable data sets. By default, our data loader will check whether this has a done to get item. And if it doesn't, it'll treat it as iterable. If it does, it'll treat it as indexed. But you can override that by saying indexed equals false. So this case is going to give you the same, exactly the same thing as before, but it's doing it by calling next rather than done to get item. But this is useful, as I say, if you've got really, really, really huge data sets or data sets that are coming over the network or something like that. OK. So that is some of that behavior. Yes. Absolutely. You can change it to something that comes as a keyword argument. Sure. OK. So these are kind of more just interesting tests rather than additional functionality. But in a data loader, when you have multiple workers, one of the things that's difficult is ensuring that everything comes back in the correct order. So here I've created a data loader that simply returns the item from a list. So I've overwritten list. That adds a random bit of sleep each time. And so there are just some tests here, as you can see, that check that even if we have multiple workers, that we actually get back the results in the correct order. And you can also see by running percent time, we can observe that it is actually using the additional workers to run more quickly. And so here's a similar example. At this time, I've simulated a queue. So this is an example that only has done to error, doesn't have done to get item. So if I put this into my data loader, then it's only able to iterate. So here's a really good example of what it looks like to work with an iteratable only queue with variable latency. This is kind of like what something streaming over a network, for example, might look like. And so here's a test that we get back the right thing. And now in this case, because our sleepy queue only has a done to error, it doesn't have done to get item. That means there's no guarantee about what order things will come back in. So we end up with something that's shuffled this time, which I think answers the question of juvion111, which is what would happen if shuffle and indexed are both false. So it's a great question. So if indexed is false, then shuffle doesn't do anything. And specifically what happens is, in terms of how it's implemented, the sampler, here is the sampler, if you have indexed, then it returns an infinite list of integers. And if it's not indexed, then it returns an infinite list of none. So when you shuffle, then you're going to end up with an infinite list of none in shuffled order. So it doesn't actually do anything interesting. And so this is an interesting little class which we'll get to, but basically for creating infinite lists. You'll see, like a lot of the stuff we're doing is much more functional than a lot of normal Python programming, which makes it kind of easier to write, easier to understand, more concise. But we kind of had to create some things to make it easier. So things like infinite lists are things which are pretty important in this style of programming. So that's why we have a little class to easily create these two main types of infinite lists. And so if you haven't done much kind of more functional style programming in Python, definitely check out the iter tools standard library package, because that contains lots and lots and lots of useful things for this style of programming. And as you can see, for example, it has a way to slice into potentially infinite iterators. So this is how you get the first n things out of a potentially infinitely sized list. In this case, it definitely is an infinitely sized list. OK. So let's look at some interesting things here. One is that randl doesn't have to be written like this. What we could do is we could create a function called create, or we could say a function called rand item, like so. So we're now not inheriting from the data loader. We're just creating a function called rand item. And instead of creating a randl, we could create a normal data loader. But then we're going to pass in create item equals underscore rand item. Oops. And item. Ah, yes. And so that's now not going to take a self. Great. OK. So as you can see, this is now doing exactly the same thing. That and that are identical. And the reason why is we've kind of added this really nice functionality that we use all over the place where nearly anywhere that we kind of add callbacks or customizability through inheritance, you can also do it by passing in a parameter with the name of the method that you want to override. And we use this quite a lot because if you've just got some simple little function that you want to change, overriding is just more awkward than we would like you to force you to do. And particularly for newer users to Python, we don't want them to force them to have to understand inheritance and OO and stuff like that. So this is kind of much easier for new users for a lot of things. So for a lot of our documentation and lessons and stuff like that, we'll be able to do a lot of stuff without having to first teach OO and inheritance and stuff like that. The way it works behind the scenes is super fun. So let's have a look at create item. And yes, great question from Hiromi. When you do it this way, you don't get to use any state because you don't get passed in self. So if you want state, then you have to inherit. If you don't care about state, then you don't. So let's see how create item works. Create item is one of the things in this long list of underscore methods. As you can see, it's just a list of strings. Underscore methods is a special name. And it's a special name that is going to be looked at by the funks, quags, decorator. And the funks, quags, decorator is going to look for this special name. And it's going to say, oh, the quags in this init are not actually unknown. The quags is actually this list. And so what it's going to do is it's going to automatically grab any quags with these names, and it's going to replace the methods that we have with the thing that's passed to that quag. So again, one of the issues with quags, as we've talked about, is that they're terrible for kind of discoverability and documentation because you don't really know what you can pass to quags. But pretty much everything we use in quags, we make sure that we fix that. So if we look at Dataloader and hit Shift-Tab, you'll see that all the methods are listed. You'll also see that we have here assert not quags. And the reason for that is that at func quags will remove all of these methods from quags once it processes them. So that way, you can be sure that you haven't accidentally passed in something that wasn't recognized. So if instead of, for example, doing createBatchEquals, we did createBackEquals, then we will get that assertion error. So we know we've made a mistake. So again, we're kind of trying to get the best of both worlds, concise, understandable code, but also discoverability and avoiding nasty bugs. OK. So, all right. Another tiny little trick, that one that I just find super handy, is storeAtra. Here is something that I do all the time. I'll go like self.dataset, self.vs, et cetera, equals data set, vs, et cetera. So very, very often when we're setting up a class, we basically store a bunch of parameters inside self. The problem with doing it this way is you have to make sure that the orders match correctly. If you add or remove stuff from the parameter list, you have to remember to add or remove it here. You have to make sure the names are the same. So there's a lot of repetition and opportunity for bugs. So storeAtra does exactly what I just described, but you only have to list the things, list of things to store once. So it's a minor convenience, but it definitely avoids a few bugs that I come across and a little bit less typing, a little bit less reading to have to do. But I think that's handy. OK. Let me know if anybody sees anything else here that looks interesting. Because we're doing a lot of stuff lazily, you'll see a lot more yields and yield froms than you're probably used to. So to understand fast AI version 2 foundations better, you will need to make sure that you understand how those work in Python pretty well. If anybody has confusions about that, please ask on the forums. But I did find that by kind of using this lazy approach, function-lish approach, we did end up with something that I found much easier to understand, had much less bugs, and also was just much less code to write. So it was definitely worth doing. OK. So then there's this one ugly bit of code, which is basically we create an object of type FakeLoader. And the reason for that is that PyTorch assumes that it's going to be working with an object which has this specific list of things in it. So we basically had to create an object with that specific list of things and set them to the values that it expected. And then it calls iter on that object. And so that's where we then pass it back to our DataLoader. So that's what this little bit of ugly code is. It does mean, unfortunately, that we have also tied our code to some various private things inside PyTorch, which means that from version to version that might change. And so if you start using master and you get errors on things like, I don't know, underscore auto-collation doesn't exist, it probably means they've changed the name of some internal thing. So we're going to be keeping an eye on that. Maybe we can even encourage the PyTorch team to make stuff a little bit less coupled. But for now, that's a bit of ugliness that we have to deal with. Something else to mention is PyTorch, I think they added it to 1.2. Maybe it was earlier. They've added a thing called the workerInit function, which is a handy little callback in their DataLoader infrastructure, which will be called every time a new process is fired off. So we've written a little function for the workerInformation function here that calls PyTorch's getWorkerInfo, which basically will tell us what data set is this process working with, how many workers are there, and what's the ID of this worker into that list of workers. And so we do some, you know, so this way we can ensure that each worker gets a separate random seed, so we're not going to have horrible things with random seeds being duplicated. We also use this information to parallelize things automatically inside the DataLoader. As you can see, we've got, yes, we set the offset and the number of workers, and then we use that here in our sampler to ensure that each process handles a contiguous set of things which is different to each other process. Okay, so PyTorch by default calls something called defaultCollate to take all the items from your data sets and turn them into a batch if you have a batch size. Otherwise it calls something called defaultConvert. So we have created our own fastaiConvert and fastaiCollate which handle a few types which PyTorch does not handle. So it doesn't really change any behavior but it means more things should work correctly. So createBatch by default simply uses collate or convert as appropriate depending on whether your batch size is none or not. But again, you can replace that with something else and later on you'll see a place where we do that. I think the language model DataLoader does that. In fact, that might be an interesting thing just to briefly mention. For those of you that have looked at language models before, the language model DataLoader in fastai version 1 and most libraries tends to be big, confusing, horrible things. Look at this. This is the entire language model DataLoader in fastai version 2 and it's because we were able to inherit from our new DataLoader and use some of these callbacks. This actually has all of the advanced behavior of approximate shuffling and stuff like that which you want in a modern language model DataLoader. We'll look at this in more detail later but it's an example of the huge wins that we get from this new DataLoader. Things get so, so much easier. Okay, so now I think we can go back and look at what is a transform DataLoader in more detail because a transform DataLoader is something which inherits the DataLoader. And it gets a bunch of quags which as we mentioned thanks to at delegates are passed to the DataLoader in this case is the superclass but they're all going to be correctly documented and they're all going to be tab completed and everything thanks to at delegates. So what's the additional stuff that trfmdl does that DataLoader doesn't do? Well the first thing it does is this specific list of callbacks turned into transform pipelines. So basically after you grab a single item from the data set there will be a pipeline. After you collect, sorry, after you've got all of the items together but before you collect them there will be this callback and after everything's been turned into a collated batch there'll be this callback and these are all going to be pipelines. So you can see here these are all in underscore dltrfms. So we're going to go through each of those and we're going to turn them into a pipeline. Okay so it's just going to grab it and turn it into a pipeline. Remember how I mentioned before that there are item transforms and tuple transforms and item tuple transforms are the only ones that have the special behavior that it's going to be applied to each element of a tuple. And I mentioned that kind of stuff, all the tufmdl and tufmds and everything will handle all that for you and here it is doing it here. Basically before batch is the only one that's going to be done as a whole item and the other ones will be done as tuple transforms. And then I also mentioned that it handles setup so this is the place where it's going to call setup for us if you have a setup. We'll be talking more about setup when we look at pipelines in more detail. But the main thing to recognize is that tufmdl is a data loader, it has the callbacks, these same three callbacks but these three callbacks are now actual transformation pipelines which means they handle decodes and all that kind of thing. So a tufmdl we're going to call under init needs to be able to decode things. So how is it going to decode things? So one of the things it's going to need to know is what types to decode to. And this is tricky for inference for example because for inference in production all you're getting back from your model is a tensor and that tensor could represent anything. So our tufmdl actually has to remember what types it has and so it actually stores them. The types that come out of the data set are stored away in retainDS and the types that come out of the data at the end of the patching is stored away in retainDL. And they're stored as partial functions and we'll see this function called retainTypes later but this is the function that knows how to ensure that things keep their types. And so we just grab, so we get one item out of our batch just in order that we can find out what type types it has in it and then we turn that into our collated batch in order to find what types are in that. So that way when we go decode we can put back the types that we're meant to have. So that's what's going on there. So basically what that means then is that when we say show batch we can say decode, it will put back the correct types and then call the decode bits of all of our different pipelines and then allow us to show it. If we can show the whole batch at once, which would be the case for something like tabular or for things like most other kinds of data sets, we're going to have to display each part of the tuple separately, which is what this is doing here. So we end up with something where we can show batch, decode batch, et cetera. Okay. So L10 is asking, will there be a walkthrough tomorrow? Yes, there's a walkthrough every day. Well, you just need to look on the FastAI version 2 daily code walkthroughs where we will keep your information updated. Oh, wait, but today's Friday. So not today, Thursday. Yeah. So it's going to be every weekday, so including tomorrow. Okay. So let me know if you have any requests for where you would like me to head next. Is that all of that one? So if I look where we've done, we've kind of, we've done 01C, we've done 08, we've done 05. So maybe we should look at pipeline. So that's going to be a big rabbit hole. So what I suggest we do is we wrap up for today, and tomorrow let's look at pipeline, and then that might get us into transforms, and that's going to be a lot of fun. All right. Thanks, everybody. I will see you all tomorrow.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.76, "text": " Hi folks, can you hear me okay?", "tokens": [2421, 4024, 11, 393, 291, 1568, 385, 1392, 30], "temperature": 0.0, "avg_logprob": -0.5629412174224854, "compression_ratio": 1.0344827586206897, "no_speech_prob": 0.3765779137611389}, {"id": 1, "seek": 1176, "start": 11.76, "end": 31.0, "text": " Can you see everything okay?", "tokens": [1664, 291, 536, 1203, 1392, 30], "temperature": 0.0, "avg_logprob": -0.43275193245180194, "compression_ratio": 1.0740740740740742, "no_speech_prob": 0.0005774137680418789}, {"id": 2, "seek": 1176, "start": 31.0, "end": 35.84, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.43275193245180194, "compression_ratio": 1.0740740740740742, "no_speech_prob": 0.0005774137680418789}, {"id": 3, "seek": 1176, "start": 35.84, "end": 38.8, "text": " Oh, Pedro's here too. Hello Pedro.", "tokens": [876, 11, 26662, 311, 510, 886, 13, 2425, 26662, 13], "temperature": 0.0, "avg_logprob": -0.43275193245180194, "compression_ratio": 1.0740740740740742, "no_speech_prob": 0.0005774137680418789}, {"id": 4, "seek": 1176, "start": 38.8, "end": 41.52, "text": " Hello everybody.", "tokens": [2425, 2201, 13], "temperature": 0.0, "avg_logprob": -0.43275193245180194, "compression_ratio": 1.0740740740740742, "no_speech_prob": 0.0005774137680418789}, {"id": 5, "seek": 4152, "start": 41.52, "end": 55.32000000000001, "text": " Okay, so if you hadn't noticed yet, I just pinned the daily code walkthroughs thread,", "tokens": [1033, 11, 370, 498, 291, 8782, 380, 5694, 1939, 11, 286, 445, 33802, 264, 5212, 3089, 1792, 11529, 82, 7207, 11], "temperature": 0.0, "avg_logprob": -0.2157586614290873, "compression_ratio": 1.312, "no_speech_prob": 0.0007999211666174233}, {"id": 6, "seek": 4152, "start": 55.32000000000001, "end": 62.32000000000001, "text": " and at the top of it, I'm just going to keep the video and notes for each one.", "tokens": [293, 412, 264, 1192, 295, 309, 11, 286, 478, 445, 516, 281, 1066, 264, 960, 293, 5570, 337, 1184, 472, 13], "temperature": 0.0, "avg_logprob": -0.2157586614290873, "compression_ratio": 1.312, "no_speech_prob": 0.0007999211666174233}, {"id": 7, "seek": 6232, "start": 62.32, "end": 74.36, "text": " And so special extra big thanks to Elena on ictost again for her notes, and also to Vishnu.", "tokens": [400, 370, 2121, 2857, 955, 3231, 281, 39603, 322, 220, 985, 555, 797, 337, 720, 5570, 11, 293, 611, 281, 36752, 16241, 13], "temperature": 0.0, "avg_logprob": -0.23136115560726245, "compression_ratio": 1.2781954887218046, "no_speech_prob": 0.00011762635404011235}, {"id": 8, "seek": 6232, "start": 74.36, "end": 85.6, "text": " So we've got two sets of notes at the moment, which is super helpful, I think.", "tokens": [407, 321, 600, 658, 732, 6352, 295, 5570, 412, 264, 1623, 11, 597, 307, 1687, 4961, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.23136115560726245, "compression_ratio": 1.2781954887218046, "no_speech_prob": 0.00011762635404011235}, {"id": 9, "seek": 8560, "start": 85.6, "end": 93.32, "text": " So let's see, and if anybody has any questions about previous lessons after they've had a", "tokens": [407, 718, 311, 536, 11, 293, 498, 4472, 575, 604, 1651, 466, 3894, 8820, 934, 436, 600, 632, 257], "temperature": 0.0, "avg_logprob": -0.1195666335877918, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.157546446658671e-05}, {"id": 10, "seek": 8560, "start": 93.32, "end": 98.75999999999999, "text": " chance to look at them, feel free to ask during the calls as well, of course, or feel free", "tokens": [2931, 281, 574, 412, 552, 11, 841, 1737, 281, 1029, 1830, 264, 5498, 382, 731, 11, 295, 1164, 11, 420, 841, 1737], "temperature": 0.0, "avg_logprob": -0.1195666335877918, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.157546446658671e-05}, {"id": 11, "seek": 8560, "start": 98.75999999999999, "end": 105.6, "text": " to ask about pretty much anything, although I reserve the right to not answer, but I will", "tokens": [281, 1029, 466, 1238, 709, 1340, 11, 4878, 286, 17824, 264, 558, 281, 406, 1867, 11, 457, 286, 486], "temperature": 0.0, "avg_logprob": -0.1195666335877918, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.157546446658671e-05}, {"id": 12, "seek": 8560, "start": 105.6, "end": 110.67999999999999, "text": " certainly answer if I think it might be of general interest to people.", "tokens": [3297, 1867, 498, 286, 519, 309, 1062, 312, 295, 2674, 1179, 281, 561, 13], "temperature": 0.0, "avg_logprob": -0.1195666335877918, "compression_ratio": 1.5934579439252337, "no_speech_prob": 9.157546446658671e-05}, {"id": 13, "seek": 11068, "start": 110.68, "end": 117.72000000000001, "text": " So we were looking at data core yesterday.", "tokens": [407, 321, 645, 1237, 412, 1412, 4965, 5186, 13], "temperature": 0.0, "avg_logprob": -0.26631256512233187, "compression_ratio": 1.0476190476190477, "no_speech_prob": 1.2803907338820864e-05}, {"id": 14, "seek": 11068, "start": 117.72000000000001, "end": 133.12, "text": " Before we look at that, I might point out 08.", "tokens": [4546, 321, 574, 412, 300, 11, 286, 1062, 935, 484, 1958, 23, 13], "temperature": 0.0, "avg_logprob": -0.26631256512233187, "compression_ratio": 1.0476190476190477, "no_speech_prob": 1.2803907338820864e-05}, {"id": 15, "seek": 13312, "start": 133.12, "end": 140.56, "text": " I made some minor changes to one of the 08 versions, which specifically is the Siamese", "tokens": [286, 1027, 512, 6696, 2962, 281, 472, 295, 264, 1958, 23, 9606, 11, 597, 4682, 307, 264, 318, 2918, 1130], "temperature": 0.0, "avg_logprob": -0.22752735614776612, "compression_ratio": 1.554945054945055, "no_speech_prob": 1.4736914636159781e-05}, {"id": 16, "seek": 13312, "start": 140.56, "end": 142.52, "text": " model dataset one.", "tokens": [2316, 28872, 472, 13], "temperature": 0.0, "avg_logprob": -0.22752735614776612, "compression_ratio": 1.554945054945055, "no_speech_prob": 1.4736914636159781e-05}, {"id": 17, "seek": 13312, "start": 142.52, "end": 153.52, "text": " I moved the Siamese image type, which derives from tuple, up to the top, and I then removed", "tokens": [286, 4259, 264, 318, 2918, 1130, 3256, 2010, 11, 597, 1163, 1539, 490, 2604, 781, 11, 493, 281, 264, 1192, 11, 293, 286, 550, 7261], "temperature": 0.0, "avg_logprob": -0.22752735614776612, "compression_ratio": 1.554945054945055, "no_speech_prob": 1.4736914636159781e-05}, {"id": 18, "seek": 13312, "start": 153.52, "end": 163.08, "text": " the Siamese image.create method, because we weren't really using it for anything, and", "tokens": [264, 318, 2918, 1130, 3256, 13, 14066, 473, 3170, 11, 570, 321, 4999, 380, 534, 1228, 309, 337, 1340, 11, 293], "temperature": 0.0, "avg_logprob": -0.22752735614776612, "compression_ratio": 1.554945054945055, "no_speech_prob": 1.4736914636159781e-05}, {"id": 19, "seek": 16308, "start": 163.08, "end": 168.64000000000001, "text": " instead, I realized we can actually just make the pipeline just say Siamese image, so that's", "tokens": [2602, 11, 286, 5334, 321, 393, 767, 445, 652, 264, 15517, 445, 584, 318, 2918, 1130, 3256, 11, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.1681527388723273, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.3920529201859608e-05}, {"id": 20, "seek": 16308, "start": 168.64000000000001, "end": 173.44, "text": " going to call Siamese image as a function at the end.", "tokens": [516, 281, 818, 318, 2918, 1130, 3256, 382, 257, 2445, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1681527388723273, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.3920529201859608e-05}, {"id": 21, "seek": 16308, "start": 173.44, "end": 178.92000000000002, "text": " In other words, it's going to call its constructor, so it's going to create the Siamese image", "tokens": [682, 661, 2283, 11, 309, 311, 516, 281, 818, 1080, 47479, 11, 370, 309, 311, 516, 281, 1884, 264, 318, 2918, 1130, 3256], "temperature": 0.0, "avg_logprob": -0.1681527388723273, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.3920529201859608e-05}, {"id": 22, "seek": 16308, "start": 178.92000000000002, "end": 181.64000000000001, "text": " from the tuple.", "tokens": [490, 264, 2604, 781, 13], "temperature": 0.0, "avg_logprob": -0.1681527388723273, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.3920529201859608e-05}, {"id": 23, "seek": 16308, "start": 181.64000000000001, "end": 191.52, "text": " I also, rather than deriving from tuple transform to create open and resize, this is actually", "tokens": [286, 611, 11, 2831, 813, 1163, 2123, 490, 2604, 781, 4088, 281, 1884, 1269, 293, 50069, 11, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.1681527388723273, "compression_ratio": 1.7587939698492463, "no_speech_prob": 2.3920529201859608e-05}, {"id": 24, "seek": 19152, "start": 191.52, "end": 199.0, "text": " an easier way to do it, is you can just create a tuple transform and pass in a function.", "tokens": [364, 3571, 636, 281, 360, 309, 11, 307, 291, 393, 445, 1884, 257, 2604, 781, 4088, 293, 1320, 294, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.10924950066734762, "compression_ratio": 1.5568862275449102, "no_speech_prob": 5.014599082642235e-06}, {"id": 25, "seek": 19152, "start": 199.0, "end": 205.36, "text": " So in this case, you just have to make sure that the function resized image is defined", "tokens": [407, 294, 341, 1389, 11, 291, 445, 362, 281, 652, 988, 300, 264, 2445, 725, 1602, 3256, 307, 7642], "temperature": 0.0, "avg_logprob": -0.10924950066734762, "compression_ratio": 1.5568862275449102, "no_speech_prob": 5.014599082642235e-06}, {"id": 26, "seek": 19152, "start": 205.36, "end": 217.76000000000002, "text": " to such that the first parameter has a type, if you want your transform to be typed.", "tokens": [281, 1270, 300, 264, 700, 13075, 575, 257, 2010, 11, 498, 291, 528, 428, 4088, 281, 312, 33941, 13], "temperature": 0.0, "avg_logprob": -0.10924950066734762, "compression_ratio": 1.5568862275449102, "no_speech_prob": 5.014599082642235e-06}, {"id": 27, "seek": 21776, "start": 217.76, "end": 225.79999999999998, "text": " Since we're using these kind of mid-level things, it's a little bit more complicated", "tokens": [4162, 321, 434, 1228, 613, 733, 295, 2062, 12, 12418, 721, 11, 309, 311, 257, 707, 857, 544, 6179], "temperature": 0.0, "avg_logprob": -0.1141878931145919, "compression_ratio": 1.59375, "no_speech_prob": 5.507447895070072e-06}, {"id": 28, "seek": 21776, "start": 225.79999999999998, "end": 226.79999999999998, "text": " than usual.", "tokens": [813, 7713, 13], "temperature": 0.0, "avg_logprob": -0.1141878931145919, "compression_ratio": 1.59375, "no_speech_prob": 5.507447895070072e-06}, {"id": 29, "seek": 21776, "start": 226.79999999999998, "end": 229.39999999999998, "text": " Normally, you don't have to create a tuple transform.", "tokens": [17424, 11, 291, 500, 380, 362, 281, 1884, 257, 2604, 781, 4088, 13], "temperature": 0.0, "avg_logprob": -0.1141878931145919, "compression_ratio": 1.59375, "no_speech_prob": 5.507447895070072e-06}, {"id": 30, "seek": 21776, "start": 229.39999999999998, "end": 237.95999999999998, "text": " If you create a data source or a transformed list or a transformed dataset, then it will", "tokens": [759, 291, 1884, 257, 1412, 4009, 420, 257, 16894, 1329, 420, 257, 16894, 28872, 11, 550, 309, 486], "temperature": 0.0, "avg_logprob": -0.1141878931145919, "compression_ratio": 1.59375, "no_speech_prob": 5.507447895070072e-06}, {"id": 31, "seek": 21776, "start": 237.95999999999998, "end": 242.79999999999998, "text": " automatically turn any functions into the right kind of transform.", "tokens": [6772, 1261, 604, 6828, 666, 264, 558, 733, 295, 4088, 13], "temperature": 0.0, "avg_logprob": -0.1141878931145919, "compression_ratio": 1.59375, "no_speech_prob": 5.507447895070072e-06}, {"id": 32, "seek": 24280, "start": 242.8, "end": 247.84, "text": " So we're kind of doing more advanced stuff than probably most users would have to worry", "tokens": [407, 321, 434, 733, 295, 884, 544, 7339, 1507, 813, 1391, 881, 5022, 576, 362, 281, 3292], "temperature": 0.0, "avg_logprob": -0.1414483250051305, "compression_ratio": 1.5573770491803278, "no_speech_prob": 5.9544645409914665e-06}, {"id": 33, "seek": 24280, "start": 247.84, "end": 255.68, "text": " about, but I think that people who are on this call are interested in learning about", "tokens": [466, 11, 457, 286, 519, 300, 561, 567, 366, 322, 341, 818, 366, 3102, 294, 2539, 466], "temperature": 0.0, "avg_logprob": -0.1414483250051305, "compression_ratio": 1.5573770491803278, "no_speech_prob": 5.9544645409914665e-06}, {"id": 34, "seek": 24280, "start": 255.68, "end": 257.8, "text": " the more advanced stuff.", "tokens": [264, 544, 7339, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1414483250051305, "compression_ratio": 1.5573770491803278, "no_speech_prob": 5.9544645409914665e-06}, {"id": 35, "seek": 24280, "start": 257.8, "end": 268.84000000000003, "text": " But if you look at the versions after that, we didn't have to do anything much in terms", "tokens": [583, 498, 291, 574, 412, 264, 9606, 934, 300, 11, 321, 994, 380, 362, 281, 360, 1340, 709, 294, 2115], "temperature": 0.0, "avg_logprob": -0.1414483250051305, "compression_ratio": 1.5573770491803278, "no_speech_prob": 5.9544645409914665e-06}, {"id": 36, "seek": 26884, "start": 268.84, "end": 273.0, "text": " of making this all work.", "tokens": [295, 1455, 341, 439, 589, 13], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 37, "seek": 26884, "start": 273.0, "end": 274.0, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 38, "seek": 26884, "start": 274.0, "end": 281.35999999999996, "text": " So, yeah, we didn't have to worry about tuple transforms or item transforms or whatever,", "tokens": [407, 11, 1338, 11, 321, 994, 380, 362, 281, 3292, 466, 2604, 781, 35592, 420, 3174, 35592, 420, 2035, 11], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 39, "seek": 26884, "start": 281.35999999999996, "end": 286.08, "text": " because these things like tiff and ds know how to handle different parts of the pipeline", "tokens": [570, 613, 721, 411, 256, 3661, 293, 274, 82, 458, 577, 281, 4813, 819, 3166, 295, 264, 15517], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 40, "seek": 26884, "start": 286.08, "end": 287.08, "text": " appropriately.", "tokens": [23505, 13], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 41, "seek": 26884, "start": 287.08, "end": 288.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 42, "seek": 26884, "start": 288.08, "end": 292.79999999999995, "text": " So that's something I thought I would mention.", "tokens": [407, 300, 311, 746, 286, 1194, 286, 576, 2152, 13], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 43, "seek": 26884, "start": 292.79999999999995, "end": 294.4, "text": " And you can also see that for segmentation.", "tokens": [400, 291, 393, 611, 536, 300, 337, 9469, 399, 13], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 44, "seek": 26884, "start": 294.4, "end": 298.15999999999997, "text": " Again, we didn't have to do anything much at all.", "tokens": [3764, 11, 321, 994, 380, 362, 281, 360, 1340, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.2810202107846158, "compression_ratio": 1.6157205240174672, "no_speech_prob": 9.22284925763961e-06}, {"id": 45, "seek": 29816, "start": 298.16, "end": 305.24, "text": " We just defined normal functions and stuff like that.", "tokens": [492, 445, 7642, 2710, 6828, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.3079222679138184, "compression_ratio": 1.3098591549295775, "no_speech_prob": 4.157221610512352e-06}, {"id": 46, "seek": 29816, "start": 305.24, "end": 307.68, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3079222679138184, "compression_ratio": 1.3098591549295775, "no_speech_prob": 4.157221610512352e-06}, {"id": 47, "seek": 29816, "start": 307.68, "end": 316.24, "text": " So we were looking at 05.", "tokens": [407, 321, 645, 1237, 412, 1958, 20, 13], "temperature": 0.0, "avg_logprob": -0.3079222679138184, "compression_ratio": 1.3098591549295775, "no_speech_prob": 4.157221610512352e-06}, {"id": 48, "seek": 29816, "start": 316.24, "end": 321.28000000000003, "text": " And I think we did, we finished with mnist, right?", "tokens": [400, 286, 519, 321, 630, 11, 321, 4335, 365, 275, 77, 468, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.3079222679138184, "compression_ratio": 1.3098591549295775, "no_speech_prob": 4.157221610512352e-06}, {"id": 49, "seek": 29816, "start": 321.28000000000003, "end": 326.0, "text": " And then we were starting to look at tiff and dl.", "tokens": [400, 550, 321, 645, 2891, 281, 574, 412, 256, 3661, 293, 37873, 13], "temperature": 0.0, "avg_logprob": -0.3079222679138184, "compression_ratio": 1.3098591549295775, "no_speech_prob": 4.157221610512352e-06}, {"id": 50, "seek": 32600, "start": 326.0, "end": 330.4, "text": " And so rather than going into the details of tiff and dl, let's just look at some examples", "tokens": [400, 370, 2831, 813, 516, 666, 264, 4365, 295, 256, 3661, 293, 37873, 11, 718, 311, 445, 574, 412, 512, 5110], "temperature": 0.0, "avg_logprob": -0.11765811867909888, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.7502425180282444e-05}, {"id": 51, "seek": 32600, "start": 330.4, "end": 332.16, "text": " of things that can use tiff and dl.", "tokens": [295, 721, 300, 393, 764, 256, 3661, 293, 37873, 13], "temperature": 0.0, "avg_logprob": -0.11765811867909888, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.7502425180282444e-05}, {"id": 52, "seek": 32600, "start": 332.16, "end": 340.92, "text": " So in data core, we have to find a few more transforms.", "tokens": [407, 294, 1412, 4965, 11, 321, 362, 281, 915, 257, 1326, 544, 35592, 13], "temperature": 0.0, "avg_logprob": -0.11765811867909888, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.7502425180282444e-05}, {"id": 53, "seek": 32600, "start": 340.92, "end": 348.08, "text": " As you can see, here's an example of a transform, which is CUDA.", "tokens": [1018, 291, 393, 536, 11, 510, 311, 364, 1365, 295, 257, 4088, 11, 597, 307, 29777, 7509, 13], "temperature": 0.0, "avg_logprob": -0.11765811867909888, "compression_ratio": 1.496969696969697, "no_speech_prob": 1.7502425180282444e-05}, {"id": 54, "seek": 34808, "start": 348.08, "end": 362.52, "text": " So CUDA is a transform which will move a batch to some device, which defaults, this is now", "tokens": [407, 29777, 7509, 307, 257, 4088, 597, 486, 1286, 257, 15245, 281, 512, 4302, 11, 597, 7576, 82, 11, 341, 307, 586], "temperature": 0.0, "avg_logprob": -0.3177245457967122, "compression_ratio": 1.168141592920354, "no_speech_prob": 8.01325950305909e-06}, {"id": 55, "seek": 34808, "start": 362.52, "end": 364.56, "text": " incorrect documentation.", "tokens": [18424, 14333, 13], "temperature": 0.0, "avg_logprob": -0.3177245457967122, "compression_ratio": 1.168141592920354, "no_speech_prob": 8.01325950305909e-06}, {"id": 56, "seek": 34808, "start": 364.56, "end": 367.56, "text": " So let's fix it.", "tokens": [407, 718, 311, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.3177245457967122, "compression_ratio": 1.168141592920354, "no_speech_prob": 8.01325950305909e-06}, {"id": 57, "seek": 36756, "start": 367.56, "end": 389.0, "text": " Default device is what it defaults to.", "tokens": [9548, 5107, 4302, 307, 437, 309, 7576, 82, 281, 13], "temperature": 0.0, "avg_logprob": -0.22147495930011457, "compression_ratio": 1.064102564102564, "no_speech_prob": 1.3006769222556613e-05}, {"id": 58, "seek": 36756, "start": 389.0, "end": 393.88, "text": " So this might be interesting to some of you.", "tokens": [407, 341, 1062, 312, 1880, 281, 512, 295, 291, 13], "temperature": 0.0, "avg_logprob": -0.22147495930011457, "compression_ratio": 1.064102564102564, "no_speech_prob": 1.3006769222556613e-05}, {"id": 59, "seek": 39388, "start": 393.88, "end": 399.32, "text": " Default device, as you can see, returns a device.", "tokens": [9548, 5107, 4302, 11, 382, 291, 393, 536, 11, 11247, 257, 4302, 13], "temperature": 0.0, "avg_logprob": -0.09477816836934694, "compression_ratio": 1.4821428571428572, "no_speech_prob": 4.936811819789e-06}, {"id": 60, "seek": 39388, "start": 399.32, "end": 402.04, "text": " And so I share a server with some other people.", "tokens": [400, 370, 286, 2073, 257, 7154, 365, 512, 661, 561, 13], "temperature": 0.0, "avg_logprob": -0.09477816836934694, "compression_ratio": 1.4821428571428572, "no_speech_prob": 4.936811819789e-06}, {"id": 61, "seek": 39388, "start": 402.04, "end": 406.64, "text": " So we've each decided which GPU is going to be our device.", "tokens": [407, 321, 600, 1184, 3047, 597, 18407, 307, 516, 281, 312, 527, 4302, 13], "temperature": 0.0, "avg_logprob": -0.09477816836934694, "compression_ratio": 1.4821428571428572, "no_speech_prob": 4.936811819789e-06}, {"id": 62, "seek": 39388, "start": 406.64, "end": 409.56, "text": " So I'm using device number 5.", "tokens": [407, 286, 478, 1228, 4302, 1230, 1025, 13], "temperature": 0.0, "avg_logprob": -0.09477816836934694, "compression_ratio": 1.4821428571428572, "no_speech_prob": 4.936811819789e-06}, {"id": 63, "seek": 39388, "start": 409.56, "end": 418.64, "text": " You can change the default device by just passing in a device.", "tokens": [509, 393, 1319, 264, 7576, 4302, 538, 445, 8437, 294, 257, 4302, 13], "temperature": 0.0, "avg_logprob": -0.09477816836934694, "compression_ratio": 1.4821428571428572, "no_speech_prob": 4.936811819789e-06}, {"id": 64, "seek": 41864, "start": 418.64, "end": 430.76, "text": " So in fact, you can just say, forch.cuda.setDevice.", "tokens": [407, 294, 1186, 11, 291, 393, 445, 584, 11, 337, 339, 13, 66, 11152, 13, 3854, 11089, 85, 573, 13], "temperature": 0.0, "avg_logprob": -0.2003869576887651, "compression_ratio": 1.3605442176870748, "no_speech_prob": 2.6841682938538725e-06}, {"id": 65, "seek": 41864, "start": 430.76, "end": 433.59999999999997, "text": " And then that will change it.", "tokens": [400, 550, 300, 486, 1319, 309, 13], "temperature": 0.0, "avg_logprob": -0.2003869576887651, "compression_ratio": 1.3605442176870748, "no_speech_prob": 2.6841682938538725e-06}, {"id": 66, "seek": 41864, "start": 433.59999999999997, "end": 441.47999999999996, "text": " The reason we have a default device function is that you can pass in useCuda true or false.", "tokens": [440, 1778, 321, 362, 257, 7576, 4302, 2445, 307, 300, 291, 393, 1320, 294, 764, 34, 11152, 2074, 420, 7908, 13], "temperature": 0.0, "avg_logprob": -0.2003869576887651, "compression_ratio": 1.3605442176870748, "no_speech_prob": 2.6841682938538725e-06}, {"id": 67, "seek": 41864, "start": 441.47999999999996, "end": 442.76, "text": " And so it's kind of handy.", "tokens": [400, 370, 309, 311, 733, 295, 13239, 13], "temperature": 0.0, "avg_logprob": -0.2003869576887651, "compression_ratio": 1.3605442176870748, "no_speech_prob": 2.6841682938538725e-06}, {"id": 68, "seek": 44276, "start": 442.76, "end": 452.44, "text": " You can just pass in useCuda equals false, and it will set the default device to CPU.", "tokens": [509, 393, 445, 1320, 294, 764, 34, 11152, 6915, 7908, 11, 293, 309, 486, 992, 264, 7576, 4302, 281, 13199, 13], "temperature": 0.0, "avg_logprob": -0.1080926548350941, "compression_ratio": 1.5114942528735633, "no_speech_prob": 4.565826657199068e-06}, {"id": 69, "seek": 44276, "start": 452.44, "end": 461.52, "text": " Otherwise, it will set the default device to whatever you put into torch.cuda.setDevice.", "tokens": [10328, 11, 309, 486, 992, 264, 7576, 4302, 281, 2035, 291, 829, 666, 27822, 13, 66, 11152, 13, 3854, 11089, 85, 573, 13], "temperature": 0.0, "avg_logprob": -0.1080926548350941, "compression_ratio": 1.5114942528735633, "no_speech_prob": 4.565826657199068e-06}, {"id": 70, "seek": 44276, "start": 461.52, "end": 467.88, "text": " This is a really nice, easy way to cause all of fast AI to go between CUDA and non-CUDA,", "tokens": [639, 307, 257, 534, 1481, 11, 1858, 636, 281, 3082, 439, 295, 2370, 7318, 281, 352, 1296, 29777, 7509, 293, 2107, 12, 25864, 7509, 11], "temperature": 0.0, "avg_logprob": -0.1080926548350941, "compression_ratio": 1.5114942528735633, "no_speech_prob": 4.565826657199068e-06}, {"id": 71, "seek": 46788, "start": 467.88, "end": 473.08, "text": " and also to ensure that the same device is used everywhere if you wish for it to be used", "tokens": [293, 611, 281, 5586, 300, 264, 912, 4302, 307, 1143, 5315, 498, 291, 3172, 337, 309, 281, 312, 1143], "temperature": 0.0, "avg_logprob": -0.11895136038462321, "compression_ratio": 1.68944099378882, "no_speech_prob": 2.2252484086493496e-06}, {"id": 72, "seek": 46788, "start": 473.08, "end": 474.08, "text": " everywhere.", "tokens": [5315, 13], "temperature": 0.0, "avg_logprob": -0.11895136038462321, "compression_ratio": 1.68944099378882, "no_speech_prob": 2.2252484086493496e-06}, {"id": 73, "seek": 46788, "start": 474.08, "end": 476.8, "text": " You never have to use the default device.", "tokens": [509, 1128, 362, 281, 764, 264, 7576, 4302, 13], "temperature": 0.0, "avg_logprob": -0.11895136038462321, "compression_ratio": 1.68944099378882, "no_speech_prob": 2.2252484086493496e-06}, {"id": 74, "seek": 46788, "start": 476.8, "end": 482.76, "text": " You can also always just pass in a specific device to the transform.", "tokens": [509, 393, 611, 1009, 445, 1320, 294, 257, 2685, 4302, 281, 264, 4088, 13], "temperature": 0.0, "avg_logprob": -0.11895136038462321, "compression_ratio": 1.68944099378882, "no_speech_prob": 2.2252484086493496e-06}, {"id": 75, "seek": 46788, "start": 482.76, "end": 490.12, "text": " But if you pass in none, then it's going to use the default.", "tokens": [583, 498, 291, 1320, 294, 6022, 11, 550, 309, 311, 516, 281, 764, 264, 7576, 13], "temperature": 0.0, "avg_logprob": -0.11895136038462321, "compression_ratio": 1.68944099378882, "no_speech_prob": 2.2252484086493496e-06}, {"id": 76, "seek": 49012, "start": 490.12, "end": 498.08, "text": " So I mentioned transforms look a lot like functions.", "tokens": [407, 286, 2835, 35592, 574, 257, 688, 411, 6828, 13], "temperature": 0.0, "avg_logprob": -0.11051747534010145, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.186344757632469e-07}, {"id": 77, "seek": 49012, "start": 498.08, "end": 502.12, "text": " As you can see, here we're creating the CUDA transform, and then we call it just like a", "tokens": [1018, 291, 393, 536, 11, 510, 321, 434, 4084, 264, 29777, 7509, 4088, 11, 293, 550, 321, 818, 309, 445, 411, 257], "temperature": 0.0, "avg_logprob": -0.11051747534010145, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.186344757632469e-07}, {"id": 78, "seek": 49012, "start": 502.12, "end": 505.8, "text": " function.", "tokens": [2445, 13], "temperature": 0.0, "avg_logprob": -0.11051747534010145, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.186344757632469e-07}, {"id": 79, "seek": 49012, "start": 505.8, "end": 510.32, "text": " And when we do call them like a function, they're going to call in codes.", "tokens": [400, 562, 321, 360, 818, 552, 411, 257, 2445, 11, 436, 434, 516, 281, 818, 294, 14211, 13], "temperature": 0.0, "avg_logprob": -0.11051747534010145, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.186344757632469e-07}, {"id": 80, "seek": 49012, "start": 510.32, "end": 514.24, "text": " And the reasons that we don't just use a function, there's a couple.", "tokens": [400, 264, 4112, 300, 321, 500, 380, 445, 764, 257, 2445, 11, 456, 311, 257, 1916, 13], "temperature": 0.0, "avg_logprob": -0.11051747534010145, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.186344757632469e-07}, {"id": 81, "seek": 49012, "start": 514.24, "end": 516.92, "text": " The first is that you might want to have some state.", "tokens": [440, 700, 307, 300, 291, 1062, 528, 281, 362, 512, 1785, 13], "temperature": 0.0, "avg_logprob": -0.11051747534010145, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.186344757632469e-07}, {"id": 82, "seek": 49012, "start": 516.92, "end": 519.52, "text": " So in this case, we wanted some state here.", "tokens": [407, 294, 341, 1389, 11, 321, 1415, 512, 1785, 510, 13], "temperature": 0.0, "avg_logprob": -0.11051747534010145, "compression_ratio": 1.7647058823529411, "no_speech_prob": 8.186344757632469e-07}, {"id": 83, "seek": 51952, "start": 519.52, "end": 525.68, "text": " Now, of course, you could do that with a partial instead.", "tokens": [823, 11, 295, 1164, 11, 291, 727, 360, 300, 365, 257, 14641, 2602, 13], "temperature": 0.0, "avg_logprob": -0.10562106406334604, "compression_ratio": 1.5867768595041323, "no_speech_prob": 3.4465465432731435e-06}, {"id": 84, "seek": 51952, "start": 525.68, "end": 529.88, "text": " But this is just a nice, simple way to consistently do it.", "tokens": [583, 341, 307, 445, 257, 1481, 11, 2199, 636, 281, 14961, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.10562106406334604, "compression_ratio": 1.5867768595041323, "no_speech_prob": 3.4465465432731435e-06}, {"id": 85, "seek": 51952, "start": 529.88, "end": 534.84, "text": " And then the second reason is because it's nice to have this decodes method.", "tokens": [400, 550, 264, 1150, 1778, 307, 570, 309, 311, 1481, 281, 362, 341, 979, 4789, 3170, 13], "temperature": 0.0, "avg_logprob": -0.10562106406334604, "compression_ratio": 1.5867768595041323, "no_speech_prob": 3.4465465432731435e-06}, {"id": 86, "seek": 51952, "start": 534.84, "end": 538.3199999999999, "text": " And so this is quite nice that we now have a CUDA method that automatically puts things", "tokens": [400, 370, 341, 307, 1596, 1481, 300, 321, 586, 362, 257, 29777, 7509, 3170, 300, 6772, 8137, 721], "temperature": 0.0, "avg_logprob": -0.10562106406334604, "compression_ratio": 1.5867768595041323, "no_speech_prob": 3.4465465432731435e-06}, {"id": 87, "seek": 51952, "start": 538.3199999999999, "end": 540.72, "text": " back on the CPU when you're done.", "tokens": [646, 322, 264, 13199, 562, 291, 434, 1096, 13], "temperature": 0.0, "avg_logprob": -0.10562106406334604, "compression_ratio": 1.5867768595041323, "no_speech_prob": 3.4465465432731435e-06}, {"id": 88, "seek": 51952, "start": 540.72, "end": 549.36, "text": " It's going to really help to avoid memory leaks and stuff like that.", "tokens": [467, 311, 516, 281, 534, 854, 281, 5042, 4675, 28885, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.10562106406334604, "compression_ratio": 1.5867768595041323, "no_speech_prob": 3.4465465432731435e-06}, {"id": 89, "seek": 54936, "start": 549.36, "end": 557.48, "text": " So like a lot of the more advanced functionality in Fast AI tends to be stuff which is like", "tokens": [407, 411, 257, 688, 295, 264, 544, 7339, 14980, 294, 15968, 7318, 12258, 281, 312, 1507, 597, 307, 411], "temperature": 0.0, "avg_logprob": -0.18383004448630594, "compression_ratio": 1.3806818181818181, "no_speech_prob": 1.308149762735411e-06}, {"id": 90, "seek": 54936, "start": 557.48, "end": 558.48, "text": " pretty optional.", "tokens": [1238, 17312, 13], "temperature": 0.0, "avg_logprob": -0.18383004448630594, "compression_ratio": 1.3806818181818181, "no_speech_prob": 1.308149762735411e-06}, {"id": 91, "seek": 54936, "start": 558.48, "end": 562.94, "text": " So you don't have to use a CUDA transform.", "tokens": [407, 291, 500, 380, 362, 281, 764, 257, 29777, 7509, 4088, 13], "temperature": 0.0, "avg_logprob": -0.18383004448630594, "compression_ratio": 1.3806818181818181, "no_speech_prob": 1.308149762735411e-06}, {"id": 92, "seek": 54936, "start": 562.94, "end": 572.2, "text": " You could just put in a pipeline to device as a function.", "tokens": [509, 727, 445, 829, 294, 257, 15517, 281, 4302, 382, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.18383004448630594, "compression_ratio": 1.3806818181818181, "no_speech_prob": 1.308149762735411e-06}, {"id": 93, "seek": 54936, "start": 572.2, "end": 575.12, "text": " And that'll work absolutely fine.", "tokens": [400, 300, 603, 589, 3122, 2489, 13], "temperature": 0.0, "avg_logprob": -0.18383004448630594, "compression_ratio": 1.3806818181818181, "no_speech_prob": 1.308149762735411e-06}, {"id": 94, "seek": 57512, "start": 575.12, "end": 579.8, "text": " But it's not going to do the extra stuff of making sure that you don't have memory leaks", "tokens": [583, 309, 311, 406, 516, 281, 360, 264, 2857, 1507, 295, 1455, 988, 300, 291, 500, 380, 362, 4675, 28885], "temperature": 0.0, "avg_logprob": -0.1247780846386421, "compression_ratio": 1.5, "no_speech_prob": 4.888262310487335e-07}, {"id": 95, "seek": 57512, "start": 579.8, "end": 582.96, "text": " and stuff by automatically calling decodes for you.", "tokens": [293, 1507, 538, 6772, 5141, 979, 4789, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1247780846386421, "compression_ratio": 1.5, "no_speech_prob": 4.888262310487335e-07}, {"id": 96, "seek": 57512, "start": 582.96, "end": 591.44, "text": " So most of the more advanced functionality in Fast AI is designed to both be optional", "tokens": [407, 881, 295, 264, 544, 7339, 14980, 294, 15968, 7318, 307, 4761, 281, 1293, 312, 17312], "temperature": 0.0, "avg_logprob": -0.1247780846386421, "compression_ratio": 1.5, "no_speech_prob": 4.888262310487335e-07}, {"id": 97, "seek": 57512, "start": 591.44, "end": 599.6, "text": " and also have intuitive behavior, like do what you would expect it to do.", "tokens": [293, 611, 362, 21769, 5223, 11, 411, 360, 437, 291, 576, 2066, 309, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1247780846386421, "compression_ratio": 1.5, "no_speech_prob": 4.888262310487335e-07}, {"id": 98, "seek": 57512, "start": 599.6, "end": 603.78, "text": " So that's the CUDA transform.", "tokens": [407, 300, 311, 264, 29777, 7509, 4088, 13], "temperature": 0.0, "avg_logprob": -0.1247780846386421, "compression_ratio": 1.5, "no_speech_prob": 4.888262310487335e-07}, {"id": 99, "seek": 60378, "start": 603.78, "end": 608.48, "text": " One interesting feature that you might have noticed we've seen before is there's a decorator", "tokens": [1485, 1880, 4111, 300, 291, 1062, 362, 5694, 321, 600, 1612, 949, 307, 456, 311, 257, 7919, 1639], "temperature": 0.0, "avg_logprob": -0.13778852499448335, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.0784720618394203e-05}, {"id": 100, "seek": 60378, "start": 608.48, "end": 612.54, "text": " called at docs that we use quite a lot.", "tokens": [1219, 412, 45623, 300, 321, 764, 1596, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.13778852499448335, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.0784720618394203e-05}, {"id": 101, "seek": 60378, "start": 612.54, "end": 615.0799999999999, "text": " And what does at docs do?", "tokens": [400, 437, 775, 412, 45623, 360, 30], "temperature": 0.0, "avg_logprob": -0.13778852499448335, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.0784720618394203e-05}, {"id": 102, "seek": 60378, "start": 615.0799999999999, "end": 621.64, "text": " Well, you can see it underneath when I say show doc CUDA.decodes.", "tokens": [1042, 11, 291, 393, 536, 309, 7223, 562, 286, 584, 855, 3211, 29777, 7509, 13, 42821, 4789, 13], "temperature": 0.0, "avg_logprob": -0.13778852499448335, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.0784720618394203e-05}, {"id": 103, "seek": 60378, "start": 621.64, "end": 624.8, "text": " It's returning this doc string.", "tokens": [467, 311, 12678, 341, 3211, 6798, 13], "temperature": 0.0, "avg_logprob": -0.13778852499448335, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.0784720618394203e-05}, {"id": 104, "seek": 60378, "start": 624.8, "end": 627.24, "text": " And the doc string is over here.", "tokens": [400, 264, 3211, 6798, 307, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.13778852499448335, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.0784720618394203e-05}, {"id": 105, "seek": 60378, "start": 627.24, "end": 631.56, "text": " So basically what at docs is going to do is it's going to try and find an attribute called", "tokens": [407, 1936, 437, 412, 45623, 307, 516, 281, 360, 307, 309, 311, 516, 281, 853, 293, 915, 364, 19667, 1219], "temperature": 0.0, "avg_logprob": -0.13778852499448335, "compression_ratio": 1.6170212765957446, "no_speech_prob": 2.0784720618394203e-05}, {"id": 106, "seek": 63156, "start": 631.56, "end": 635.04, "text": " underscore docs, which is going to be a dictionary.", "tokens": [37556, 45623, 11, 597, 307, 516, 281, 312, 257, 25890, 13], "temperature": 0.0, "avg_logprob": -0.1705453122248415, "compression_ratio": 1.8220338983050848, "no_speech_prob": 3.288710558990715e-06}, {"id": 107, "seek": 63156, "start": 635.04, "end": 637.8, "text": " And it's going to use that for the doc strings.", "tokens": [400, 309, 311, 516, 281, 764, 300, 337, 264, 3211, 13985, 13], "temperature": 0.0, "avg_logprob": -0.1705453122248415, "compression_ratio": 1.8220338983050848, "no_speech_prob": 3.288710558990715e-06}, {"id": 108, "seek": 63156, "start": 637.8, "end": 640.0, "text": " You don't have to use it for all the doc strings.", "tokens": [509, 500, 380, 362, 281, 764, 309, 337, 439, 264, 3211, 13985, 13], "temperature": 0.0, "avg_logprob": -0.1705453122248415, "compression_ratio": 1.8220338983050848, "no_speech_prob": 3.288710558990715e-06}, {"id": 109, "seek": 63156, "start": 640.0, "end": 643.8, "text": " But we very, very often, because our code is so simple, we very, very often have one", "tokens": [583, 321, 588, 11, 588, 2049, 11, 570, 527, 3089, 307, 370, 2199, 11, 321, 588, 11, 588, 2049, 362, 472], "temperature": 0.0, "avg_logprob": -0.1705453122248415, "compression_ratio": 1.8220338983050848, "no_speech_prob": 3.288710558990715e-06}, {"id": 110, "seek": 63156, "start": 643.8, "end": 649.0799999999999, "text": " liners, which if you add a doc string to that, it's going to become a three liner, which", "tokens": [22896, 433, 11, 597, 498, 291, 909, 257, 3211, 6798, 281, 300, 11, 309, 311, 516, 281, 1813, 257, 1045, 24468, 11, 597], "temperature": 0.0, "avg_logprob": -0.1705453122248415, "compression_ratio": 1.8220338983050848, "no_speech_prob": 3.288710558990715e-06}, {"id": 111, "seek": 63156, "start": 649.0799999999999, "end": 653.68, "text": " is just going to take up a whole lot of stuff.", "tokens": [307, 445, 516, 281, 747, 493, 257, 1379, 688, 295, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1705453122248415, "compression_ratio": 1.8220338983050848, "no_speech_prob": 3.288710558990715e-06}, {"id": 112, "seek": 63156, "start": 653.68, "end": 658.76, "text": " So this way, you can have much more concise at all classes.", "tokens": [407, 341, 636, 11, 291, 393, 362, 709, 544, 44882, 412, 439, 5359, 13], "temperature": 0.0, "avg_logprob": -0.1705453122248415, "compression_ratio": 1.8220338983050848, "no_speech_prob": 3.288710558990715e-06}, {"id": 113, "seek": 65876, "start": 658.76, "end": 663.68, "text": " I also quite like having all the documentation in one place, personally.", "tokens": [286, 611, 1596, 411, 1419, 439, 264, 14333, 294, 472, 1081, 11, 5665, 13], "temperature": 0.0, "avg_logprob": -0.19018212231722745, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.1659345545922406e-05}, {"id": 114, "seek": 65876, "start": 663.68, "end": 666.68, "text": " So Joseph asks, what is B?", "tokens": [407, 11170, 8962, 11, 437, 307, 363, 30], "temperature": 0.0, "avg_logprob": -0.19018212231722745, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.1659345545922406e-05}, {"id": 115, "seek": 65876, "start": 666.68, "end": 671.72, "text": " So B is being, so let's look at that from a couple of directions.", "tokens": [407, 363, 307, 885, 11, 370, 718, 311, 574, 412, 300, 490, 257, 1916, 295, 11095, 13], "temperature": 0.0, "avg_logprob": -0.19018212231722745, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.1659345545922406e-05}, {"id": 116, "seek": 65876, "start": 671.72, "end": 675.96, "text": " The first is what's being passed to encodes.", "tokens": [440, 700, 307, 437, 311, 885, 4678, 281, 2058, 4789, 13], "temperature": 0.0, "avg_logprob": -0.19018212231722745, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.1659345545922406e-05}, {"id": 117, "seek": 65876, "start": 675.96, "end": 681.04, "text": " Encodes is going to be passed whatever you pass to the callable, which in this case would", "tokens": [29584, 4789, 307, 516, 281, 312, 4678, 2035, 291, 1320, 281, 264, 818, 712, 11, 597, 294, 341, 1389, 576], "temperature": 0.0, "avg_logprob": -0.19018212231722745, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.1659345545922406e-05}, {"id": 118, "seek": 65876, "start": 681.04, "end": 682.6, "text": " be tensor 1.", "tokens": [312, 40863, 502, 13], "temperature": 0.0, "avg_logprob": -0.19018212231722745, "compression_ratio": 1.5048076923076923, "no_speech_prob": 1.1659345545922406e-05}, {"id": 119, "seek": 68260, "start": 682.6, "end": 690.6, "text": " Just like in nn.module forward, you'll get passed whatever is passed to the callable.", "tokens": [1449, 411, 294, 297, 77, 13, 8014, 2271, 2128, 11, 291, 603, 483, 4678, 2035, 307, 4678, 281, 264, 818, 712, 13], "temperature": 0.0, "avg_logprob": -0.18317508697509766, "compression_ratio": 1.5951219512195123, "no_speech_prob": 1.7880516907098354e-06}, {"id": 120, "seek": 68260, "start": 690.6, "end": 697.0400000000001, "text": " Specifically, B in this case is standing for as Kevin's guest batch.", "tokens": [26058, 11, 363, 294, 341, 1389, 307, 4877, 337, 382, 9954, 311, 8341, 15245, 13], "temperature": 0.0, "avg_logprob": -0.18317508697509766, "compression_ratio": 1.5951219512195123, "no_speech_prob": 1.7880516907098354e-06}, {"id": 121, "seek": 68260, "start": 697.0400000000001, "end": 704.5600000000001, "text": " And so basically, if you put this transform into the data loader in after batch, then", "tokens": [400, 370, 1936, 11, 498, 291, 829, 341, 4088, 666, 264, 1412, 3677, 260, 294, 934, 15245, 11, 550], "temperature": 0.0, "avg_logprob": -0.18317508697509766, "compression_ratio": 1.5951219512195123, "no_speech_prob": 1.7880516907098354e-06}, {"id": 122, "seek": 68260, "start": 704.5600000000001, "end": 706.88, "text": " it's going to get a complete batch.", "tokens": [309, 311, 516, 281, 483, 257, 3566, 15245, 13], "temperature": 0.0, "avg_logprob": -0.18317508697509766, "compression_ratio": 1.5951219512195123, "no_speech_prob": 1.7880516907098354e-06}, {"id": 123, "seek": 68260, "start": 706.88, "end": 711.6, "text": " And it'll pop it onto whatever device you request.", "tokens": [400, 309, 603, 1665, 309, 3911, 2035, 4302, 291, 5308, 13], "temperature": 0.0, "avg_logprob": -0.18317508697509766, "compression_ratio": 1.5951219512195123, "no_speech_prob": 1.7880516907098354e-06}, {"id": 124, "seek": 71160, "start": 711.6, "end": 718.16, "text": " Which by default, if you have a GPU, will be your first GPU.", "tokens": [3013, 538, 7576, 11, 498, 291, 362, 257, 18407, 11, 486, 312, 428, 700, 18407, 13], "temperature": 0.0, "avg_logprob": -0.11440071105957031, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.905417997884797e-06}, {"id": 125, "seek": 71160, "start": 718.16, "end": 726.8000000000001, "text": " You'll see here that the test is passing in a tuple containing a single tensor.", "tokens": [509, 603, 536, 510, 300, 264, 1500, 307, 8437, 294, 257, 2604, 781, 19273, 257, 2167, 40863, 13], "temperature": 0.0, "avg_logprob": -0.11440071105957031, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.905417997884797e-06}, {"id": 126, "seek": 71160, "start": 726.8000000000001, "end": 729.1, "text": " And of course, that's normally what batches contain.", "tokens": [400, 295, 1164, 11, 300, 311, 5646, 437, 15245, 279, 5304, 13], "temperature": 0.0, "avg_logprob": -0.11440071105957031, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.905417997884797e-06}, {"id": 127, "seek": 71160, "start": 729.1, "end": 731.0, "text": " They contain tuples of tensors.", "tokens": [814, 5304, 2604, 2622, 295, 10688, 830, 13], "temperature": 0.0, "avg_logprob": -0.11440071105957031, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.905417997884797e-06}, {"id": 128, "seek": 71160, "start": 731.0, "end": 735.0400000000001, "text": " Normally they'd have two tensors, x and a y.", "tokens": [17424, 436, 1116, 362, 732, 10688, 830, 11, 2031, 293, 257, 288, 13], "temperature": 0.0, "avg_logprob": -0.11440071105957031, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.905417997884797e-06}, {"id": 129, "seek": 71160, "start": 735.0400000000001, "end": 741.32, "text": " And so things like two device and two CPU will work perfectly fine on tuples as well.", "tokens": [400, 370, 721, 411, 732, 4302, 293, 732, 13199, 486, 589, 6239, 2489, 322, 2604, 2622, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11440071105957031, "compression_ratio": 1.5964125560538116, "no_speech_prob": 3.905417997884797e-06}, {"id": 130, "seek": 74132, "start": 741.32, "end": 746.5600000000001, "text": " They will just move everything in that tuple across to the device or the CPU.", "tokens": [814, 486, 445, 1286, 1203, 294, 300, 2604, 781, 2108, 281, 264, 4302, 420, 264, 13199, 13], "temperature": 0.0, "avg_logprob": -0.0984920064608256, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.288716698079952e-06}, {"id": 131, "seek": 74132, "start": 746.5600000000001, "end": 749.1600000000001, "text": " The tuples could also contain dictionaries or lists.", "tokens": [440, 2604, 2622, 727, 611, 5304, 22352, 4889, 420, 14511, 13], "temperature": 0.0, "avg_logprob": -0.0984920064608256, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.288716698079952e-06}, {"id": 132, "seek": 74132, "start": 749.1600000000001, "end": 751.24, "text": " It just does it recursively.", "tokens": [467, 445, 775, 309, 20560, 3413, 13], "temperature": 0.0, "avg_logprob": -0.0984920064608256, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.288716698079952e-06}, {"id": 133, "seek": 74132, "start": 751.24, "end": 760.5600000000001, "text": " So if we have a look at two device, you'll see that what it does is it sets up a function", "tokens": [407, 498, 321, 362, 257, 574, 412, 732, 4302, 11, 291, 603, 536, 300, 437, 309, 775, 307, 309, 6352, 493, 257, 2445], "temperature": 0.0, "avg_logprob": -0.0984920064608256, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.288716698079952e-06}, {"id": 134, "seek": 74132, "start": 760.5600000000001, "end": 766.08, "text": " which calls the.to function in PyTorch.", "tokens": [597, 5498, 264, 2411, 1353, 2445, 294, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.0984920064608256, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.288716698079952e-06}, {"id": 135, "seek": 74132, "start": 766.08, "end": 770.36, "text": " And it's going to pass it through to the device.", "tokens": [400, 309, 311, 516, 281, 1320, 309, 807, 281, 264, 4302, 13], "temperature": 0.0, "avg_logprob": -0.0984920064608256, "compression_ratio": 1.5504587155963303, "no_speech_prob": 3.288716698079952e-06}, {"id": 136, "seek": 77036, "start": 770.36, "end": 771.96, "text": " But then it doesn't just call that function.", "tokens": [583, 550, 309, 1177, 380, 445, 818, 300, 2445, 13], "temperature": 0.0, "avg_logprob": -0.17408468916609482, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.6701109416317195e-05}, {"id": 137, "seek": 77036, "start": 771.96, "end": 776.16, "text": " It calls apply that function to that batch.", "tokens": [467, 5498, 3079, 300, 2445, 281, 300, 15245, 13], "temperature": 0.0, "avg_logprob": -0.17408468916609482, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.6701109416317195e-05}, {"id": 138, "seek": 77036, "start": 776.16, "end": 781.28, "text": " And so what apply does is it's super handy function for PyTorch is it's going to apply", "tokens": [400, 370, 437, 3079, 775, 307, 309, 311, 1687, 13239, 2445, 337, 9953, 51, 284, 339, 307, 309, 311, 516, 281, 3079], "temperature": 0.0, "avg_logprob": -0.17408468916609482, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.6701109416317195e-05}, {"id": 139, "seek": 77036, "start": 781.28, "end": 791.8000000000001, "text": " this function recursively to any lists, dictionaries, and tuples inside that argument.", "tokens": [341, 2445, 20560, 3413, 281, 604, 14511, 11, 22352, 4889, 11, 293, 2604, 2622, 1854, 300, 6770, 13], "temperature": 0.0, "avg_logprob": -0.17408468916609482, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.6701109416317195e-05}, {"id": 140, "seek": 77036, "start": 791.8000000000001, "end": 794.48, "text": " So apply.", "tokens": [407, 3079, 13], "temperature": 0.0, "avg_logprob": -0.17408468916609482, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.6701109416317195e-05}, {"id": 141, "seek": 79448, "start": 794.48, "end": 800.6, "text": " As you can see, it's basically looking for things that are lists or instances and calling", "tokens": [1018, 291, 393, 536, 11, 309, 311, 1936, 1237, 337, 721, 300, 366, 14511, 420, 14519, 293, 5141], "temperature": 0.0, "avg_logprob": -0.15911451116338507, "compression_ratio": 1.6755725190839694, "no_speech_prob": 3.446553591857082e-06}, {"id": 142, "seek": 79448, "start": 800.6, "end": 803.24, "text": " them appropriately.", "tokens": [552, 23505, 13], "temperature": 0.0, "avg_logprob": -0.15911451116338507, "compression_ratio": 1.6755725190839694, "no_speech_prob": 3.446553591857082e-06}, {"id": 143, "seek": 79448, "start": 803.24, "end": 805.16, "text": " PyTorch has an apply as well.", "tokens": [9953, 51, 284, 339, 575, 364, 3079, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15911451116338507, "compression_ratio": 1.6755725190839694, "no_speech_prob": 3.446553591857082e-06}, {"id": 144, "seek": 79448, "start": 805.16, "end": 808.96, "text": " But this is actually a bit different because rather than changing something, it's actually", "tokens": [583, 341, 307, 767, 257, 857, 819, 570, 2831, 813, 4473, 746, 11, 309, 311, 767], "temperature": 0.0, "avg_logprob": -0.15911451116338507, "compression_ratio": 1.6755725190839694, "no_speech_prob": 3.446553591857082e-06}, {"id": 145, "seek": 79448, "start": 808.96, "end": 811.12, "text": " returning the result of applying.", "tokens": [12678, 264, 1874, 295, 9275, 13], "temperature": 0.0, "avg_logprob": -0.15911451116338507, "compression_ratio": 1.6755725190839694, "no_speech_prob": 3.446553591857082e-06}, {"id": 146, "seek": 79448, "start": 811.12, "end": 816.24, "text": " And it's making every effort to keep all the types consistent as we do everywhere.", "tokens": [400, 309, 311, 1455, 633, 4630, 281, 1066, 439, 264, 3467, 8398, 382, 321, 360, 5315, 13], "temperature": 0.0, "avg_logprob": -0.15911451116338507, "compression_ratio": 1.6755725190839694, "no_speech_prob": 3.446553591857082e-06}, {"id": 147, "seek": 79448, "start": 816.24, "end": 822.0, "text": " So for example, if your list-y thing is actually some subclass, a tuple or list, it'll make", "tokens": [407, 337, 1365, 11, 498, 428, 1329, 12, 88, 551, 307, 767, 512, 1422, 11665, 11, 257, 2604, 781, 420, 1329, 11, 309, 603, 652], "temperature": 0.0, "avg_logprob": -0.15911451116338507, "compression_ratio": 1.6755725190839694, "no_speech_prob": 3.446553591857082e-06}, {"id": 148, "seek": 82200, "start": 822.0, "end": 826.76, "text": " sure that it keeps that subclass all the way through.", "tokens": [988, 300, 309, 5965, 300, 1422, 11665, 439, 264, 636, 807, 13], "temperature": 0.0, "avg_logprob": -0.25824080875941685, "compression_ratio": 1.4698795180722892, "no_speech_prob": 2.0580350792442914e-06}, {"id": 149, "seek": 82200, "start": 826.76, "end": 829.76, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.25824080875941685, "compression_ratio": 1.4698795180722892, "no_speech_prob": 2.0580350792442914e-06}, {"id": 150, "seek": 82200, "start": 829.76, "end": 835.72, "text": " For those of you that haven't looked at the documentation framework in Fast.ai V1, you", "tokens": [1171, 729, 295, 291, 300, 2378, 380, 2956, 412, 264, 14333, 8388, 294, 15968, 13, 1301, 691, 16, 11, 291], "temperature": 0.0, "avg_logprob": -0.25824080875941685, "compression_ratio": 1.4698795180722892, "no_speech_prob": 2.0580350792442914e-06}, {"id": 151, "seek": 82200, "start": 835.72, "end": 838.48, "text": " might not have seen showDoc before.", "tokens": [1062, 406, 362, 1612, 855, 35, 905, 949, 13], "temperature": 0.0, "avg_logprob": -0.25824080875941685, "compression_ratio": 1.4698795180722892, "no_speech_prob": 2.0580350792442914e-06}, {"id": 152, "seek": 82200, "start": 838.48, "end": 844.3, "text": " showDoc is a function which returns, as you see, documentation.", "tokens": [855, 35, 905, 307, 257, 2445, 597, 11247, 11, 382, 291, 536, 11, 14333, 13], "temperature": 0.0, "avg_logprob": -0.25824080875941685, "compression_ratio": 1.4698795180722892, "no_speech_prob": 2.0580350792442914e-06}, {"id": 153, "seek": 84430, "start": 844.3, "end": 856.8, "text": " So if we go to the documentation, let's take a look at Puda.", "tokens": [407, 498, 321, 352, 281, 264, 14333, 11, 718, 311, 747, 257, 574, 412, 430, 11152, 13], "temperature": 0.0, "avg_logprob": -0.28721473693847654, "compression_ratio": 1.236220472440945, "no_speech_prob": 4.860378794546705e-06}, {"id": 154, "seek": 84430, "start": 856.8, "end": 871.16, "text": " Here is, you can see, Puda.encodes return batch to CPU, which is obviously the wrong", "tokens": [1692, 307, 11, 291, 393, 536, 11, 430, 11152, 13, 22660, 4789, 2736, 15245, 281, 13199, 11, 597, 307, 2745, 264, 2085], "temperature": 0.0, "avg_logprob": -0.28721473693847654, "compression_ratio": 1.236220472440945, "no_speech_prob": 4.860378794546705e-06}, {"id": 155, "seek": 84430, "start": 871.16, "end": 872.16, "text": " way around.", "tokens": [636, 926, 13], "temperature": 0.0, "avg_logprob": -0.28721473693847654, "compression_ratio": 1.236220472440945, "no_speech_prob": 4.860378794546705e-06}, {"id": 156, "seek": 87216, "start": 872.16, "end": 875.64, "text": " That should say encodes.", "tokens": [663, 820, 584, 2058, 4789, 13], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 157, "seek": 87216, "start": 875.64, "end": 876.64, "text": " Puda.encodes.", "tokens": [430, 11152, 13, 22660, 4789, 13], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 158, "seek": 87216, "start": 876.64, "end": 877.64, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 159, "seek": 87216, "start": 877.64, "end": 878.64, "text": " Never mind.", "tokens": [7344, 1575, 13], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 160, "seek": 87216, "start": 878.64, "end": 886.56, "text": " Let's do decodes since I had that the right way around.", "tokens": [961, 311, 360, 979, 4789, 1670, 286, 632, 300, 264, 558, 636, 926, 13], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 161, "seek": 87216, "start": 886.56, "end": 892.0, "text": " Puda.decodes return batch to CPU.", "tokens": [430, 11152, 13, 42821, 4789, 2736, 15245, 281, 13199, 13], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 162, "seek": 87216, "start": 892.0, "end": 897.52, "text": " So basically what showDoc does is it creates the markdown that's going to appear in the", "tokens": [407, 1936, 437, 855, 35, 905, 775, 307, 309, 7829, 264, 1491, 5093, 300, 311, 516, 281, 4204, 294, 264], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 163, "seek": 87216, "start": 897.52, "end": 899.72, "text": " documentation.", "tokens": [14333, 13], "temperature": 0.0, "avg_logprob": -0.28696411038622444, "compression_ratio": 1.4285714285714286, "no_speech_prob": 1.8162006654165452e-06}, {"id": 164, "seek": 89972, "start": 899.72, "end": 903.88, "text": " This is a bit different to how a lot of documentation generators work.", "tokens": [639, 307, 257, 857, 819, 281, 577, 257, 688, 295, 14333, 38662, 589, 13], "temperature": 0.0, "avg_logprob": -0.11464860267245892, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.2958772660786053e-06}, {"id": 165, "seek": 89972, "start": 903.88, "end": 909.76, "text": " I really like it because it basically allows us to auto-include the kind of automatic documentation", "tokens": [286, 534, 411, 309, 570, 309, 1936, 4045, 505, 281, 8399, 12, 4647, 32334, 264, 733, 295, 12509, 14333], "temperature": 0.0, "avg_logprob": -0.11464860267245892, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.2958772660786053e-06}, {"id": 166, "seek": 89972, "start": 909.76, "end": 911.4, "text": " anywhere we like.", "tokens": [4992, 321, 411, 13], "temperature": 0.0, "avg_logprob": -0.11464860267245892, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.2958772660786053e-06}, {"id": 167, "seek": 89972, "start": 911.4, "end": 917.52, "text": " But then we can add our own markdown and code at any point.", "tokens": [583, 550, 321, 393, 909, 527, 1065, 1491, 5093, 293, 3089, 412, 604, 935, 13], "temperature": 0.0, "avg_logprob": -0.11464860267245892, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.2958772660786053e-06}, {"id": 168, "seek": 89972, "start": 917.52, "end": 922.8000000000001, "text": " So it kind of lets us construct documentation which has auto-generated bits and also manual", "tokens": [407, 309, 733, 295, 6653, 505, 7690, 14333, 597, 575, 8399, 12, 21848, 770, 9239, 293, 611, 9688], "temperature": 0.0, "avg_logprob": -0.11464860267245892, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.2958772660786053e-06}, {"id": 169, "seek": 89972, "start": 922.8000000000001, "end": 924.2, "text": " bits as well.", "tokens": [9239, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11464860267245892, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.2958772660786053e-06}, {"id": 170, "seek": 89972, "start": 924.2, "end": 927.36, "text": " And we find that super handy.", "tokens": [400, 321, 915, 300, 1687, 13239, 13], "temperature": 0.0, "avg_logprob": -0.11464860267245892, "compression_ratio": 1.6842105263157894, "no_speech_prob": 2.2958772660786053e-06}, {"id": 171, "seek": 92736, "start": 927.36, "end": 933.5600000000001, "text": " When you export the Notebooks to HTML, it will automatically remove the showDoc line.", "tokens": [1133, 291, 10725, 264, 11633, 15170, 281, 17995, 11, 309, 486, 6772, 4159, 264, 855, 35, 905, 1622, 13], "temperature": 0.0, "avg_logprob": -0.15891653491604713, "compression_ratio": 1.64, "no_speech_prob": 3.4465510907466523e-06}, {"id": 172, "seek": 92736, "start": 933.5600000000001, "end": 938.08, "text": " So as you can see, you can't actually see it saying showDoc.", "tokens": [407, 382, 291, 393, 536, 11, 291, 393, 380, 767, 536, 309, 1566, 855, 35, 905, 13], "temperature": 0.0, "avg_logprob": -0.15891653491604713, "compression_ratio": 1.64, "no_speech_prob": 3.4465510907466523e-06}, {"id": 173, "seek": 92736, "start": 938.08, "end": 944.8000000000001, "text": " The other thing to note is that for classes and functions, you don't have to include the", "tokens": [440, 661, 551, 281, 3637, 307, 300, 337, 5359, 293, 6828, 11, 291, 500, 380, 362, 281, 4090, 264], "temperature": 0.0, "avg_logprob": -0.15891653491604713, "compression_ratio": 1.64, "no_speech_prob": 3.4465510907466523e-06}, {"id": 174, "seek": 92736, "start": 944.8000000000001, "end": 945.8000000000001, "text": " showDoc.", "tokens": [855, 35, 905, 13], "temperature": 0.0, "avg_logprob": -0.15891653491604713, "compression_ratio": 1.64, "no_speech_prob": 3.4465510907466523e-06}, {"id": 175, "seek": 92736, "start": 945.8000000000001, "end": 948.16, "text": " It will automatically add it for you.", "tokens": [467, 486, 6772, 909, 309, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15891653491604713, "compression_ratio": 1.64, "no_speech_prob": 3.4465510907466523e-06}, {"id": 176, "seek": 92736, "start": 948.16, "end": 956.5600000000001, "text": " But for methods, you do have to put it there.", "tokens": [583, 337, 7150, 11, 291, 360, 362, 281, 829, 309, 456, 13], "temperature": 0.0, "avg_logprob": -0.15891653491604713, "compression_ratio": 1.64, "no_speech_prob": 3.4465510907466523e-06}, {"id": 177, "seek": 95656, "start": 956.56, "end": 957.56, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1771746516227722, "compression_ratio": 1.6612021857923498, "no_speech_prob": 6.854172170278616e-06}, {"id": 178, "seek": 95656, "start": 957.56, "end": 962.2399999999999, "text": " So that's kind of everything there.", "tokens": [407, 300, 311, 733, 295, 1203, 456, 13], "temperature": 0.0, "avg_logprob": -0.1771746516227722, "compression_ratio": 1.6612021857923498, "no_speech_prob": 6.854172170278616e-06}, {"id": 179, "seek": 95656, "start": 962.2399999999999, "end": 966.16, "text": " And you can see in the documentation, the showDoc kind of creates things like the source", "tokens": [400, 291, 393, 536, 294, 264, 14333, 11, 264, 855, 35, 905, 733, 295, 7829, 721, 411, 264, 4009], "temperature": 0.0, "avg_logprob": -0.1771746516227722, "compression_ratio": 1.6612021857923498, "no_speech_prob": 6.854172170278616e-06}, {"id": 180, "seek": 95656, "start": 966.16, "end": 971.2199999999999, "text": " links which, if you look at the bottom of the screen, you can see that the source link", "tokens": [6123, 597, 11, 498, 291, 574, 412, 264, 2767, 295, 264, 2568, 11, 291, 393, 536, 300, 264, 4009, 2113], "temperature": 0.0, "avg_logprob": -0.1771746516227722, "compression_ratio": 1.6612021857923498, "no_speech_prob": 6.854172170278616e-06}, {"id": 181, "seek": 95656, "start": 971.2199999999999, "end": 978.5999999999999, "text": " when I'm inside the Notebook will link to the Notebook that defines the function.", "tokens": [562, 286, 478, 1854, 264, 11633, 2939, 486, 2113, 281, 264, 11633, 2939, 300, 23122, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1771746516227722, "compression_ratio": 1.6612021857923498, "no_speech_prob": 6.854172170278616e-06}, {"id": 182, "seek": 97860, "start": 978.6, "end": 989.0400000000001, "text": " Whereas in the documentation, the source link will link to the... actually, this isn't quite", "tokens": [13813, 294, 264, 14333, 11, 264, 4009, 2113, 486, 2113, 281, 264, 485, 767, 11, 341, 1943, 380, 1596], "temperature": 0.0, "avg_logprob": -0.13468039420343214, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.74799548505689e-06}, {"id": 183, "seek": 97860, "start": 989.0400000000001, "end": 990.0400000000001, "text": " working yet.", "tokens": [1364, 1939, 13], "temperature": 0.0, "avg_logprob": -0.13468039420343214, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.74799548505689e-06}, {"id": 184, "seek": 97860, "start": 990.0400000000001, "end": 998.96, "text": " I think once this is working, this should be linking to the GitHub where that's defined.", "tokens": [286, 519, 1564, 341, 307, 1364, 11, 341, 820, 312, 25775, 281, 264, 23331, 689, 300, 311, 7642, 13], "temperature": 0.0, "avg_logprob": -0.13468039420343214, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.74799548505689e-06}, {"id": 185, "seek": 97860, "start": 998.96, "end": 1005.44, "text": " So that's something I should note down to fix.", "tokens": [407, 300, 311, 746, 286, 820, 3637, 760, 281, 3191, 13], "temperature": 0.0, "avg_logprob": -0.13468039420343214, "compression_ratio": 1.535031847133758, "no_speech_prob": 6.74799548505689e-06}, {"id": 186, "seek": 100544, "start": 1005.44, "end": 1015.6800000000001, "text": " It's working in Fast.io v1, so it should be easy to fix.", "tokens": [467, 311, 1364, 294, 15968, 13, 1004, 371, 16, 11, 370, 309, 820, 312, 1858, 281, 3191, 13], "temperature": 0.0, "avg_logprob": -0.27965487162272135, "compression_ratio": 1.3481012658227849, "no_speech_prob": 3.3931135021703085e-06}, {"id": 187, "seek": 100544, "start": 1015.6800000000001, "end": 1016.6800000000001, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.27965487162272135, "compression_ratio": 1.3481012658227849, "no_speech_prob": 3.3931135021703085e-06}, {"id": 188, "seek": 100544, "start": 1016.6800000000001, "end": 1019.5200000000001, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.27965487162272135, "compression_ratio": 1.3481012658227849, "no_speech_prob": 3.3931135021703085e-06}, {"id": 189, "seek": 100544, "start": 1019.5200000000001, "end": 1022.6400000000001, "text": " So you can see here our tests.", "tokens": [407, 291, 393, 536, 510, 527, 6921, 13], "temperature": 0.0, "avg_logprob": -0.27965487162272135, "compression_ratio": 1.3481012658227849, "no_speech_prob": 3.3931135021703085e-06}, {"id": 190, "seek": 100544, "start": 1022.6400000000001, "end": 1025.28, "text": " So we create the transform.", "tokens": [407, 321, 1884, 264, 4088, 13], "temperature": 0.0, "avg_logprob": -0.27965487162272135, "compression_ratio": 1.3481012658227849, "no_speech_prob": 3.3931135021703085e-06}, {"id": 191, "seek": 100544, "start": 1025.28, "end": 1028.04, "text": " We call it as if it's a function.", "tokens": [492, 818, 309, 382, 498, 309, 311, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.27965487162272135, "compression_ratio": 1.3481012658227849, "no_speech_prob": 3.3931135021703085e-06}, {"id": 192, "seek": 100544, "start": 1028.04, "end": 1035.3200000000002, "text": " We test that it is a tuple that contains this.", "tokens": [492, 1500, 300, 309, 307, 257, 2604, 781, 300, 8306, 341, 13], "temperature": 0.0, "avg_logprob": -0.27965487162272135, "compression_ratio": 1.3481012658227849, "no_speech_prob": 3.3931135021703085e-06}, {"id": 193, "seek": 103532, "start": 1035.32, "end": 1041.6799999999998, "text": " And we test that the type is something that's now codified.", "tokens": [400, 321, 1500, 300, 264, 2010, 307, 746, 300, 311, 586, 17656, 2587, 13], "temperature": 0.0, "avg_logprob": -0.1541691430007355, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.844895672955317e-06}, {"id": 194, "seek": 103532, "start": 1041.6799999999998, "end": 1044.96, "text": " And then we should be able to decode that thing we just created.", "tokens": [400, 550, 321, 820, 312, 1075, 281, 979, 1429, 300, 551, 321, 445, 2942, 13], "temperature": 0.0, "avg_logprob": -0.1541691430007355, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.844895672955317e-06}, {"id": 195, "seek": 103532, "start": 1044.96, "end": 1049.52, "text": " And now it should be uncodified.", "tokens": [400, 586, 309, 820, 312, 6219, 378, 2587, 13], "temperature": 0.0, "avg_logprob": -0.1541691430007355, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.844895672955317e-06}, {"id": 196, "seek": 103532, "start": 1049.52, "end": 1055.32, "text": " So as I mentioned last time, putting something that's a byte, making it into a float, takes", "tokens": [407, 382, 286, 2835, 1036, 565, 11, 3372, 746, 300, 311, 257, 40846, 11, 1455, 309, 666, 257, 15706, 11, 2516], "temperature": 0.0, "avg_logprob": -0.1541691430007355, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.844895672955317e-06}, {"id": 197, "seek": 103532, "start": 1055.32, "end": 1059.52, "text": " a surprisingly long time on the CPU.", "tokens": [257, 17600, 938, 565, 322, 264, 13199, 13], "temperature": 0.0, "avg_logprob": -0.1541691430007355, "compression_ratio": 1.580110497237569, "no_speech_prob": 3.844895672955317e-06}, {"id": 198, "seek": 105952, "start": 1059.52, "end": 1067.52, "text": " So by doing this as a PyTorch transform, we can automatically have it run on the GPU if", "tokens": [407, 538, 884, 341, 382, 257, 9953, 51, 284, 339, 4088, 11, 321, 393, 6772, 362, 309, 1190, 322, 264, 18407, 498], "temperature": 0.0, "avg_logprob": -0.08928363800048827, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.6015593448391883e-06}, {"id": 199, "seek": 105952, "start": 1067.52, "end": 1071.8799999999999, "text": " we put it in that part of the pipeline.", "tokens": [321, 829, 309, 294, 300, 644, 295, 264, 15517, 13], "temperature": 0.0, "avg_logprob": -0.08928363800048827, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.6015593448391883e-06}, {"id": 200, "seek": 105952, "start": 1071.8799999999999, "end": 1079.24, "text": " And so here you'll see we've got one pair of encodes decodes for a tensor image and", "tokens": [400, 370, 510, 291, 603, 536, 321, 600, 658, 472, 6119, 295, 2058, 4789, 979, 4789, 337, 257, 40863, 3256, 293], "temperature": 0.0, "avg_logprob": -0.08928363800048827, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.6015593448391883e-06}, {"id": 201, "seek": 105952, "start": 1079.24, "end": 1082.96, "text": " one pair of encodes decodes for a tensor mask.", "tokens": [472, 6119, 295, 2058, 4789, 979, 4789, 337, 257, 40863, 6094, 13], "temperature": 0.0, "avg_logprob": -0.08928363800048827, "compression_ratio": 1.5731707317073171, "no_speech_prob": 2.6015593448391883e-06}, {"id": 202, "seek": 108296, "start": 1082.96, "end": 1091.64, "text": " And the reason why, as you can see, let's move them so we put the encodes next to each", "tokens": [400, 264, 1778, 983, 11, 382, 291, 393, 536, 11, 718, 311, 1286, 552, 370, 321, 829, 264, 2058, 4789, 958, 281, 1184], "temperature": 0.0, "avg_logprob": -0.29749840327671595, "compression_ratio": 1.361963190184049, "no_speech_prob": 3.0894575502316e-06}, {"id": 203, "seek": 108296, "start": 1091.64, "end": 1092.64, "text": " other.", "tokens": [661, 13], "temperature": 0.0, "avg_logprob": -0.29749840327671595, "compression_ratio": 1.361963190184049, "no_speech_prob": 3.0894575502316e-06}, {"id": 204, "seek": 108296, "start": 1092.64, "end": 1093.64, "text": " The...", "tokens": [440, 485], "temperature": 0.0, "avg_logprob": -0.29749840327671595, "compression_ratio": 1.361963190184049, "no_speech_prob": 3.0894575502316e-06}, {"id": 205, "seek": 108296, "start": 1093.64, "end": 1096.1200000000001, "text": " I shouldn't need that.", "tokens": [286, 4659, 380, 643, 300, 13], "temperature": 0.0, "avg_logprob": -0.29749840327671595, "compression_ratio": 1.361963190184049, "no_speech_prob": 3.0894575502316e-06}, {"id": 206, "seek": 108296, "start": 1096.1200000000001, "end": 1107.72, "text": " The tensor mask version converts to a long if you're trying to divide by 255, if you", "tokens": [440, 40863, 6094, 3037, 38874, 281, 257, 938, 498, 291, 434, 1382, 281, 9845, 538, 3552, 20, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.29749840327671595, "compression_ratio": 1.361963190184049, "no_speech_prob": 3.0894575502316e-06}, {"id": 207, "seek": 108296, "start": 1107.72, "end": 1108.72, "text": " requested it.", "tokens": [16436, 309, 13], "temperature": 0.0, "avg_logprob": -0.29749840327671595, "compression_ratio": 1.361963190184049, "no_speech_prob": 3.0894575502316e-06}, {"id": 208, "seek": 110872, "start": 1108.72, "end": 1116.24, "text": " In fact, it's going to convert it to a long either way, which makes sense because masks", "tokens": [682, 1186, 11, 309, 311, 516, 281, 7620, 309, 281, 257, 938, 2139, 636, 11, 597, 1669, 2020, 570, 11830], "temperature": 0.0, "avg_logprob": -0.17570411836778796, "compression_ratio": 1.4269662921348314, "no_speech_prob": 2.684138735276065e-06}, {"id": 209, "seek": 110872, "start": 1116.24, "end": 1121.1200000000001, "text": " have to be long.", "tokens": [362, 281, 312, 938, 13], "temperature": 0.0, "avg_logprob": -0.17570411836778796, "compression_ratio": 1.4269662921348314, "no_speech_prob": 2.684138735276065e-06}, {"id": 210, "seek": 110872, "start": 1121.1200000000001, "end": 1130.0, "text": " And then the decodes, if you're decoding a tensor image, then sometimes you end up with", "tokens": [400, 550, 264, 979, 4789, 11, 498, 291, 434, 979, 8616, 257, 40863, 3256, 11, 550, 2171, 291, 917, 493, 365], "temperature": 0.0, "avg_logprob": -0.17570411836778796, "compression_ratio": 1.4269662921348314, "no_speech_prob": 2.684138735276065e-06}, {"id": 211, "seek": 110872, "start": 1130.0, "end": 1132.6200000000001, "text": " like 1.00001.", "tokens": [411, 502, 13, 33202, 16, 13], "temperature": 0.0, "avg_logprob": -0.17570411836778796, "compression_ratio": 1.4269662921348314, "no_speech_prob": 2.684138735276065e-06}, {"id": 212, "seek": 110872, "start": 1132.6200000000001, "end": 1136.08, "text": " So we clamp it to avoid those redundant errors.", "tokens": [407, 321, 17690, 309, 281, 5042, 729, 40997, 13603, 13], "temperature": 0.0, "avg_logprob": -0.17570411836778796, "compression_ratio": 1.4269662921348314, "no_speech_prob": 2.684138735276065e-06}, {"id": 213, "seek": 113608, "start": 1136.08, "end": 1139.8, "text": " For masks, we don't have to because it was a long, so we don't have those floating point", "tokens": [1171, 11830, 11, 321, 500, 380, 362, 281, 570, 309, 390, 257, 938, 11, 370, 321, 500, 380, 362, 729, 12607, 935], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 214, "seek": 113608, "start": 1139.8, "end": 1140.8, "text": " issues.", "tokens": [2663, 13], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 215, "seek": 113608, "start": 1140.8, "end": 1142.8, "text": " So this is kind of...", "tokens": [407, 341, 307, 733, 295, 485], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 216, "seek": 113608, "start": 1142.8, "end": 1148.24, "text": " Yeah, this is where this auto dispatch stuff is super handy.", "tokens": [865, 11, 341, 307, 689, 341, 8399, 36729, 1507, 307, 1687, 13239, 13], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 217, "seek": 113608, "start": 1148.24, "end": 1154.12, "text": " And again, you don't have to use any of this, but it makes life a lot easier for a lot of", "tokens": [400, 797, 11, 291, 500, 380, 362, 281, 764, 604, 295, 341, 11, 457, 309, 1669, 993, 257, 688, 3571, 337, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 218, "seek": 113608, "start": 1154.12, "end": 1156.6, "text": " things if you do.", "tokens": [721, 498, 291, 360, 13], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 219, "seek": 113608, "start": 1156.6, "end": 1160.1999999999998, "text": " So Kevin, I think I've answered your question, but let me know if I haven't.", "tokens": [407, 9954, 11, 286, 519, 286, 600, 10103, 428, 1168, 11, 457, 718, 385, 458, 498, 286, 2378, 380, 13], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 220, "seek": 113608, "start": 1160.1999999999998, "end": 1162.4399999999998, "text": " I think the answer is it doesn't need it.", "tokens": [286, 519, 264, 1867, 307, 309, 1177, 380, 643, 309, 13], "temperature": 0.0, "avg_logprob": -0.18166548358507392, "compression_ratio": 1.6916666666666667, "no_speech_prob": 3.373626896063797e-05}, {"id": 221, "seek": 116244, "start": 1162.44, "end": 1168.76, "text": " And the reason it shouldn't need it is that it's got a tensor mask coming in.", "tokens": [400, 264, 1778, 309, 4659, 380, 643, 309, 307, 300, 309, 311, 658, 257, 40863, 6094, 1348, 294, 13], "temperature": 0.0, "avg_logprob": -0.13172580034304887, "compression_ratio": 1.497175141242938, "no_speech_prob": 4.611158146872185e-05}, {"id": 222, "seek": 116244, "start": 1168.76, "end": 1175.92, "text": " And so by default, if you don't have a return type and it returns a superclass of the thing", "tokens": [400, 370, 538, 7576, 11, 498, 291, 500, 380, 362, 257, 2736, 2010, 293, 309, 11247, 257, 1687, 11665, 295, 264, 551], "temperature": 0.0, "avg_logprob": -0.13172580034304887, "compression_ratio": 1.497175141242938, "no_speech_prob": 4.611158146872185e-05}, {"id": 223, "seek": 116244, "start": 1175.92, "end": 1179.0, "text": " that it gets, it will cast it to the subclass.", "tokens": [300, 309, 2170, 11, 309, 486, 4193, 309, 281, 264, 1422, 11665, 13], "temperature": 0.0, "avg_logprob": -0.13172580034304887, "compression_ratio": 1.497175141242938, "no_speech_prob": 4.611158146872185e-05}, {"id": 224, "seek": 116244, "start": 1179.0, "end": 1186.8, "text": " So yeah, this is all passing without that.", "tokens": [407, 1338, 11, 341, 307, 439, 8437, 1553, 300, 13], "temperature": 0.0, "avg_logprob": -0.13172580034304887, "compression_ratio": 1.497175141242938, "no_speech_prob": 4.611158146872185e-05}, {"id": 225, "seek": 116244, "start": 1186.8, "end": 1190.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.13172580034304887, "compression_ratio": 1.497175141242938, "no_speech_prob": 4.611158146872185e-05}, {"id": 226, "seek": 119064, "start": 1190.64, "end": 1196.24, "text": " So I think that since it's doing nothing, I don't think we should need a decodes at", "tokens": [407, 286, 519, 300, 1670, 309, 311, 884, 1825, 11, 286, 500, 380, 519, 321, 820, 643, 257, 979, 4789, 412], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 227, "seek": 119064, "start": 1196.24, "end": 1202.2800000000002, "text": " all because by definition decodes does nothing.", "tokens": [439, 570, 538, 7123, 979, 4789, 775, 1825, 13], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 228, "seek": 119064, "start": 1202.2800000000002, "end": 1205.2800000000002, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 229, "seek": 119064, "start": 1205.2800000000002, "end": 1207.3200000000002, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 230, "seek": 119064, "start": 1207.3200000000002, "end": 1210.5600000000002, "text": " That's interesting.", "tokens": [663, 311, 1880, 13], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 231, "seek": 119064, "start": 1210.5600000000002, "end": 1215.2, "text": " There's about a 20 second delay.", "tokens": [821, 311, 466, 257, 945, 1150, 8577, 13], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 232, "seek": 119064, "start": 1215.2, "end": 1219.18, "text": " I think there's an option in YouTube video to say low latency.", "tokens": [286, 519, 456, 311, 364, 3614, 294, 3088, 960, 281, 584, 2295, 27043, 13], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 233, "seek": 119064, "start": 1219.18, "end": 1220.18, "text": " We could try that.", "tokens": [492, 727, 853, 300, 13], "temperature": 0.0, "avg_logprob": -0.3377220199768802, "compression_ratio": 1.4919786096256684, "no_speech_prob": 4.611073018168099e-05}, {"id": 234, "seek": 122018, "start": 1220.18, "end": 1222.16, "text": " I think the quality might be a little bit less good.", "tokens": [286, 519, 264, 3125, 1062, 312, 257, 707, 857, 1570, 665, 13], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 235, "seek": 122018, "start": 1222.16, "end": 1224.28, "text": " So we'll try that next time.", "tokens": [407, 321, 603, 853, 300, 958, 565, 13], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 236, "seek": 122018, "start": 1224.28, "end": 1229.76, "text": " Remind me if I forget and we'll see if the quality is still good enough.", "tokens": [4080, 471, 385, 498, 286, 2870, 293, 321, 603, 536, 498, 264, 3125, 307, 920, 665, 1547, 13], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 237, "seek": 122018, "start": 1229.76, "end": 1231.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 238, "seek": 122018, "start": 1231.44, "end": 1237.2, "text": " So normalization, like here's an example of where the transform stuff is just so, so,", "tokens": [407, 2710, 2144, 11, 411, 510, 311, 364, 1365, 295, 689, 264, 4088, 1507, 307, 445, 370, 11, 370, 11], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 239, "seek": 122018, "start": 1237.2, "end": 1239.52, "text": " so, so helpful.", "tokens": [370, 11, 370, 4961, 13], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 240, "seek": 122018, "start": 1239.52, "end": 1244.64, "text": " Because it's so annoying in fast.ai v1 and every other library having to remember to", "tokens": [1436, 309, 311, 370, 11304, 294, 2370, 13, 1301, 371, 16, 293, 633, 661, 6405, 1419, 281, 1604, 281], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 241, "seek": 122018, "start": 1244.64, "end": 1248.96, "text": " denormalize stuff when you want to display it.", "tokens": [1441, 24440, 1125, 1507, 562, 291, 528, 281, 4674, 309, 13], "temperature": 0.0, "avg_logprob": -0.23006344685512306, "compression_ratio": 1.5823293172690762, "no_speech_prob": 2.3687418888584943e-06}, {"id": 242, "seek": 124896, "start": 1248.96, "end": 1255.08, "text": " But thanks to decodable transforms, we have state containing the mean and standard deviation", "tokens": [583, 3231, 281, 979, 378, 712, 35592, 11, 321, 362, 1785, 19273, 264, 914, 293, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.15451231736403245, "compression_ratio": 1.392156862745098, "no_speech_prob": 3.187532911397284e-06}, {"id": 243, "seek": 124896, "start": 1255.08, "end": 1257.16, "text": " that we want.", "tokens": [300, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.15451231736403245, "compression_ratio": 1.392156862745098, "no_speech_prob": 3.187532911397284e-06}, {"id": 244, "seek": 124896, "start": 1257.16, "end": 1260.92, "text": " And so now we have decodes.", "tokens": [400, 370, 586, 321, 362, 979, 4789, 13], "temperature": 0.0, "avg_logprob": -0.15451231736403245, "compression_ratio": 1.392156862745098, "no_speech_prob": 3.187532911397284e-06}, {"id": 245, "seek": 124896, "start": 1260.92, "end": 1264.4, "text": " So that is super helpful, right?", "tokens": [407, 300, 307, 1687, 4961, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15451231736403245, "compression_ratio": 1.392156862745098, "no_speech_prob": 3.187532911397284e-06}, {"id": 246, "seek": 124896, "start": 1264.4, "end": 1268.3600000000001, "text": " So here's how it works.", "tokens": [407, 510, 311, 577, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.15451231736403245, "compression_ratio": 1.392156862745098, "no_speech_prob": 3.187532911397284e-06}, {"id": 247, "seek": 124896, "start": 1268.3600000000001, "end": 1273.16, "text": " Let's see, normalize.", "tokens": [961, 311, 536, 11, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.15451231736403245, "compression_ratio": 1.392156862745098, "no_speech_prob": 3.187532911397284e-06}, {"id": 248, "seek": 127316, "start": 1273.16, "end": 1279.3200000000002, "text": " So we can create some data loader transform pipeline that does CUDA, then converts to", "tokens": [407, 321, 393, 1884, 512, 1412, 3677, 260, 4088, 15517, 300, 775, 29777, 7509, 11, 550, 38874, 281], "temperature": 0.0, "avg_logprob": -0.13977916538715363, "compression_ratio": 1.5911949685534592, "no_speech_prob": 1.67971438713721e-06}, {"id": 249, "seek": 127316, "start": 1279.3200000000002, "end": 1285.8000000000002, "text": " a float, and then normalizes with some mean and standard deviation.", "tokens": [257, 15706, 11, 293, 550, 2710, 5660, 365, 512, 914, 293, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.13977916538715363, "compression_ratio": 1.5911949685534592, "no_speech_prob": 1.67971438713721e-06}, {"id": 250, "seek": 127316, "start": 1285.8000000000002, "end": 1289.6000000000001, "text": " So then we can get a transform data loader from that.", "tokens": [407, 550, 321, 393, 483, 257, 4088, 1412, 3677, 260, 490, 300, 13], "temperature": 0.0, "avg_logprob": -0.13977916538715363, "compression_ratio": 1.5911949685534592, "no_speech_prob": 1.67971438713721e-06}, {"id": 251, "seek": 127316, "start": 1289.6000000000001, "end": 1294.8000000000002, "text": " So that's our after batch transform pipeline.", "tokens": [407, 300, 311, 527, 934, 15245, 4088, 15517, 13], "temperature": 0.0, "avg_logprob": -0.13977916538715363, "compression_ratio": 1.5911949685534592, "no_speech_prob": 1.67971438713721e-06}, {"id": 252, "seek": 129480, "start": 1294.8, "end": 1305.36, "text": " And so now we can grab a batch and we can decode it.", "tokens": [400, 370, 586, 321, 393, 4444, 257, 15245, 293, 321, 393, 979, 1429, 309, 13], "temperature": 0.0, "avg_logprob": -0.18686223030090332, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.6536823750357144e-06}, {"id": 253, "seek": 129480, "start": 1305.36, "end": 1314.52, "text": " And as you can see, we end up with the right means and standard deviations for everything", "tokens": [400, 382, 291, 393, 536, 11, 321, 917, 493, 365, 264, 558, 1355, 293, 3832, 31219, 763, 337, 1203], "temperature": 0.0, "avg_logprob": -0.18686223030090332, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.6536823750357144e-06}, {"id": 254, "seek": 129480, "start": 1314.52, "end": 1315.94, "text": " and we can display it.", "tokens": [293, 321, 393, 4674, 309, 13], "temperature": 0.0, "avg_logprob": -0.18686223030090332, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.6536823750357144e-06}, {"id": 255, "seek": 129480, "start": 1315.94, "end": 1320.36, "text": " So that is super great.", "tokens": [407, 300, 307, 1687, 869, 13], "temperature": 0.0, "avg_logprob": -0.18686223030090332, "compression_ratio": 1.4210526315789473, "no_speech_prob": 1.6536823750357144e-06}, {"id": 256, "seek": 132036, "start": 1320.36, "end": 1337.28, "text": " Okay, so that's a slightly strange test.", "tokens": [1033, 11, 370, 300, 311, 257, 4748, 5861, 1500, 13], "temperature": 0.0, "avg_logprob": -0.4394092219216483, "compression_ratio": 0.8333333333333334, "no_speech_prob": 6.401541031664237e-05}, {"id": 257, "seek": 133728, "start": 1337.28, "end": 1352.24, "text": " I think we're just getting lucky there because x.mean.", "tokens": [286, 519, 321, 434, 445, 1242, 6356, 456, 570, 2031, 13, 1398, 282, 13], "temperature": 0.0, "avg_logprob": -0.3160021263256408, "compression_ratio": 1.3984962406015038, "no_speech_prob": 2.507061799406074e-05}, {"id": 258, "seek": 133728, "start": 1352.24, "end": 1353.24, "text": " I see.", "tokens": [286, 536, 13], "temperature": 0.0, "avg_logprob": -0.3160021263256408, "compression_ratio": 1.3984962406015038, "no_speech_prob": 2.507061799406074e-05}, {"id": 259, "seek": 133728, "start": 1353.24, "end": 1355.8799999999999, "text": " It's because we're not using a real mean and standard deviation.", "tokens": [467, 311, 570, 321, 434, 406, 1228, 257, 957, 914, 293, 3832, 25163, 13], "temperature": 0.0, "avg_logprob": -0.3160021263256408, "compression_ratio": 1.3984962406015038, "no_speech_prob": 2.507061799406074e-05}, {"id": 260, "seek": 133728, "start": 1355.8799999999999, "end": 1358.24, "text": " So the mean ends up being less than zero.", "tokens": [407, 264, 914, 5314, 493, 885, 1570, 813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.3160021263256408, "compression_ratio": 1.3984962406015038, "no_speech_prob": 2.507061799406074e-05}, {"id": 261, "seek": 133728, "start": 1358.24, "end": 1364.68, "text": " That makes sense.", "tokens": [663, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.3160021263256408, "compression_ratio": 1.3984962406015038, "no_speech_prob": 2.507061799406074e-05}, {"id": 262, "seek": 136468, "start": 1364.68, "end": 1372.0, "text": " BroadcastVec is just a minor little convenience function which our mean and standard deviation", "tokens": [14074, 3734, 53, 3045, 307, 445, 257, 6696, 707, 19283, 2445, 597, 527, 914, 293, 3832, 25163], "temperature": 0.0, "avg_logprob": -0.16254447400569916, "compression_ratio": 1.6432748538011697, "no_speech_prob": 8.397808414883912e-06}, {"id": 263, "seek": 136468, "start": 1372.0, "end": 1377.76, "text": " needs to be broadcast over the batch dimension correctly.", "tokens": [2203, 281, 312, 9975, 670, 264, 15245, 10139, 8944, 13], "temperature": 0.0, "avg_logprob": -0.16254447400569916, "compression_ratio": 1.6432748538011697, "no_speech_prob": 8.397808414883912e-06}, {"id": 264, "seek": 136468, "start": 1377.76, "end": 1385.96, "text": " So that's what BroadcastVec is doing is it's broadcasting to create a rank four tensor by", "tokens": [407, 300, 311, 437, 14074, 3734, 53, 3045, 307, 884, 307, 309, 311, 30024, 281, 1884, 257, 6181, 1451, 40863, 538], "temperature": 0.0, "avg_logprob": -0.16254447400569916, "compression_ratio": 1.6432748538011697, "no_speech_prob": 8.397808414883912e-06}, {"id": 265, "seek": 136468, "start": 1385.96, "end": 1393.68, "text": " broadcasting over the first dimension.", "tokens": [30024, 670, 264, 700, 10139, 13], "temperature": 0.0, "avg_logprob": -0.16254447400569916, "compression_ratio": 1.6432748538011697, "no_speech_prob": 8.397808414883912e-06}, {"id": 266, "seek": 139368, "start": 1393.68, "end": 1398.88, "text": " And so there's certainly room for us to improve normalize which we should do I guess by maybe", "tokens": [400, 370, 456, 311, 3297, 1808, 337, 505, 281, 3470, 2710, 1125, 597, 321, 820, 360, 286, 2041, 538, 1310], "temperature": 0.0, "avg_logprob": -0.1929833016744474, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.5007954011234688e-06}, {"id": 267, "seek": 139368, "start": 1398.88, "end": 1405.3600000000001, "text": " adding a setup method which automatically calculates a mean and standard deviation from", "tokens": [5127, 257, 8657, 3170, 597, 6772, 4322, 1024, 257, 914, 293, 3832, 25163, 490], "temperature": 0.0, "avg_logprob": -0.1929833016744474, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.5007954011234688e-06}, {"id": 268, "seek": 139368, "start": 1405.3600000000001, "end": 1407.68, "text": " one batch of data or something like that.", "tokens": [472, 15245, 295, 1412, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1929833016744474, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.5007954011234688e-06}, {"id": 269, "seek": 139368, "start": 1407.68, "end": 1412.2, "text": " For now, as you can see, it's all pretty manual.", "tokens": [1171, 586, 11, 382, 291, 393, 536, 11, 309, 311, 439, 1238, 9688, 13], "temperature": 0.0, "avg_logprob": -0.1929833016744474, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.5007954011234688e-06}, {"id": 270, "seek": 139368, "start": 1412.2, "end": 1417.16, "text": " Maybe we should make BroadcastVec happen automatically as well.", "tokens": [2704, 321, 820, 652, 14074, 3734, 53, 3045, 1051, 6772, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1929833016744474, "compression_ratio": 1.5272727272727273, "no_speech_prob": 3.5007954011234688e-06}, {"id": 271, "seek": 141716, "start": 1417.16, "end": 1424.5600000000002, "text": " Let's add that.", "tokens": [961, 311, 909, 300, 13], "temperature": 0.0, "avg_logprob": -0.44409381018744576, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.1380620839627227e-06}, {"id": 272, "seek": 141716, "start": 1424.5600000000002, "end": 1428.8400000000001, "text": " Normalize.setup.", "tokens": [21277, 1125, 13, 3854, 1010, 13], "temperature": 0.0, "avg_logprob": -0.44409381018744576, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.1380620839627227e-06}, {"id": 273, "seek": 141716, "start": 1428.8400000000001, "end": 1437.48, "text": " So remember setup is the thing that is going to be passed the data, the training data that", "tokens": [407, 1604, 8657, 307, 264, 551, 300, 307, 516, 281, 312, 4678, 264, 1412, 11, 264, 3097, 1412, 300], "temperature": 0.0, "avg_logprob": -0.44409381018744576, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.1380620839627227e-06}, {"id": 274, "seek": 141716, "start": 1437.48, "end": 1444.8000000000002, "text": " we're using so we can use that to automatically set things up.", "tokens": [321, 434, 1228, 370, 321, 393, 764, 300, 281, 6772, 992, 721, 493, 13], "temperature": 0.0, "avg_logprob": -0.44409381018744576, "compression_ratio": 1.441860465116279, "no_speech_prob": 3.1380620839627227e-06}, {"id": 275, "seek": 144480, "start": 1444.8, "end": 1449.36, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.25471358463681976, "compression_ratio": 1.4736842105263157, "no_speech_prob": 7.81138055572228e-07}, {"id": 276, "seek": 144480, "start": 1449.36, "end": 1453.2, "text": " So that's normalize.", "tokens": [407, 300, 311, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.25471358463681976, "compression_ratio": 1.4736842105263157, "no_speech_prob": 7.81138055572228e-07}, {"id": 277, "seek": 144480, "start": 1453.2, "end": 1464.44, "text": " So one of the things that sometimes you fast AI users complain about is data bunch as being", "tokens": [407, 472, 295, 264, 721, 300, 2171, 291, 2370, 7318, 5022, 11024, 466, 307, 1412, 3840, 382, 885], "temperature": 0.0, "avg_logprob": -0.25471358463681976, "compression_ratio": 1.4736842105263157, "no_speech_prob": 7.81138055572228e-07}, {"id": 278, "seek": 144480, "start": 1464.44, "end": 1469.12, "text": " like an abstraction that gets used quite widely in fast AI and things like a complex thing", "tokens": [411, 364, 37765, 300, 2170, 1143, 1596, 13371, 294, 2370, 7318, 293, 721, 411, 257, 3997, 551], "temperature": 0.0, "avg_logprob": -0.25471358463681976, "compression_ratio": 1.4736842105263157, "no_speech_prob": 7.81138055572228e-07}, {"id": 279, "seek": 144480, "start": 1469.12, "end": 1470.12, "text": " to understand.", "tokens": [281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.25471358463681976, "compression_ratio": 1.4736842105263157, "no_speech_prob": 7.81138055572228e-07}, {"id": 280, "seek": 147012, "start": 1470.12, "end": 1477.0, "text": " So here is the entire definition of data bunch in fast AI version 2.", "tokens": [407, 510, 307, 264, 2302, 7123, 295, 1412, 3840, 294, 2370, 7318, 3037, 568, 13], "temperature": 0.0, "avg_logprob": -0.11446253089017647, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.6797129092083196e-06}, {"id": 281, "seek": 147012, "start": 1477.0, "end": 1481.56, "text": " So hopefully we can all agree that this is not something we're going to have to spend", "tokens": [407, 4696, 321, 393, 439, 3986, 300, 341, 307, 406, 746, 321, 434, 516, 281, 362, 281, 3496], "temperature": 0.0, "avg_logprob": -0.11446253089017647, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.6797129092083196e-06}, {"id": 282, "seek": 147012, "start": 1481.56, "end": 1483.7199999999998, "text": " a lot of time getting our heads around.", "tokens": [257, 688, 295, 565, 1242, 527, 8050, 926, 13], "temperature": 0.0, "avg_logprob": -0.11446253089017647, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.6797129092083196e-06}, {"id": 283, "seek": 147012, "start": 1483.7199999999998, "end": 1491.28, "text": " A data bunch quite literally is simply something that contains a training data loader and a", "tokens": [316, 1412, 3840, 1596, 3736, 307, 2935, 746, 300, 8306, 257, 3097, 1412, 3677, 260, 293, 257], "temperature": 0.0, "avg_logprob": -0.11446253089017647, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.6797129092083196e-06}, {"id": 284, "seek": 147012, "start": 1491.28, "end": 1493.56, "text": " validation data loader.", "tokens": [24071, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.11446253089017647, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.6797129092083196e-06}, {"id": 285, "seek": 147012, "start": 1493.56, "end": 1495.36, "text": " So that's literally all it is.", "tokens": [407, 300, 311, 3736, 439, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.11446253089017647, "compression_ratio": 1.6634146341463414, "no_speech_prob": 1.6797129092083196e-06}, {"id": 286, "seek": 149536, "start": 1495.36, "end": 1505.0, "text": " And so if you don't want to use our data bunch class, you could literally just go, you know,", "tokens": [400, 370, 498, 291, 500, 380, 528, 281, 764, 527, 1412, 3840, 1508, 11, 291, 727, 3736, 445, 352, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1859148661295573, "compression_ratio": 1.5735294117647058, "no_speech_prob": 1.9333472209837055e-06}, {"id": 287, "seek": 149536, "start": 1505.0, "end": 1519.8, "text": " data bunch equals named tuple trainDL equals whatever, validDL equals whatever, and now", "tokens": [1412, 3840, 6915, 4926, 2604, 781, 3847, 35, 43, 6915, 2035, 11, 7363, 35, 43, 6915, 2035, 11, 293, 586], "temperature": 0.0, "avg_logprob": -0.1859148661295573, "compression_ratio": 1.5735294117647058, "no_speech_prob": 1.9333472209837055e-06}, {"id": 288, "seek": 149536, "start": 1519.8, "end": 1523.04, "text": " you can use that as a data bunch.", "tokens": [291, 393, 764, 300, 382, 257, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.1859148661295573, "compression_ratio": 1.5735294117647058, "no_speech_prob": 1.9333472209837055e-06}, {"id": 289, "seek": 152304, "start": 1523.04, "end": 1529.84, "text": " But there's no particular reason to because you could just go data bunch equals data bunch", "tokens": [583, 456, 311, 572, 1729, 1778, 281, 570, 291, 727, 445, 352, 1412, 3840, 6915, 1412, 3840], "temperature": 0.0, "avg_logprob": -0.13152146613460847, "compression_ratio": 1.693467336683417, "no_speech_prob": 5.682363735104445e-06}, {"id": 290, "seek": 152304, "start": 1529.84, "end": 1536.24, "text": " and pass in your training data loader and your validation data loader and again you're", "tokens": [293, 1320, 294, 428, 3097, 1412, 3677, 260, 293, 428, 24071, 1412, 3677, 260, 293, 797, 291, 434], "temperature": 0.0, "avg_logprob": -0.13152146613460847, "compression_ratio": 1.693467336683417, "no_speech_prob": 5.682363735104445e-06}, {"id": 291, "seek": 152304, "start": 1536.24, "end": 1537.24, "text": " all done.", "tokens": [439, 1096, 13], "temperature": 0.0, "avg_logprob": -0.13152146613460847, "compression_ratio": 1.693467336683417, "no_speech_prob": 5.682363735104445e-06}, {"id": 292, "seek": 152304, "start": 1537.24, "end": 1544.1599999999999, "text": " So as long as you've got some kind of object that has a trainDL and a validDL.", "tokens": [407, 382, 938, 382, 291, 600, 658, 512, 733, 295, 2657, 300, 575, 257, 3847, 35, 43, 293, 257, 7363, 35, 43, 13], "temperature": 0.0, "avg_logprob": -0.13152146613460847, "compression_ratio": 1.693467336683417, "no_speech_prob": 5.682363735104445e-06}, {"id": 293, "seek": 152304, "start": 1544.1599999999999, "end": 1548.32, "text": " But so data bunch is just something which creates that object for you.", "tokens": [583, 370, 1412, 3840, 307, 445, 746, 597, 7829, 300, 2657, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.13152146613460847, "compression_ratio": 1.693467336683417, "no_speech_prob": 5.682363735104445e-06}, {"id": 294, "seek": 154832, "start": 1548.32, "end": 1553.0, "text": " There's quite a few little tricks though to make it nice and concise and again you don't", "tokens": [821, 311, 1596, 257, 1326, 707, 11733, 1673, 281, 652, 309, 1481, 293, 44882, 293, 797, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.14715507155970523, "compression_ratio": 1.5901639344262295, "no_speech_prob": 9.368443897983525e-06}, {"id": 295, "seek": 154832, "start": 1553.0, "end": 1559.76, "text": " need to learn all these tricks if you're not interested in kind of finding ways to write", "tokens": [643, 281, 1466, 439, 613, 11733, 498, 291, 434, 406, 3102, 294, 733, 295, 5006, 2098, 281, 2464], "temperature": 0.0, "avg_logprob": -0.14715507155970523, "compression_ratio": 1.5901639344262295, "no_speech_prob": 9.368443897983525e-06}, {"id": 296, "seek": 154832, "start": 1559.76, "end": 1563.4399999999998, "text": " Python in concise ways.", "tokens": [15329, 294, 44882, 2098, 13], "temperature": 0.0, "avg_logprob": -0.14715507155970523, "compression_ratio": 1.5901639344262295, "no_speech_prob": 9.368443897983525e-06}, {"id": 297, "seek": 154832, "start": 1563.4399999999998, "end": 1566.8, "text": " But if you are interested, you might like some of these tricks.", "tokens": [583, 498, 291, 366, 3102, 11, 291, 1062, 411, 512, 295, 613, 11733, 13], "temperature": 0.0, "avg_logprob": -0.14715507155970523, "compression_ratio": 1.5901639344262295, "no_speech_prob": 9.368443897983525e-06}, {"id": 298, "seek": 154832, "start": 1566.8, "end": 1571.48, "text": " The first one is getAtra.", "tokens": [440, 700, 472, 307, 483, 18684, 424, 13], "temperature": 0.0, "avg_logprob": -0.14715507155970523, "compression_ratio": 1.5901639344262295, "no_speech_prob": 9.368443897983525e-06}, {"id": 299, "seek": 157148, "start": 1571.48, "end": 1581.92, "text": " So getAtra is basically a wrapper around thunder getAtra which is part of Python and it's a", "tokens": [407, 483, 18684, 424, 307, 1936, 257, 46906, 926, 19898, 483, 18684, 424, 597, 307, 644, 295, 15329, 293, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.15398078389687114, "compression_ratio": 1.68, "no_speech_prob": 2.2958975023357198e-06}, {"id": 300, "seek": 157148, "start": 1581.92, "end": 1583.94, "text": " really handy thing in Python.", "tokens": [534, 13239, 551, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.15398078389687114, "compression_ratio": 1.68, "no_speech_prob": 2.2958975023357198e-06}, {"id": 301, "seek": 157148, "start": 1583.94, "end": 1590.2, "text": " What it lets you do is if you define it in a class and then you call some attribute of", "tokens": [708, 309, 6653, 291, 360, 307, 498, 291, 6964, 309, 294, 257, 1508, 293, 550, 291, 818, 512, 19667, 295], "temperature": 0.0, "avg_logprob": -0.15398078389687114, "compression_ratio": 1.68, "no_speech_prob": 2.2958975023357198e-06}, {"id": 302, "seek": 157148, "start": 1590.2, "end": 1595.46, "text": " that class that doesn't exist, then it called Python will call getAtra.", "tokens": [300, 1508, 300, 1177, 380, 2514, 11, 550, 309, 1219, 15329, 486, 818, 483, 18684, 424, 13], "temperature": 0.0, "avg_logprob": -0.15398078389687114, "compression_ratio": 1.68, "no_speech_prob": 2.2958975023357198e-06}, {"id": 303, "seek": 157148, "start": 1595.46, "end": 1596.72, "text": " So you can Google it.", "tokens": [407, 291, 393, 3329, 309, 13], "temperature": 0.0, "avg_logprob": -0.15398078389687114, "compression_ratio": 1.68, "no_speech_prob": 2.2958975023357198e-06}, {"id": 304, "seek": 157148, "start": 1596.72, "end": 1600.4, "text": " There's lots of information online about that if you want to know about it.", "tokens": [821, 311, 3195, 295, 1589, 2950, 466, 300, 498, 291, 528, 281, 458, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.15398078389687114, "compression_ratio": 1.68, "no_speech_prob": 2.2958975023357198e-06}, {"id": 305, "seek": 160040, "start": 1600.4, "end": 1604.2800000000002, "text": " But defining getAtra has a couple of problems.", "tokens": [583, 17827, 483, 18684, 424, 575, 257, 1916, 295, 2740, 13], "temperature": 0.0, "avg_logprob": -0.11923636268166934, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.9333369891683105e-06}, {"id": 306, "seek": 160040, "start": 1604.2800000000002, "end": 1611.72, "text": " The first problem is that it's going to grab everything that isn't defined and that can", "tokens": [440, 700, 1154, 307, 300, 309, 311, 516, 281, 4444, 1203, 300, 1943, 380, 7642, 293, 300, 393], "temperature": 0.0, "avg_logprob": -0.11923636268166934, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.9333369891683105e-06}, {"id": 307, "seek": 160040, "start": 1611.72, "end": 1618.72, "text": " hide errors because you might be calling something that doesn't exist by mistake by some typo", "tokens": [6479, 13603, 570, 291, 1062, 312, 5141, 746, 300, 1177, 380, 2514, 538, 6146, 538, 512, 2125, 78], "temperature": 0.0, "avg_logprob": -0.11923636268166934, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.9333369891683105e-06}, {"id": 308, "seek": 160040, "start": 1618.72, "end": 1623.48, "text": " or something and you end up with some like weird exception or worse still you might end", "tokens": [420, 746, 293, 291, 917, 493, 365, 512, 411, 3657, 11183, 420, 5324, 920, 291, 1062, 917], "temperature": 0.0, "avg_logprob": -0.11923636268166934, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.9333369891683105e-06}, {"id": 309, "seek": 160040, "start": 1623.48, "end": 1629.2, "text": " up with unexpected behavior and not an exception.", "tokens": [493, 365, 13106, 5223, 293, 406, 364, 11183, 13], "temperature": 0.0, "avg_logprob": -0.11923636268166934, "compression_ratio": 1.7264150943396226, "no_speech_prob": 1.9333369891683105e-06}, {"id": 310, "seek": 162920, "start": 1629.2, "end": 1637.3600000000001, "text": " The second problem is that you don't get tab completion because the stuff that's in thunder", "tokens": [440, 1150, 1154, 307, 300, 291, 500, 380, 483, 4421, 19372, 570, 264, 1507, 300, 311, 294, 19898], "temperature": 0.0, "avg_logprob": -0.12378590232447574, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.507525202119723e-06}, {"id": 311, "seek": 162920, "start": 1637.3600000000001, "end": 1646.52, "text": " getAtra, Python doesn't know what can be handled there so it has no way to do tab completion.", "tokens": [483, 18684, 424, 11, 15329, 1177, 380, 458, 437, 393, 312, 18033, 456, 370, 309, 575, 572, 636, 281, 360, 4421, 19372, 13], "temperature": 0.0, "avg_logprob": -0.12378590232447574, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.507525202119723e-06}, {"id": 312, "seek": 162920, "start": 1646.52, "end": 1649.68, "text": " So I will show you a couple of cool things though.", "tokens": [407, 286, 486, 855, 291, 257, 1916, 295, 1627, 721, 1673, 13], "temperature": 0.0, "avg_logprob": -0.12378590232447574, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.507525202119723e-06}, {"id": 313, "seek": 162920, "start": 1649.68, "end": 1655.2, "text": " If we define use the base class getAtra, it's going to give us exactly the same behavior", "tokens": [759, 321, 6964, 764, 264, 3096, 1508, 483, 18684, 424, 11, 309, 311, 516, 281, 976, 505, 2293, 264, 912, 5223], "temperature": 0.0, "avg_logprob": -0.12378590232447574, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.507525202119723e-06}, {"id": 314, "seek": 162920, "start": 1655.2, "end": 1657.8400000000001, "text": " as thunder getAtra does in Python.", "tokens": [382, 19898, 483, 18684, 424, 775, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.12378590232447574, "compression_ratio": 1.6363636363636365, "no_speech_prob": 5.507525202119723e-06}, {"id": 315, "seek": 165784, "start": 1657.84, "end": 1664.28, "text": " And specifically it's going to look for an attribute called default in your class and", "tokens": [400, 4682, 309, 311, 516, 281, 574, 337, 364, 19667, 1219, 7576, 294, 428, 1508, 293], "temperature": 0.0, "avg_logprob": -0.2698043228743912, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.255325504549546e-06}, {"id": 316, "seek": 165784, "start": 1664.28, "end": 1670.78, "text": " anything that's not understood, it's going to instead look for that attribute in default.", "tokens": [1340, 300, 311, 406, 7320, 11, 309, 311, 516, 281, 2602, 574, 337, 300, 19667, 294, 7576, 13], "temperature": 0.0, "avg_logprob": -0.2698043228743912, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.255325504549546e-06}, {"id": 317, "seek": 165784, "start": 1670.78, "end": 1674.24, "text": " And the reason for that we want that in data bunch is it's very handy to be able to say", "tokens": [400, 264, 1778, 337, 300, 321, 528, 300, 294, 1412, 3840, 307, 309, 311, 588, 13239, 281, 312, 1075, 281, 584], "temperature": 0.0, "avg_logprob": -0.2698043228743912, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.255325504549546e-06}, {"id": 318, "seek": 165784, "start": 1674.24, "end": 1684.24, "text": " for example, so data bunch dot trainDL instance.", "tokens": [337, 1365, 11, 370, 1412, 3840, 5893, 3847, 35, 43, 5197, 13], "temperature": 0.0, "avg_logprob": -0.2698043228743912, "compression_ratio": 1.6774193548387097, "no_speech_prob": 5.255325504549546e-06}, {"id": 319, "seek": 168424, "start": 1684.24, "end": 1694.24, "text": " For example, there's a data set.", "tokens": [1171, 1365, 11, 456, 311, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.18714242585947816, "compression_ratio": 1.4745762711864407, "no_speech_prob": 1.267914853997354e-06}, {"id": 320, "seek": 168424, "start": 1694.24, "end": 1698.96, "text": " But if you just say data bunch dot data set, that would be nice to be able to assume that", "tokens": [583, 498, 291, 445, 584, 1412, 3840, 5893, 1412, 992, 11, 300, 576, 312, 1481, 281, 312, 1075, 281, 6552, 300], "temperature": 0.0, "avg_logprob": -0.18714242585947816, "compression_ratio": 1.4745762711864407, "no_speech_prob": 1.267914853997354e-06}, {"id": 321, "seek": 168424, "start": 1698.96, "end": 1704.56, "text": " I'm talking about the training set by default.", "tokens": [286, 478, 1417, 466, 264, 3097, 992, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.18714242585947816, "compression_ratio": 1.4745762711864407, "no_speech_prob": 1.267914853997354e-06}, {"id": 322, "seek": 168424, "start": 1704.56, "end": 1711.18, "text": " And so this is where getAtra is super handy rather than having to define, you know, another", "tokens": [400, 370, 341, 307, 689, 483, 18684, 424, 307, 1687, 13239, 2831, 813, 1419, 281, 6964, 11, 291, 458, 11, 1071], "temperature": 0.0, "avg_logprob": -0.18714242585947816, "compression_ratio": 1.4745762711864407, "no_speech_prob": 1.267914853997354e-06}, {"id": 323, "seek": 171118, "start": 1711.18, "end": 1715.3600000000001, "text": " example would be one batch.", "tokens": [1365, 576, 312, 472, 15245, 13], "temperature": 0.0, "avg_logprob": -0.20658764309353297, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.425411589181749e-06}, {"id": 324, "seek": 171118, "start": 1715.3600000000001, "end": 1722.3600000000001, "text": " You know, this is the same as trainDL dot one batch.", "tokens": [509, 458, 11, 341, 307, 264, 912, 382, 3847, 35, 43, 5893, 472, 15245, 13], "temperature": 0.0, "avg_logprob": -0.20658764309353297, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.425411589181749e-06}, {"id": 325, "seek": 171118, "start": 1722.3600000000001, "end": 1732.04, "text": " Yes, Petro, that would be very helpful to add those tasks as GitHub issues.", "tokens": [1079, 11, 10472, 340, 11, 300, 576, 312, 588, 4961, 281, 909, 729, 9608, 382, 23331, 2663, 13], "temperature": 0.0, "avg_logprob": -0.20658764309353297, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.425411589181749e-06}, {"id": 326, "seek": 171118, "start": 1732.04, "end": 1734.4, "text": " Although in this case, they kind of say consider blah.", "tokens": [5780, 294, 341, 1389, 11, 436, 733, 295, 584, 1949, 12288, 13], "temperature": 0.0, "avg_logprob": -0.20658764309353297, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.425411589181749e-06}, {"id": 327, "seek": 171118, "start": 1734.4, "end": 1738.0, "text": " So the issue should say consider blah rather than do it because I'm not quite sure yet", "tokens": [407, 264, 2734, 820, 584, 1949, 12288, 2831, 813, 360, 309, 570, 286, 478, 406, 1596, 988, 1939], "temperature": 0.0, "avg_logprob": -0.20658764309353297, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.425411589181749e-06}, {"id": 328, "seek": 171118, "start": 1738.0, "end": 1740.92, "text": " if it's a good idea.", "tokens": [498, 309, 311, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.20658764309353297, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.425411589181749e-06}, {"id": 329, "seek": 174092, "start": 1740.92, "end": 1743.8400000000001, "text": " The first one is certainly something to fix.", "tokens": [440, 700, 472, 307, 3297, 746, 281, 3191, 13], "temperature": 0.0, "avg_logprob": -0.22542616299220494, "compression_ratio": 1.5992063492063493, "no_speech_prob": 6.747765382897342e-06}, {"id": 330, "seek": 174092, "start": 1743.8400000000001, "end": 1748.92, "text": " So thanks for that suggestion.", "tokens": [407, 3231, 337, 300, 16541, 13], "temperature": 0.0, "avg_logprob": -0.22542616299220494, "compression_ratio": 1.5992063492063493, "no_speech_prob": 6.747765382897342e-06}, {"id": 331, "seek": 174092, "start": 1748.92, "end": 1751.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.22542616299220494, "compression_ratio": 1.5992063492063493, "no_speech_prob": 6.747765382897342e-06}, {"id": 332, "seek": 174092, "start": 1751.44, "end": 1756.28, "text": " So yeah, we'd love to be able to, you know, not have to write all those different versions.", "tokens": [407, 1338, 11, 321, 1116, 959, 281, 312, 1075, 281, 11, 291, 458, 11, 406, 362, 281, 2464, 439, 729, 819, 9606, 13], "temperature": 0.0, "avg_logprob": -0.22542616299220494, "compression_ratio": 1.5992063492063493, "no_speech_prob": 6.747765382897342e-06}, {"id": 333, "seek": 174092, "start": 1756.28, "end": 1759.64, "text": " So done to getAtra is super handy for that.", "tokens": [407, 1096, 281, 483, 18684, 424, 307, 1687, 13239, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.22542616299220494, "compression_ratio": 1.5992063492063493, "no_speech_prob": 6.747765382897342e-06}, {"id": 334, "seek": 174092, "start": 1759.64, "end": 1765.72, "text": " But as I said, you know, you have these problems of things like if you pass through some typo,", "tokens": [583, 382, 286, 848, 11, 291, 458, 11, 291, 362, 613, 2740, 295, 721, 411, 498, 291, 1320, 807, 512, 2125, 78, 11], "temperature": 0.0, "avg_logprob": -0.22542616299220494, "compression_ratio": 1.5992063492063493, "no_speech_prob": 6.747765382897342e-06}, {"id": 335, "seek": 174092, "start": 1765.72, "end": 1770.8000000000002, "text": " so I've accidentally said not one batch but on batch, I'd get some weird error or it might", "tokens": [370, 286, 600, 15715, 848, 406, 472, 15245, 457, 322, 15245, 11, 286, 1116, 483, 512, 3657, 6713, 420, 309, 1062], "temperature": 0.0, "avg_logprob": -0.22542616299220494, "compression_ratio": 1.5992063492063493, "no_speech_prob": 6.747765382897342e-06}, {"id": 336, "seek": 177080, "start": 1770.8, "end": 1772.96, "text": " even give me the wrong behavior.", "tokens": [754, 976, 385, 264, 2085, 5223, 13], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 337, "seek": 177080, "start": 1772.96, "end": 1775.04, "text": " GetAtra subclass fixes all that.", "tokens": [3240, 18684, 424, 1422, 11665, 32539, 439, 300, 13], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 338, "seek": 177080, "start": 1775.04, "end": 1780.32, "text": " So for example, this correctly gives me an attribute error on batch.", "tokens": [407, 337, 1365, 11, 341, 8944, 2709, 385, 364, 19667, 6713, 322, 15245, 13], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 339, "seek": 177080, "start": 1780.32, "end": 1784.48, "text": " So it tells me clearly that this is an attribute that doesn't exist.", "tokens": [407, 309, 5112, 385, 4448, 300, 341, 307, 364, 19667, 300, 1177, 380, 2514, 13], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 340, "seek": 177080, "start": 1784.48, "end": 1785.84, "text": " And here's the other cool thing.", "tokens": [400, 510, 311, 264, 661, 1627, 551, 13], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 341, "seek": 177080, "start": 1785.84, "end": 1790.72, "text": " If I press tab, I get tab completion.", "tokens": [759, 286, 1886, 4421, 11, 286, 483, 4421, 19372, 13], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 342, "seek": 177080, "start": 1790.72, "end": 1794.12, "text": " Or press tab, I can see all the possible things.", "tokens": [1610, 1886, 4421, 11, 286, 393, 536, 439, 264, 1944, 721, 13], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 343, "seek": 177080, "start": 1794.12, "end": 1799.96, "text": " So getAtra fixes the subclass, fixes all of the things I mentioned that I'm aware of as", "tokens": [407, 483, 18684, 424, 32539, 264, 1422, 11665, 11, 32539, 439, 295, 264, 721, 286, 2835, 300, 286, 478, 3650, 295, 382], "temperature": 0.0, "avg_logprob": -0.17429841083029043, "compression_ratio": 1.7196652719665273, "no_speech_prob": 4.637787242245395e-06}, {"id": 344, "seek": 179996, "start": 1799.96, "end": 1803.08, "text": " problems in done to getAtra in Python.", "tokens": [2740, 294, 1096, 281, 483, 18684, 424, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.1510944601930218, "compression_ratio": 1.5132275132275133, "no_speech_prob": 4.0928407543106005e-06}, {"id": 345, "seek": 179996, "start": 1803.08, "end": 1806.56, "text": " So like this is an example of where we're trying to take the stuff that's already in", "tokens": [407, 411, 341, 307, 364, 1365, 295, 689, 321, 434, 1382, 281, 747, 264, 1507, 300, 311, 1217, 294], "temperature": 0.0, "avg_logprob": -0.1510944601930218, "compression_ratio": 1.5132275132275133, "no_speech_prob": 4.0928407543106005e-06}, {"id": 346, "seek": 179996, "start": 1806.56, "end": 1810.64, "text": " Python and make it better.", "tokens": [15329, 293, 652, 309, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1510944601930218, "compression_ratio": 1.5132275132275133, "no_speech_prob": 4.0928407543106005e-06}, {"id": 347, "seek": 179996, "start": 1810.64, "end": 1818.8400000000001, "text": " You know, more self-documenting, harder to make mistakes, stuff like that.", "tokens": [509, 458, 11, 544, 2698, 12, 67, 30439, 278, 11, 6081, 281, 652, 8038, 11, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1510944601930218, "compression_ratio": 1.5132275132275133, "no_speech_prob": 4.0928407543106005e-06}, {"id": 348, "seek": 179996, "start": 1818.8400000000001, "end": 1826.4, "text": " So the way that this works is you both inherit from getAtra.", "tokens": [407, 264, 636, 300, 341, 1985, 307, 291, 1293, 21389, 490, 483, 18684, 424, 13], "temperature": 0.0, "avg_logprob": -0.1510944601930218, "compression_ratio": 1.5132275132275133, "no_speech_prob": 4.0928407543106005e-06}, {"id": 349, "seek": 182640, "start": 1826.4, "end": 1832.3200000000002, "text": " You have to add some attribute called default.", "tokens": [509, 362, 281, 909, 512, 19667, 1219, 7576, 13], "temperature": 0.0, "avg_logprob": -0.10034354728988454, "compression_ratio": 1.6963350785340314, "no_speech_prob": 1.0129885595233645e-05}, {"id": 350, "seek": 182640, "start": 1832.3200000000002, "end": 1837.88, "text": " And that's what any unknown attributes will be passed down to.", "tokens": [400, 300, 311, 437, 604, 9841, 17212, 486, 312, 4678, 760, 281, 13], "temperature": 0.0, "avg_logprob": -0.10034354728988454, "compression_ratio": 1.6963350785340314, "no_speech_prob": 1.0129885595233645e-05}, {"id": 351, "seek": 182640, "start": 1837.88, "end": 1843.6000000000001, "text": " And then optionally, you can define a special attribute called underscore extra that contains", "tokens": [400, 550, 3614, 379, 11, 291, 393, 6964, 257, 2121, 19667, 1219, 37556, 2857, 300, 8306], "temperature": 0.0, "avg_logprob": -0.10034354728988454, "compression_ratio": 1.6963350785340314, "no_speech_prob": 1.0129885595233645e-05}, {"id": 352, "seek": 182640, "start": 1843.6000000000001, "end": 1845.94, "text": " a listed strings.", "tokens": [257, 10052, 13985, 13], "temperature": 0.0, "avg_logprob": -0.10034354728988454, "compression_ratio": 1.6963350785340314, "no_speech_prob": 1.0129885595233645e-05}, {"id": 353, "seek": 182640, "start": 1845.94, "end": 1850.96, "text": " And that will be only those things will be delegated to.", "tokens": [400, 300, 486, 312, 787, 729, 721, 486, 312, 15824, 770, 281, 13], "temperature": 0.0, "avg_logprob": -0.10034354728988454, "compression_ratio": 1.6963350785340314, "no_speech_prob": 1.0129885595233645e-05}, {"id": 354, "seek": 182640, "start": 1850.96, "end": 1855.18, "text": " So it won't delegate to every possible thing.", "tokens": [407, 309, 1582, 380, 40999, 281, 633, 1944, 551, 13], "temperature": 0.0, "avg_logprob": -0.10034354728988454, "compression_ratio": 1.6963350785340314, "no_speech_prob": 1.0129885595233645e-05}, {"id": 355, "seek": 185518, "start": 1855.18, "end": 1857.3200000000002, "text": " You don't have to define underscore extra.", "tokens": [509, 500, 380, 362, 281, 6964, 37556, 2857, 13], "temperature": 0.0, "avg_logprob": -0.20257375481423368, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.3931237339857034e-06}, {"id": 356, "seek": 185518, "start": 1857.3200000000002, "end": 1858.3200000000002, "text": " So I'll show you.", "tokens": [407, 286, 603, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.20257375481423368, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.3931237339857034e-06}, {"id": 357, "seek": 185518, "start": 1858.3200000000002, "end": 1864.2, "text": " If I comment it out, right, and then I go data bunch.", "tokens": [759, 286, 2871, 309, 484, 11, 558, 11, 293, 550, 286, 352, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.20257375481423368, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.3931237339857034e-06}, {"id": 358, "seek": 185518, "start": 1864.2, "end": 1868.52, "text": " And I press tab.", "tokens": [400, 286, 1886, 4421, 13], "temperature": 0.0, "avg_logprob": -0.20257375481423368, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.3931237339857034e-06}, {"id": 359, "seek": 185518, "start": 1868.52, "end": 1870.68, "text": " You'll see I've got more things here now.", "tokens": [509, 603, 536, 286, 600, 658, 544, 721, 510, 586, 13], "temperature": 0.0, "avg_logprob": -0.20257375481423368, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.3931237339857034e-06}, {"id": 360, "seek": 185518, "start": 1870.68, "end": 1877.98, "text": " And so what it does by default is that underscore extra by default will actually dynamically", "tokens": [400, 370, 437, 309, 775, 538, 7576, 307, 300, 37556, 2857, 538, 7576, 486, 767, 43492], "temperature": 0.0, "avg_logprob": -0.20257375481423368, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.3931237339857034e-06}, {"id": 361, "seek": 185518, "start": 1877.98, "end": 1882.54, "text": " find all of the attributes inside self.default.", "tokens": [915, 439, 295, 264, 17212, 1854, 2698, 13, 20595, 5107, 13], "temperature": 0.0, "avg_logprob": -0.20257375481423368, "compression_ratio": 1.5544554455445545, "no_speech_prob": 3.3931237339857034e-06}, {"id": 362, "seek": 188254, "start": 1882.54, "end": 1887.24, "text": " If underscore extra is not defined, it will include all of the ones that don't start with", "tokens": [759, 37556, 2857, 307, 406, 7642, 11, 309, 486, 4090, 439, 295, 264, 2306, 300, 500, 380, 722, 365], "temperature": 0.0, "avg_logprob": -0.17117946705919632, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.905448920704657e-06}, {"id": 363, "seek": 188254, "start": 1887.24, "end": 1888.24, "text": " an underscore.", "tokens": [364, 37556, 13], "temperature": 0.0, "avg_logprob": -0.17117946705919632, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.905448920704657e-06}, {"id": 364, "seek": 188254, "start": 1888.24, "end": 1891.6399999999999, "text": " And they're all going to be included in your tab completion list.", "tokens": [400, 436, 434, 439, 516, 281, 312, 5556, 294, 428, 4421, 19372, 1329, 13], "temperature": 0.0, "avg_logprob": -0.17117946705919632, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.905448920704657e-06}, {"id": 365, "seek": 188254, "start": 1891.6399999999999, "end": 1895.8799999999999, "text": " In this case, we didn't really want to include everything.", "tokens": [682, 341, 1389, 11, 321, 994, 380, 534, 528, 281, 4090, 1203, 13], "temperature": 0.0, "avg_logprob": -0.17117946705919632, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.905448920704657e-06}, {"id": 366, "seek": 188254, "start": 1895.8799999999999, "end": 1903.72, "text": " So we kind of try to keep things manageable by saying this is the subset of stuff we expect.", "tokens": [407, 321, 733, 295, 853, 281, 1066, 721, 38798, 538, 1566, 341, 307, 264, 25993, 295, 1507, 321, 2066, 13], "temperature": 0.0, "avg_logprob": -0.17117946705919632, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.905448920704657e-06}, {"id": 367, "seek": 188254, "start": 1903.72, "end": 1905.2, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.17117946705919632, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.905448920704657e-06}, {"id": 368, "seek": 188254, "start": 1905.2, "end": 1908.08, "text": " So that's one nice thing.", "tokens": [407, 300, 311, 472, 1481, 551, 13], "temperature": 0.0, "avg_logprob": -0.17117946705919632, "compression_ratio": 1.5784753363228698, "no_speech_prob": 3.905448920704657e-06}, {"id": 369, "seek": 190808, "start": 1908.08, "end": 1914.76, "text": " You'll see we're defining at docs, which allows us to add documentation to everything.", "tokens": [509, 603, 536, 321, 434, 17827, 412, 45623, 11, 597, 4045, 505, 281, 909, 14333, 281, 1203, 13], "temperature": 0.0, "avg_logprob": -0.19430330716646635, "compression_ratio": 1.4157303370786516, "no_speech_prob": 2.6015873118012678e-06}, {"id": 370, "seek": 190808, "start": 1914.76, "end": 1919.28, "text": " Ethan's asked a question about Swift.", "tokens": [23984, 311, 2351, 257, 1168, 466, 25539, 13], "temperature": 0.0, "avg_logprob": -0.19430330716646635, "compression_ratio": 1.4157303370786516, "no_speech_prob": 2.6015873118012678e-06}, {"id": 371, "seek": 190808, "start": 1919.28, "end": 1928.1599999999999, "text": " It would be best to probably ask that on the Swift forum.", "tokens": [467, 576, 312, 1151, 281, 1391, 1029, 300, 322, 264, 25539, 17542, 13], "temperature": 0.0, "avg_logprob": -0.19430330716646635, "compression_ratio": 1.4157303370786516, "no_speech_prob": 2.6015873118012678e-06}, {"id": 372, "seek": 190808, "start": 1928.1599999999999, "end": 1933.8799999999999, "text": " So Kevin's asking about tab completion on chain commands.", "tokens": [407, 9954, 311, 3365, 466, 4421, 19372, 322, 5021, 16901, 13], "temperature": 0.0, "avg_logprob": -0.19430330716646635, "compression_ratio": 1.4157303370786516, "no_speech_prob": 2.6015873118012678e-06}, {"id": 373, "seek": 190808, "start": 1933.8799999999999, "end": 1937.24, "text": " Not really.", "tokens": [1726, 534, 13], "temperature": 0.0, "avg_logprob": -0.19430330716646635, "compression_ratio": 1.4157303370786516, "no_speech_prob": 2.6015873118012678e-06}, {"id": 374, "seek": 193724, "start": 1937.24, "end": 1944.84, "text": " This is the issue with Jupyter, basically, which is really an issue with Python.", "tokens": [639, 307, 264, 2734, 365, 22125, 88, 391, 11, 1936, 11, 597, 307, 534, 364, 2734, 365, 15329, 13], "temperature": 0.0, "avg_logprob": -0.13172373833594384, "compression_ratio": 1.6576086956521738, "no_speech_prob": 2.6841544240596704e-06}, {"id": 375, "seek": 193724, "start": 1944.84, "end": 1950.16, "text": " It can't really do tab completion on something that calls a function because it would need", "tokens": [467, 393, 380, 534, 360, 4421, 19372, 322, 746, 300, 5498, 257, 2445, 570, 309, 576, 643], "temperature": 0.0, "avg_logprob": -0.13172373833594384, "compression_ratio": 1.6576086956521738, "no_speech_prob": 2.6841544240596704e-06}, {"id": 376, "seek": 193724, "start": 1950.16, "end": 1953.7, "text": " to call the function to know how to tab complete it.", "tokens": [281, 818, 264, 2445, 281, 458, 577, 281, 4421, 3566, 309, 13], "temperature": 0.0, "avg_logprob": -0.13172373833594384, "compression_ratio": 1.6576086956521738, "no_speech_prob": 2.6841544240596704e-06}, {"id": 377, "seek": 193724, "start": 1953.7, "end": 1958.4, "text": " So in Jupyter, you just have to create a separate line that calls each function.", "tokens": [407, 294, 22125, 88, 391, 11, 291, 445, 362, 281, 1884, 257, 4994, 1622, 300, 5498, 1184, 2445, 13], "temperature": 0.0, "avg_logprob": -0.13172373833594384, "compression_ratio": 1.6576086956521738, "no_speech_prob": 2.6841544240596704e-06}, {"id": 378, "seek": 195840, "start": 1958.4, "end": 1967.44, "text": " But we're doing a lot less chain commands in FastAI version 2, partly for that reason", "tokens": [583, 321, 434, 884, 257, 688, 1570, 5021, 16901, 294, 15968, 48698, 3037, 568, 11, 17031, 337, 300, 1778], "temperature": 0.0, "avg_logprob": -0.16912628474988436, "compression_ratio": 1.4845360824742269, "no_speech_prob": 7.934424957056763e-07}, {"id": 379, "seek": 195840, "start": 1967.44, "end": 1971.1000000000001, "text": " and partly because of some of the ideas that came out of the Swift work.", "tokens": [293, 17031, 570, 295, 512, 295, 264, 3487, 300, 1361, 484, 295, 264, 25539, 589, 13], "temperature": 0.0, "avg_logprob": -0.16912628474988436, "compression_ratio": 1.4845360824742269, "no_speech_prob": 7.934424957056763e-07}, {"id": 380, "seek": 195840, "start": 1971.1000000000001, "end": 1974.8000000000002, "text": " So you'll find it's less of a problem.", "tokens": [407, 291, 603, 915, 309, 311, 1570, 295, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.16912628474988436, "compression_ratio": 1.4845360824742269, "no_speech_prob": 7.934424957056763e-07}, {"id": 381, "seek": 195840, "start": 1974.8000000000002, "end": 1983.6000000000001, "text": " But for things that are properties like this, the tab completion does chain correctly.", "tokens": [583, 337, 721, 300, 366, 7221, 411, 341, 11, 264, 4421, 19372, 775, 5021, 8944, 13], "temperature": 0.0, "avg_logprob": -0.16912628474988436, "compression_ratio": 1.4845360824742269, "no_speech_prob": 7.934424957056763e-07}, {"id": 382, "seek": 195840, "start": 1983.6000000000001, "end": 1986.0400000000002, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.16912628474988436, "compression_ratio": 1.4845360824742269, "no_speech_prob": 7.934424957056763e-07}, {"id": 383, "seek": 198604, "start": 1986.04, "end": 1989.24, "text": " So that's what underscore extra is.", "tokens": [407, 300, 311, 437, 37556, 2857, 307, 13], "temperature": 0.0, "avg_logprob": -0.1680162924307364, "compression_ratio": 1.6736401673640167, "no_speech_prob": 1.5205609997792635e-05}, {"id": 384, "seek": 198604, "start": 1989.24, "end": 1991.82, "text": " So now you understand what the DUNDA init is.", "tokens": [407, 586, 291, 1223, 437, 264, 413, 3979, 7509, 3157, 307, 13], "temperature": 0.0, "avg_logprob": -0.1680162924307364, "compression_ratio": 1.6736401673640167, "no_speech_prob": 1.5205609997792635e-05}, {"id": 385, "seek": 198604, "start": 1991.82, "end": 1996.3799999999999, "text": " So important to recognize that a data bunch is super flexible because you can pass in", "tokens": [407, 1021, 281, 5521, 300, 257, 1412, 3840, 307, 1687, 11358, 570, 291, 393, 1320, 294], "temperature": 0.0, "avg_logprob": -0.1680162924307364, "compression_ratio": 1.6736401673640167, "no_speech_prob": 1.5205609997792635e-05}, {"id": 386, "seek": 198604, "start": 1996.3799999999999, "end": 1998.24, "text": " as many data loaders as you like.", "tokens": [382, 867, 1412, 3677, 433, 382, 291, 411, 13], "temperature": 0.0, "avg_logprob": -0.1680162924307364, "compression_ratio": 1.6736401673640167, "no_speech_prob": 1.5205609997792635e-05}, {"id": 387, "seek": 198604, "start": 1998.24, "end": 2002.68, "text": " So you can have not just a training and validation set, but a test set, multiple validation sets,", "tokens": [407, 291, 393, 362, 406, 445, 257, 3097, 293, 24071, 992, 11, 457, 257, 1500, 992, 11, 3866, 24071, 6352, 11], "temperature": 0.0, "avg_logprob": -0.1680162924307364, "compression_ratio": 1.6736401673640167, "no_speech_prob": 1.5205609997792635e-05}, {"id": 388, "seek": 198604, "start": 2002.68, "end": 2004.04, "text": " and so forth.", "tokens": [293, 370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.1680162924307364, "compression_ratio": 1.6736401673640167, "no_speech_prob": 1.5205609997792635e-05}, {"id": 389, "seek": 198604, "start": 2004.04, "end": 2012.1599999999999, "text": " So then we define DUNDA get item so that you can, as you see, index directly into your", "tokens": [407, 550, 321, 6964, 413, 3979, 7509, 483, 3174, 370, 300, 291, 393, 11, 382, 291, 536, 11, 8186, 3838, 666, 428], "temperature": 0.0, "avg_logprob": -0.1680162924307364, "compression_ratio": 1.6736401673640167, "no_speech_prob": 1.5205609997792635e-05}, {"id": 390, "seek": 201216, "start": 2012.16, "end": 2016.28, "text": " data bunch to get the nth data loader out of it.", "tokens": [1412, 3840, 281, 483, 264, 297, 392, 1412, 3677, 260, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13873638186538428, "compression_ratio": 1.3503649635036497, "no_speech_prob": 4.029421234008623e-06}, {"id": 391, "seek": 201216, "start": 2016.28, "end": 2021.3600000000001, "text": " So as you can see, it's just returning soft.dls.", "tokens": [407, 382, 291, 393, 536, 11, 309, 311, 445, 12678, 2787, 13, 67, 11784, 13], "temperature": 0.0, "avg_logprob": -0.13873638186538428, "compression_ratio": 1.3503649635036497, "no_speech_prob": 4.029421234008623e-06}, {"id": 392, "seek": 201216, "start": 2021.3600000000001, "end": 2026.64, "text": " And so now that we've defined get item, we basically want to be able to say at property", "tokens": [400, 370, 586, 300, 321, 600, 7642, 483, 3174, 11, 321, 1936, 528, 281, 312, 1075, 281, 584, 412, 4707], "temperature": 0.0, "avg_logprob": -0.13873638186538428, "compression_ratio": 1.3503649635036497, "no_speech_prob": 4.029421234008623e-06}, {"id": 393, "seek": 202664, "start": 2026.64, "end": 2043.2800000000002, "text": " def train dl self, self return self 0.", "tokens": [1060, 3847, 37873, 2698, 11, 2698, 2736, 2698, 1958, 13], "temperature": 0.0, "avg_logprob": -0.21405379836623734, "compression_ratio": 1.2470588235294118, "no_speech_prob": 8.267772500403225e-06}, {"id": 394, "seek": 202664, "start": 2043.2800000000002, "end": 2048.0, "text": " And we want it to be able to do valid dl.", "tokens": [400, 321, 528, 309, 281, 312, 1075, 281, 360, 7363, 37873, 13], "temperature": 0.0, "avg_logprob": -0.21405379836623734, "compression_ratio": 1.2470588235294118, "no_speech_prob": 8.267772500403225e-06}, {"id": 395, "seek": 202664, "start": 2048.0, "end": 2050.6, "text": " And that would be self 1.", "tokens": [400, 300, 576, 312, 2698, 502, 13], "temperature": 0.0, "avg_logprob": -0.21405379836623734, "compression_ratio": 1.2470588235294118, "no_speech_prob": 8.267772500403225e-06}, {"id": 396, "seek": 205060, "start": 2050.6, "end": 2056.7999999999997, "text": " And so that's a lot of lines of code for not doing very much stuff.", "tokens": [400, 370, 300, 311, 257, 688, 295, 3876, 295, 3089, 337, 406, 884, 588, 709, 1507, 13], "temperature": 0.0, "avg_logprob": -0.08605588032649114, "compression_ratio": 1.4233128834355828, "no_speech_prob": 4.5659248826268595e-06}, {"id": 397, "seek": 205060, "start": 2056.7999999999997, "end": 2062.16, "text": " So in FastAI, there's a thing called add props, which stands for add properties.", "tokens": [407, 294, 15968, 48698, 11, 456, 311, 257, 551, 1219, 909, 26173, 11, 597, 7382, 337, 909, 7221, 13], "temperature": 0.0, "avg_logprob": -0.08605588032649114, "compression_ratio": 1.4233128834355828, "no_speech_prob": 4.5659248826268595e-06}, {"id": 398, "seek": 205060, "start": 2062.16, "end": 2072.88, "text": " And that is going to go through numbers from 0 to n minus 1, which by default is 2.", "tokens": [400, 300, 307, 516, 281, 352, 807, 3547, 490, 1958, 281, 297, 3175, 502, 11, 597, 538, 7576, 307, 568, 13], "temperature": 0.0, "avg_logprob": -0.08605588032649114, "compression_ratio": 1.4233128834355828, "no_speech_prob": 4.5659248826268595e-06}, {"id": 399, "seek": 207288, "start": 2072.88, "end": 2081.6, "text": " It will create a new function with this definition, and it will make it into a property.", "tokens": [467, 486, 1884, 257, 777, 2445, 365, 341, 7123, 11, 293, 309, 486, 652, 309, 666, 257, 4707, 13], "temperature": 0.0, "avg_logprob": -0.12948154916568677, "compression_ratio": 1.7203791469194314, "no_speech_prob": 4.8603997129248455e-06}, {"id": 400, "seek": 207288, "start": 2081.6, "end": 2085.42, "text": " So here we're creating those two properties in one go, and the properties are called train", "tokens": [407, 510, 321, 434, 4084, 729, 732, 7221, 294, 472, 352, 11, 293, 264, 7221, 366, 1219, 3847], "temperature": 0.0, "avg_logprob": -0.12948154916568677, "compression_ratio": 1.7203791469194314, "no_speech_prob": 4.8603997129248455e-06}, {"id": 401, "seek": 207288, "start": 2085.42, "end": 2090.76, "text": " dl and valid dl, and they respectively are going to return x0 and x1.", "tokens": [37873, 293, 7363, 37873, 11, 293, 436, 25009, 366, 516, 281, 2736, 2031, 15, 293, 2031, 16, 13], "temperature": 0.0, "avg_logprob": -0.12948154916568677, "compression_ratio": 1.7203791469194314, "no_speech_prob": 4.8603997129248455e-06}, {"id": 402, "seek": 207288, "start": 2090.76, "end": 2095.06, "text": " So they'll do exactly the same as this.", "tokens": [407, 436, 603, 360, 2293, 264, 912, 382, 341, 13], "temperature": 0.0, "avg_logprob": -0.12948154916568677, "compression_ratio": 1.7203791469194314, "no_speech_prob": 4.8603997129248455e-06}, {"id": 403, "seek": 207288, "start": 2095.06, "end": 2101.56, "text": " And so here's the same thing for getting x0 and x1.dataset as properties.", "tokens": [400, 370, 510, 311, 264, 912, 551, 337, 1242, 2031, 15, 293, 2031, 16, 13, 20367, 296, 302, 382, 7221, 13], "temperature": 0.0, "avg_logprob": -0.12948154916568677, "compression_ratio": 1.7203791469194314, "no_speech_prob": 4.8603997129248455e-06}, {"id": 404, "seek": 210156, "start": 2101.56, "end": 2106.88, "text": " So again, these are shortcuts which come up quite a lot in FastAI code, because a lot", "tokens": [407, 797, 11, 613, 366, 34620, 597, 808, 493, 1596, 257, 688, 294, 15968, 48698, 3089, 11, 570, 257, 688], "temperature": 0.0, "avg_logprob": -0.1868131620074631, "compression_ratio": 1.638655462184874, "no_speech_prob": 5.682344180968357e-06}, {"id": 405, "seek": 210156, "start": 2106.88, "end": 2111.7999999999997, "text": " often we want the train version of something and a validation version of something.", "tokens": [2049, 321, 528, 264, 3847, 3037, 295, 746, 293, 257, 24071, 3037, 295, 746, 13], "temperature": 0.0, "avg_logprob": -0.1868131620074631, "compression_ratio": 1.638655462184874, "no_speech_prob": 5.682344180968357e-06}, {"id": 406, "seek": 210156, "start": 2111.7999999999997, "end": 2113.22, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1868131620074631, "compression_ratio": 1.638655462184874, "no_speech_prob": 5.682344180968357e-06}, {"id": 407, "seek": 210156, "start": 2113.22, "end": 2119.96, "text": " So we end up with a super little concise bit of code, and that's our data bunch.", "tokens": [407, 321, 917, 493, 365, 257, 1687, 707, 44882, 857, 295, 3089, 11, 293, 300, 311, 527, 1412, 3840, 13], "temperature": 0.0, "avg_logprob": -0.1868131620074631, "compression_ratio": 1.638655462184874, "no_speech_prob": 5.682344180968357e-06}, {"id": 408, "seek": 210156, "start": 2119.96, "end": 2120.96, "text": " We can grab one batch.", "tokens": [492, 393, 4444, 472, 15245, 13], "temperature": 0.0, "avg_logprob": -0.1868131620074631, "compression_ratio": 1.638655462184874, "no_speech_prob": 5.682344180968357e-06}, {"id": 409, "seek": 210156, "start": 2120.96, "end": 2127.88, "text": " In fact, for train dl, we don't need to say train dl because we have gedactra, which you", "tokens": [682, 1186, 11, 337, 3847, 37873, 11, 321, 500, 380, 643, 281, 584, 3847, 37873, 570, 321, 362, 19238, 578, 424, 11, 597, 291], "temperature": 0.0, "avg_logprob": -0.1868131620074631, "compression_ratio": 1.638655462184874, "no_speech_prob": 5.682344180968357e-06}, {"id": 410, "seek": 210156, "start": 2127.88, "end": 2130.84, "text": " can see is tested here.", "tokens": [393, 536, 307, 8246, 510, 13], "temperature": 0.0, "avg_logprob": -0.1868131620074631, "compression_ratio": 1.638655462184874, "no_speech_prob": 5.682344180968357e-06}, {"id": 411, "seek": 213084, "start": 2130.84, "end": 2138.2000000000003, "text": " And one batch is simply calling, as we saw last time, next.idr.tdl.", "tokens": [400, 472, 15245, 307, 2935, 5141, 11, 382, 321, 1866, 1036, 565, 11, 958, 13, 327, 81, 13, 83, 67, 75, 13], "temperature": 0.0, "avg_logprob": -0.33492547587344523, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.2805181540898047e-05}, {"id": 412, "seek": 213084, "start": 2138.2000000000003, "end": 2141.84, "text": " So we're just making sure that they're all the same, which they are.", "tokens": [407, 321, 434, 445, 1455, 988, 300, 436, 434, 439, 264, 912, 11, 597, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.33492547587344523, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.2805181540898047e-05}, {"id": 413, "seek": 213084, "start": 2141.84, "end": 2147.36, "text": " And then there's the test of getItem.", "tokens": [400, 550, 456, 311, 264, 1500, 295, 483, 3522, 443, 13], "temperature": 0.0, "avg_logprob": -0.33492547587344523, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.2805181540898047e-05}, {"id": 414, "seek": 213084, "start": 2147.36, "end": 2153.8, "text": " And that should be up there.", "tokens": [400, 300, 820, 312, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.33492547587344523, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.2805181540898047e-05}, {"id": 415, "seek": 213084, "start": 2153.8, "end": 2155.52, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.33492547587344523, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.2805181540898047e-05}, {"id": 416, "seek": 213084, "start": 2155.52, "end": 2157.1200000000003, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.33492547587344523, "compression_ratio": 1.379746835443038, "no_speech_prob": 1.2805181540898047e-05}, {"id": 417, "seek": 215712, "start": 2157.12, "end": 2164.16, "text": " So that is data core.", "tokens": [407, 300, 307, 1412, 4965, 13], "temperature": 0.0, "avg_logprob": -0.13659120055864443, "compression_ratio": 1.3543307086614174, "no_speech_prob": 3.1380695872940123e-06}, {"id": 418, "seek": 215712, "start": 2164.16, "end": 2170.96, "text": " If people have requests as to where to go next, feel free to tell me.", "tokens": [759, 561, 362, 12475, 382, 281, 689, 281, 352, 958, 11, 841, 1737, 281, 980, 385, 13], "temperature": 0.0, "avg_logprob": -0.13659120055864443, "compression_ratio": 1.3543307086614174, "no_speech_prob": 3.1380695872940123e-06}, {"id": 419, "seek": 215712, "start": 2170.96, "end": 2181.2799999999997, "text": " Otherwise, I think what we'll do is we'll go and have a look at the data loader.", "tokens": [10328, 11, 286, 519, 437, 321, 603, 360, 307, 321, 603, 352, 293, 362, 257, 574, 412, 264, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.13659120055864443, "compression_ratio": 1.3543307086614174, "no_speech_prob": 3.1380695872940123e-06}, {"id": 420, "seek": 218128, "start": 2181.28, "end": 2189.1600000000003, "text": " And so we're getting into some deep code now.", "tokens": [400, 370, 321, 434, 1242, 666, 512, 2452, 3089, 586, 13], "temperature": 0.0, "avg_logprob": -0.1629653431120373, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.36875257542124e-06}, {"id": 421, "seek": 218128, "start": 2189.1600000000003, "end": 2192.5600000000004, "text": " Stuff that starts with O1 is going to be deep code.", "tokens": [31347, 300, 3719, 365, 422, 16, 307, 516, 281, 312, 2452, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1629653431120373, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.36875257542124e-06}, {"id": 422, "seek": 218128, "start": 2192.5600000000004, "end": 2196.32, "text": " And the reason I'm going here now is so that we can actually have an excuse to look at", "tokens": [400, 264, 1778, 286, 478, 516, 510, 586, 307, 370, 300, 321, 393, 767, 362, 364, 8960, 281, 574, 412], "temperature": 0.0, "avg_logprob": -0.1629653431120373, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.36875257542124e-06}, {"id": 423, "seek": 218128, "start": 2196.32, "end": 2199.52, "text": " some of the deep code.", "tokens": [512, 295, 264, 2452, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1629653431120373, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.36875257542124e-06}, {"id": 424, "seek": 218128, "start": 2199.52, "end": 2210.0, "text": " The data loader, which is here, is designed to be a replacement for high torch data loader.", "tokens": [440, 1412, 3677, 260, 11, 597, 307, 510, 11, 307, 4761, 281, 312, 257, 14419, 337, 1090, 27822, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.1629653431120373, "compression_ratio": 1.5736842105263158, "no_speech_prob": 2.36875257542124e-06}, {"id": 425, "seek": 221000, "start": 2210.0, "end": 2216.96, "text": " Yes, meta classes, I think we will get to, Kevin, pretty soon.", "tokens": [1079, 11, 19616, 5359, 11, 286, 519, 321, 486, 483, 281, 11, 9954, 11, 1238, 2321, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 426, "seek": 221000, "start": 2216.96, "end": 2218.24, "text": " We're kind of heading in that direction.", "tokens": [492, 434, 733, 295, 9864, 294, 300, 3513, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 427, "seek": 221000, "start": 2218.24, "end": 2221.44, "text": " But I want to kind of see examples of them first.", "tokens": [583, 286, 528, 281, 733, 295, 536, 5110, 295, 552, 700, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 428, "seek": 221000, "start": 2221.44, "end": 2222.44, "text": " So I think we're going to go from here.", "tokens": [407, 286, 519, 321, 434, 516, 281, 352, 490, 510, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 429, "seek": 221000, "start": 2222.44, "end": 2223.44, "text": " Then we might go to transforms.", "tokens": [1396, 321, 1062, 352, 281, 35592, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 430, "seek": 221000, "start": 2223.44, "end": 2227.8, "text": " Then we might end up at meta classes.", "tokens": [1396, 321, 1062, 917, 493, 412, 19616, 5359, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 431, "seek": 221000, "start": 2227.8, "end": 2232.36, "text": " So this is designed to be a replacement for the high torch data loader.", "tokens": [407, 341, 307, 4761, 281, 312, 257, 14419, 337, 264, 1090, 27822, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 432, "seek": 221000, "start": 2232.36, "end": 2233.92, "text": " Why replace the high torch data loader?", "tokens": [1545, 7406, 264, 1090, 27822, 1412, 3677, 260, 30], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 433, "seek": 221000, "start": 2233.92, "end": 2235.56, "text": " There's a few reasons.", "tokens": [821, 311, 257, 1326, 4112, 13], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 434, "seek": 221000, "start": 2235.56, "end": 2239.72, "text": " The biggest one is that I just kept finding that there were things we wanted to do with", "tokens": [440, 3880, 472, 307, 300, 286, 445, 4305, 5006, 300, 456, 645, 721, 321, 1415, 281, 360, 365], "temperature": 0.0, "avg_logprob": -0.18416786193847656, "compression_ratio": 1.7419354838709677, "no_speech_prob": 5.255190444586333e-06}, {"id": 435, "seek": 223972, "start": 2239.72, "end": 2243.9599999999996, "text": " the high torch data loader that it didn't provide the hooks to do.", "tokens": [264, 1090, 27822, 1412, 3677, 260, 300, 309, 994, 380, 2893, 264, 26485, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 436, "seek": 223972, "start": 2243.9599999999996, "end": 2248.4399999999996, "text": " So we had to keep creating our own new classes.", "tokens": [407, 321, 632, 281, 1066, 4084, 527, 1065, 777, 5359, 13], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 437, "seek": 223972, "start": 2248.4399999999996, "end": 2250.9599999999996, "text": " And so we just had a lot of complicated code.", "tokens": [400, 370, 321, 445, 632, 257, 688, 295, 6179, 3089, 13], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 438, "seek": 223972, "start": 2250.9599999999996, "end": 2258.7999999999997, "text": " The second reason is that the high torch data loader is very, I really don't like the code", "tokens": [440, 1150, 1778, 307, 300, 264, 1090, 27822, 1412, 3677, 260, 307, 588, 11, 286, 534, 500, 380, 411, 264, 3089], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 439, "seek": 223972, "start": 2258.7999999999997, "end": 2259.7999999999997, "text": " in it.", "tokens": [294, 309, 13], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 440, "seek": 223972, "start": 2259.7999999999997, "end": 2260.7999999999997, "text": " It's very awkward.", "tokens": [467, 311, 588, 11411, 13], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 441, "seek": 223972, "start": 2260.7999999999997, "end": 2261.7999999999997, "text": " It's hard to understand.", "tokens": [467, 311, 1152, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 442, "seek": 223972, "start": 2261.7999999999997, "end": 2265.08, "text": " And there's lots of different pieces that are all tightly coupled together and make", "tokens": [400, 456, 311, 3195, 295, 819, 3755, 300, 366, 439, 21952, 29482, 1214, 293, 652], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 443, "seek": 223972, "start": 2265.08, "end": 2267.04, "text": " all kinds of assumptions about each other.", "tokens": [439, 3685, 295, 17695, 466, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.11829154244784651, "compression_ratio": 1.7023809523809523, "no_speech_prob": 5.682204118784284e-06}, {"id": 444, "seek": 226704, "start": 2267.04, "end": 2273.2799999999997, "text": " It's really hard to work through and understand and fix issues and add things to it.", "tokens": [467, 311, 534, 1152, 281, 589, 807, 293, 1223, 293, 3191, 2663, 293, 909, 721, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.14308466038233797, "compression_ratio": 1.4689265536723164, "no_speech_prob": 2.058018935713335e-06}, {"id": 445, "seek": 226704, "start": 2273.2799999999997, "end": 2278.56, "text": " So it just, yeah, I was really pretty dissatisfied with it.", "tokens": [407, 309, 445, 11, 1338, 11, 286, 390, 534, 1238, 7802, 38502, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.14308466038233797, "compression_ratio": 1.4689265536723164, "no_speech_prob": 2.058018935713335e-06}, {"id": 446, "seek": 226704, "start": 2278.56, "end": 2281.24, "text": " So we've created our own.", "tokens": [407, 321, 600, 2942, 527, 1065, 13], "temperature": 0.0, "avg_logprob": -0.14308466038233797, "compression_ratio": 1.4689265536723164, "no_speech_prob": 2.058018935713335e-06}, {"id": 447, "seek": 226704, "start": 2281.24, "end": 2294.72, "text": " Having said that, high torch data loader is based on a very rock solid, well tested, fast", "tokens": [10222, 848, 300, 11, 1090, 27822, 1412, 3677, 260, 307, 2361, 322, 257, 588, 3727, 5100, 11, 731, 8246, 11, 2370], "temperature": 0.0, "avg_logprob": -0.14308466038233797, "compression_ratio": 1.4689265536723164, "no_speech_prob": 2.058018935713335e-06}, {"id": 448, "seek": 229472, "start": 2294.72, "end": 2298.16, "text": " processing system.", "tokens": [9007, 1185, 13], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 449, "seek": 229472, "start": 2298.16, "end": 2299.52, "text": " And we wanted to use that.", "tokens": [400, 321, 1415, 281, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 450, "seek": 229472, "start": 2299.52, "end": 2305.12, "text": " And so the good news is that that multi-processing system is actually pulled out in high torch", "tokens": [400, 370, 264, 665, 2583, 307, 300, 300, 4825, 12, 41075, 278, 1185, 307, 767, 7373, 484, 294, 1090, 27822], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 451, "seek": 229472, "start": 2305.12, "end": 2308.8399999999997, "text": " into something called underscore multi-processing data loader.", "tokens": [666, 746, 1219, 37556, 4825, 12, 41075, 278, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 452, "seek": 229472, "start": 2308.8399999999997, "end": 2310.16, "text": " So we can just use that.", "tokens": [407, 321, 393, 445, 764, 300, 13], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 453, "seek": 229472, "start": 2310.16, "end": 2312.7599999999998, "text": " So we don't have to rewrite any of that.", "tokens": [407, 321, 500, 380, 362, 281, 28132, 604, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 454, "seek": 229472, "start": 2312.7599999999998, "end": 2320.2, "text": " Unfortunately, as I mentioned, high torch classes are all kind of tightly coupled and", "tokens": [8590, 11, 382, 286, 2835, 11, 1090, 27822, 5359, 366, 439, 733, 295, 21952, 29482, 293], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 455, "seek": 229472, "start": 2320.2, "end": 2322.12, "text": " make lots of assumptions about each other.", "tokens": [652, 3195, 295, 17695, 466, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.19074681225944967, "compression_ratio": 1.6936170212765957, "no_speech_prob": 2.4299164579133503e-05}, {"id": 456, "seek": 232212, "start": 2322.12, "end": 2327.04, "text": " So to actually use this, we've had to do a little bit of slightly ugly code, but it's", "tokens": [407, 281, 767, 764, 341, 11, 321, 600, 632, 281, 360, 257, 707, 857, 295, 4748, 12246, 3089, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.1628056721514966, "compression_ratio": 1.5077720207253886, "no_speech_prob": 1.4367290077643702e-06}, {"id": 457, "seek": 232212, "start": 2327.04, "end": 2328.04, "text": " not too much.", "tokens": [406, 886, 709, 13], "temperature": 0.0, "avg_logprob": -0.1628056721514966, "compression_ratio": 1.5077720207253886, "no_speech_prob": 1.4367290077643702e-06}, {"id": 458, "seek": 232212, "start": 2328.04, "end": 2331.88, "text": " Specifically, it's these lines of code here.", "tokens": [26058, 11, 309, 311, 613, 3876, 295, 3089, 510, 13], "temperature": 0.0, "avg_logprob": -0.1628056721514966, "compression_ratio": 1.5077720207253886, "no_speech_prob": 1.4367290077643702e-06}, {"id": 459, "seek": 232212, "start": 2331.88, "end": 2334.2, "text": " But we'll come back to them later.", "tokens": [583, 321, 603, 808, 646, 281, 552, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1628056721514966, "compression_ratio": 1.5077720207253886, "no_speech_prob": 1.4367290077643702e-06}, {"id": 460, "seek": 232212, "start": 2334.2, "end": 2340.7599999999998, "text": " But the only reason they exist is so that we can kind of sneak our way into the high", "tokens": [583, 264, 787, 1778, 436, 2514, 307, 370, 300, 321, 393, 733, 295, 13164, 527, 636, 666, 264, 1090], "temperature": 0.0, "avg_logprob": -0.1628056721514966, "compression_ratio": 1.5077720207253886, "no_speech_prob": 1.4367290077643702e-06}, {"id": 461, "seek": 232212, "start": 2340.7599999999998, "end": 2345.1, "text": " torch data loading system.", "tokens": [27822, 1412, 15114, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1628056721514966, "compression_ratio": 1.5077720207253886, "no_speech_prob": 1.4367290077643702e-06}, {"id": 462, "seek": 234510, "start": 2345.1, "end": 2353.98, "text": " So a data loader, you can use it in much the same way as the normal high torch data loader.", "tokens": [407, 257, 1412, 3677, 260, 11, 291, 393, 764, 309, 294, 709, 264, 912, 636, 382, 264, 2710, 1090, 27822, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.08543693728563262, "compression_ratio": 1.658682634730539, "no_speech_prob": 6.786697781535622e-07}, {"id": 463, "seek": 234510, "start": 2353.98, "end": 2363.16, "text": " So if we start with a data set, and remember a data set is anything that has a length and", "tokens": [407, 498, 321, 722, 365, 257, 1412, 992, 11, 293, 1604, 257, 1412, 992, 307, 1340, 300, 575, 257, 4641, 293], "temperature": 0.0, "avg_logprob": -0.08543693728563262, "compression_ratio": 1.658682634730539, "no_speech_prob": 6.786697781535622e-07}, {"id": 464, "seek": 234510, "start": 2363.16, "end": 2365.56, "text": " you can index into.", "tokens": [291, 393, 8186, 666, 13], "temperature": 0.0, "avg_logprob": -0.08543693728563262, "compression_ratio": 1.658682634730539, "no_speech_prob": 6.786697781535622e-07}, {"id": 465, "seek": 234510, "start": 2365.56, "end": 2369.88, "text": " So for example, this list of letters is a data set.", "tokens": [407, 337, 1365, 11, 341, 1329, 295, 7825, 307, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08543693728563262, "compression_ratio": 1.658682634730539, "no_speech_prob": 6.786697781535622e-07}, {"id": 466, "seek": 234510, "start": 2369.88, "end": 2373.4, "text": " It works as a data set.", "tokens": [467, 1985, 382, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.08543693728563262, "compression_ratio": 1.658682634730539, "no_speech_prob": 6.786697781535622e-07}, {"id": 467, "seek": 237340, "start": 2373.4, "end": 2379.84, "text": " So we can say data loader, pass in our data set, pass in a batch size, say whether or", "tokens": [407, 321, 393, 584, 1412, 3677, 260, 11, 1320, 294, 527, 1412, 992, 11, 1320, 294, 257, 15245, 2744, 11, 584, 1968, 420], "temperature": 0.0, "avg_logprob": -0.10215565176571116, "compression_ratio": 1.5543478260869565, "no_speech_prob": 7.766811904730275e-06}, {"id": 468, "seek": 237340, "start": 2379.84, "end": 2387.4, "text": " not you want to drop the last batch if it's not a size four, and say how many multi-processing", "tokens": [406, 291, 528, 281, 3270, 264, 1036, 15245, 498, 309, 311, 406, 257, 2744, 1451, 11, 293, 584, 577, 867, 4825, 12, 41075, 278], "temperature": 0.0, "avg_logprob": -0.10215565176571116, "compression_ratio": 1.5543478260869565, "no_speech_prob": 7.766811904730275e-06}, {"id": 469, "seek": 237340, "start": 2387.4, "end": 2389.44, "text": " workers to do.", "tokens": [5600, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.10215565176571116, "compression_ratio": 1.5543478260869565, "no_speech_prob": 7.766811904730275e-06}, {"id": 470, "seek": 237340, "start": 2389.44, "end": 2399.48, "text": " And then if we take that and we run two epochs of it, grab all the elements in each epoch,", "tokens": [400, 550, 498, 321, 747, 300, 293, 321, 1190, 732, 30992, 28346, 295, 309, 11, 4444, 439, 264, 4959, 294, 1184, 30992, 339, 11], "temperature": 0.0, "avg_logprob": -0.10215565176571116, "compression_ratio": 1.5543478260869565, "no_speech_prob": 7.766811904730275e-06}, {"id": 471, "seek": 239948, "start": 2399.48, "end": 2406.92, "text": " and then join each batch together with no separator, and then join each set of batches", "tokens": [293, 550, 3917, 1184, 15245, 1214, 365, 572, 3128, 1639, 11, 293, 550, 3917, 1184, 992, 295, 15245, 279], "temperature": 0.0, "avg_logprob": -0.1662941098213196, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.8573051622515777e-06}, {"id": 472, "seek": 239948, "start": 2406.92, "end": 2410.6, "text": " together with a space between them, we get back that.", "tokens": [1214, 365, 257, 1901, 1296, 552, 11, 321, 483, 646, 300, 13], "temperature": 0.0, "avg_logprob": -0.1662941098213196, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.8573051622515777e-06}, {"id": 473, "seek": 239948, "start": 2410.6, "end": 2419.16, "text": " OK, so there's all the letters with the, as you can see, the last batch disappears.", "tokens": [2264, 11, 370, 456, 311, 439, 264, 7825, 365, 264, 11, 382, 291, 393, 536, 11, 264, 1036, 15245, 25527, 13], "temperature": 0.0, "avg_logprob": -0.1662941098213196, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.8573051622515777e-06}, {"id": 474, "seek": 239948, "start": 2419.16, "end": 2425.2, "text": " If we do it without dropLast equals true, then we do get the last bit.", "tokens": [759, 321, 360, 309, 1553, 3270, 42385, 6915, 2074, 11, 550, 321, 360, 483, 264, 1036, 857, 13], "temperature": 0.0, "avg_logprob": -0.1662941098213196, "compression_ratio": 1.6857142857142857, "no_speech_prob": 2.8573051622515777e-06}, {"id": 475, "seek": 242520, "start": 2425.2, "end": 2434.08, "text": " And we can have more than as many workers as we like by just passing that in.", "tokens": [400, 321, 393, 362, 544, 813, 382, 867, 5600, 382, 321, 411, 538, 445, 8437, 300, 294, 13], "temperature": 0.0, "avg_logprob": -0.1377139034041439, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.443978928567958e-06}, {"id": 476, "seek": 242520, "start": 2434.08, "end": 2441.6, "text": " You can pass in things that can be turned into tensors, like for example ints, and just", "tokens": [509, 393, 1320, 294, 721, 300, 393, 312, 3574, 666, 10688, 830, 11, 411, 337, 1365, 560, 82, 11, 293, 445], "temperature": 0.0, "avg_logprob": -0.1377139034041439, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.443978928567958e-06}, {"id": 477, "seek": 242520, "start": 2441.6, "end": 2446.56, "text": " like the, this is all the same as the PyTorch data loader, it will turn those into batches", "tokens": [411, 264, 11, 341, 307, 439, 264, 912, 382, 264, 9953, 51, 284, 339, 1412, 3677, 260, 11, 309, 486, 1261, 729, 666, 15245, 279], "temperature": 0.0, "avg_logprob": -0.1377139034041439, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.443978928567958e-06}, {"id": 478, "seek": 242520, "start": 2446.56, "end": 2449.3599999999997, "text": " of tensors, as you can see.", "tokens": [295, 10688, 830, 11, 382, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.1377139034041439, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.443978928567958e-06}, {"id": 479, "seek": 244936, "start": 2449.36, "end": 2458.52, "text": " So this is the testing, test equal type tests that this thing and this thing are the same,", "tokens": [407, 341, 307, 264, 4997, 11, 1500, 2681, 2010, 6921, 300, 341, 551, 293, 341, 551, 366, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.18877103057088732, "compression_ratio": 1.6576086956521738, "no_speech_prob": 5.173858880880289e-06}, {"id": 480, "seek": 244936, "start": 2458.52, "end": 2460.76, "text": " and even have exactly the same types.", "tokens": [293, 754, 362, 2293, 264, 912, 3467, 13], "temperature": 0.0, "avg_logprob": -0.18877103057088732, "compression_ratio": 1.6576086956521738, "no_speech_prob": 5.173858880880289e-06}, {"id": 481, "seek": 244936, "start": 2460.76, "end": 2466.2000000000003, "text": " Normally test equals just checks that collections have the same contents.", "tokens": [17424, 1500, 6915, 445, 13834, 300, 16641, 362, 264, 912, 15768, 13], "temperature": 0.0, "avg_logprob": -0.18877103057088732, "compression_ratio": 1.6576086956521738, "no_speech_prob": 5.173858880880289e-06}, {"id": 482, "seek": 244936, "start": 2466.2000000000003, "end": 2473.2000000000003, "text": " OK, so that's kind of the basic behavior that you would expect to see in a normal PyTorch", "tokens": [2264, 11, 370, 300, 311, 733, 295, 264, 3875, 5223, 300, 291, 576, 2066, 281, 536, 294, 257, 2710, 9953, 51, 284, 339], "temperature": 0.0, "avg_logprob": -0.18877103057088732, "compression_ratio": 1.6576086956521738, "no_speech_prob": 5.173858880880289e-06}, {"id": 483, "seek": 244936, "start": 2473.2000000000003, "end": 2477.6, "text": " data loader.", "tokens": [1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.18877103057088732, "compression_ratio": 1.6576086956521738, "no_speech_prob": 5.173858880880289e-06}, {"id": 484, "seek": 247760, "start": 2477.6, "end": 2481.3199999999997, "text": " But we also have some hooks that you can add in.", "tokens": [583, 321, 611, 362, 512, 26485, 300, 291, 393, 909, 294, 13], "temperature": 0.0, "avg_logprob": -0.1663519969353309, "compression_ratio": 1.8555555555555556, "no_speech_prob": 3.169257615809329e-05}, {"id": 485, "seek": 247760, "start": 2481.3199999999997, "end": 2485.08, "text": " So one of the hooks is afterItter.", "tokens": [407, 472, 295, 264, 26485, 307, 934, 3522, 391, 13], "temperature": 0.0, "avg_logprob": -0.1663519969353309, "compression_ratio": 1.8555555555555556, "no_speech_prob": 3.169257615809329e-05}, {"id": 486, "seek": 247760, "start": 2485.08, "end": 2492.68, "text": " And so afterItter is a hook that will run at the end of each iteration.", "tokens": [400, 370, 934, 3522, 391, 307, 257, 6328, 300, 486, 1190, 412, 264, 917, 295, 1184, 24784, 13], "temperature": 0.0, "avg_logprob": -0.1663519969353309, "compression_ratio": 1.8555555555555556, "no_speech_prob": 3.169257615809329e-05}, {"id": 487, "seek": 247760, "start": 2492.68, "end": 2498.16, "text": " And so this is just something that's going to, let's see what T3 is.", "tokens": [400, 370, 341, 307, 445, 746, 300, 311, 516, 281, 11, 718, 311, 536, 437, 314, 18, 307, 13], "temperature": 0.0, "avg_logprob": -0.1663519969353309, "compression_ratio": 1.8555555555555556, "no_speech_prob": 3.169257615809329e-05}, {"id": 488, "seek": 247760, "start": 2498.16, "end": 2504.96, "text": " T3 is just some tensor, and so it's just something that's going to just put, set some, set T3.f", "tokens": [314, 18, 307, 445, 512, 40863, 11, 293, 370, 309, 311, 445, 746, 300, 311, 516, 281, 445, 829, 11, 992, 512, 11, 992, 314, 18, 13, 69], "temperature": 0.0, "avg_logprob": -0.1663519969353309, "compression_ratio": 1.8555555555555556, "no_speech_prob": 3.169257615809329e-05}, {"id": 489, "seek": 247760, "start": 2504.96, "end": 2506.6, "text": " to something.", "tokens": [281, 746, 13], "temperature": 0.0, "avg_logprob": -0.1663519969353309, "compression_ratio": 1.8555555555555556, "no_speech_prob": 3.169257615809329e-05}, {"id": 490, "seek": 250660, "start": 2506.6, "end": 2509.68, "text": " And so after we run this, T3.f equals that thing.", "tokens": [400, 370, 934, 321, 1190, 341, 11, 314, 18, 13, 69, 6915, 300, 551, 13], "temperature": 0.0, "avg_logprob": -0.25605907178904913, "compression_ratio": 1.4596273291925466, "no_speech_prob": 2.601597088869312e-06}, {"id": 491, "seek": 250660, "start": 2509.68, "end": 2516.2, "text": " So you can add code that runs after an iteration.", "tokens": [407, 291, 393, 909, 3089, 300, 6676, 934, 364, 24784, 13], "temperature": 0.0, "avg_logprob": -0.25605907178904913, "compression_ratio": 1.4596273291925466, "no_speech_prob": 2.601597088869312e-06}, {"id": 492, "seek": 250660, "start": 2516.2, "end": 2524.72, "text": " Then let's see this one.", "tokens": [1396, 718, 311, 536, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.25605907178904913, "compression_ratio": 1.4596273291925466, "no_speech_prob": 2.601597088869312e-06}, {"id": 493, "seek": 250660, "start": 2524.72, "end": 2527.0, "text": " That's just the same as before.", "tokens": [663, 311, 445, 264, 912, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.25605907178904913, "compression_ratio": 1.4596273291925466, "no_speech_prob": 2.601597088869312e-06}, {"id": 494, "seek": 250660, "start": 2527.0, "end": 2533.0, "text": " You can also pass a generator to a data loader, and it will work fine as well.", "tokens": [509, 393, 611, 1320, 257, 19265, 281, 257, 1412, 3677, 260, 11, 293, 309, 486, 589, 2489, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.25605907178904913, "compression_ratio": 1.4596273291925466, "no_speech_prob": 2.601597088869312e-06}, {"id": 495, "seek": 253300, "start": 2533.0, "end": 2540.64, "text": " OK, so that's all kind of normal data loader behavior.", "tokens": [2264, 11, 370, 300, 311, 439, 733, 295, 2710, 1412, 3677, 260, 5223, 13], "temperature": 0.0, "avg_logprob": -0.10630603169285974, "compression_ratio": 1.5404040404040404, "no_speech_prob": 7.889221706136595e-06}, {"id": 496, "seek": 253300, "start": 2540.64, "end": 2544.36, "text": " Then there's more stuff we can do.", "tokens": [1396, 456, 311, 544, 1507, 321, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.10630603169285974, "compression_ratio": 1.5404040404040404, "no_speech_prob": 7.889221706136595e-06}, {"id": 497, "seek": 253300, "start": 2544.36, "end": 2551.4, "text": " And specifically, we can define, rather than just saying afterItter, there's actually a", "tokens": [400, 4682, 11, 321, 393, 6964, 11, 2831, 813, 445, 1566, 934, 3522, 391, 11, 456, 311, 767, 257], "temperature": 0.0, "avg_logprob": -0.10630603169285974, "compression_ratio": 1.5404040404040404, "no_speech_prob": 7.889221706136595e-06}, {"id": 498, "seek": 253300, "start": 2551.4, "end": 2559.18, "text": " whole list of callbacks that we can, as you can see, that we can define.", "tokens": [1379, 1329, 295, 818, 17758, 300, 321, 393, 11, 382, 291, 393, 536, 11, 300, 321, 393, 6964, 13], "temperature": 0.0, "avg_logprob": -0.10630603169285974, "compression_ratio": 1.5404040404040404, "no_speech_prob": 7.889221706136595e-06}, {"id": 499, "seek": 253300, "start": 2559.18, "end": 2562.04, "text": " And we use them all over the place throughout Fast.ai.", "tokens": [400, 321, 764, 552, 439, 670, 264, 1081, 3710, 15968, 13, 1301, 13], "temperature": 0.0, "avg_logprob": -0.10630603169285974, "compression_ratio": 1.5404040404040404, "no_speech_prob": 7.889221706136595e-06}, {"id": 500, "seek": 256204, "start": 2562.04, "end": 2565.64, "text": " We've already seen afterItter, which runs after each iteration.", "tokens": [492, 600, 1217, 1612, 934, 3522, 391, 11, 597, 6676, 934, 1184, 24784, 13], "temperature": 0.0, "avg_logprob": -0.15135998768849415, "compression_ratio": 1.832535885167464, "no_speech_prob": 1.750273077050224e-05}, {"id": 501, "seek": 256204, "start": 2565.64, "end": 2569.52, "text": " There's, sorry, after all the iterations.", "tokens": [821, 311, 11, 2597, 11, 934, 439, 264, 36540, 13], "temperature": 0.0, "avg_logprob": -0.15135998768849415, "compression_ratio": 1.832535885167464, "no_speech_prob": 1.750273077050224e-05}, {"id": 502, "seek": 256204, "start": 2569.52, "end": 2575.24, "text": " There's beforeItter, which will run before all the iterations.", "tokens": [821, 311, 949, 3522, 391, 11, 597, 486, 1190, 949, 439, 264, 36540, 13], "temperature": 0.0, "avg_logprob": -0.15135998768849415, "compression_ratio": 1.832535885167464, "no_speech_prob": 1.750273077050224e-05}, {"id": 503, "seek": 256204, "start": 2575.24, "end": 2582.4, "text": " And then if you look at the code, you can see here is the iterator.", "tokens": [400, 550, 498, 291, 574, 412, 264, 3089, 11, 291, 393, 536, 510, 307, 264, 17138, 1639, 13], "temperature": 0.0, "avg_logprob": -0.15135998768849415, "compression_ratio": 1.832535885167464, "no_speech_prob": 1.750273077050224e-05}, {"id": 504, "seek": 256204, "start": 2582.4, "end": 2585.84, "text": " So here's beforeItter, here's afterItter.", "tokens": [407, 510, 311, 949, 3522, 391, 11, 510, 311, 934, 3522, 391, 13], "temperature": 0.0, "avg_logprob": -0.15135998768849415, "compression_ratio": 1.832535885167464, "no_speech_prob": 1.750273077050224e-05}, {"id": 505, "seek": 256204, "start": 2585.84, "end": 2589.88, "text": " This is the slightly awkward thing that we need to fit in with the PyTorch multi-processing", "tokens": [639, 307, 264, 4748, 11411, 551, 300, 321, 643, 281, 3318, 294, 365, 264, 9953, 51, 284, 339, 4825, 12, 41075, 278], "temperature": 0.0, "avg_logprob": -0.15135998768849415, "compression_ratio": 1.832535885167464, "no_speech_prob": 1.750273077050224e-05}, {"id": 506, "seek": 256204, "start": 2589.88, "end": 2590.88, "text": " data loader.", "tokens": [1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.15135998768849415, "compression_ratio": 1.832535885167464, "no_speech_prob": 1.750273077050224e-05}, {"id": 507, "seek": 259088, "start": 2590.88, "end": 2602.2400000000002, "text": " But what it's going to do then is it's going to call, it's going to basically use this", "tokens": [583, 437, 309, 311, 516, 281, 360, 550, 307, 309, 311, 516, 281, 818, 11, 309, 311, 516, 281, 1936, 764, 341], "temperature": 0.0, "avg_logprob": -0.16767568941469546, "compression_ratio": 1.80625, "no_speech_prob": 1.2029538083879743e-05}, {"id": 508, "seek": 259088, "start": 2602.2400000000002, "end": 2609.8, "text": " thing here, which is going to call sampler to get the samples, which is basically using", "tokens": [551, 510, 11, 597, 307, 516, 281, 818, 3247, 22732, 281, 483, 264, 10938, 11, 597, 307, 1936, 1228], "temperature": 0.0, "avg_logprob": -0.16767568941469546, "compression_ratio": 1.80625, "no_speech_prob": 1.2029538083879743e-05}, {"id": 509, "seek": 259088, "start": 2609.8, "end": 2612.36, "text": " this as something very similar to the PyTorch's samplers.", "tokens": [341, 382, 746, 588, 2531, 281, 264, 9953, 51, 284, 339, 311, 3247, 564, 433, 13], "temperature": 0.0, "avg_logprob": -0.16767568941469546, "compression_ratio": 1.80625, "no_speech_prob": 1.2029538083879743e-05}, {"id": 510, "seek": 259088, "start": 2612.36, "end": 2616.52, "text": " And it's going to call createBatches for each iteration.", "tokens": [400, 309, 311, 516, 281, 818, 1884, 33, 852, 279, 337, 1184, 24784, 13], "temperature": 0.0, "avg_logprob": -0.16767568941469546, "compression_ratio": 1.80625, "no_speech_prob": 1.2029538083879743e-05}, {"id": 511, "seek": 261652, "start": 2616.52, "end": 2623.84, "text": " And createBatches is going to go through everything in the sampler.", "tokens": [400, 1884, 33, 852, 279, 307, 516, 281, 352, 807, 1203, 294, 264, 3247, 22732, 13], "temperature": 0.0, "avg_logprob": -0.12915467364447458, "compression_ratio": 1.4274193548387097, "no_speech_prob": 1.3007022062083706e-05}, {"id": 512, "seek": 261652, "start": 2623.84, "end": 2627.04, "text": " And it's going to map doItem over it.", "tokens": [400, 309, 311, 516, 281, 4471, 360, 3522, 443, 670, 309, 13], "temperature": 0.0, "avg_logprob": -0.12915467364447458, "compression_ratio": 1.4274193548387097, "no_speech_prob": 1.3007022062083706e-05}, {"id": 513, "seek": 261652, "start": 2627.04, "end": 2632.84, "text": " And doItem will first call createItem, and then it will call afterItem.", "tokens": [400, 360, 3522, 443, 486, 700, 818, 1884, 3522, 443, 11, 293, 550, 309, 486, 818, 934, 3522, 443, 13], "temperature": 0.0, "avg_logprob": -0.12915467364447458, "compression_ratio": 1.4274193548387097, "no_speech_prob": 1.3007022062083706e-05}, {"id": 514, "seek": 263284, "start": 2632.84, "end": 2647.4, "text": " And then after that, it will then use a function called chunked, which is basically going to", "tokens": [400, 550, 934, 300, 11, 309, 486, 550, 764, 257, 2445, 1219, 16635, 292, 11, 597, 307, 1936, 516, 281], "temperature": 0.0, "avg_logprob": -0.15869119797629871, "compression_ratio": 1.6875, "no_speech_prob": 4.637847723643063e-06}, {"id": 515, "seek": 263284, "start": 2647.4, "end": 2651.52, "text": " take, it's basically going to create batches out of our list.", "tokens": [747, 11, 309, 311, 1936, 516, 281, 1884, 15245, 279, 484, 295, 527, 1329, 13], "temperature": 0.0, "avg_logprob": -0.15869119797629871, "compression_ratio": 1.6875, "no_speech_prob": 4.637847723643063e-06}, {"id": 516, "seek": 263284, "start": 2651.52, "end": 2654.52, "text": " But it's all done lazily.", "tokens": [583, 309, 311, 439, 1096, 19320, 953, 13], "temperature": 0.0, "avg_logprob": -0.15869119797629871, "compression_ratio": 1.6875, "no_speech_prob": 4.637847723643063e-06}, {"id": 517, "seek": 263284, "start": 2654.52, "end": 2656.0, "text": " And then they're going to call doBatch.", "tokens": [400, 550, 436, 434, 516, 281, 818, 360, 33, 852, 13], "temperature": 0.0, "avg_logprob": -0.15869119797629871, "compression_ratio": 1.6875, "no_speech_prob": 4.637847723643063e-06}, {"id": 518, "seek": 263284, "start": 2656.0, "end": 2660.84, "text": " So just like we had doItem to create our items, doBatch creates our batches.", "tokens": [407, 445, 411, 321, 632, 360, 3522, 443, 281, 1884, 527, 4754, 11, 360, 33, 852, 7829, 527, 15245, 279, 13], "temperature": 0.0, "avg_logprob": -0.15869119797629871, "compression_ratio": 1.6875, "no_speech_prob": 4.637847723643063e-06}, {"id": 519, "seek": 266084, "start": 2660.84, "end": 2666.28, "text": " And so that will call beforeBatch, and then createBatch.", "tokens": [400, 370, 300, 486, 818, 949, 33, 852, 11, 293, 550, 1884, 33, 852, 13], "temperature": 0.0, "avg_logprob": -0.21194306815542827, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.341982715530321e-05}, {"id": 520, "seek": 266084, "start": 2666.28, "end": 2668.48, "text": " This is the thing to retain types.", "tokens": [639, 307, 264, 551, 281, 18340, 3467, 13], "temperature": 0.0, "avg_logprob": -0.21194306815542827, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.341982715530321e-05}, {"id": 521, "seek": 266084, "start": 2668.48, "end": 2671.44, "text": " And then finally, afterBatch.", "tokens": [400, 550, 2721, 11, 934, 33, 852, 13], "temperature": 0.0, "avg_logprob": -0.21194306815542827, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.341982715530321e-05}, {"id": 522, "seek": 266084, "start": 2671.44, "end": 2678.1200000000003, "text": " And so the idea here is that you can replace any of those things.", "tokens": [400, 370, 264, 1558, 510, 307, 300, 291, 393, 7406, 604, 295, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.21194306815542827, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.341982715530321e-05}, {"id": 523, "seek": 266084, "start": 2678.1200000000003, "end": 2690.76, "text": " Things like beforeBatch, afterBatch, and afterItem are all things which default to no up to.", "tokens": [9514, 411, 949, 33, 852, 11, 934, 33, 852, 11, 293, 934, 3522, 443, 366, 439, 721, 597, 7576, 281, 572, 493, 281, 13], "temperature": 0.0, "avg_logprob": -0.21194306815542827, "compression_ratio": 1.6470588235294117, "no_speech_prob": 1.341982715530321e-05}, {"id": 524, "seek": 269076, "start": 2690.76, "end": 2694.36, "text": " So all these things default to no operation.", "tokens": [407, 439, 613, 721, 7576, 281, 572, 6916, 13], "temperature": 0.0, "avg_logprob": -0.12621742248535156, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.425452061695978e-06}, {"id": 525, "seek": 269076, "start": 2694.36, "end": 2699.6000000000004, "text": " So in other words, you can just use them as callbacks.", "tokens": [407, 294, 661, 2283, 11, 291, 393, 445, 764, 552, 382, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.12621742248535156, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.425452061695978e-06}, {"id": 526, "seek": 269076, "start": 2699.6000000000004, "end": 2704.84, "text": " But you can actually change everything.", "tokens": [583, 291, 393, 767, 1319, 1203, 13], "temperature": 0.0, "avg_logprob": -0.12621742248535156, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.425452061695978e-06}, {"id": 527, "seek": 269076, "start": 2704.84, "end": 2710.36, "text": " So we'll see examples of that over time, but I'll show you some examples.", "tokens": [407, 321, 603, 536, 5110, 295, 300, 670, 565, 11, 457, 286, 603, 855, 291, 512, 5110, 13], "temperature": 0.0, "avg_logprob": -0.12621742248535156, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.425452061695978e-06}, {"id": 528, "seek": 269076, "start": 2710.36, "end": 2716.6400000000003, "text": " So example number one is here's a subclass of Dataloader.", "tokens": [407, 1365, 1230, 472, 307, 510, 311, 257, 1422, 11665, 295, 9315, 10334, 8312, 13], "temperature": 0.0, "avg_logprob": -0.12621742248535156, "compression_ratio": 1.5224719101123596, "no_speech_prob": 4.425452061695978e-06}, {"id": 529, "seek": 271664, "start": 2716.64, "end": 2720.8799999999997, "text": " And in this subclass of Dataloader, we override createItem.", "tokens": [400, 294, 341, 1422, 11665, 295, 9315, 10334, 8312, 11, 321, 42321, 1884, 3522, 443, 13], "temperature": 0.0, "avg_logprob": -0.09693830353873116, "compression_ratio": 1.7149122807017543, "no_speech_prob": 1.5534923250015709e-06}, {"id": 530, "seek": 271664, "start": 2720.8799999999997, "end": 2728.0, "text": " So createItem normally grabs the ith element of a data set, assuming we have some sample", "tokens": [407, 1884, 3522, 443, 5646, 30028, 264, 309, 71, 4478, 295, 257, 1412, 992, 11, 11926, 321, 362, 512, 6889], "temperature": 0.0, "avg_logprob": -0.09693830353873116, "compression_ratio": 1.7149122807017543, "no_speech_prob": 1.5534923250015709e-06}, {"id": 531, "seek": 271664, "start": 2728.0, "end": 2729.2, "text": " that we want.", "tokens": [300, 321, 528, 13], "temperature": 0.0, "avg_logprob": -0.09693830353873116, "compression_ratio": 1.7149122807017543, "no_speech_prob": 1.5534923250015709e-06}, {"id": 532, "seek": 271664, "start": 2729.2, "end": 2731.16, "text": " That's what createItem normally does.", "tokens": [663, 311, 437, 1884, 3522, 443, 5646, 775, 13], "temperature": 0.0, "avg_logprob": -0.09693830353873116, "compression_ratio": 1.7149122807017543, "no_speech_prob": 1.5534923250015709e-06}, {"id": 533, "seek": 271664, "start": 2731.16, "end": 2733.96, "text": " So now we're overriding it to do something else.", "tokens": [407, 586, 321, 434, 670, 81, 2819, 309, 281, 360, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.09693830353873116, "compression_ratio": 1.7149122807017543, "no_speech_prob": 1.5534923250015709e-06}, {"id": 534, "seek": 271664, "start": 2733.96, "end": 2740.12, "text": " And specifically, it's going to return some random number.", "tokens": [400, 4682, 11, 309, 311, 516, 281, 2736, 512, 4974, 1230, 13], "temperature": 0.0, "avg_logprob": -0.09693830353873116, "compression_ratio": 1.7149122807017543, "no_speech_prob": 1.5534923250015709e-06}, {"id": 535, "seek": 271664, "start": 2740.12, "end": 2745.16, "text": " And so you can see it's going to return that random number if it's less than 0.95.", "tokens": [400, 370, 291, 393, 536, 309, 311, 516, 281, 2736, 300, 4974, 1230, 498, 309, 311, 1570, 813, 1958, 13, 15718, 13], "temperature": 0.0, "avg_logprob": -0.09693830353873116, "compression_ratio": 1.7149122807017543, "no_speech_prob": 1.5534923250015709e-06}, {"id": 536, "seek": 274516, "start": 2745.16, "end": 2748.92, "text": " Otherwise, it will stop.", "tokens": [10328, 11, 309, 486, 1590, 13], "temperature": 0.0, "avg_logprob": -0.20252154529958533, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.2289075357330148e-06}, {"id": 537, "seek": 274516, "start": 2748.92, "end": 2750.3999999999996, "text": " What does stop?", "tokens": [708, 775, 1590, 30], "temperature": 0.0, "avg_logprob": -0.20252154529958533, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.2289075357330148e-06}, {"id": 538, "seek": 274516, "start": 2750.3999999999996, "end": 2757.92, "text": " Stop is simply something that will raise a stop iteration exception.", "tokens": [5535, 307, 2935, 746, 300, 486, 5300, 257, 1590, 24784, 11183, 13], "temperature": 0.0, "avg_logprob": -0.20252154529958533, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.2289075357330148e-06}, {"id": 539, "seek": 274516, "start": 2757.92, "end": 2764.96, "text": " For those of you that don't know, in Python, the way that generators, iterators, stuff like", "tokens": [1171, 729, 295, 291, 300, 500, 380, 458, 11, 294, 15329, 11, 264, 636, 300, 38662, 11, 17138, 3391, 11, 1507, 411], "temperature": 0.0, "avg_logprob": -0.20252154529958533, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.2289075357330148e-06}, {"id": 540, "seek": 274516, "start": 2764.96, "end": 2770.8399999999997, "text": " that say they've finished is they raise a stop iteration exception.", "tokens": [300, 584, 436, 600, 4335, 307, 436, 5300, 257, 1590, 24784, 11183, 13], "temperature": 0.0, "avg_logprob": -0.20252154529958533, "compression_ratio": 1.6402439024390243, "no_speech_prob": 1.2289075357330148e-06}, {"id": 541, "seek": 277084, "start": 2770.84, "end": 2778.2400000000002, "text": " So Python actually uses exceptions for control flow, which is a really interesting insight.", "tokens": [407, 15329, 767, 4960, 22847, 337, 1969, 3095, 11, 597, 307, 257, 534, 1880, 11269, 13], "temperature": 0.0, "avg_logprob": -0.14633169927095113, "compression_ratio": 1.5458937198067633, "no_speech_prob": 1.392517674503324e-06}, {"id": 542, "seek": 277084, "start": 2778.2400000000002, "end": 2788.1200000000003, "text": " So in this case, we can create I mean, obviously, this particular example is not something we're", "tokens": [407, 294, 341, 1389, 11, 321, 393, 1884, 286, 914, 11, 2745, 11, 341, 1729, 1365, 307, 406, 746, 321, 434], "temperature": 0.0, "avg_logprob": -0.14633169927095113, "compression_ratio": 1.5458937198067633, "no_speech_prob": 1.392517674503324e-06}, {"id": 543, "seek": 277084, "start": 2788.1200000000003, "end": 2789.1200000000003, "text": " doing in the real world.", "tokens": [884, 294, 264, 957, 1002, 13], "temperature": 0.0, "avg_logprob": -0.14633169927095113, "compression_ratio": 1.5458937198067633, "no_speech_prob": 1.392517674503324e-06}, {"id": 544, "seek": 277084, "start": 2789.1200000000003, "end": 2794.2000000000003, "text": " But you can imagine creating a createItem that keeps reading something from a network", "tokens": [583, 291, 393, 3811, 4084, 257, 1884, 3522, 443, 300, 5965, 3760, 746, 490, 257, 3209], "temperature": 0.0, "avg_logprob": -0.14633169927095113, "compression_ratio": 1.5458937198067633, "no_speech_prob": 1.392517674503324e-06}, {"id": 545, "seek": 277084, "start": 2794.2000000000003, "end": 2796.6400000000003, "text": " stream, for example.", "tokens": [4309, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.14633169927095113, "compression_ratio": 1.5458937198067633, "no_speech_prob": 1.392517674503324e-06}, {"id": 546, "seek": 279664, "start": 2796.64, "end": 2804.96, "text": " And when it gets some, I don't know, end of network stream error, it will stop.", "tokens": [400, 562, 309, 2170, 512, 11, 286, 500, 380, 458, 11, 917, 295, 3209, 4309, 6713, 11, 309, 486, 1590, 13], "temperature": 0.0, "avg_logprob": -0.12307875951131185, "compression_ratio": 1.616504854368932, "no_speech_prob": 1.4593603054890991e-06}, {"id": 547, "seek": 279664, "start": 2804.96, "end": 2809.96, "text": " So you can kind of easily create streaming data loaders in this way.", "tokens": [407, 291, 393, 733, 295, 3612, 1884, 11791, 1412, 3677, 433, 294, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.12307875951131185, "compression_ratio": 1.616504854368932, "no_speech_prob": 1.4593603054890991e-06}, {"id": 548, "seek": 279664, "start": 2809.96, "end": 2813.48, "text": " And so here's an example, basically, of a simple streaming data loader.", "tokens": [400, 370, 510, 311, 364, 1365, 11, 1936, 11, 295, 257, 2199, 11791, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.12307875951131185, "compression_ratio": 1.616504854368932, "no_speech_prob": 1.4593603054890991e-06}, {"id": 549, "seek": 279664, "start": 2813.48, "end": 2820.16, "text": " So that's pretty fun.", "tokens": [407, 300, 311, 1238, 1019, 13], "temperature": 0.0, "avg_logprob": -0.12307875951131185, "compression_ratio": 1.616504854368932, "no_speech_prob": 1.4593603054890991e-06}, {"id": 550, "seek": 279664, "start": 2820.16, "end": 2825.18, "text": " And one of the interesting things about this is you can pass in numWorkers to be something", "tokens": [400, 472, 295, 264, 1880, 721, 466, 341, 307, 291, 393, 1320, 294, 1031, 28846, 433, 281, 312, 746], "temperature": 0.0, "avg_logprob": -0.12307875951131185, "compression_ratio": 1.616504854368932, "no_speech_prob": 1.4593603054890991e-06}, {"id": 551, "seek": 282518, "start": 2825.18, "end": 2827.2, "text": " other than zero.", "tokens": [661, 813, 4018, 13], "temperature": 0.0, "avg_logprob": -0.11917860563411269, "compression_ratio": 1.565, "no_speech_prob": 1.9947053715441143e-06}, {"id": 552, "seek": 282518, "start": 2827.2, "end": 2833.04, "text": " And what that's going to do is it's going to create, in this case, four streaming data", "tokens": [400, 437, 300, 311, 516, 281, 360, 307, 309, 311, 516, 281, 1884, 11, 294, 341, 1389, 11, 1451, 11791, 1412], "temperature": 0.0, "avg_logprob": -0.11917860563411269, "compression_ratio": 1.565, "no_speech_prob": 1.9947053715441143e-06}, {"id": 553, "seek": 282518, "start": 2833.04, "end": 2837.72, "text": " loaders, which is a kind of really interesting idea.", "tokens": [3677, 433, 11, 597, 307, 257, 733, 295, 534, 1880, 1558, 13], "temperature": 0.0, "avg_logprob": -0.11917860563411269, "compression_ratio": 1.565, "no_speech_prob": 1.9947053715441143e-06}, {"id": 554, "seek": 282518, "start": 2837.72, "end": 2845.68, "text": " So as you can see, you end up with more batches than the zero numWorkers version, because", "tokens": [407, 382, 291, 393, 536, 11, 291, 917, 493, 365, 544, 15245, 279, 813, 264, 4018, 1031, 28846, 433, 3037, 11, 570], "temperature": 0.0, "avg_logprob": -0.11917860563411269, "compression_ratio": 1.565, "no_speech_prob": 1.9947053715441143e-06}, {"id": 555, "seek": 282518, "start": 2845.68, "end": 2850.7599999999998, "text": " you've got more streaming workers, and they're all doing this job.", "tokens": [291, 600, 658, 544, 11791, 5600, 11, 293, 436, 434, 439, 884, 341, 1691, 13], "temperature": 0.0, "avg_logprob": -0.11917860563411269, "compression_ratio": 1.565, "no_speech_prob": 1.9947053715441143e-06}, {"id": 556, "seek": 285076, "start": 2850.76, "end": 2856.48, "text": " So they're all doing it totally independently.", "tokens": [407, 436, 434, 439, 884, 309, 3879, 21761, 13], "temperature": 0.0, "avg_logprob": -0.11748504638671875, "compression_ratio": 1.5786802030456852, "no_speech_prob": 4.8603797040414065e-06}, {"id": 557, "seek": 285076, "start": 2856.48, "end": 2861.2200000000003, "text": " So that's pretty interesting, I think.", "tokens": [407, 300, 311, 1238, 1880, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.11748504638671875, "compression_ratio": 1.5786802030456852, "no_speech_prob": 4.8603797040414065e-06}, {"id": 558, "seek": 285076, "start": 2861.2200000000003, "end": 2865.0, "text": " So if you don't set a batch size, so here I've got batch size equals four, if you don't", "tokens": [407, 498, 291, 500, 380, 992, 257, 15245, 2744, 11, 370, 510, 286, 600, 658, 15245, 2744, 6915, 1451, 11, 498, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.11748504638671875, "compression_ratio": 1.5786802030456852, "no_speech_prob": 4.8603797040414065e-06}, {"id": 559, "seek": 285076, "start": 2865.0, "end": 2872.1400000000003, "text": " set a batch size, then we don't do any batching.", "tokens": [992, 257, 15245, 2744, 11, 550, 321, 500, 380, 360, 604, 15245, 278, 13], "temperature": 0.0, "avg_logprob": -0.11748504638671875, "compression_ratio": 1.5786802030456852, "no_speech_prob": 4.8603797040414065e-06}, {"id": 560, "seek": 285076, "start": 2872.1400000000003, "end": 2880.2000000000003, "text": " And this is actually something which is also built into the new PyTorch 1.2 data loader.", "tokens": [400, 341, 307, 767, 746, 597, 307, 611, 3094, 666, 264, 777, 9953, 51, 284, 339, 502, 13, 17, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.11748504638671875, "compression_ratio": 1.5786802030456852, "no_speech_prob": 4.8603797040414065e-06}, {"id": 561, "seek": 288020, "start": 2880.2, "end": 2884.3199999999997, "text": " This idea of you can have a batch size of none.", "tokens": [639, 1558, 295, 291, 393, 362, 257, 15245, 2744, 295, 6022, 13], "temperature": 0.0, "avg_logprob": -0.12712319691975912, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3845805369783193e-05}, {"id": 562, "seek": 288020, "start": 2884.3199999999997, "end": 2888.96, "text": " And so if you don't pass a batch size in, so here I'm, remember that letters is all", "tokens": [400, 370, 498, 291, 500, 380, 1320, 257, 15245, 2744, 294, 11, 370, 510, 286, 478, 11, 1604, 300, 7825, 307, 439], "temperature": 0.0, "avg_logprob": -0.12712319691975912, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3845805369783193e-05}, {"id": 563, "seek": 288020, "start": 2888.96, "end": 2891.72, "text": " the letters of the alphabet, lowercase.", "tokens": [264, 7825, 295, 264, 23339, 11, 3126, 9765, 13], "temperature": 0.0, "avg_logprob": -0.12712319691975912, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3845805369783193e-05}, {"id": 564, "seek": 288020, "start": 2891.72, "end": 2904.48, "text": " So if I just do a data loader on letters, then if I listify that data loader, I literally", "tokens": [407, 498, 286, 445, 360, 257, 1412, 3677, 260, 322, 7825, 11, 550, 498, 286, 1329, 2505, 300, 1412, 3677, 260, 11, 286, 3736], "temperature": 0.0, "avg_logprob": -0.12712319691975912, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3845805369783193e-05}, {"id": 565, "seek": 288020, "start": 2904.48, "end": 2907.3599999999997, "text": " am going to end up with exactly the same thing that I started with, because it's not turning", "tokens": [669, 516, 281, 917, 493, 365, 2293, 264, 912, 551, 300, 286, 1409, 365, 11, 570, 309, 311, 406, 6246], "temperature": 0.0, "avg_logprob": -0.12712319691975912, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3845805369783193e-05}, {"id": 566, "seek": 288020, "start": 2907.3599999999997, "end": 2909.7999999999997, "text": " them into batches at all.", "tokens": [552, 666, 15245, 279, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12712319691975912, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.3845805369783193e-05}, {"id": 567, "seek": 290980, "start": 2909.8, "end": 2913.0, "text": " So I'll get back 26 things.", "tokens": [407, 286, 603, 483, 646, 7551, 721, 13], "temperature": 0.0, "avg_logprob": -0.12927791371065028, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.4285224096965976e-05}, {"id": 568, "seek": 290980, "start": 2913.0, "end": 2917.76, "text": " I can shuffle a data loader, of course.", "tokens": [286, 393, 39426, 257, 1412, 3677, 260, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.12927791371065028, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.4285224096965976e-05}, {"id": 569, "seek": 290980, "start": 2917.76, "end": 2922.1600000000003, "text": " And if I do that, I should end up with exactly the same thing that I started with, but in", "tokens": [400, 498, 286, 360, 300, 11, 286, 820, 917, 493, 365, 2293, 264, 912, 551, 300, 286, 1409, 365, 11, 457, 294], "temperature": 0.0, "avg_logprob": -0.12927791371065028, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.4285224096965976e-05}, {"id": 570, "seek": 290980, "start": 2922.1600000000003, "end": 2923.6000000000004, "text": " a shuffled version.", "tokens": [257, 402, 33974, 3037, 13], "temperature": 0.0, "avg_logprob": -0.12927791371065028, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.4285224096965976e-05}, {"id": 571, "seek": 290980, "start": 2923.6000000000004, "end": 2928.6400000000003, "text": " And so we actually have a thing called testShuffled that checks that the two arguments have the", "tokens": [400, 370, 321, 767, 362, 257, 551, 1219, 1500, 7774, 33974, 300, 13834, 300, 264, 732, 12869, 362, 264], "temperature": 0.0, "avg_logprob": -0.12927791371065028, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.4285224096965976e-05}, {"id": 572, "seek": 290980, "start": 2928.6400000000003, "end": 2933.28, "text": " same contents in different orders.", "tokens": [912, 15768, 294, 819, 9470, 13], "temperature": 0.0, "avg_logprob": -0.12927791371065028, "compression_ratio": 1.5634517766497462, "no_speech_prob": 1.4285224096965976e-05}, {"id": 573, "seek": 293328, "start": 2933.28, "end": 2942.0400000000004, "text": " For randl, you could pass in a keyword argument whose value can be set to 0.95 or any other", "tokens": [1171, 367, 474, 75, 11, 291, 727, 1320, 294, 257, 20428, 6770, 6104, 2158, 393, 312, 992, 281, 1958, 13, 15718, 420, 604, 661], "temperature": 0.0, "avg_logprob": -0.26053058269412016, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.0451310117787216e-05}, {"id": 574, "seek": 293328, "start": 2942.0400000000004, "end": 2944.0400000000004, "text": " number less than 1.", "tokens": [1230, 1570, 813, 502, 13], "temperature": 0.0, "avg_logprob": -0.26053058269412016, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.0451310117787216e-05}, {"id": 575, "seek": 293328, "start": 2944.0400000000004, "end": 2947.6400000000003, "text": " I'm not sure I understand that.", "tokens": [286, 478, 406, 988, 286, 1223, 300, 13], "temperature": 0.0, "avg_logprob": -0.26053058269412016, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.0451310117787216e-05}, {"id": 576, "seek": 293328, "start": 2947.6400000000003, "end": 2948.6400000000003, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.26053058269412016, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.0451310117787216e-05}, {"id": 577, "seek": 293328, "start": 2948.6400000000003, "end": 2949.6400000000003, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.26053058269412016, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.0451310117787216e-05}, {"id": 578, "seek": 293328, "start": 2949.6400000000003, "end": 2956.84, "text": " So that's what that is.", "tokens": [407, 300, 311, 437, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.26053058269412016, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.0451310117787216e-05}, {"id": 579, "seek": 293328, "start": 2956.84, "end": 2962.5600000000004, "text": " Something else that you can do is, you can see I'm passing in a data set here to my data", "tokens": [6595, 1646, 300, 291, 393, 360, 307, 11, 291, 393, 536, 286, 478, 8437, 294, 257, 1412, 992, 510, 281, 452, 1412], "temperature": 0.0, "avg_logprob": -0.26053058269412016, "compression_ratio": 1.459016393442623, "no_speech_prob": 1.0451310117787216e-05}, {"id": 580, "seek": 296256, "start": 2962.56, "end": 2964.16, "text": " loader, which is pretty normal.", "tokens": [3677, 260, 11, 597, 307, 1238, 2710, 13], "temperature": 0.0, "avg_logprob": -0.14263861754844928, "compression_ratio": 1.6576923076923078, "no_speech_prob": 7.071694653859595e-06}, {"id": 581, "seek": 296256, "start": 2964.16, "end": 2971.24, "text": " But sometimes your data set might not have a done to get item.", "tokens": [583, 2171, 428, 1412, 992, 1062, 406, 362, 257, 1096, 281, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.14263861754844928, "compression_ratio": 1.6576923076923078, "no_speech_prob": 7.071694653859595e-06}, {"id": 582, "seek": 296256, "start": 2971.24, "end": 2974.32, "text": " So you might have a data set that only has a next.", "tokens": [407, 291, 1062, 362, 257, 1412, 992, 300, 787, 575, 257, 958, 13], "temperature": 0.0, "avg_logprob": -0.14263861754844928, "compression_ratio": 1.6576923076923078, "no_speech_prob": 7.071694653859595e-06}, {"id": 583, "seek": 296256, "start": 2974.32, "end": 2979.0, "text": " And so this would be the case if, for example, your data set was some kind of infinite stream,", "tokens": [400, 370, 341, 576, 312, 264, 1389, 498, 11, 337, 1365, 11, 428, 1412, 992, 390, 512, 733, 295, 13785, 4309, 11], "temperature": 0.0, "avg_logprob": -0.14263861754844928, "compression_ratio": 1.6576923076923078, "no_speech_prob": 7.071694653859595e-06}, {"id": 584, "seek": 296256, "start": 2979.0, "end": 2982.36, "text": " like a network link.", "tokens": [411, 257, 3209, 2113, 13], "temperature": 0.0, "avg_logprob": -0.14263861754844928, "compression_ratio": 1.6576923076923078, "no_speech_prob": 7.071694653859595e-06}, {"id": 585, "seek": 296256, "start": 2982.36, "end": 2987.7999999999997, "text": " Or it could be like some coming from a file system containing hundreds of millions of", "tokens": [1610, 309, 727, 312, 411, 512, 1348, 490, 257, 3991, 1185, 19273, 6779, 295, 6803, 295], "temperature": 0.0, "avg_logprob": -0.14263861754844928, "compression_ratio": 1.6576923076923078, "no_speech_prob": 7.071694653859595e-06}, {"id": 586, "seek": 296256, "start": 2987.7999999999997, "end": 2991.6, "text": " files, and it's so big you don't want to enumerate the whole lot, or something like", "tokens": [7098, 11, 293, 309, 311, 370, 955, 291, 500, 380, 528, 281, 465, 15583, 473, 264, 1379, 688, 11, 420, 746, 411], "temperature": 0.0, "avg_logprob": -0.14263861754844928, "compression_ratio": 1.6576923076923078, "no_speech_prob": 7.071694653859595e-06}, {"id": 587, "seek": 299160, "start": 2991.6, "end": 2992.6, "text": " that.", "tokens": [300, 13], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 588, "seek": 299160, "start": 2992.6, "end": 2996.8399999999997, "text": " And again, this is something that's also in PyTorch 1.2.", "tokens": [400, 797, 11, 341, 307, 746, 300, 311, 611, 294, 9953, 51, 284, 339, 502, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 589, "seek": 299160, "start": 2996.8399999999997, "end": 3001.4, "text": " It's the idea of kind of non-indexed or iterable data sets.", "tokens": [467, 311, 264, 1558, 295, 733, 295, 2107, 12, 471, 3121, 292, 420, 17138, 712, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 590, "seek": 299160, "start": 3001.4, "end": 3008.2, "text": " By default, our data loader will check whether this has a done to get item.", "tokens": [3146, 7576, 11, 527, 1412, 3677, 260, 486, 1520, 1968, 341, 575, 257, 1096, 281, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 591, "seek": 299160, "start": 3008.2, "end": 3010.72, "text": " And if it doesn't, it'll treat it as iterable.", "tokens": [400, 498, 309, 1177, 380, 11, 309, 603, 2387, 309, 382, 17138, 712, 13], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 592, "seek": 299160, "start": 3010.72, "end": 3012.96, "text": " If it does, it'll treat it as indexed.", "tokens": [759, 309, 775, 11, 309, 603, 2387, 309, 382, 8186, 292, 13], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 593, "seek": 299160, "start": 3012.96, "end": 3016.56, "text": " But you can override that by saying indexed equals false.", "tokens": [583, 291, 393, 42321, 300, 538, 1566, 8186, 292, 6915, 7908, 13], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 594, "seek": 299160, "start": 3016.56, "end": 3019.8399999999997, "text": " So this case is going to give you the same, exactly the same thing as before, but it's", "tokens": [407, 341, 1389, 307, 516, 281, 976, 291, 264, 912, 11, 2293, 264, 912, 551, 382, 949, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.16212982521917588, "compression_ratio": 1.6692607003891051, "no_speech_prob": 1.9637911918835016e-06}, {"id": 595, "seek": 301984, "start": 3019.84, "end": 3025.6800000000003, "text": " doing it by calling next rather than done to get item.", "tokens": [884, 309, 538, 5141, 958, 2831, 813, 1096, 281, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.3129026239568537, "compression_ratio": 1.4745762711864407, "no_speech_prob": 3.555906459951075e-06}, {"id": 596, "seek": 301984, "start": 3025.6800000000003, "end": 3029.88, "text": " But this is useful, as I say, if you've got really, really, really huge data sets or data", "tokens": [583, 341, 307, 4420, 11, 382, 286, 584, 11, 498, 291, 600, 658, 534, 11, 534, 11, 534, 2603, 1412, 6352, 420, 1412], "temperature": 0.0, "avg_logprob": -0.3129026239568537, "compression_ratio": 1.4745762711864407, "no_speech_prob": 3.555906459951075e-06}, {"id": 597, "seek": 301984, "start": 3029.88, "end": 3034.2000000000003, "text": " sets that are coming over the network or something like that.", "tokens": [6352, 300, 366, 1348, 670, 264, 3209, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.3129026239568537, "compression_ratio": 1.4745762711864407, "no_speech_prob": 3.555906459951075e-06}, {"id": 598, "seek": 301984, "start": 3034.2000000000003, "end": 3037.6200000000003, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.3129026239568537, "compression_ratio": 1.4745762711864407, "no_speech_prob": 3.555906459951075e-06}, {"id": 599, "seek": 301984, "start": 3037.6200000000003, "end": 3043.7200000000003, "text": " So that is some of that behavior.", "tokens": [407, 300, 307, 512, 295, 300, 5223, 13], "temperature": 0.0, "avg_logprob": -0.3129026239568537, "compression_ratio": 1.4745762711864407, "no_speech_prob": 3.555906459951075e-06}, {"id": 600, "seek": 301984, "start": 3043.7200000000003, "end": 3045.6000000000004, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.3129026239568537, "compression_ratio": 1.4745762711864407, "no_speech_prob": 3.555906459951075e-06}, {"id": 601, "seek": 301984, "start": 3045.6000000000004, "end": 3048.32, "text": " Absolutely.", "tokens": [7021, 13], "temperature": 0.0, "avg_logprob": -0.3129026239568537, "compression_ratio": 1.4745762711864407, "no_speech_prob": 3.555906459951075e-06}, {"id": 602, "seek": 304832, "start": 3048.32, "end": 3054.7200000000003, "text": " You can change it to something that comes as a keyword argument.", "tokens": [509, 393, 1319, 309, 281, 746, 300, 1487, 382, 257, 20428, 6770, 13], "temperature": 0.0, "avg_logprob": -0.1637584410215679, "compression_ratio": 1.502415458937198, "no_speech_prob": 6.240745278773829e-06}, {"id": 603, "seek": 304832, "start": 3054.7200000000003, "end": 3055.7200000000003, "text": " Sure.", "tokens": [4894, 13], "temperature": 0.0, "avg_logprob": -0.1637584410215679, "compression_ratio": 1.502415458937198, "no_speech_prob": 6.240745278773829e-06}, {"id": 604, "seek": 304832, "start": 3055.7200000000003, "end": 3056.7200000000003, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1637584410215679, "compression_ratio": 1.502415458937198, "no_speech_prob": 6.240745278773829e-06}, {"id": 605, "seek": 304832, "start": 3056.7200000000003, "end": 3062.92, "text": " So these are kind of more just interesting tests rather than additional functionality.", "tokens": [407, 613, 366, 733, 295, 544, 445, 1880, 6921, 2831, 813, 4497, 14980, 13], "temperature": 0.0, "avg_logprob": -0.1637584410215679, "compression_ratio": 1.502415458937198, "no_speech_prob": 6.240745278773829e-06}, {"id": 606, "seek": 304832, "start": 3062.92, "end": 3068.44, "text": " But in a data loader, when you have multiple workers, one of the things that's difficult", "tokens": [583, 294, 257, 1412, 3677, 260, 11, 562, 291, 362, 3866, 5600, 11, 472, 295, 264, 721, 300, 311, 2252], "temperature": 0.0, "avg_logprob": -0.1637584410215679, "compression_ratio": 1.502415458937198, "no_speech_prob": 6.240745278773829e-06}, {"id": 607, "seek": 304832, "start": 3068.44, "end": 3072.0, "text": " is ensuring that everything comes back in the correct order.", "tokens": [307, 16882, 300, 1203, 1487, 646, 294, 264, 3006, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1637584410215679, "compression_ratio": 1.502415458937198, "no_speech_prob": 6.240745278773829e-06}, {"id": 608, "seek": 307200, "start": 3072.0, "end": 3081.04, "text": " So here I've created a data loader that simply returns the item from a list.", "tokens": [407, 510, 286, 600, 2942, 257, 1412, 3677, 260, 300, 2935, 11247, 264, 3174, 490, 257, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1235054089472844, "compression_ratio": 1.6300813008130082, "no_speech_prob": 8.059373044488893e-07}, {"id": 609, "seek": 307200, "start": 3081.04, "end": 3083.28, "text": " So I've overwritten list.", "tokens": [407, 286, 600, 670, 26859, 1329, 13], "temperature": 0.0, "avg_logprob": -0.1235054089472844, "compression_ratio": 1.6300813008130082, "no_speech_prob": 8.059373044488893e-07}, {"id": 610, "seek": 307200, "start": 3083.28, "end": 3086.08, "text": " That adds a random bit of sleep each time.", "tokens": [663, 10860, 257, 4974, 857, 295, 2817, 1184, 565, 13], "temperature": 0.0, "avg_logprob": -0.1235054089472844, "compression_ratio": 1.6300813008130082, "no_speech_prob": 8.059373044488893e-07}, {"id": 611, "seek": 307200, "start": 3086.08, "end": 3090.74, "text": " And so there are just some tests here, as you can see, that check that even if we have", "tokens": [400, 370, 456, 366, 445, 512, 6921, 510, 11, 382, 291, 393, 536, 11, 300, 1520, 300, 754, 498, 321, 362], "temperature": 0.0, "avg_logprob": -0.1235054089472844, "compression_ratio": 1.6300813008130082, "no_speech_prob": 8.059373044488893e-07}, {"id": 612, "seek": 307200, "start": 3090.74, "end": 3094.72, "text": " multiple workers, that we actually get back the results in the correct order.", "tokens": [3866, 5600, 11, 300, 321, 767, 483, 646, 264, 3542, 294, 264, 3006, 1668, 13], "temperature": 0.0, "avg_logprob": -0.1235054089472844, "compression_ratio": 1.6300813008130082, "no_speech_prob": 8.059373044488893e-07}, {"id": 613, "seek": 307200, "start": 3094.72, "end": 3100.52, "text": " And you can also see by running percent time, we can observe that it is actually using the", "tokens": [400, 291, 393, 611, 536, 538, 2614, 3043, 565, 11, 321, 393, 11441, 300, 309, 307, 767, 1228, 264], "temperature": 0.0, "avg_logprob": -0.1235054089472844, "compression_ratio": 1.6300813008130082, "no_speech_prob": 8.059373044488893e-07}, {"id": 614, "seek": 310052, "start": 3100.52, "end": 3107.08, "text": " additional workers to run more quickly.", "tokens": [4497, 5600, 281, 1190, 544, 2661, 13], "temperature": 0.0, "avg_logprob": -0.13242220878601074, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.339171704894397e-06}, {"id": 615, "seek": 310052, "start": 3107.08, "end": 3109.28, "text": " And so here's a similar example.", "tokens": [400, 370, 510, 311, 257, 2531, 1365, 13], "temperature": 0.0, "avg_logprob": -0.13242220878601074, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.339171704894397e-06}, {"id": 616, "seek": 310052, "start": 3109.28, "end": 3113.72, "text": " At this time, I've simulated a queue.", "tokens": [1711, 341, 565, 11, 286, 600, 41713, 257, 18639, 13], "temperature": 0.0, "avg_logprob": -0.13242220878601074, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.339171704894397e-06}, {"id": 617, "seek": 310052, "start": 3113.72, "end": 3117.68, "text": " So this is an example that only has done to error, doesn't have done to get item.", "tokens": [407, 341, 307, 364, 1365, 300, 787, 575, 1096, 281, 6713, 11, 1177, 380, 362, 1096, 281, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.13242220878601074, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.339171704894397e-06}, {"id": 618, "seek": 310052, "start": 3117.68, "end": 3123.58, "text": " So if I put this into my data loader, then it's only able to iterate.", "tokens": [407, 498, 286, 829, 341, 666, 452, 1412, 3677, 260, 11, 550, 309, 311, 787, 1075, 281, 44497, 13], "temperature": 0.0, "avg_logprob": -0.13242220878601074, "compression_ratio": 1.5144508670520231, "no_speech_prob": 6.339171704894397e-06}, {"id": 619, "seek": 312358, "start": 3123.58, "end": 3134.64, "text": " So here's a really good example of what it looks like to work with an iteratable only", "tokens": [407, 510, 311, 257, 534, 665, 1365, 295, 437, 309, 1542, 411, 281, 589, 365, 364, 17138, 31415, 787], "temperature": 0.0, "avg_logprob": -0.13651296827528211, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.63780543213943e-06}, {"id": 620, "seek": 312358, "start": 3134.64, "end": 3136.52, "text": " queue with variable latency.", "tokens": [18639, 365, 7006, 27043, 13], "temperature": 0.0, "avg_logprob": -0.13651296827528211, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.63780543213943e-06}, {"id": 621, "seek": 312358, "start": 3136.52, "end": 3142.6, "text": " This is kind of like what something streaming over a network, for example, might look like.", "tokens": [639, 307, 733, 295, 411, 437, 746, 11791, 670, 257, 3209, 11, 337, 1365, 11, 1062, 574, 411, 13], "temperature": 0.0, "avg_logprob": -0.13651296827528211, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.63780543213943e-06}, {"id": 622, "seek": 312358, "start": 3142.6, "end": 3147.1, "text": " And so here's a test that we get back the right thing.", "tokens": [400, 370, 510, 311, 257, 1500, 300, 321, 483, 646, 264, 558, 551, 13], "temperature": 0.0, "avg_logprob": -0.13651296827528211, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.63780543213943e-06}, {"id": 623, "seek": 312358, "start": 3147.1, "end": 3152.64, "text": " And now in this case, because our sleepy queue only has a done to error, it doesn't have", "tokens": [400, 586, 294, 341, 1389, 11, 570, 527, 24908, 18639, 787, 575, 257, 1096, 281, 6713, 11, 309, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.13651296827528211, "compression_ratio": 1.6203703703703705, "no_speech_prob": 4.63780543213943e-06}, {"id": 624, "seek": 315264, "start": 3152.64, "end": 3153.96, "text": " done to get item.", "tokens": [1096, 281, 483, 3174, 13], "temperature": 0.0, "avg_logprob": -0.1715136678595292, "compression_ratio": 1.624413145539906, "no_speech_prob": 8.664286724524572e-06}, {"id": 625, "seek": 315264, "start": 3153.96, "end": 3157.14, "text": " That means there's no guarantee about what order things will come back in.", "tokens": [663, 1355, 456, 311, 572, 10815, 466, 437, 1668, 721, 486, 808, 646, 294, 13], "temperature": 0.0, "avg_logprob": -0.1715136678595292, "compression_ratio": 1.624413145539906, "no_speech_prob": 8.664286724524572e-06}, {"id": 626, "seek": 315264, "start": 3157.14, "end": 3163.2799999999997, "text": " So we end up with something that's shuffled this time, which I think answers the question", "tokens": [407, 321, 917, 493, 365, 746, 300, 311, 402, 33974, 341, 565, 11, 597, 286, 519, 6338, 264, 1168], "temperature": 0.0, "avg_logprob": -0.1715136678595292, "compression_ratio": 1.624413145539906, "no_speech_prob": 8.664286724524572e-06}, {"id": 627, "seek": 315264, "start": 3163.2799999999997, "end": 3169.24, "text": " of juvion111, which is what would happen if shuffle and indexed are both false.", "tokens": [295, 361, 9350, 313, 5348, 16, 11, 597, 307, 437, 576, 1051, 498, 39426, 293, 8186, 292, 366, 1293, 7908, 13], "temperature": 0.0, "avg_logprob": -0.1715136678595292, "compression_ratio": 1.624413145539906, "no_speech_prob": 8.664286724524572e-06}, {"id": 628, "seek": 315264, "start": 3169.24, "end": 3171.7599999999998, "text": " So it's a great question.", "tokens": [407, 309, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1715136678595292, "compression_ratio": 1.624413145539906, "no_speech_prob": 8.664286724524572e-06}, {"id": 629, "seek": 315264, "start": 3171.7599999999998, "end": 3176.92, "text": " So if indexed is false, then shuffle doesn't do anything.", "tokens": [407, 498, 8186, 292, 307, 7908, 11, 550, 39426, 1177, 380, 360, 1340, 13], "temperature": 0.0, "avg_logprob": -0.1715136678595292, "compression_ratio": 1.624413145539906, "no_speech_prob": 8.664286724524572e-06}, {"id": 630, "seek": 317692, "start": 3176.92, "end": 3188.16, "text": " And specifically what happens is, in terms of how it's implemented, the sampler, here", "tokens": [400, 4682, 437, 2314, 307, 11, 294, 2115, 295, 577, 309, 311, 12270, 11, 264, 3247, 22732, 11, 510], "temperature": 0.0, "avg_logprob": -0.15128864347934723, "compression_ratio": 1.7153284671532847, "no_speech_prob": 2.026125912379939e-06}, {"id": 631, "seek": 317692, "start": 3188.16, "end": 3198.7200000000003, "text": " is the sampler, if you have indexed, then it returns an infinite list of integers.", "tokens": [307, 264, 3247, 22732, 11, 498, 291, 362, 8186, 292, 11, 550, 309, 11247, 364, 13785, 1329, 295, 41674, 13], "temperature": 0.0, "avg_logprob": -0.15128864347934723, "compression_ratio": 1.7153284671532847, "no_speech_prob": 2.026125912379939e-06}, {"id": 632, "seek": 317692, "start": 3198.7200000000003, "end": 3205.08, "text": " And if it's not indexed, then it returns an infinite list of none.", "tokens": [400, 498, 309, 311, 406, 8186, 292, 11, 550, 309, 11247, 364, 13785, 1329, 295, 6022, 13], "temperature": 0.0, "avg_logprob": -0.15128864347934723, "compression_ratio": 1.7153284671532847, "no_speech_prob": 2.026125912379939e-06}, {"id": 633, "seek": 320508, "start": 3205.08, "end": 3209.4, "text": " So when you shuffle, then you're going to end up with an infinite list of none in shuffled", "tokens": [407, 562, 291, 39426, 11, 550, 291, 434, 516, 281, 917, 493, 365, 364, 13785, 1329, 295, 6022, 294, 402, 33974], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 634, "seek": 320508, "start": 3209.4, "end": 3210.7999999999997, "text": " order.", "tokens": [1668, 13], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 635, "seek": 320508, "start": 3210.7999999999997, "end": 3217.24, "text": " So it doesn't actually do anything interesting.", "tokens": [407, 309, 1177, 380, 767, 360, 1340, 1880, 13], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 636, "seek": 320508, "start": 3217.24, "end": 3220.96, "text": " And so this is an interesting little class which we'll get to, but basically for creating", "tokens": [400, 370, 341, 307, 364, 1880, 707, 1508, 597, 321, 603, 483, 281, 11, 457, 1936, 337, 4084], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 637, "seek": 320508, "start": 3220.96, "end": 3221.96, "text": " infinite lists.", "tokens": [13785, 14511, 13], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 638, "seek": 320508, "start": 3221.96, "end": 3226.44, "text": " You'll see, like a lot of the stuff we're doing is much more functional than a lot of", "tokens": [509, 603, 536, 11, 411, 257, 688, 295, 264, 1507, 321, 434, 884, 307, 709, 544, 11745, 813, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 639, "seek": 320508, "start": 3226.44, "end": 3231.12, "text": " normal Python programming, which makes it kind of easier to write, easier to understand,", "tokens": [2710, 15329, 9410, 11, 597, 1669, 309, 733, 295, 3571, 281, 2464, 11, 3571, 281, 1223, 11], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 640, "seek": 320508, "start": 3231.12, "end": 3232.12, "text": " more concise.", "tokens": [544, 44882, 13], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 641, "seek": 320508, "start": 3232.12, "end": 3234.92, "text": " But we kind of had to create some things to make it easier.", "tokens": [583, 321, 733, 295, 632, 281, 1884, 512, 721, 281, 652, 309, 3571, 13], "temperature": 0.0, "avg_logprob": -0.16957740485668182, "compression_ratio": 1.7241379310344827, "no_speech_prob": 1.0451347407069989e-05}, {"id": 642, "seek": 323492, "start": 3234.92, "end": 3240.28, "text": " So things like infinite lists are things which are pretty important in this style of programming.", "tokens": [407, 721, 411, 13785, 14511, 366, 721, 597, 366, 1238, 1021, 294, 341, 3758, 295, 9410, 13], "temperature": 0.0, "avg_logprob": -0.13323777721774194, "compression_ratio": 1.7591836734693878, "no_speech_prob": 1.3845529792888556e-05}, {"id": 643, "seek": 323492, "start": 3240.28, "end": 3248.8, "text": " So that's why we have a little class to easily create these two main types of infinite lists.", "tokens": [407, 300, 311, 983, 321, 362, 257, 707, 1508, 281, 3612, 1884, 613, 732, 2135, 3467, 295, 13785, 14511, 13], "temperature": 0.0, "avg_logprob": -0.13323777721774194, "compression_ratio": 1.7591836734693878, "no_speech_prob": 1.3845529792888556e-05}, {"id": 644, "seek": 323492, "start": 3248.8, "end": 3252.88, "text": " And so if you haven't done much kind of more functional style programming in Python, definitely", "tokens": [400, 370, 498, 291, 2378, 380, 1096, 709, 733, 295, 544, 11745, 3758, 9410, 294, 15329, 11, 2138], "temperature": 0.0, "avg_logprob": -0.13323777721774194, "compression_ratio": 1.7591836734693878, "no_speech_prob": 1.3845529792888556e-05}, {"id": 645, "seek": 323492, "start": 3252.88, "end": 3258.4, "text": " check out the iter tools standard library package, because that contains lots and lots", "tokens": [1520, 484, 264, 17138, 3873, 3832, 6405, 7372, 11, 570, 300, 8306, 3195, 293, 3195], "temperature": 0.0, "avg_logprob": -0.13323777721774194, "compression_ratio": 1.7591836734693878, "no_speech_prob": 1.3845529792888556e-05}, {"id": 646, "seek": 323492, "start": 3258.4, "end": 3263.08, "text": " and lots of useful things for this style of programming.", "tokens": [293, 3195, 295, 4420, 721, 337, 341, 3758, 295, 9410, 13], "temperature": 0.0, "avg_logprob": -0.13323777721774194, "compression_ratio": 1.7591836734693878, "no_speech_prob": 1.3845529792888556e-05}, {"id": 647, "seek": 326308, "start": 3263.08, "end": 3268.4, "text": " And as you can see, for example, it has a way to slice into potentially infinite iterators.", "tokens": [400, 382, 291, 393, 536, 11, 337, 1365, 11, 309, 575, 257, 636, 281, 13153, 666, 7263, 13785, 17138, 3391, 13], "temperature": 0.0, "avg_logprob": -0.19095137278238933, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.89248221431626e-05}, {"id": 648, "seek": 326308, "start": 3268.4, "end": 3275.04, "text": " So this is how you get the first n things out of a potentially infinitely sized list.", "tokens": [407, 341, 307, 577, 291, 483, 264, 700, 297, 721, 484, 295, 257, 7263, 36227, 20004, 1329, 13], "temperature": 0.0, "avg_logprob": -0.19095137278238933, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.89248221431626e-05}, {"id": 649, "seek": 326308, "start": 3275.04, "end": 3281.08, "text": " In this case, it definitely is an infinitely sized list.", "tokens": [682, 341, 1389, 11, 309, 2138, 307, 364, 36227, 20004, 1329, 13], "temperature": 0.0, "avg_logprob": -0.19095137278238933, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.89248221431626e-05}, {"id": 650, "seek": 326308, "start": 3281.08, "end": 3282.84, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.19095137278238933, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.89248221431626e-05}, {"id": 651, "seek": 326308, "start": 3282.84, "end": 3287.52, "text": " So let's look at some interesting things here.", "tokens": [407, 718, 311, 574, 412, 512, 1880, 721, 510, 13], "temperature": 0.0, "avg_logprob": -0.19095137278238933, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.89248221431626e-05}, {"id": 652, "seek": 328752, "start": 3287.52, "end": 3294.84, "text": " One is that randl doesn't have to be written like this.", "tokens": [1485, 307, 300, 367, 474, 75, 1177, 380, 362, 281, 312, 3720, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1669498737041767, "compression_ratio": 1.5347222222222223, "no_speech_prob": 1.64421035151463e-05}, {"id": 653, "seek": 328752, "start": 3294.84, "end": 3305.16, "text": " What we could do is we could create a function called create, or we could say a function", "tokens": [708, 321, 727, 360, 307, 321, 727, 1884, 257, 2445, 1219, 1884, 11, 420, 321, 727, 584, 257, 2445], "temperature": 0.0, "avg_logprob": -0.1669498737041767, "compression_ratio": 1.5347222222222223, "no_speech_prob": 1.64421035151463e-05}, {"id": 654, "seek": 328752, "start": 3305.16, "end": 3312.72, "text": " called rand item, like so.", "tokens": [1219, 367, 474, 3174, 11, 411, 370, 13], "temperature": 0.0, "avg_logprob": -0.1669498737041767, "compression_ratio": 1.5347222222222223, "no_speech_prob": 1.64421035151463e-05}, {"id": 655, "seek": 328752, "start": 3312.72, "end": 3315.08, "text": " So we're now not inheriting from the data loader.", "tokens": [407, 321, 434, 586, 406, 9484, 1748, 490, 264, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.1669498737041767, "compression_ratio": 1.5347222222222223, "no_speech_prob": 1.64421035151463e-05}, {"id": 656, "seek": 331508, "start": 3315.08, "end": 3317.6, "text": " We're just creating a function called rand item.", "tokens": [492, 434, 445, 4084, 257, 2445, 1219, 367, 474, 3174, 13], "temperature": 0.0, "avg_logprob": -0.32675883349250345, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.0129724614671431e-05}, {"id": 657, "seek": 331508, "start": 3317.6, "end": 3324.24, "text": " And instead of creating a randl, we could create a normal data loader.", "tokens": [400, 2602, 295, 4084, 257, 367, 474, 75, 11, 321, 727, 1884, 257, 2710, 1412, 3677, 260, 13], "temperature": 0.0, "avg_logprob": -0.32675883349250345, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.0129724614671431e-05}, {"id": 658, "seek": 331508, "start": 3324.24, "end": 3332.68, "text": " But then we're going to pass in create item equals underscore rand item.", "tokens": [583, 550, 321, 434, 516, 281, 1320, 294, 1884, 3174, 6915, 37556, 367, 474, 3174, 13], "temperature": 0.0, "avg_logprob": -0.32675883349250345, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.0129724614671431e-05}, {"id": 659, "seek": 331508, "start": 3332.68, "end": 3333.68, "text": " Oops.", "tokens": [21726, 13], "temperature": 0.0, "avg_logprob": -0.32675883349250345, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.0129724614671431e-05}, {"id": 660, "seek": 331508, "start": 3333.68, "end": 3336.68, "text": " And item.", "tokens": [400, 3174, 13], "temperature": 0.0, "avg_logprob": -0.32675883349250345, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.0129724614671431e-05}, {"id": 661, "seek": 331508, "start": 3336.68, "end": 3340.7999999999997, "text": " Ah, yes.", "tokens": [2438, 11, 2086, 13], "temperature": 0.0, "avg_logprob": -0.32675883349250345, "compression_ratio": 1.5069444444444444, "no_speech_prob": 1.0129724614671431e-05}, {"id": 662, "seek": 334080, "start": 3340.8, "end": 3346.6000000000004, "text": " And so that's now not going to take a self.", "tokens": [400, 370, 300, 311, 586, 406, 516, 281, 747, 257, 2698, 13], "temperature": 0.0, "avg_logprob": -0.2145819523755242, "compression_ratio": 1.4223602484472049, "no_speech_prob": 5.862672423972981e-06}, {"id": 663, "seek": 334080, "start": 3346.6000000000004, "end": 3348.6000000000004, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.2145819523755242, "compression_ratio": 1.4223602484472049, "no_speech_prob": 5.862672423972981e-06}, {"id": 664, "seek": 334080, "start": 3348.6000000000004, "end": 3350.88, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.2145819523755242, "compression_ratio": 1.4223602484472049, "no_speech_prob": 5.862672423972981e-06}, {"id": 665, "seek": 334080, "start": 3350.88, "end": 3355.8, "text": " So as you can see, this is now doing exactly the same thing.", "tokens": [407, 382, 291, 393, 536, 11, 341, 307, 586, 884, 2293, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.2145819523755242, "compression_ratio": 1.4223602484472049, "no_speech_prob": 5.862672423972981e-06}, {"id": 666, "seek": 334080, "start": 3355.8, "end": 3360.88, "text": " That and that are identical.", "tokens": [663, 293, 300, 366, 14800, 13], "temperature": 0.0, "avg_logprob": -0.2145819523755242, "compression_ratio": 1.4223602484472049, "no_speech_prob": 5.862672423972981e-06}, {"id": 667, "seek": 334080, "start": 3360.88, "end": 3365.7200000000003, "text": " And the reason why is we've kind of added this really nice functionality that we use", "tokens": [400, 264, 1778, 983, 307, 321, 600, 733, 295, 3869, 341, 534, 1481, 14980, 300, 321, 764], "temperature": 0.0, "avg_logprob": -0.2145819523755242, "compression_ratio": 1.4223602484472049, "no_speech_prob": 5.862672423972981e-06}, {"id": 668, "seek": 336572, "start": 3365.72, "end": 3372.9199999999996, "text": " all over the place where nearly anywhere that we kind of add callbacks or customizability", "tokens": [439, 670, 264, 1081, 689, 6217, 4992, 300, 321, 733, 295, 909, 818, 17758, 420, 2375, 590, 2310], "temperature": 0.0, "avg_logprob": -0.10564014945231692, "compression_ratio": 1.5204081632653061, "no_speech_prob": 6.681429454147292e-07}, {"id": 669, "seek": 336572, "start": 3372.9199999999996, "end": 3383.0, "text": " through inheritance, you can also do it by passing in a parameter with the name of the", "tokens": [807, 32122, 11, 291, 393, 611, 360, 309, 538, 8437, 294, 257, 13075, 365, 264, 1315, 295, 264], "temperature": 0.0, "avg_logprob": -0.10564014945231692, "compression_ratio": 1.5204081632653061, "no_speech_prob": 6.681429454147292e-07}, {"id": 670, "seek": 336572, "start": 3383.0, "end": 3386.6, "text": " method that you want to override.", "tokens": [3170, 300, 291, 528, 281, 42321, 13], "temperature": 0.0, "avg_logprob": -0.10564014945231692, "compression_ratio": 1.5204081632653061, "no_speech_prob": 6.681429454147292e-07}, {"id": 671, "seek": 336572, "start": 3386.6, "end": 3392.9199999999996, "text": " And we use this quite a lot because if you've just got some simple little function that", "tokens": [400, 321, 764, 341, 1596, 257, 688, 570, 498, 291, 600, 445, 658, 512, 2199, 707, 2445, 300], "temperature": 0.0, "avg_logprob": -0.10564014945231692, "compression_ratio": 1.5204081632653061, "no_speech_prob": 6.681429454147292e-07}, {"id": 672, "seek": 339292, "start": 3392.92, "end": 3400.0, "text": " you want to change, overriding is just more awkward than we would like you to force you", "tokens": [291, 528, 281, 1319, 11, 670, 81, 2819, 307, 445, 544, 11411, 813, 321, 576, 411, 291, 281, 3464, 291], "temperature": 0.0, "avg_logprob": -0.11604315563313013, "compression_ratio": 1.7092511013215859, "no_speech_prob": 4.425328825163888e-06}, {"id": 673, "seek": 339292, "start": 3400.0, "end": 3401.0, "text": " to do.", "tokens": [281, 360, 13], "temperature": 0.0, "avg_logprob": -0.11604315563313013, "compression_ratio": 1.7092511013215859, "no_speech_prob": 4.425328825163888e-06}, {"id": 674, "seek": 339292, "start": 3401.0, "end": 3405.7200000000003, "text": " And particularly for newer users to Python, we don't want them to force them to have to", "tokens": [400, 4098, 337, 17628, 5022, 281, 15329, 11, 321, 500, 380, 528, 552, 281, 3464, 552, 281, 362, 281], "temperature": 0.0, "avg_logprob": -0.11604315563313013, "compression_ratio": 1.7092511013215859, "no_speech_prob": 4.425328825163888e-06}, {"id": 675, "seek": 339292, "start": 3405.7200000000003, "end": 3410.92, "text": " understand inheritance and OO and stuff like that.", "tokens": [1223, 32122, 293, 422, 46, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.11604315563313013, "compression_ratio": 1.7092511013215859, "no_speech_prob": 4.425328825163888e-06}, {"id": 676, "seek": 339292, "start": 3410.92, "end": 3415.04, "text": " So this is kind of much easier for new users for a lot of things.", "tokens": [407, 341, 307, 733, 295, 709, 3571, 337, 777, 5022, 337, 257, 688, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.11604315563313013, "compression_ratio": 1.7092511013215859, "no_speech_prob": 4.425328825163888e-06}, {"id": 677, "seek": 339292, "start": 3415.04, "end": 3419.4, "text": " So for a lot of our documentation and lessons and stuff like that, we'll be able to do a", "tokens": [407, 337, 257, 688, 295, 527, 14333, 293, 8820, 293, 1507, 411, 300, 11, 321, 603, 312, 1075, 281, 360, 257], "temperature": 0.0, "avg_logprob": -0.11604315563313013, "compression_ratio": 1.7092511013215859, "no_speech_prob": 4.425328825163888e-06}, {"id": 678, "seek": 341940, "start": 3419.4, "end": 3427.04, "text": " lot of stuff without having to first teach OO and inheritance and stuff like that.", "tokens": [688, 295, 1507, 1553, 1419, 281, 700, 2924, 422, 46, 293, 32122, 293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1444343990749783, "compression_ratio": 1.5844748858447488, "no_speech_prob": 1.8924161850009114e-05}, {"id": 679, "seek": 341940, "start": 3427.04, "end": 3430.04, "text": " The way it works behind the scenes is super fun.", "tokens": [440, 636, 309, 1985, 2261, 264, 8026, 307, 1687, 1019, 13], "temperature": 0.0, "avg_logprob": -0.1444343990749783, "compression_ratio": 1.5844748858447488, "no_speech_prob": 1.8924161850009114e-05}, {"id": 680, "seek": 341940, "start": 3430.04, "end": 3433.36, "text": " So let's have a look at create item.", "tokens": [407, 718, 311, 362, 257, 574, 412, 1884, 3174, 13], "temperature": 0.0, "avg_logprob": -0.1444343990749783, "compression_ratio": 1.5844748858447488, "no_speech_prob": 1.8924161850009114e-05}, {"id": 681, "seek": 341940, "start": 3433.36, "end": 3437.02, "text": " And yes, great question from Hiromi.", "tokens": [400, 2086, 11, 869, 1168, 490, 23192, 9220, 13], "temperature": 0.0, "avg_logprob": -0.1444343990749783, "compression_ratio": 1.5844748858447488, "no_speech_prob": 1.8924161850009114e-05}, {"id": 682, "seek": 341940, "start": 3437.02, "end": 3441.44, "text": " When you do it this way, you don't get to use any state because you don't get passed", "tokens": [1133, 291, 360, 309, 341, 636, 11, 291, 500, 380, 483, 281, 764, 604, 1785, 570, 291, 500, 380, 483, 4678], "temperature": 0.0, "avg_logprob": -0.1444343990749783, "compression_ratio": 1.5844748858447488, "no_speech_prob": 1.8924161850009114e-05}, {"id": 683, "seek": 341940, "start": 3441.44, "end": 3443.28, "text": " in self.", "tokens": [294, 2698, 13], "temperature": 0.0, "avg_logprob": -0.1444343990749783, "compression_ratio": 1.5844748858447488, "no_speech_prob": 1.8924161850009114e-05}, {"id": 684, "seek": 341940, "start": 3443.28, "end": 3446.92, "text": " So if you want state, then you have to inherit.", "tokens": [407, 498, 291, 528, 1785, 11, 550, 291, 362, 281, 21389, 13], "temperature": 0.0, "avg_logprob": -0.1444343990749783, "compression_ratio": 1.5844748858447488, "no_speech_prob": 1.8924161850009114e-05}, {"id": 685, "seek": 344692, "start": 3446.92, "end": 3451.2000000000003, "text": " If you don't care about state, then you don't.", "tokens": [759, 291, 500, 380, 1127, 466, 1785, 11, 550, 291, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.12961511513621538, "compression_ratio": 1.6414141414141414, "no_speech_prob": 7.071472282405011e-06}, {"id": 686, "seek": 344692, "start": 3451.2000000000003, "end": 3454.6800000000003, "text": " So let's see how create item works.", "tokens": [407, 718, 311, 536, 577, 1884, 3174, 1985, 13], "temperature": 0.0, "avg_logprob": -0.12961511513621538, "compression_ratio": 1.6414141414141414, "no_speech_prob": 7.071472282405011e-06}, {"id": 687, "seek": 344692, "start": 3454.6800000000003, "end": 3461.56, "text": " Create item is one of the things in this long list of underscore methods.", "tokens": [20248, 3174, 307, 472, 295, 264, 721, 294, 341, 938, 1329, 295, 37556, 7150, 13], "temperature": 0.0, "avg_logprob": -0.12961511513621538, "compression_ratio": 1.6414141414141414, "no_speech_prob": 7.071472282405011e-06}, {"id": 688, "seek": 344692, "start": 3461.56, "end": 3466.08, "text": " As you can see, it's just a list of strings.", "tokens": [1018, 291, 393, 536, 11, 309, 311, 445, 257, 1329, 295, 13985, 13], "temperature": 0.0, "avg_logprob": -0.12961511513621538, "compression_ratio": 1.6414141414141414, "no_speech_prob": 7.071472282405011e-06}, {"id": 689, "seek": 344692, "start": 3466.08, "end": 3467.84, "text": " Underscore methods is a special name.", "tokens": [2719, 433, 12352, 7150, 307, 257, 2121, 1315, 13], "temperature": 0.0, "avg_logprob": -0.12961511513621538, "compression_ratio": 1.6414141414141414, "no_speech_prob": 7.071472282405011e-06}, {"id": 690, "seek": 344692, "start": 3467.84, "end": 3475.6, "text": " And it's a special name that is going to be looked at by the funks, quags, decorator.", "tokens": [400, 309, 311, 257, 2121, 1315, 300, 307, 516, 281, 312, 2956, 412, 538, 264, 1019, 1694, 11, 421, 12109, 11, 7919, 1639, 13], "temperature": 0.0, "avg_logprob": -0.12961511513621538, "compression_ratio": 1.6414141414141414, "no_speech_prob": 7.071472282405011e-06}, {"id": 691, "seek": 347560, "start": 3475.6, "end": 3481.4, "text": " And the funks, quags, decorator is going to look for this special name.", "tokens": [400, 264, 1019, 1694, 11, 421, 12109, 11, 7919, 1639, 307, 516, 281, 574, 337, 341, 2121, 1315, 13], "temperature": 0.0, "avg_logprob": -0.08899291769250647, "compression_ratio": 1.694267515923567, "no_speech_prob": 7.29560224499437e-06}, {"id": 692, "seek": 347560, "start": 3481.4, "end": 3488.24, "text": " And it's going to say, oh, the quags in this init are not actually unknown.", "tokens": [400, 309, 311, 516, 281, 584, 11, 1954, 11, 264, 421, 12109, 294, 341, 3157, 366, 406, 767, 9841, 13], "temperature": 0.0, "avg_logprob": -0.08899291769250647, "compression_ratio": 1.694267515923567, "no_speech_prob": 7.29560224499437e-06}, {"id": 693, "seek": 347560, "start": 3488.24, "end": 3493.44, "text": " The quags is actually this list.", "tokens": [440, 421, 12109, 307, 767, 341, 1329, 13], "temperature": 0.0, "avg_logprob": -0.08899291769250647, "compression_ratio": 1.694267515923567, "no_speech_prob": 7.29560224499437e-06}, {"id": 694, "seek": 347560, "start": 3493.44, "end": 3501.72, "text": " And so what it's going to do is it's going to automatically grab any quags with these", "tokens": [400, 370, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 6772, 4444, 604, 421, 12109, 365, 613], "temperature": 0.0, "avg_logprob": -0.08899291769250647, "compression_ratio": 1.694267515923567, "no_speech_prob": 7.29560224499437e-06}, {"id": 695, "seek": 350172, "start": 3501.72, "end": 3510.7999999999997, "text": " names, and it's going to replace the methods that we have with the thing that's passed", "tokens": [5288, 11, 293, 309, 311, 516, 281, 7406, 264, 7150, 300, 321, 362, 365, 264, 551, 300, 311, 4678], "temperature": 0.0, "avg_logprob": -0.10212186370233092, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.594227786787087e-06}, {"id": 696, "seek": 350172, "start": 3510.7999999999997, "end": 3512.9199999999996, "text": " to that quag.", "tokens": [281, 300, 421, 559, 13], "temperature": 0.0, "avg_logprob": -0.10212186370233092, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.594227786787087e-06}, {"id": 697, "seek": 350172, "start": 3512.9199999999996, "end": 3517.72, "text": " So again, one of the issues with quags, as we've talked about, is that they're terrible", "tokens": [407, 797, 11, 472, 295, 264, 2663, 365, 421, 12109, 11, 382, 321, 600, 2825, 466, 11, 307, 300, 436, 434, 6237], "temperature": 0.0, "avg_logprob": -0.10212186370233092, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.594227786787087e-06}, {"id": 698, "seek": 350172, "start": 3517.72, "end": 3522.56, "text": " for kind of discoverability and documentation because you don't really know what you can", "tokens": [337, 733, 295, 4411, 2310, 293, 14333, 570, 291, 500, 380, 534, 458, 437, 291, 393], "temperature": 0.0, "avg_logprob": -0.10212186370233092, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.594227786787087e-06}, {"id": 699, "seek": 350172, "start": 3522.56, "end": 3524.12, "text": " pass to quags.", "tokens": [1320, 281, 421, 12109, 13], "temperature": 0.0, "avg_logprob": -0.10212186370233092, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.594227786787087e-06}, {"id": 700, "seek": 350172, "start": 3524.12, "end": 3529.2799999999997, "text": " But pretty much everything we use in quags, we make sure that we fix that.", "tokens": [583, 1238, 709, 1203, 321, 764, 294, 421, 12109, 11, 321, 652, 988, 300, 321, 3191, 300, 13], "temperature": 0.0, "avg_logprob": -0.10212186370233092, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.594227786787087e-06}, {"id": 701, "seek": 352928, "start": 3529.28, "end": 3542.28, "text": " So if we look at Dataloader and hit Shift-Tab, you'll see that all the methods are listed.", "tokens": [407, 498, 321, 574, 412, 9315, 10334, 8312, 293, 2045, 28304, 12, 51, 455, 11, 291, 603, 536, 300, 439, 264, 7150, 366, 10052, 13], "temperature": 0.0, "avg_logprob": -0.13550428442052892, "compression_ratio": 1.5238095238095237, "no_speech_prob": 1.6536815792278503e-06}, {"id": 702, "seek": 352928, "start": 3542.28, "end": 3548.4, "text": " You'll also see that we have here assert not quags.", "tokens": [509, 603, 611, 536, 300, 321, 362, 510, 19810, 406, 421, 12109, 13], "temperature": 0.0, "avg_logprob": -0.13550428442052892, "compression_ratio": 1.5238095238095237, "no_speech_prob": 1.6536815792278503e-06}, {"id": 703, "seek": 352928, "start": 3548.4, "end": 3556.6600000000003, "text": " And the reason for that is that at func quags will remove all of these methods from quags", "tokens": [400, 264, 1778, 337, 300, 307, 300, 412, 1019, 66, 421, 12109, 486, 4159, 439, 295, 613, 7150, 490, 421, 12109], "temperature": 0.0, "avg_logprob": -0.13550428442052892, "compression_ratio": 1.5238095238095237, "no_speech_prob": 1.6536815792278503e-06}, {"id": 704, "seek": 352928, "start": 3556.6600000000003, "end": 3558.6000000000004, "text": " once it processes them.", "tokens": [1564, 309, 7555, 552, 13], "temperature": 0.0, "avg_logprob": -0.13550428442052892, "compression_ratio": 1.5238095238095237, "no_speech_prob": 1.6536815792278503e-06}, {"id": 705, "seek": 355860, "start": 3558.6, "end": 3563.0, "text": " So that way, you can be sure that you haven't accidentally passed in something that wasn't", "tokens": [407, 300, 636, 11, 291, 393, 312, 988, 300, 291, 2378, 380, 15715, 4678, 294, 746, 300, 2067, 380], "temperature": 0.0, "avg_logprob": -0.1298933227856954, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.4823422134213615e-06}, {"id": 706, "seek": 355860, "start": 3563.0, "end": 3564.2, "text": " recognized.", "tokens": [9823, 13], "temperature": 0.0, "avg_logprob": -0.1298933227856954, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.4823422134213615e-06}, {"id": 707, "seek": 355860, "start": 3564.2, "end": 3571.56, "text": " So if instead of, for example, doing createBatchEquals, we did createBackEquals, then we will get", "tokens": [407, 498, 2602, 295, 11, 337, 1365, 11, 884, 1884, 33, 852, 36, 358, 1124, 11, 321, 630, 1884, 28404, 36, 358, 1124, 11, 550, 321, 486, 483], "temperature": 0.0, "avg_logprob": -0.1298933227856954, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.4823422134213615e-06}, {"id": 708, "seek": 355860, "start": 3571.56, "end": 3572.62, "text": " that assertion error.", "tokens": [300, 19810, 313, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1298933227856954, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.4823422134213615e-06}, {"id": 709, "seek": 355860, "start": 3572.62, "end": 3575.52, "text": " So we know we've made a mistake.", "tokens": [407, 321, 458, 321, 600, 1027, 257, 6146, 13], "temperature": 0.0, "avg_logprob": -0.1298933227856954, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.4823422134213615e-06}, {"id": 710, "seek": 355860, "start": 3575.52, "end": 3579.8399999999997, "text": " So again, we're kind of trying to get the best of both worlds, concise, understandable", "tokens": [407, 797, 11, 321, 434, 733, 295, 1382, 281, 483, 264, 1151, 295, 1293, 13401, 11, 44882, 11, 25648], "temperature": 0.0, "avg_logprob": -0.1298933227856954, "compression_ratio": 1.5545454545454545, "no_speech_prob": 1.4823422134213615e-06}, {"id": 711, "seek": 357984, "start": 3579.84, "end": 3589.04, "text": " code, but also discoverability and avoiding nasty bugs.", "tokens": [3089, 11, 457, 611, 4411, 2310, 293, 20220, 17923, 15120, 13], "temperature": 0.0, "avg_logprob": -0.2684042817455227, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.2289056030567735e-06}, {"id": 712, "seek": 357984, "start": 3589.04, "end": 3591.6800000000003, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.2684042817455227, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.2289056030567735e-06}, {"id": 713, "seek": 357984, "start": 3591.6800000000003, "end": 3596.56, "text": " So, all right.", "tokens": [407, 11, 439, 558, 13], "temperature": 0.0, "avg_logprob": -0.2684042817455227, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.2289056030567735e-06}, {"id": 714, "seek": 357984, "start": 3596.56, "end": 3602.92, "text": " Another tiny little trick, that one that I just find super handy, is storeAtra.", "tokens": [3996, 5870, 707, 4282, 11, 300, 472, 300, 286, 445, 915, 1687, 13239, 11, 307, 3531, 18684, 424, 13], "temperature": 0.0, "avg_logprob": -0.2684042817455227, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.2289056030567735e-06}, {"id": 715, "seek": 357984, "start": 3602.92, "end": 3605.8, "text": " Here is something that I do all the time.", "tokens": [1692, 307, 746, 300, 286, 360, 439, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.2684042817455227, "compression_ratio": 1.3333333333333333, "no_speech_prob": 1.2289056030567735e-06}, {"id": 716, "seek": 360580, "start": 3605.8, "end": 3617.32, "text": " I'll go like self.dataset, self.vs, et cetera, equals data set, vs, et cetera.", "tokens": [286, 603, 352, 411, 2698, 13, 20367, 296, 302, 11, 2698, 13, 36959, 11, 1030, 11458, 11, 6915, 1412, 992, 11, 12041, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1538721231313852, "compression_ratio": 1.7763713080168777, "no_speech_prob": 4.222759798722109e-06}, {"id": 717, "seek": 360580, "start": 3617.32, "end": 3623.04, "text": " So very, very often when we're setting up a class, we basically store a bunch of parameters", "tokens": [407, 588, 11, 588, 2049, 562, 321, 434, 3287, 493, 257, 1508, 11, 321, 1936, 3531, 257, 3840, 295, 9834], "temperature": 0.0, "avg_logprob": -0.1538721231313852, "compression_ratio": 1.7763713080168777, "no_speech_prob": 4.222759798722109e-06}, {"id": 718, "seek": 360580, "start": 3623.04, "end": 3624.6400000000003, "text": " inside self.", "tokens": [1854, 2698, 13], "temperature": 0.0, "avg_logprob": -0.1538721231313852, "compression_ratio": 1.7763713080168777, "no_speech_prob": 4.222759798722109e-06}, {"id": 719, "seek": 360580, "start": 3624.6400000000003, "end": 3629.1200000000003, "text": " The problem with doing it this way is you have to make sure that the orders match correctly.", "tokens": [440, 1154, 365, 884, 309, 341, 636, 307, 291, 362, 281, 652, 988, 300, 264, 9470, 2995, 8944, 13], "temperature": 0.0, "avg_logprob": -0.1538721231313852, "compression_ratio": 1.7763713080168777, "no_speech_prob": 4.222759798722109e-06}, {"id": 720, "seek": 360580, "start": 3629.1200000000003, "end": 3632.44, "text": " If you add or remove stuff from the parameter list, you have to remember to add or remove", "tokens": [759, 291, 909, 420, 4159, 1507, 490, 264, 13075, 1329, 11, 291, 362, 281, 1604, 281, 909, 420, 4159], "temperature": 0.0, "avg_logprob": -0.1538721231313852, "compression_ratio": 1.7763713080168777, "no_speech_prob": 4.222759798722109e-06}, {"id": 721, "seek": 360580, "start": 3632.44, "end": 3633.44, "text": " it here.", "tokens": [309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1538721231313852, "compression_ratio": 1.7763713080168777, "no_speech_prob": 4.222759798722109e-06}, {"id": 722, "seek": 360580, "start": 3633.44, "end": 3635.6800000000003, "text": " You have to make sure the names are the same.", "tokens": [509, 362, 281, 652, 988, 264, 5288, 366, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.1538721231313852, "compression_ratio": 1.7763713080168777, "no_speech_prob": 4.222759798722109e-06}, {"id": 723, "seek": 363568, "start": 3635.68, "end": 3639.8799999999997, "text": " So there's a lot of repetition and opportunity for bugs.", "tokens": [407, 456, 311, 257, 688, 295, 30432, 293, 2650, 337, 15120, 13], "temperature": 0.0, "avg_logprob": -0.22364497184753418, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.7265638184180716e-06}, {"id": 724, "seek": 363568, "start": 3639.8799999999997, "end": 3648.3599999999997, "text": " So storeAtra does exactly what I just described, but you only have to list the things, list", "tokens": [407, 3531, 18684, 424, 775, 2293, 437, 286, 445, 7619, 11, 457, 291, 787, 362, 281, 1329, 264, 721, 11, 1329], "temperature": 0.0, "avg_logprob": -0.22364497184753418, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.7265638184180716e-06}, {"id": 725, "seek": 363568, "start": 3648.3599999999997, "end": 3650.0, "text": " of things to store once.", "tokens": [295, 721, 281, 3531, 1564, 13], "temperature": 0.0, "avg_logprob": -0.22364497184753418, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.7265638184180716e-06}, {"id": 726, "seek": 363568, "start": 3650.0, "end": 3657.44, "text": " So it's a minor convenience, but it definitely avoids a few bugs that I come across and a", "tokens": [407, 309, 311, 257, 6696, 19283, 11, 457, 309, 2138, 3641, 3742, 257, 1326, 15120, 300, 286, 808, 2108, 293, 257], "temperature": 0.0, "avg_logprob": -0.22364497184753418, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.7265638184180716e-06}, {"id": 727, "seek": 363568, "start": 3657.44, "end": 3660.0, "text": " little bit less typing, a little bit less reading to have to do.", "tokens": [707, 857, 1570, 18444, 11, 257, 707, 857, 1570, 3760, 281, 362, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.22364497184753418, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.7265638184180716e-06}, {"id": 728, "seek": 363568, "start": 3660.0, "end": 3663.0, "text": " But I think that's handy.", "tokens": [583, 286, 519, 300, 311, 13239, 13], "temperature": 0.0, "avg_logprob": -0.22364497184753418, "compression_ratio": 1.631336405529954, "no_speech_prob": 3.7265638184180716e-06}, {"id": 729, "seek": 366300, "start": 3663.0, "end": 3666.0, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.13811000375186697, "compression_ratio": 1.5229357798165137, "no_speech_prob": 3.944165291613899e-05}, {"id": 730, "seek": 366300, "start": 3666.0, "end": 3674.76, "text": " Let me know if anybody sees anything else here that looks interesting.", "tokens": [961, 385, 458, 498, 4472, 8194, 1340, 1646, 510, 300, 1542, 1880, 13], "temperature": 0.0, "avg_logprob": -0.13811000375186697, "compression_ratio": 1.5229357798165137, "no_speech_prob": 3.944165291613899e-05}, {"id": 731, "seek": 366300, "start": 3674.76, "end": 3680.0, "text": " Because we're doing a lot of stuff lazily, you'll see a lot more yields and yield froms", "tokens": [1436, 321, 434, 884, 257, 688, 295, 1507, 19320, 953, 11, 291, 603, 536, 257, 688, 544, 32168, 293, 11257, 490, 82], "temperature": 0.0, "avg_logprob": -0.13811000375186697, "compression_ratio": 1.5229357798165137, "no_speech_prob": 3.944165291613899e-05}, {"id": 732, "seek": 366300, "start": 3680.0, "end": 3682.28, "text": " than you're probably used to.", "tokens": [813, 291, 434, 1391, 1143, 281, 13], "temperature": 0.0, "avg_logprob": -0.13811000375186697, "compression_ratio": 1.5229357798165137, "no_speech_prob": 3.944165291613899e-05}, {"id": 733, "seek": 366300, "start": 3682.28, "end": 3688.92, "text": " So to understand fast AI version 2 foundations better, you will need to make sure that you", "tokens": [407, 281, 1223, 2370, 7318, 3037, 568, 22467, 1101, 11, 291, 486, 643, 281, 652, 988, 300, 291], "temperature": 0.0, "avg_logprob": -0.13811000375186697, "compression_ratio": 1.5229357798165137, "no_speech_prob": 3.944165291613899e-05}, {"id": 734, "seek": 366300, "start": 3688.92, "end": 3691.96, "text": " understand how those work in Python pretty well.", "tokens": [1223, 577, 729, 589, 294, 15329, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.13811000375186697, "compression_ratio": 1.5229357798165137, "no_speech_prob": 3.944165291613899e-05}, {"id": 735, "seek": 369196, "start": 3691.96, "end": 3697.96, "text": " If anybody has confusions about that, please ask on the forums.", "tokens": [759, 4472, 575, 1497, 27255, 466, 300, 11, 1767, 1029, 322, 264, 26998, 13], "temperature": 0.0, "avg_logprob": -0.1866931120554606, "compression_ratio": 1.5169082125603865, "no_speech_prob": 5.862449143023696e-06}, {"id": 736, "seek": 369196, "start": 3697.96, "end": 3707.32, "text": " But I did find that by kind of using this lazy approach, function-lish approach, we", "tokens": [583, 286, 630, 915, 300, 538, 733, 295, 1228, 341, 14847, 3109, 11, 2445, 12, 1933, 3109, 11, 321], "temperature": 0.0, "avg_logprob": -0.1866931120554606, "compression_ratio": 1.5169082125603865, "no_speech_prob": 5.862449143023696e-06}, {"id": 737, "seek": 369196, "start": 3707.32, "end": 3712.7200000000003, "text": " did end up with something that I found much easier to understand, had much less bugs,", "tokens": [630, 917, 493, 365, 746, 300, 286, 1352, 709, 3571, 281, 1223, 11, 632, 709, 1570, 15120, 11], "temperature": 0.0, "avg_logprob": -0.1866931120554606, "compression_ratio": 1.5169082125603865, "no_speech_prob": 5.862449143023696e-06}, {"id": 738, "seek": 369196, "start": 3712.7200000000003, "end": 3714.64, "text": " and also was just much less code to write.", "tokens": [293, 611, 390, 445, 709, 1570, 3089, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.1866931120554606, "compression_ratio": 1.5169082125603865, "no_speech_prob": 5.862449143023696e-06}, {"id": 739, "seek": 369196, "start": 3714.64, "end": 3716.84, "text": " So it was definitely worth doing.", "tokens": [407, 309, 390, 2138, 3163, 884, 13], "temperature": 0.0, "avg_logprob": -0.1866931120554606, "compression_ratio": 1.5169082125603865, "no_speech_prob": 5.862449143023696e-06}, {"id": 740, "seek": 369196, "start": 3716.84, "end": 3718.4, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1866931120554606, "compression_ratio": 1.5169082125603865, "no_speech_prob": 5.862449143023696e-06}, {"id": 741, "seek": 371840, "start": 3718.4, "end": 3728.12, "text": " So then there's this one ugly bit of code, which is basically we create an object of", "tokens": [407, 550, 456, 311, 341, 472, 12246, 857, 295, 3089, 11, 597, 307, 1936, 321, 1884, 364, 2657, 295], "temperature": 0.0, "avg_logprob": -0.09783807667818936, "compression_ratio": 1.4634146341463414, "no_speech_prob": 5.507430614670739e-06}, {"id": 742, "seek": 371840, "start": 3728.12, "end": 3732.36, "text": " type FakeLoader.", "tokens": [2010, 40469, 31645, 8312, 13], "temperature": 0.0, "avg_logprob": -0.09783807667818936, "compression_ratio": 1.4634146341463414, "no_speech_prob": 5.507430614670739e-06}, {"id": 743, "seek": 371840, "start": 3732.36, "end": 3741.92, "text": " And the reason for that is that PyTorch assumes that it's going to be working with an object", "tokens": [400, 264, 1778, 337, 300, 307, 300, 9953, 51, 284, 339, 37808, 300, 309, 311, 516, 281, 312, 1364, 365, 364, 2657], "temperature": 0.0, "avg_logprob": -0.09783807667818936, "compression_ratio": 1.4634146341463414, "no_speech_prob": 5.507430614670739e-06}, {"id": 744, "seek": 371840, "start": 3741.92, "end": 3745.6800000000003, "text": " which has this specific list of things in it.", "tokens": [597, 575, 341, 2685, 1329, 295, 721, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.09783807667818936, "compression_ratio": 1.4634146341463414, "no_speech_prob": 5.507430614670739e-06}, {"id": 745, "seek": 374568, "start": 3745.68, "end": 3748.96, "text": " So we basically had to create an object with that specific list of things and set them", "tokens": [407, 321, 1936, 632, 281, 1884, 364, 2657, 365, 300, 2685, 1329, 295, 721, 293, 992, 552], "temperature": 0.0, "avg_logprob": -0.15951488329016644, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.0415048968279734e-06}, {"id": 746, "seek": 374568, "start": 3748.96, "end": 3752.08, "text": " to the values that it expected.", "tokens": [281, 264, 4190, 300, 309, 5176, 13], "temperature": 0.0, "avg_logprob": -0.15951488329016644, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.0415048968279734e-06}, {"id": 747, "seek": 374568, "start": 3752.08, "end": 3757.3599999999997, "text": " And then it calls iter on that object.", "tokens": [400, 550, 309, 5498, 17138, 322, 300, 2657, 13], "temperature": 0.0, "avg_logprob": -0.15951488329016644, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.0415048968279734e-06}, {"id": 748, "seek": 374568, "start": 3757.3599999999997, "end": 3761.2, "text": " And so that's where we then pass it back to our DataLoader.", "tokens": [400, 370, 300, 311, 689, 321, 550, 1320, 309, 646, 281, 527, 11888, 31645, 8312, 13], "temperature": 0.0, "avg_logprob": -0.15951488329016644, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.0415048968279734e-06}, {"id": 749, "seek": 374568, "start": 3761.2, "end": 3764.48, "text": " So that's what this little bit of ugly code is.", "tokens": [407, 300, 311, 437, 341, 707, 857, 295, 12246, 3089, 307, 13], "temperature": 0.0, "avg_logprob": -0.15951488329016644, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.0415048968279734e-06}, {"id": 750, "seek": 374568, "start": 3764.48, "end": 3770.04, "text": " It does mean, unfortunately, that we have also tied our code to some various private", "tokens": [467, 775, 914, 11, 7015, 11, 300, 321, 362, 611, 9601, 527, 3089, 281, 512, 3683, 4551], "temperature": 0.0, "avg_logprob": -0.15951488329016644, "compression_ratio": 1.583710407239819, "no_speech_prob": 3.0415048968279734e-06}, {"id": 751, "seek": 377004, "start": 3770.04, "end": 3775.72, "text": " things inside PyTorch, which means that from version to version that might change.", "tokens": [721, 1854, 9953, 51, 284, 339, 11, 597, 1355, 300, 490, 3037, 281, 3037, 300, 1062, 1319, 13], "temperature": 0.0, "avg_logprob": -0.11306967825259802, "compression_ratio": 1.57421875, "no_speech_prob": 9.972578482120298e-06}, {"id": 752, "seek": 377004, "start": 3775.72, "end": 3782.08, "text": " And so if you start using master and you get errors on things like, I don't know, underscore", "tokens": [400, 370, 498, 291, 722, 1228, 4505, 293, 291, 483, 13603, 322, 721, 411, 11, 286, 500, 380, 458, 11, 37556], "temperature": 0.0, "avg_logprob": -0.11306967825259802, "compression_ratio": 1.57421875, "no_speech_prob": 9.972578482120298e-06}, {"id": 753, "seek": 377004, "start": 3782.08, "end": 3785.68, "text": " auto-collation doesn't exist, it probably means they've changed the name of some internal", "tokens": [8399, 12, 33891, 399, 1177, 380, 2514, 11, 309, 1391, 1355, 436, 600, 3105, 264, 1315, 295, 512, 6920], "temperature": 0.0, "avg_logprob": -0.11306967825259802, "compression_ratio": 1.57421875, "no_speech_prob": 9.972578482120298e-06}, {"id": 754, "seek": 377004, "start": 3785.68, "end": 3786.68, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.11306967825259802, "compression_ratio": 1.57421875, "no_speech_prob": 9.972578482120298e-06}, {"id": 755, "seek": 377004, "start": 3786.68, "end": 3788.7599999999998, "text": " So we're going to be keeping an eye on that.", "tokens": [407, 321, 434, 516, 281, 312, 5145, 364, 3313, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.11306967825259802, "compression_ratio": 1.57421875, "no_speech_prob": 9.972578482120298e-06}, {"id": 756, "seek": 377004, "start": 3788.7599999999998, "end": 3794.64, "text": " Maybe we can even encourage the PyTorch team to make stuff a little bit less coupled.", "tokens": [2704, 321, 393, 754, 5373, 264, 9953, 51, 284, 339, 1469, 281, 652, 1507, 257, 707, 857, 1570, 29482, 13], "temperature": 0.0, "avg_logprob": -0.11306967825259802, "compression_ratio": 1.57421875, "no_speech_prob": 9.972578482120298e-06}, {"id": 757, "seek": 379464, "start": 3794.64, "end": 3804.64, "text": " But for now, that's a bit of ugliness that we have to deal with.", "tokens": [583, 337, 586, 11, 300, 311, 257, 857, 295, 344, 7191, 1324, 300, 321, 362, 281, 2028, 365, 13], "temperature": 0.0, "avg_logprob": -0.09318833808376364, "compression_ratio": 1.3850574712643677, "no_speech_prob": 2.6425509531691205e-06}, {"id": 758, "seek": 379464, "start": 3804.64, "end": 3813.48, "text": " Something else to mention is PyTorch, I think they added it to 1.2.", "tokens": [6595, 1646, 281, 2152, 307, 9953, 51, 284, 339, 11, 286, 519, 436, 3869, 309, 281, 502, 13, 17, 13], "temperature": 0.0, "avg_logprob": -0.09318833808376364, "compression_ratio": 1.3850574712643677, "no_speech_prob": 2.6425509531691205e-06}, {"id": 759, "seek": 379464, "start": 3813.48, "end": 3814.48, "text": " Maybe it was earlier.", "tokens": [2704, 309, 390, 3071, 13], "temperature": 0.0, "avg_logprob": -0.09318833808376364, "compression_ratio": 1.3850574712643677, "no_speech_prob": 2.6425509531691205e-06}, {"id": 760, "seek": 379464, "start": 3814.48, "end": 3821.72, "text": " They've added a thing called the workerInit function, which is a handy little callback", "tokens": [814, 600, 3869, 257, 551, 1219, 264, 11346, 4575, 270, 2445, 11, 597, 307, 257, 13239, 707, 818, 3207], "temperature": 0.0, "avg_logprob": -0.09318833808376364, "compression_ratio": 1.3850574712643677, "no_speech_prob": 2.6425509531691205e-06}, {"id": 761, "seek": 382172, "start": 3821.72, "end": 3828.4399999999996, "text": " in their DataLoader infrastructure, which will be called every time a new process is", "tokens": [294, 641, 11888, 31645, 8312, 6896, 11, 597, 486, 312, 1219, 633, 565, 257, 777, 1399, 307], "temperature": 0.0, "avg_logprob": -0.09075169922203145, "compression_ratio": 1.6238938053097345, "no_speech_prob": 6.854101684439229e-06}, {"id": 762, "seek": 382172, "start": 3828.4399999999996, "end": 3830.6, "text": " fired off.", "tokens": [11777, 766, 13], "temperature": 0.0, "avg_logprob": -0.09075169922203145, "compression_ratio": 1.6238938053097345, "no_speech_prob": 6.854101684439229e-06}, {"id": 763, "seek": 382172, "start": 3830.6, "end": 3835.7999999999997, "text": " So we've written a little function for the workerInformation function here that calls", "tokens": [407, 321, 600, 3720, 257, 707, 2445, 337, 264, 11346, 4575, 8663, 2445, 510, 300, 5498], "temperature": 0.0, "avg_logprob": -0.09075169922203145, "compression_ratio": 1.6238938053097345, "no_speech_prob": 6.854101684439229e-06}, {"id": 764, "seek": 382172, "start": 3835.7999999999997, "end": 3842.56, "text": " PyTorch's getWorkerInfo, which basically will tell us what data set is this process working", "tokens": [9953, 51, 284, 339, 311, 483, 28846, 260, 4575, 16931, 11, 597, 1936, 486, 980, 505, 437, 1412, 992, 307, 341, 1399, 1364], "temperature": 0.0, "avg_logprob": -0.09075169922203145, "compression_ratio": 1.6238938053097345, "no_speech_prob": 6.854101684439229e-06}, {"id": 765, "seek": 382172, "start": 3842.56, "end": 3849.08, "text": " with, how many workers are there, and what's the ID of this worker into that list of workers.", "tokens": [365, 11, 577, 867, 5600, 366, 456, 11, 293, 437, 311, 264, 7348, 295, 341, 11346, 666, 300, 1329, 295, 5600, 13], "temperature": 0.0, "avg_logprob": -0.09075169922203145, "compression_ratio": 1.6238938053097345, "no_speech_prob": 6.854101684439229e-06}, {"id": 766, "seek": 384908, "start": 3849.08, "end": 3856.12, "text": " And so we do some, you know, so this way we can ensure that each worker gets a separate", "tokens": [400, 370, 321, 360, 512, 11, 291, 458, 11, 370, 341, 636, 321, 393, 5586, 300, 1184, 11346, 2170, 257, 4994], "temperature": 0.0, "avg_logprob": -0.13351224752572866, "compression_ratio": 1.5, "no_speech_prob": 8.800811883702409e-06}, {"id": 767, "seek": 384908, "start": 3856.12, "end": 3862.72, "text": " random seed, so we're not going to have horrible things with random seeds being duplicated.", "tokens": [4974, 8871, 11, 370, 321, 434, 406, 516, 281, 362, 9263, 721, 365, 4974, 9203, 885, 1581, 564, 3587, 13], "temperature": 0.0, "avg_logprob": -0.13351224752572866, "compression_ratio": 1.5, "no_speech_prob": 8.800811883702409e-06}, {"id": 768, "seek": 384908, "start": 3862.72, "end": 3873.68, "text": " We also use this information to parallelize things automatically inside the DataLoader.", "tokens": [492, 611, 764, 341, 1589, 281, 8952, 1125, 721, 6772, 1854, 264, 11888, 31645, 8312, 13], "temperature": 0.0, "avg_logprob": -0.13351224752572866, "compression_ratio": 1.5, "no_speech_prob": 8.800811883702409e-06}, {"id": 769, "seek": 387368, "start": 3873.68, "end": 3882.7599999999998, "text": " As you can see, we've got, yes, we set the offset and the number of workers, and then", "tokens": [1018, 291, 393, 536, 11, 321, 600, 658, 11, 2086, 11, 321, 992, 264, 18687, 293, 264, 1230, 295, 5600, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.18695281723798332, "compression_ratio": 1.5, "no_speech_prob": 2.9943996651127236e-06}, {"id": 770, "seek": 387368, "start": 3882.7599999999998, "end": 3890.2799999999997, "text": " we use that here in our sampler to ensure that each process handles a contiguous set", "tokens": [321, 764, 300, 510, 294, 527, 3247, 22732, 281, 5586, 300, 1184, 1399, 18722, 257, 660, 30525, 992], "temperature": 0.0, "avg_logprob": -0.18695281723798332, "compression_ratio": 1.5, "no_speech_prob": 2.9943996651127236e-06}, {"id": 771, "seek": 387368, "start": 3890.2799999999997, "end": 3896.68, "text": " of things which is different to each other process.", "tokens": [295, 721, 597, 307, 819, 281, 1184, 661, 1399, 13], "temperature": 0.0, "avg_logprob": -0.18695281723798332, "compression_ratio": 1.5, "no_speech_prob": 2.9943996651127236e-06}, {"id": 772, "seek": 389668, "start": 3896.68, "end": 3921.7999999999997, "text": " Okay, so PyTorch by default calls something called defaultCollate to take all the items", "tokens": [1033, 11, 370, 9953, 51, 284, 339, 538, 7576, 5498, 746, 1219, 7576, 35294, 473, 281, 747, 439, 264, 4754], "temperature": 0.0, "avg_logprob": -0.25599753856658936, "compression_ratio": 1.0875, "no_speech_prob": 1.2805183359887451e-05}, {"id": 773, "seek": 392180, "start": 3921.8, "end": 3927.7200000000003, "text": " from your data sets and turn them into a batch if you have a batch size.", "tokens": [490, 428, 1412, 6352, 293, 1261, 552, 666, 257, 15245, 498, 291, 362, 257, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.16529871133657603, "compression_ratio": 1.5222929936305734, "no_speech_prob": 5.771733412984759e-06}, {"id": 774, "seek": 392180, "start": 3927.7200000000003, "end": 3936.6000000000004, "text": " Otherwise it calls something called defaultConvert.", "tokens": [10328, 309, 5498, 746, 1219, 7576, 9838, 3281, 13], "temperature": 0.0, "avg_logprob": -0.16529871133657603, "compression_ratio": 1.5222929936305734, "no_speech_prob": 5.771733412984759e-06}, {"id": 775, "seek": 392180, "start": 3936.6000000000004, "end": 3946.28, "text": " So we have created our own fastaiConvert and fastaiCollate which handle a few types which", "tokens": [407, 321, 362, 2942, 527, 1065, 2370, 1301, 9838, 3281, 293, 2370, 1301, 35294, 473, 597, 4813, 257, 1326, 3467, 597], "temperature": 0.0, "avg_logprob": -0.16529871133657603, "compression_ratio": 1.5222929936305734, "no_speech_prob": 5.771733412984759e-06}, {"id": 776, "seek": 392180, "start": 3946.28, "end": 3950.76, "text": " PyTorch does not handle.", "tokens": [9953, 51, 284, 339, 775, 406, 4813, 13], "temperature": 0.0, "avg_logprob": -0.16529871133657603, "compression_ratio": 1.5222929936305734, "no_speech_prob": 5.771733412984759e-06}, {"id": 777, "seek": 395076, "start": 3950.76, "end": 3955.2000000000003, "text": " So it doesn't really change any behavior but it means more things should work correctly.", "tokens": [407, 309, 1177, 380, 534, 1319, 604, 5223, 457, 309, 1355, 544, 721, 820, 589, 8944, 13], "temperature": 0.0, "avg_logprob": -0.18751874959693765, "compression_ratio": 1.5745454545454545, "no_speech_prob": 9.08027413970558e-06}, {"id": 778, "seek": 395076, "start": 3955.2000000000003, "end": 3963.48, "text": " So createBatch by default simply uses collate or convert as appropriate depending on whether", "tokens": [407, 1884, 33, 852, 538, 7576, 2935, 4960, 1263, 473, 420, 7620, 382, 6854, 5413, 322, 1968], "temperature": 0.0, "avg_logprob": -0.18751874959693765, "compression_ratio": 1.5745454545454545, "no_speech_prob": 9.08027413970558e-06}, {"id": 779, "seek": 395076, "start": 3963.48, "end": 3966.4, "text": " your batch size is none or not.", "tokens": [428, 15245, 2744, 307, 6022, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.18751874959693765, "compression_ratio": 1.5745454545454545, "no_speech_prob": 9.08027413970558e-06}, {"id": 780, "seek": 395076, "start": 3966.4, "end": 3970.7200000000003, "text": " But again, you can replace that with something else and later on you'll see a place where", "tokens": [583, 797, 11, 291, 393, 7406, 300, 365, 746, 1646, 293, 1780, 322, 291, 603, 536, 257, 1081, 689], "temperature": 0.0, "avg_logprob": -0.18751874959693765, "compression_ratio": 1.5745454545454545, "no_speech_prob": 9.08027413970558e-06}, {"id": 781, "seek": 395076, "start": 3970.7200000000003, "end": 3971.7200000000003, "text": " we do that.", "tokens": [321, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.18751874959693765, "compression_ratio": 1.5745454545454545, "no_speech_prob": 9.08027413970558e-06}, {"id": 782, "seek": 395076, "start": 3971.7200000000003, "end": 3975.0, "text": " I think the language model DataLoader does that.", "tokens": [286, 519, 264, 2856, 2316, 11888, 31645, 8312, 775, 300, 13], "temperature": 0.0, "avg_logprob": -0.18751874959693765, "compression_ratio": 1.5745454545454545, "no_speech_prob": 9.08027413970558e-06}, {"id": 783, "seek": 395076, "start": 3975.0, "end": 3978.88, "text": " In fact, that might be an interesting thing just to briefly mention.", "tokens": [682, 1186, 11, 300, 1062, 312, 364, 1880, 551, 445, 281, 10515, 2152, 13], "temperature": 0.0, "avg_logprob": -0.18751874959693765, "compression_ratio": 1.5745454545454545, "no_speech_prob": 9.08027413970558e-06}, {"id": 784, "seek": 397888, "start": 3978.88, "end": 3985.44, "text": " For those of you that have looked at language models before, the language model DataLoader", "tokens": [1171, 729, 295, 291, 300, 362, 2956, 412, 2856, 5245, 949, 11, 264, 2856, 2316, 11888, 31645, 8312], "temperature": 0.0, "avg_logprob": -0.1566961267021265, "compression_ratio": 1.7376237623762376, "no_speech_prob": 3.84487657356658e-06}, {"id": 785, "seek": 397888, "start": 3985.44, "end": 3996.1600000000003, "text": " in fastai version 1 and most libraries tends to be big, confusing, horrible things.", "tokens": [294, 2370, 1301, 3037, 502, 293, 881, 15148, 12258, 281, 312, 955, 11, 13181, 11, 9263, 721, 13], "temperature": 0.0, "avg_logprob": -0.1566961267021265, "compression_ratio": 1.7376237623762376, "no_speech_prob": 3.84487657356658e-06}, {"id": 786, "seek": 397888, "start": 3996.1600000000003, "end": 3997.1600000000003, "text": " Look at this.", "tokens": [2053, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.1566961267021265, "compression_ratio": 1.7376237623762376, "no_speech_prob": 3.84487657356658e-06}, {"id": 787, "seek": 397888, "start": 3997.1600000000003, "end": 4001.6, "text": " This is the entire language model DataLoader in fastai version 2 and it's because we were", "tokens": [639, 307, 264, 2302, 2856, 2316, 11888, 31645, 8312, 294, 2370, 1301, 3037, 568, 293, 309, 311, 570, 321, 645], "temperature": 0.0, "avg_logprob": -0.1566961267021265, "compression_ratio": 1.7376237623762376, "no_speech_prob": 3.84487657356658e-06}, {"id": 788, "seek": 397888, "start": 4001.6, "end": 4008.0, "text": " able to inherit from our new DataLoader and use some of these callbacks.", "tokens": [1075, 281, 21389, 490, 527, 777, 11888, 31645, 8312, 293, 764, 512, 295, 613, 818, 17758, 13], "temperature": 0.0, "avg_logprob": -0.1566961267021265, "compression_ratio": 1.7376237623762376, "no_speech_prob": 3.84487657356658e-06}, {"id": 789, "seek": 400800, "start": 4008.0, "end": 4017.76, "text": " This actually has all of the advanced behavior of approximate shuffling and stuff like that", "tokens": [639, 767, 575, 439, 295, 264, 7339, 5223, 295, 30874, 402, 1245, 1688, 293, 1507, 411, 300], "temperature": 0.0, "avg_logprob": -0.21578860592532467, "compression_ratio": 1.5, "no_speech_prob": 8.267536941275466e-06}, {"id": 790, "seek": 400800, "start": 4017.76, "end": 4020.56, "text": " which you want in a modern language model DataLoader.", "tokens": [597, 291, 528, 294, 257, 4363, 2856, 2316, 11888, 31645, 8312, 13], "temperature": 0.0, "avg_logprob": -0.21578860592532467, "compression_ratio": 1.5, "no_speech_prob": 8.267536941275466e-06}, {"id": 791, "seek": 400800, "start": 4020.56, "end": 4026.0, "text": " We'll look at this in more detail later but it's an example of the huge wins that we get", "tokens": [492, 603, 574, 412, 341, 294, 544, 2607, 1780, 457, 309, 311, 364, 1365, 295, 264, 2603, 10641, 300, 321, 483], "temperature": 0.0, "avg_logprob": -0.21578860592532467, "compression_ratio": 1.5, "no_speech_prob": 8.267536941275466e-06}, {"id": 792, "seek": 400800, "start": 4026.0, "end": 4027.28, "text": " from this new DataLoader.", "tokens": [490, 341, 777, 11888, 31645, 8312, 13], "temperature": 0.0, "avg_logprob": -0.21578860592532467, "compression_ratio": 1.5, "no_speech_prob": 8.267536941275466e-06}, {"id": 793, "seek": 400800, "start": 4027.28, "end": 4031.48, "text": " Things get so, so much easier.", "tokens": [9514, 483, 370, 11, 370, 709, 3571, 13], "temperature": 0.0, "avg_logprob": -0.21578860592532467, "compression_ratio": 1.5, "no_speech_prob": 8.267536941275466e-06}, {"id": 794, "seek": 403148, "start": 4031.48, "end": 4044.64, "text": " Okay, so now I think we can go back and look at what is a transform DataLoader in more", "tokens": [1033, 11, 370, 586, 286, 519, 321, 393, 352, 646, 293, 574, 412, 437, 307, 257, 4088, 11888, 31645, 8312, 294, 544], "temperature": 0.0, "avg_logprob": -0.20842475891113282, "compression_ratio": 1.6130952380952381, "no_speech_prob": 1.8058151908917353e-05}, {"id": 795, "seek": 403148, "start": 4044.64, "end": 4053.6, "text": " detail because a transform DataLoader is something which inherits the DataLoader.", "tokens": [2607, 570, 257, 4088, 11888, 31645, 8312, 307, 746, 597, 9484, 1208, 264, 11888, 31645, 8312, 13], "temperature": 0.0, "avg_logprob": -0.20842475891113282, "compression_ratio": 1.6130952380952381, "no_speech_prob": 1.8058151908917353e-05}, {"id": 796, "seek": 403148, "start": 4053.6, "end": 4061.44, "text": " And it gets a bunch of quags which as we mentioned thanks to at delegates are passed to the DataLoader", "tokens": [400, 309, 2170, 257, 3840, 295, 421, 12109, 597, 382, 321, 2835, 3231, 281, 412, 45756, 366, 4678, 281, 264, 11888, 31645, 8312], "temperature": 0.0, "avg_logprob": -0.20842475891113282, "compression_ratio": 1.6130952380952381, "no_speech_prob": 1.8058151908917353e-05}, {"id": 797, "seek": 406144, "start": 4061.44, "end": 4067.36, "text": " in this case is the superclass but they're all going to be correctly documented and they're", "tokens": [294, 341, 1389, 307, 264, 1687, 11665, 457, 436, 434, 439, 516, 281, 312, 8944, 23007, 293, 436, 434], "temperature": 0.0, "avg_logprob": -0.19534610230245708, "compression_ratio": 1.608695652173913, "no_speech_prob": 4.33163731941022e-05}, {"id": 798, "seek": 406144, "start": 4067.36, "end": 4070.96, "text": " all going to be tab completed and everything thanks to at delegates.", "tokens": [439, 516, 281, 312, 4421, 7365, 293, 1203, 3231, 281, 412, 45756, 13], "temperature": 0.0, "avg_logprob": -0.19534610230245708, "compression_ratio": 1.608695652173913, "no_speech_prob": 4.33163731941022e-05}, {"id": 799, "seek": 406144, "start": 4070.96, "end": 4075.52, "text": " So what's the additional stuff that trfmdl does that DataLoader doesn't do?", "tokens": [407, 437, 311, 264, 4497, 1507, 300, 504, 69, 76, 67, 75, 775, 300, 11888, 31645, 8312, 1177, 380, 360, 30], "temperature": 0.0, "avg_logprob": -0.19534610230245708, "compression_ratio": 1.608695652173913, "no_speech_prob": 4.33163731941022e-05}, {"id": 800, "seek": 406144, "start": 4075.52, "end": 4087.7200000000003, "text": " Well the first thing it does is this specific list of callbacks turned into transform pipelines.", "tokens": [1042, 264, 700, 551, 309, 775, 307, 341, 2685, 1329, 295, 818, 17758, 3574, 666, 4088, 40168, 13], "temperature": 0.0, "avg_logprob": -0.19534610230245708, "compression_ratio": 1.608695652173913, "no_speech_prob": 4.33163731941022e-05}, {"id": 801, "seek": 408772, "start": 4087.72, "end": 4096.96, "text": " So basically after you grab a single item from the data set there will be a pipeline.", "tokens": [407, 1936, 934, 291, 4444, 257, 2167, 3174, 490, 264, 1412, 992, 456, 486, 312, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.18685689264414262, "compression_ratio": 1.8101851851851851, "no_speech_prob": 9.368476639792789e-06}, {"id": 802, "seek": 408772, "start": 4096.96, "end": 4104.04, "text": " After you collect, sorry, after you've got all of the items together but before you collect", "tokens": [2381, 291, 2500, 11, 2597, 11, 934, 291, 600, 658, 439, 295, 264, 4754, 1214, 457, 949, 291, 2500], "temperature": 0.0, "avg_logprob": -0.18685689264414262, "compression_ratio": 1.8101851851851851, "no_speech_prob": 9.368476639792789e-06}, {"id": 803, "seek": 408772, "start": 4104.04, "end": 4110.0, "text": " them there will be this callback and after everything's been turned into a collated batch", "tokens": [552, 456, 486, 312, 341, 818, 3207, 293, 934, 1203, 311, 668, 3574, 666, 257, 1263, 770, 15245], "temperature": 0.0, "avg_logprob": -0.18685689264414262, "compression_ratio": 1.8101851851851851, "no_speech_prob": 9.368476639792789e-06}, {"id": 804, "seek": 408772, "start": 4110.0, "end": 4113.08, "text": " there'll be this callback and these are all going to be pipelines.", "tokens": [456, 603, 312, 341, 818, 3207, 293, 613, 366, 439, 516, 281, 312, 40168, 13], "temperature": 0.0, "avg_logprob": -0.18685689264414262, "compression_ratio": 1.8101851851851851, "no_speech_prob": 9.368476639792789e-06}, {"id": 805, "seek": 408772, "start": 4113.08, "end": 4117.04, "text": " So you can see here these are all in underscore dltrfms.", "tokens": [407, 291, 393, 536, 510, 613, 366, 439, 294, 37556, 274, 2282, 81, 69, 2592, 13], "temperature": 0.0, "avg_logprob": -0.18685689264414262, "compression_ratio": 1.8101851851851851, "no_speech_prob": 9.368476639792789e-06}, {"id": 806, "seek": 411704, "start": 4117.04, "end": 4123.44, "text": " So we're going to go through each of those and we're going to turn them into a pipeline.", "tokens": [407, 321, 434, 516, 281, 352, 807, 1184, 295, 729, 293, 321, 434, 516, 281, 1261, 552, 666, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.15243251427360202, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.3931216876226244e-06}, {"id": 807, "seek": 411704, "start": 4123.44, "end": 4131.88, "text": " Okay so it's just going to grab it and turn it into a pipeline.", "tokens": [1033, 370, 309, 311, 445, 516, 281, 4444, 309, 293, 1261, 309, 666, 257, 15517, 13], "temperature": 0.0, "avg_logprob": -0.15243251427360202, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.3931216876226244e-06}, {"id": 808, "seek": 411704, "start": 4131.88, "end": 4138.8, "text": " Remember how I mentioned before that there are item transforms and tuple transforms and", "tokens": [5459, 577, 286, 2835, 949, 300, 456, 366, 3174, 35592, 293, 2604, 781, 35592, 293], "temperature": 0.0, "avg_logprob": -0.15243251427360202, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.3931216876226244e-06}, {"id": 809, "seek": 411704, "start": 4138.8, "end": 4142.36, "text": " item tuple transforms are the only ones that have the special behavior that it's going", "tokens": [3174, 2604, 781, 35592, 366, 264, 787, 2306, 300, 362, 264, 2121, 5223, 300, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.15243251427360202, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.3931216876226244e-06}, {"id": 810, "seek": 411704, "start": 4142.36, "end": 4147.0, "text": " to be applied to each element of a tuple.", "tokens": [281, 312, 6456, 281, 1184, 4478, 295, 257, 2604, 781, 13], "temperature": 0.0, "avg_logprob": -0.15243251427360202, "compression_ratio": 1.8636363636363635, "no_speech_prob": 3.3931216876226244e-06}, {"id": 811, "seek": 414700, "start": 4147.0, "end": 4152.12, "text": " And I mentioned that kind of stuff, all the tufmdl and tufmds and everything will handle", "tokens": [400, 286, 2835, 300, 733, 295, 1507, 11, 439, 264, 2604, 69, 76, 67, 75, 293, 2604, 69, 76, 16063, 293, 1203, 486, 4813], "temperature": 0.0, "avg_logprob": -0.1428492228190104, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.75027616933221e-05}, {"id": 812, "seek": 414700, "start": 4152.12, "end": 4155.88, "text": " all that for you and here it is doing it here.", "tokens": [439, 300, 337, 291, 293, 510, 309, 307, 884, 309, 510, 13], "temperature": 0.0, "avg_logprob": -0.1428492228190104, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.75027616933221e-05}, {"id": 813, "seek": 414700, "start": 4155.88, "end": 4167.08, "text": " Basically before batch is the only one that's going to be done as a whole item and the other", "tokens": [8537, 949, 15245, 307, 264, 787, 472, 300, 311, 516, 281, 312, 1096, 382, 257, 1379, 3174, 293, 264, 661], "temperature": 0.0, "avg_logprob": -0.1428492228190104, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.75027616933221e-05}, {"id": 814, "seek": 414700, "start": 4167.08, "end": 4175.16, "text": " ones will be done as tuple transforms.", "tokens": [2306, 486, 312, 1096, 382, 2604, 781, 35592, 13], "temperature": 0.0, "avg_logprob": -0.1428492228190104, "compression_ratio": 1.5892857142857142, "no_speech_prob": 1.75027616933221e-05}, {"id": 815, "seek": 417516, "start": 4175.16, "end": 4178.4, "text": " And then I also mentioned that it handles setup so this is the place where it's going", "tokens": [400, 550, 286, 611, 2835, 300, 309, 18722, 8657, 370, 341, 307, 264, 1081, 689, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.1419388907296317, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.4389155340904836e-06}, {"id": 816, "seek": 417516, "start": 4178.4, "end": 4181.96, "text": " to call setup for us if you have a setup.", "tokens": [281, 818, 8657, 337, 505, 498, 291, 362, 257, 8657, 13], "temperature": 0.0, "avg_logprob": -0.1419388907296317, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.4389155340904836e-06}, {"id": 817, "seek": 417516, "start": 4181.96, "end": 4185.68, "text": " We'll be talking more about setup when we look at pipelines in more detail.", "tokens": [492, 603, 312, 1417, 544, 466, 8657, 562, 321, 574, 412, 40168, 294, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.1419388907296317, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.4389155340904836e-06}, {"id": 818, "seek": 417516, "start": 4185.68, "end": 4192.12, "text": " But the main thing to recognize is that tufmdl is a data loader, it has the callbacks, these", "tokens": [583, 264, 2135, 551, 281, 5521, 307, 300, 2604, 69, 76, 67, 75, 307, 257, 1412, 3677, 260, 11, 309, 575, 264, 818, 17758, 11, 613], "temperature": 0.0, "avg_logprob": -0.1419388907296317, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.4389155340904836e-06}, {"id": 819, "seek": 417516, "start": 4192.12, "end": 4198.599999999999, "text": " same three callbacks but these three callbacks are now actual transformation pipelines which", "tokens": [912, 1045, 818, 17758, 457, 613, 1045, 818, 17758, 366, 586, 3539, 9887, 40168, 597], "temperature": 0.0, "avg_logprob": -0.1419388907296317, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.4389155340904836e-06}, {"id": 820, "seek": 417516, "start": 4198.599999999999, "end": 4204.599999999999, "text": " means they handle decodes and all that kind of thing.", "tokens": [1355, 436, 4813, 979, 4789, 293, 439, 300, 733, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.1419388907296317, "compression_ratio": 1.7440944881889764, "no_speech_prob": 6.4389155340904836e-06}, {"id": 821, "seek": 420460, "start": 4204.6, "end": 4214.64, "text": " So a tufmdl we're going to call under init needs to be able to decode things.", "tokens": [407, 257, 2604, 69, 76, 67, 75, 321, 434, 516, 281, 818, 833, 3157, 2203, 281, 312, 1075, 281, 979, 1429, 721, 13], "temperature": 0.0, "avg_logprob": -0.11457815326628137, "compression_ratio": 1.5573770491803278, "no_speech_prob": 4.710809662356041e-06}, {"id": 822, "seek": 420460, "start": 4214.64, "end": 4216.240000000001, "text": " So how is it going to decode things?", "tokens": [407, 577, 307, 309, 516, 281, 979, 1429, 721, 30], "temperature": 0.0, "avg_logprob": -0.11457815326628137, "compression_ratio": 1.5573770491803278, "no_speech_prob": 4.710809662356041e-06}, {"id": 823, "seek": 420460, "start": 4216.240000000001, "end": 4223.320000000001, "text": " So one of the things it's going to need to know is what types to decode to.", "tokens": [407, 472, 295, 264, 721, 309, 311, 516, 281, 643, 281, 458, 307, 437, 3467, 281, 979, 1429, 281, 13], "temperature": 0.0, "avg_logprob": -0.11457815326628137, "compression_ratio": 1.5573770491803278, "no_speech_prob": 4.710809662356041e-06}, {"id": 824, "seek": 422332, "start": 4223.32, "end": 4237.08, "text": " And this is tricky for inference for example because for inference in production all you're", "tokens": [400, 341, 307, 12414, 337, 38253, 337, 1365, 570, 337, 38253, 294, 4265, 439, 291, 434], "temperature": 0.0, "avg_logprob": -0.10348034686729556, "compression_ratio": 1.5380116959064327, "no_speech_prob": 2.5612209810788045e-06}, {"id": 825, "seek": 422332, "start": 4237.08, "end": 4242.719999999999, "text": " getting back from your model is a tensor and that tensor could represent anything.", "tokens": [1242, 646, 490, 428, 2316, 307, 257, 40863, 293, 300, 40863, 727, 2906, 1340, 13], "temperature": 0.0, "avg_logprob": -0.10348034686729556, "compression_ratio": 1.5380116959064327, "no_speech_prob": 2.5612209810788045e-06}, {"id": 826, "seek": 422332, "start": 4242.719999999999, "end": 4251.639999999999, "text": " So our tufmdl actually has to remember what types it has and so it actually stores them.", "tokens": [407, 527, 2604, 69, 76, 67, 75, 767, 575, 281, 1604, 437, 3467, 309, 575, 293, 370, 309, 767, 9512, 552, 13], "temperature": 0.0, "avg_logprob": -0.10348034686729556, "compression_ratio": 1.5380116959064327, "no_speech_prob": 2.5612209810788045e-06}, {"id": 827, "seek": 425164, "start": 4251.64, "end": 4256.76, "text": " The types that come out of the data set are stored away in retainDS and the types that", "tokens": [440, 3467, 300, 808, 484, 295, 264, 1412, 992, 366, 12187, 1314, 294, 18340, 11844, 293, 264, 3467, 300], "temperature": 0.0, "avg_logprob": -0.11256674294159791, "compression_ratio": 1.8883928571428572, "no_speech_prob": 1.0129666407010518e-05}, {"id": 828, "seek": 425164, "start": 4256.76, "end": 4262.200000000001, "text": " come out of the data at the end of the patching is stored away in retainDL.", "tokens": [808, 484, 295, 264, 1412, 412, 264, 917, 295, 264, 9972, 278, 307, 12187, 1314, 294, 18340, 35, 43, 13], "temperature": 0.0, "avg_logprob": -0.11256674294159791, "compression_ratio": 1.8883928571428572, "no_speech_prob": 1.0129666407010518e-05}, {"id": 829, "seek": 425164, "start": 4262.200000000001, "end": 4267.0, "text": " And they're stored as partial functions and we'll see this function called retainTypes", "tokens": [400, 436, 434, 12187, 382, 14641, 6828, 293, 321, 603, 536, 341, 2445, 1219, 18340, 22464, 5190], "temperature": 0.0, "avg_logprob": -0.11256674294159791, "compression_ratio": 1.8883928571428572, "no_speech_prob": 1.0129666407010518e-05}, {"id": 830, "seek": 425164, "start": 4267.0, "end": 4271.88, "text": " later but this is the function that knows how to ensure that things keep their types.", "tokens": [1780, 457, 341, 307, 264, 2445, 300, 3255, 577, 281, 5586, 300, 721, 1066, 641, 3467, 13], "temperature": 0.0, "avg_logprob": -0.11256674294159791, "compression_ratio": 1.8883928571428572, "no_speech_prob": 1.0129666407010518e-05}, {"id": 831, "seek": 425164, "start": 4271.88, "end": 4280.22, "text": " And so we just grab, so we get one item out of our batch just in order that we can find", "tokens": [400, 370, 321, 445, 4444, 11, 370, 321, 483, 472, 3174, 484, 295, 527, 15245, 445, 294, 1668, 300, 321, 393, 915], "temperature": 0.0, "avg_logprob": -0.11256674294159791, "compression_ratio": 1.8883928571428572, "no_speech_prob": 1.0129666407010518e-05}, {"id": 832, "seek": 428022, "start": 4280.22, "end": 4286.0, "text": " out what type types it has in it and then we turn that into our collated batch in order", "tokens": [484, 437, 2010, 3467, 309, 575, 294, 309, 293, 550, 321, 1261, 300, 666, 527, 1263, 770, 15245, 294, 1668], "temperature": 0.0, "avg_logprob": -0.11646863315882308, "compression_ratio": 1.7966101694915255, "no_speech_prob": 2.156794607799384e-06}, {"id": 833, "seek": 428022, "start": 4286.0, "end": 4288.360000000001, "text": " to find what types are in that.", "tokens": [281, 915, 437, 3467, 366, 294, 300, 13], "temperature": 0.0, "avg_logprob": -0.11646863315882308, "compression_ratio": 1.7966101694915255, "no_speech_prob": 2.156794607799384e-06}, {"id": 834, "seek": 428022, "start": 4288.360000000001, "end": 4298.2, "text": " So that way when we go decode we can put back the types that we're meant to have.", "tokens": [407, 300, 636, 562, 321, 352, 979, 1429, 321, 393, 829, 646, 264, 3467, 300, 321, 434, 4140, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.11646863315882308, "compression_ratio": 1.7966101694915255, "no_speech_prob": 2.156794607799384e-06}, {"id": 835, "seek": 428022, "start": 4298.2, "end": 4301.400000000001, "text": " So that's what's going on there.", "tokens": [407, 300, 311, 437, 311, 516, 322, 456, 13], "temperature": 0.0, "avg_logprob": -0.11646863315882308, "compression_ratio": 1.7966101694915255, "no_speech_prob": 2.156794607799384e-06}, {"id": 836, "seek": 428022, "start": 4301.400000000001, "end": 4309.76, "text": " So basically what that means then is that when we say show batch we can say decode,", "tokens": [407, 1936, 437, 300, 1355, 550, 307, 300, 562, 321, 584, 855, 15245, 321, 393, 584, 979, 1429, 11], "temperature": 0.0, "avg_logprob": -0.11646863315882308, "compression_ratio": 1.7966101694915255, "no_speech_prob": 2.156794607799384e-06}, {"id": 837, "seek": 430976, "start": 4309.76, "end": 4317.4400000000005, "text": " it will put back the correct types and then call the decode bits of all of our different", "tokens": [309, 486, 829, 646, 264, 3006, 3467, 293, 550, 818, 264, 979, 1429, 9239, 295, 439, 295, 527, 819], "temperature": 0.0, "avg_logprob": -0.12051816420121626, "compression_ratio": 1.5968586387434556, "no_speech_prob": 3.7852805689908564e-06}, {"id": 838, "seek": 430976, "start": 4317.4400000000005, "end": 4324.2, "text": " pipelines and then allow us to show it.", "tokens": [40168, 293, 550, 2089, 505, 281, 855, 309, 13], "temperature": 0.0, "avg_logprob": -0.12051816420121626, "compression_ratio": 1.5968586387434556, "no_speech_prob": 3.7852805689908564e-06}, {"id": 839, "seek": 430976, "start": 4324.2, "end": 4330.8, "text": " If we can show the whole batch at once, which would be the case for something like tabular", "tokens": [759, 321, 393, 855, 264, 1379, 15245, 412, 1564, 11, 597, 576, 312, 264, 1389, 337, 746, 411, 4421, 1040], "temperature": 0.0, "avg_logprob": -0.12051816420121626, "compression_ratio": 1.5968586387434556, "no_speech_prob": 3.7852805689908564e-06}, {"id": 840, "seek": 430976, "start": 4330.8, "end": 4336.96, "text": " or for things like most other kinds of data sets, we're going to have to display each", "tokens": [420, 337, 721, 411, 881, 661, 3685, 295, 1412, 6352, 11, 321, 434, 516, 281, 362, 281, 4674, 1184], "temperature": 0.0, "avg_logprob": -0.12051816420121626, "compression_ratio": 1.5968586387434556, "no_speech_prob": 3.7852805689908564e-06}, {"id": 841, "seek": 433696, "start": 4336.96, "end": 4342.6, "text": " part of the tuple separately, which is what this is doing here.", "tokens": [644, 295, 264, 2604, 781, 14759, 11, 597, 307, 437, 341, 307, 884, 510, 13], "temperature": 0.0, "avg_logprob": -0.2649485270182292, "compression_ratio": 1.4176470588235295, "no_speech_prob": 2.1444513549795374e-05}, {"id": 842, "seek": 433696, "start": 4342.6, "end": 4355.4800000000005, "text": " So we end up with something where we can show batch, decode batch, et cetera.", "tokens": [407, 321, 917, 493, 365, 746, 689, 321, 393, 855, 15245, 11, 979, 1429, 15245, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.2649485270182292, "compression_ratio": 1.4176470588235295, "no_speech_prob": 2.1444513549795374e-05}, {"id": 843, "seek": 433696, "start": 4355.4800000000005, "end": 4359.4800000000005, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2649485270182292, "compression_ratio": 1.4176470588235295, "no_speech_prob": 2.1444513549795374e-05}, {"id": 844, "seek": 433696, "start": 4359.4800000000005, "end": 4361.84, "text": " So L10 is asking, will there be a walkthrough tomorrow?", "tokens": [407, 441, 3279, 307, 3365, 11, 486, 456, 312, 257, 1792, 11529, 4153, 30], "temperature": 0.0, "avg_logprob": -0.2649485270182292, "compression_ratio": 1.4176470588235295, "no_speech_prob": 2.1444513549795374e-05}, {"id": 845, "seek": 433696, "start": 4361.84, "end": 4363.68, "text": " Yes, there's a walkthrough every day.", "tokens": [1079, 11, 456, 311, 257, 1792, 11529, 633, 786, 13], "temperature": 0.0, "avg_logprob": -0.2649485270182292, "compression_ratio": 1.4176470588235295, "no_speech_prob": 2.1444513549795374e-05}, {"id": 846, "seek": 436368, "start": 4363.68, "end": 4369.16, "text": " Well, you just need to look on the FastAI version 2 daily code walkthroughs where we", "tokens": [1042, 11, 291, 445, 643, 281, 574, 322, 264, 15968, 48698, 3037, 568, 5212, 3089, 1792, 11529, 82, 689, 321], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 847, "seek": 436368, "start": 4369.16, "end": 4371.280000000001, "text": " will keep your information updated.", "tokens": [486, 1066, 428, 1589, 10588, 13], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 848, "seek": 436368, "start": 4371.280000000001, "end": 4373.400000000001, "text": " Oh, wait, but today's Friday.", "tokens": [876, 11, 1699, 11, 457, 965, 311, 6984, 13], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 849, "seek": 436368, "start": 4373.400000000001, "end": 4374.400000000001, "text": " So not today, Thursday.", "tokens": [407, 406, 965, 11, 10383, 13], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 850, "seek": 436368, "start": 4374.400000000001, "end": 4375.400000000001, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 851, "seek": 436368, "start": 4375.400000000001, "end": 4382.4800000000005, "text": " So it's going to be every weekday, so including tomorrow.", "tokens": [407, 309, 311, 516, 281, 312, 633, 1243, 810, 11, 370, 3009, 4153, 13], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 852, "seek": 436368, "start": 4382.4800000000005, "end": 4386.4400000000005, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 853, "seek": 436368, "start": 4386.4400000000005, "end": 4393.6, "text": " So let me know if you have any requests for where you would like me to head next.", "tokens": [407, 718, 385, 458, 498, 291, 362, 604, 12475, 337, 689, 291, 576, 411, 385, 281, 1378, 958, 13], "temperature": 0.0, "avg_logprob": -0.2912111083666484, "compression_ratio": 1.4684684684684686, "no_speech_prob": 8.267483281088062e-06}, {"id": 854, "seek": 439360, "start": 4393.6, "end": 4405.08, "text": " Is that all of that one?", "tokens": [1119, 300, 439, 295, 300, 472, 30], "temperature": 0.0, "avg_logprob": -0.2549994448398022, "compression_ratio": 1.4683544303797469, "no_speech_prob": 5.828691064380109e-05}, {"id": 855, "seek": 439360, "start": 4405.08, "end": 4413.84, "text": " So if I look where we've done, we've kind of, we've done 01C, we've done 08, we've", "tokens": [407, 498, 286, 574, 689, 321, 600, 1096, 11, 321, 600, 733, 295, 11, 321, 600, 1096, 23185, 34, 11, 321, 600, 1096, 1958, 23, 11, 321, 600], "temperature": 0.0, "avg_logprob": -0.2549994448398022, "compression_ratio": 1.4683544303797469, "no_speech_prob": 5.828691064380109e-05}, {"id": 856, "seek": 439360, "start": 4413.84, "end": 4417.64, "text": " done 05.", "tokens": [1096, 1958, 20, 13], "temperature": 0.0, "avg_logprob": -0.2549994448398022, "compression_ratio": 1.4683544303797469, "no_speech_prob": 5.828691064380109e-05}, {"id": 857, "seek": 441764, "start": 4417.64, "end": 4423.96, "text": " So maybe we should look at pipeline.", "tokens": [407, 1310, 321, 820, 574, 412, 15517, 13], "temperature": 0.0, "avg_logprob": -0.14322311958570158, "compression_ratio": 1.6203208556149733, "no_speech_prob": 1.577919192641275e-06}, {"id": 858, "seek": 441764, "start": 4423.96, "end": 4425.96, "text": " So that's going to be a big rabbit hole.", "tokens": [407, 300, 311, 516, 281, 312, 257, 955, 19509, 5458, 13], "temperature": 0.0, "avg_logprob": -0.14322311958570158, "compression_ratio": 1.6203208556149733, "no_speech_prob": 1.577919192641275e-06}, {"id": 859, "seek": 441764, "start": 4425.96, "end": 4434.76, "text": " So what I suggest we do is we wrap up for today, and tomorrow let's look at pipeline,", "tokens": [407, 437, 286, 3402, 321, 360, 307, 321, 7019, 493, 337, 965, 11, 293, 4153, 718, 311, 574, 412, 15517, 11], "temperature": 0.0, "avg_logprob": -0.14322311958570158, "compression_ratio": 1.6203208556149733, "no_speech_prob": 1.577919192641275e-06}, {"id": 860, "seek": 441764, "start": 4434.76, "end": 4439.360000000001, "text": " and then that might get us into transforms, and that's going to be a lot of fun.", "tokens": [293, 550, 300, 1062, 483, 505, 666, 35592, 11, 293, 300, 311, 516, 281, 312, 257, 688, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.14322311958570158, "compression_ratio": 1.6203208556149733, "no_speech_prob": 1.577919192641275e-06}, {"id": 861, "seek": 441764, "start": 4439.360000000001, "end": 4440.360000000001, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.14322311958570158, "compression_ratio": 1.6203208556149733, "no_speech_prob": 1.577919192641275e-06}, {"id": 862, "seek": 441764, "start": 4440.360000000001, "end": 4441.360000000001, "text": " Thanks, everybody.", "tokens": [2561, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.14322311958570158, "compression_ratio": 1.6203208556149733, "no_speech_prob": 1.577919192641275e-06}, {"id": 863, "seek": 444136, "start": 4441.36, "end": 4458.2, "text": " I will see you all tomorrow.", "tokens": [50364, 286, 486, 536, 291, 439, 4153, 13, 51206], "temperature": 0.0, "avg_logprob": -0.39233078956604006, "compression_ratio": 0.8235294117647058, "no_speech_prob": 0.0001117284846259281}], "language": "en"}